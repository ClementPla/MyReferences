@misc{13126203Spectral,
  title = {[1312.6203] {{Spectral Networks}} and {{Locally Connected Networks}} on {{Graphs}}},
  urldate = {2019-11-28}
}

@misc{13126203Spectrala,
  title = {[1312.6203] {{Spectral Networks}} and {{Locally Connected Networks}} on {{Graphs}}},
  urldate = {2019-11-28},
  howpublished = {https://arxiv.org/abs/1312.6203},
  file = {C:\Users\cleme\Zotero\storage\82RK8A4J\1312.html}
}

@misc{16TrainingDataefficient,
  title = {(16) {{Training}} Data-Efficient Image Transformers \& Distillation through Attention {\textbar} {{Request PDF}}},
  urldate = {2021-11-08}
}

@misc{16TrainingDataefficienta,
  title = {(16) {{Training}} Data-Efficient Image Transformers \& Distillation through Attention {\textbar} {{Request PDF}}},
  urldate = {2021-11-08},
  howpublished = {https://www.researchgate.net/publication/347797071\_Training\_data-efficient\_image\_transformers\_distillation\_through\_attention},
  file = {C:\Users\cleme\Zotero\storage\ES9GU9R6\347797071_Training_data-efficient_image_transformers_distillation_through_attention.html}
}

@misc{170208608Rigorous,
  title = {[1702.08608] {{Towards A Rigorous Science}} of {{Interpretable Machine Learning}}},
  urldate = {2019-12-06}
}

@misc{170208608Rigorousa,
  title = {[1702.08608] {{Towards A Rigorous Science}} of {{Interpretable Machine Learning}}},
  urldate = {2019-12-06},
  howpublished = {https://arxiv.org/abs/1702.08608},
  file = {C:\Users\cleme\Zotero\storage\WDZ9X93S\1702.html}
}

@misc{171001711Grader,
  title = {[1710.01711] {{Grader}} Variability and the Importance of Reference Standards for Evaluating Machine Learning Models for Diabetic Retinopathy},
  urldate = {2019-11-01}
}

@misc{171001711Gradera,
  title = {[1710.01711] {{Grader}} Variability and the Importance of Reference Standards for Evaluating Machine Learning Models for Diabetic Retinopathy},
  urldate = {2019-11-01},
  howpublished = {https://arxiv.org/abs/1710.01711},
  file = {C:\Users\cleme\Zotero\storage\4K8JJDA4\1710.html}
}

@misc{18PDFComprehensive,
  title = {(18) ({{PDF}}) {{A}} Comprehensive Retinal Image Dataset for the Assessment of Glaucoma from the Optic Nerve Head Analysis},
  urldate = {2019-12-26},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  langid = {english}
}

@misc{18PDFComprehensivea,
  title = {(18) ({{PDF}}) {{A}} Comprehensive Retinal Image Dataset for the Assessment of Glaucoma from the Optic Nerve Head Analysis},
  journal = {ResearchGate},
  urldate = {2019-12-26},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  howpublished = {https://www.researchgate.net/publication/306939962\_A\_comprehensive\_retinal\_image\_dataset\_for\_the\_assessment\_of\_glaucoma\_from\_the\_optic\_nerve\_head\_analysis},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\RGRMRJH7\306939962_A_comprehensive_retinal_image_dataset_for_the_assessment_of_glaucoma_from_the_optic_n.html}
}

@misc{18PDFLong,
  title = {(18) ({{PDF}}) {{Long Short-term Memory}}},
  urldate = {2019-06-26},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  langid = {english}
}

@misc{18PDFLonga,
  title = {(18) ({{PDF}}) {{Long Short-term Memory}}},
  journal = {ResearchGate},
  urldate = {2019-06-26},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  howpublished = {https://www.researchgate.net/publication/13853244\_Long\_Short-term\_Memory},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\LTGXE8DD\13853244_Long_Short-term_Memory.html}
}

@inproceedings{2020,
  title = {Explainable {{AI}} for Medical Imaging: Deep-Learning {{CNN}} Ensemble for Classification of Estrogen Receptor Status from Breast {{MRI}}},
  author = {Papanastasopoulos, Zachary and Samala, Ravi K. and Chan, Heang-Ping and Hadjiiski, Lubomir and Paramagul, Chintana and Helvie, Mark A. and Neal, Colleen H.},
  editor = {Hahn, Horst K. and Mazurowski, Maciej A.},
  year = {2020},
  month = mar,
  publisher = {SPIE},
  doi = {10.1117/12.2549298}
}

@inproceedings{2020,
  title = {Explainable {{AI}} for Medical Imaging: {{Deep-learning CNN}} Ensemble for Classification of Estrogen Receptor Status from Breast {{MRI}}},
  author = {Papanastasopoulos, Zachary and Samala, Ravi K. and Chan, Heang-Ping and Hadjiiski, Lubomir and Paramagul, Chintana and Helvie, Mark A. and Neal, Colleen H.},
  editor = {Hahn, Horst K. and Mazurowski, Maciej A.},
  year = {2020},
  month = mar,
  publisher = {SPIE},
  doi = {10.1117/12.2549298}
}

@misc{4thAsiaPacific,
  title = {The 4th {{Asia Pacific Tele-Ophthalmology Society Symposium}}},
  urldate = {2021-11-26},
  langid = {american}
}

@misc{4thAsiaPacifica,
  title = {The 4th {{Asia Pacific Tele-Ophthalmology Society Symposium}}},
  urldate = {2021-11-26},
  langid = {american},
  file = {C:\Users\cleme\Zotero\storage\R3Q6498K\2019.asiateleophth.org.html}
}

@article{9875989,
  title = {Explainability in Graph Neural Networks: {{A}} Taxonomic Survey},
  author = {Yuan, H. and Yu, H. and Gui, S. and Ji, S.},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis \&amp; Machine Intelligence},
  volume = {45},
  number = {05},
  pages = {5782--5799},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3204236},
  abstract = {Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we provide a testbed for GNN explainability, including datasets, common algorithms and evaluation metrics. Furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.},
  keywords = {biological system modeling,data models,graph neural networks,predictive models,systematics,task analysis,taxonomy}
}

@article{9875989,
  title = {Explainability in Graph Neural Networks: {{A}} Taxonomic Survey},
  author = {Yuan, H. and Yu, H. and Gui, S. and Ji, S.},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis \&amp; Machine Intelligence},
  volume = {45},
  number = {05},
  pages = {5782--5799},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3204236},
  abstract = {Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we provide a testbed for GNN explainability, including datasets, common algorithms and evaluation metrics. Furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.},
  keywords = {biological system modeling,data models,graph neural networks,predictive models,systematics,task analysis,taxonomy}
}

@article{aasanSpittingImageSuperpixel2023,
  title = {A {{Spitting Image}}: {{Superpixel Transformers}}},
  shorttitle = {A {{Spitting Image}}},
  author = {Aasan, Marius and Rivera, Ad{\'i}n Ram{\'i}rez and Kolbjornsen, Odd and Solberg, Anne Schistad},
  year = {2023},
  month = oct,
  urldate = {2024-02-13},
  abstract = {Vision Transformer (ViT) architectures treat tokenization as an inflexible, monolithic process with regular grid partitions. In this work, we propose a generalized superpixel transformer (SPiT) framework that decouples tokenization from feature extraction; a significant shift from contemporary approaches, where these are treated as an undifferentiated whole. Using on-line superpixel tokenization and scale- and shape-invariant feature extraction, we perform experiments and ablations that contrast our approach with canonical tokenization and randomized partitions as baselines. We find that modular superpixel-based tokenization provides significantly improved interpretability using state-of-the-art metrics for faithfulness while maintaining competitive classification performance, providing a space of semantically-rich models that can generalize across different vision tasks.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\LLDG2HLK\Aasan et al. - 2023 - A Spitting Image Superpixel Transformers.pdf}
}

@inproceedings{abnarQuantifyingAttentionFlow2020,
  title = {Quantifying {{Attention Flow}} in {{Transformers}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Abnar, Samira and Zuidema, Willem},
  year = {2020},
  pages = {4190--4197},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.385},
  urldate = {2021-03-03},
  abstract = {In the Transformer model, ``self-attention'' combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
  langid = {english}
}

@inproceedings{abnarQuantifyingAttentionFlow2020a,
  title = {Quantifying {{Attention Flow}} in {{Transformers}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Abnar, Samira and Zuidema, Willem},
  year = {2020},
  pages = {4190--4197},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.385},
  urldate = {2021-03-03},
  abstract = {In the Transformer model, ``self-attention'' combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\EDBPK3AM\Abnar et Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf}
}

@article{abramoffRetinalImagingImage2010,
  title = {Retinal {{Imaging}} and {{Image Analysis}}},
  author = {Abr{\`a}moff, Michael D. and Garvin, Mona K. and Sonka, Milan},
  year = {2010},
  month = jan,
  journal = {IEEE reviews in biomedical engineering},
  volume = {3},
  pages = {169--208},
  issn = {1937-3333},
  doi = {10.1109/RBME.2010.2084567},
  urldate = {2019-09-23},
  abstract = {Many important eye diseases as well as systemic diseases manifest themselves in the retina. While a number of other anatomical structures contribute to the process of vision, this review focuses on retinal imaging and image analysis. Following a brief overview of the most prevalent causes of blindness in the industrialized world that includes age-related macular degeneration, diabetic retinopathy, and glaucoma, the review is devoted to retinal imaging and image analysis methods and their clinical implications. Methods for 2-D fundus imaging and techniques for 3-D optical coherence tomography (OCT) imaging are reviewed. Special attention is given to quantitative techniques for analysis of fundus photographs with a focus on clinically relevant assessment of retinal vasculature, identification of retinal lesions, assessment of optic nerve head (ONH) shape, building retinal atlases, and to automated methods for population screening for retinal diseases. A separate section is devoted to 3-D analysis of OCT images, describing methods for segmentation and analysis of retinal layers, retinal vasculature, and 2-D/3-D detection of symptomatic exudate-associated derangements, as well as to OCT-based analysis of ONH morphology and shape. Throughout the paper, aspects of image acquisition, image analysis, and clinical relevance are treated together considering their mutually interlinked relationships.},
  pmcid = {PMC3131209},
  pmid = {22275207},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\ISSVFGK3\\10.1109@RBME.2010.2084567.pdf;C\:\\Users\\cleme\\Zotero\\storage\\WU5VXVNK\\Abràmoff et al. - 2010 - Retinal Imaging and Image Analysis.pdf}
}

@article{achantaSLICSuperpixelsCompared2012,
  title = {{{SLIC Superpixels Compared}} to {{State-of-the-Art Superpixel Methods}}},
  author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"u}sstrunk, Sabine},
  year = {2012},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {34},
  number = {11},
  pages = {2274--2282},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.120},
  urldate = {2024-02-13},
  abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
  keywords = {Approximation algorithms,clustering,Clustering algorithms,Complexity theory,Image color analysis,Image edge detection,Image segmentation,k-means,Measurement uncertainty,segmentation,Superpixels},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\L5H8YP6R\\Achanta et al. - 2012 - SLIC Superpixels Compared to State-of-the-Art Supe.pdf;C\:\\Users\\cleme\\Zotero\\storage\\GQWG4FWK\\6205760.html}
}

@article{acharyaIntegratedIndexIdentification2012,
  title = {An {{Integrated Index}} for the {{Identification}} of {{Diabetic Retinopathy Stages Using Texture Parameters}}},
  author = {Acharya, U. Rajendra and Ng, E. Y. K. and Tan, Jen-Hong and Sree, S. Vinitha and Ng, Kwan-Hoong},
  year = {2012},
  month = jun,
  journal = {Journal of Medical Systems},
  volume = {36},
  number = {3},
  pages = {2011--2020},
  issn = {1573-689X},
  doi = {10.1007/s10916-011-9663-8},
  urldate = {2019-11-19},
  abstract = {Diabetes is a condition of increase in the blood sugar level higher than the normal range. Prolonged diabetes damages the small blood vessels in the retina resulting in diabetic retinopathy (DR). DR progresses with time without any noticeable symptoms until the damage has occurred. Hence, it is very beneficial to have the regular cost effective eye screening for the diabetes subjects. This paper documents a system that can be used for automatic mass screenings of diabetic retinopathy. Four classes are identified: normal retina, non-proliferative diabetic retinopathy (NPDR), proliferative diabetic retinopathy (PDR), and macular edema (ME). We used 238 retinal fundus images in our analysis. Five different texture features such as homogeneity, correlation, short run emphasis, long run emphasis, and run percentage were extracted from the digital fundus images. These features were fed into a support vector machine classifier (SVM) for automatic classification. SVM classifier of different kernel functions (linear, radial basis function, polynomial of order 1, 2, and 3) was studied. Receiver operation characteristics (ROC) curves were plotted to select the best classifier. Our proposed system is able to identify the unknown class with an accuracy of 85.2\%, and sensitivity, specificity, and area under curve (AUC) of 98.9\%, 89.5\%, and 0.972 respectively using SVM classifier with polynomial kernel of order 3. We have also proposed a new integrated DR index (IDRI) using different features, which is able to identify the different classes with 100\% accuracy.},
  langid = {english},
  keywords = {Classifier,Diabetes,Fundus image,Index,Retinopathy,Support vector machine,Texture}
}

@article{acharyaIntegratedIndexIdentification2012a,
  title = {An {{Integrated Index}} for the {{Identification}} of {{Diabetic Retinopathy Stages Using Texture Parameters}}},
  author = {Acharya, U. Rajendra and Ng, E. Y. K. and Tan, Jen-Hong and Sree, S. Vinitha and Ng, Kwan-Hoong},
  year = {2012},
  month = jun,
  journal = {Journal of Medical Systems},
  volume = {36},
  number = {3},
  pages = {2011--2020},
  issn = {1573-689X},
  doi = {10.1007/s10916-011-9663-8},
  urldate = {2019-11-19},
  abstract = {Diabetes is a condition of increase in the blood sugar level higher than the normal range. Prolonged diabetes damages the small blood vessels in the retina resulting in diabetic retinopathy (DR). DR progresses with time without any noticeable symptoms until the damage has occurred. Hence, it is very beneficial to have the regular cost effective eye screening for the diabetes subjects. This paper documents a system that can be used for automatic mass screenings of diabetic retinopathy. Four classes are identified: normal retina, non-proliferative diabetic retinopathy (NPDR), proliferative diabetic retinopathy (PDR), and macular edema (ME). We used 238 retinal fundus images in our analysis. Five different texture features such as homogeneity, correlation, short run emphasis, long run emphasis, and run percentage were extracted from the digital fundus images. These features were fed into a support vector machine classifier (SVM) for automatic classification. SVM classifier of different kernel functions (linear, radial basis function, polynomial of order 1, 2, and 3) was studied. Receiver operation characteristics (ROC) curves were plotted to select the best classifier. Our proposed system is able to identify the unknown class with an accuracy of 85.2\%, and sensitivity, specificity, and area under curve (AUC) of 98.9\%, 89.5\%, and 0.972 respectively using SVM classifier with polynomial kernel of order 3. We have also proposed a new integrated DR index (IDRI) using different features, which is able to identify the different classes with 100\% accuracy.},
  langid = {english},
  keywords = {Classifier,Diabetes,Fundus image,Index,Retinopathy,Support vector machine,Texture},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9SYH3TQD\\acharya2011.pdf;C\:\\Users\\cleme\\Zotero\\storage\\KCZUKR8E\\Acharya et al. - 2012 - An Integrated Index for the Identification of Diab.pdf}
}

@article{acharyauApplicationHigherOrder2008,
  title = {Application of Higher Order Spectra for the Identification of Diabetes Retinopathy Stages},
  author = {Acharya U, Rajendra and Chua, Chua Kuang and Ng, E. Y. K. and Yu, Wenwei and Chee, Caroline},
  year = {2008},
  month = dec,
  journal = {Journal of Medical Systems},
  volume = {32},
  number = {6},
  pages = {481--488},
  issn = {0148-5598},
  doi = {10.1007/s10916-008-9154-8},
  abstract = {Diabetic retinopathy (DR) is a condition where the retina is damaged due to fluid leaking from the blood vessels into the retina. In extreme cases, the patient will become blind. Therefore, early detection of diabetic retinopathy is crucial to prevent blindness. Various image processing techniques have been used to identify the different stages of diabetes retinopathy. The application of non-linear features of the higher-order spectra (HOS) was found to be efficient as it is more suitable for the detection of shapes. The aim of this work is to automatically identify the normal, mild DR, moderate DR, severe DR and prolific DR. The parameters are extracted from the raw images using the HOS techniques and fed to the support vector machine (SVM) classifier. This paper presents classification of five kinds of eye classes using SVM classifier. Our protocol uses, 300 subjects consisting of five different kinds of eye disease conditions. We demonstrate a sensitivity of 82\% for the classifier with the specificity of 88\%.},
  langid = {english},
  pmid = {19058652},
  keywords = {Automated,Computer-Assisted,Diabetic Retinopathy,Humans,Image Processing,Pattern Recognition,Reproducibility of Results}
}

@article{acharyauApplicationHigherOrder2008a,
  title = {Application of Higher Order Spectra for the Identification of Diabetes Retinopathy Stages},
  author = {Acharya U, Rajendra and Chua, Chua Kuang and Ng, E. Y. K. and Yu, Wenwei and Chee, Caroline},
  year = {2008},
  month = dec,
  journal = {Journal of Medical Systems},
  volume = {32},
  number = {6},
  pages = {481--488},
  issn = {0148-5598},
  doi = {10.1007/s10916-008-9154-8},
  abstract = {Diabetic retinopathy (DR) is a condition where the retina is damaged due to fluid leaking from the blood vessels into the retina. In extreme cases, the patient will become blind. Therefore, early detection of diabetic retinopathy is crucial to prevent blindness. Various image processing techniques have been used to identify the different stages of diabetes retinopathy. The application of non-linear features of the higher-order spectra (HOS) was found to be efficient as it is more suitable for the detection of shapes. The aim of this work is to automatically identify the normal, mild DR, moderate DR, severe DR and prolific DR. The parameters are extracted from the raw images using the HOS techniques and fed to the support vector machine (SVM) classifier. This paper presents classification of five kinds of eye classes using SVM classifier. Our protocol uses, 300 subjects consisting of five different kinds of eye disease conditions. We demonstrate a sensitivity of 82\% for the classifier with the specificity of 88\%.},
  langid = {english},
  pmid = {19058652},
  keywords = {Diabetic Retinopathy,Humans,Image Processing Computer-Assisted,Pattern Recognition Automated,Reproducibility of Results},
  file = {C:\Users\cleme\Zotero\storage\ILHHQ8GC\acharyau2008.pdf}
}

@misc{adakDetectingSeverityDiabetic2023a,
  title = {Detecting {{Severity}} of {{Diabetic Retinopathy}} from {{Fundus Images}} Using {{Ensembled Transformers}}},
  author = {Adak, Chandranath and Karkera, Tejas and Chattopadhyay, Soumi and Saqib, Muhammad},
  year = {2023},
  month = jan,
  number = {arXiv:2301.00973},
  eprint = {2301.00973},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.00973},
  urldate = {2023-06-28},
  abstract = {Diabetic Retinopathy (DR) is considered one of the primary concerns due to its effect on vision loss among most people with diabetes globally. The severity of DR is mostly comprehended manually by ophthalmologists from fundus photography-based retina images. This paper deals with an automated understanding of the severity stages of DR. In the literature, researchers have focused on this automation using traditional machine learning-based algorithms and convolutional architectures. However, the past works hardly focused on essential parts of the retinal image to improve the model performance. In this paper, we adopt transformer-based learning models to capture the crucial features of retinal images to understand DR severity better. We work with ensembling image transformers, where we adopt four models, namely ViT (Vision Transformer), BEiT (Bidirectional Encoder representation for image Transformer), CaiT (Class-Attention in Image Transformers), and DeiT (Data efficient image Transformers), to infer the degree of DR severity from fundus photographs. For experiments, we used the publicly available APTOS-2019 blindness detection dataset, where the performances of the transformer-based models were quite encouraging.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\S8QA93Y5\\Adak et al. - 2023 - Detecting Severity of Diabetic Retinopathy from Fu.pdf;C\:\\Users\\cleme\\Zotero\\storage\\EISAYAMQ\\2301.html}
}

@article{adebayoSanityChecksSaliency,
  title = {Sanity {{Checks}} for {{Saliency Maps}}},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  pages = {11},
  abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings2.},
  langid = {english}
}

@article{adebayoSanityChecksSaliencya,
  title = {Sanity {{Checks}} for {{Saliency Maps}}},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  pages = {11},
  abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings2.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\5M7S2HG2\Adebayo et al. - Sanity Checks for Saliency Maps.pdf}
}

@article{agarwalEvaluatingExplainabilityGraph2023,
  title = {Evaluating Explainability for Graph Neural Networks},
  author = {Agarwal, Chirag and Queen, Owen and Lakkaraju, Himabindu and Zitnik, Marinka},
  year = {2023},
  month = mar,
  journal = {Scientific Data},
  volume = {10},
  number = {1},
  pages = {144},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-023-01974-x},
  urldate = {2023-10-03},
  abstract = {As explanations are increasingly used to understand the behavior of graph neural networks (GNNs), evaluating the quality and reliability of GNN explanations is crucial. However, assessing the quality of GNN explanations is challenging as existing graph datasets have no or unreliable ground-truth explanations. Here, we introduce a synthetic graph data generator, ShapeGGen, which can generate a variety of benchmark datasets (e.g., varying graph sizes, degree distributions, homophilic vs. heterophilic graphs) accompanied by ground-truth explanations. The flexibility to generate diverse synthetic datasets and corresponding ground-truth explanations allows ShapeGGen to mimic the data in various real-world areas. We include ShapeGGen and several real-world graph datasets in a graph explainability library, GraphXAI. In addition to synthetic and real-world graph datasets with ground-truth explanations, GraphXAI provides data loaders, data processing functions, visualizers, GNN model implementations, and evaluation metrics to benchmark GNN explainability methods.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computer science,Databases,Decision making,Statistics}
}

@article{agarwalEvaluatingExplainabilityGraph2023a,
  title = {Evaluating Explainability for Graph Neural Networks},
  author = {Agarwal, Chirag and Queen, Owen and Lakkaraju, Himabindu and Zitnik, Marinka},
  year = {2023},
  month = mar,
  journal = {Scientific Data},
  volume = {10},
  number = {1},
  pages = {144},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-023-01974-x},
  urldate = {2023-10-03},
  abstract = {As explanations are increasingly used to understand the behavior of graph neural networks (GNNs), evaluating the quality and reliability of GNN explanations is crucial. However, assessing the quality of GNN explanations is challenging as existing graph datasets have no or unreliable ground-truth explanations. Here, we introduce a synthetic graph data generator, ShapeGGen, which can generate a variety of benchmark datasets (e.g., varying graph sizes, degree distributions, homophilic vs. heterophilic graphs) accompanied by ground-truth explanations. The flexibility to generate diverse synthetic datasets and corresponding ground-truth explanations allows ShapeGGen to mimic the data in various real-world areas. We include ShapeGGen and several real-world graph datasets in a graph explainability library, GraphXAI. In addition to synthetic and real-world graph datasets with ground-truth explanations, GraphXAI provides data loaders, data processing functions, visualizers, GNN model implementations, and evaluation metrics to benchmark GNN explainability methods.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computer science,Databases,Decision making,Statistics},
  file = {C:\Users\cleme\Zotero\storage\P5VZJZ76\Agarwal et al. - 2023 - Evaluating explainability for graph neural network.pdf}
}

@article{aggarwalDiagnosticAccuracyDeep2021,
  title = {Diagnostic Accuracy of Deep Learning in Medical Imaging: A Systematic Review and Meta-Analysis},
  shorttitle = {Diagnostic Accuracy of Deep Learning in Medical Imaging},
  author = {Aggarwal, Ravi and Sounderajah, Viknesh and Martin, Guy and Ting, Daniel S. W. and Karthikesalingam, Alan and King, Dominic and Ashrafian, Hutan and Darzi, Ara},
  year = {2021},
  month = apr,
  journal = {npj Digital Medicine},
  volume = {4},
  number = {1},
  pages = {1--23},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-021-00438-z},
  urldate = {2022-07-10},
  abstract = {Deep learning (DL) has the potential to transform medical diagnostics. However, the diagnostic accuracy of DL is uncertain. Our aim was to evaluate the diagnostic accuracy of DL algorithms to identify pathology in medical imaging. Searches were conducted in Medline and EMBASE up to January 2020. We identified 11,921 studies, of which 503 were included in the systematic review. Eighty-two studies in ophthalmology, 82 in breast disease and 115 in respiratory disease were included for meta-analysis. Two hundred twenty-four studies in other specialities were included for qualitative review. Peer-reviewed studies that reported on the diagnostic accuracy of DL algorithms to identify pathology using medical imaging were included. Primary outcomes were measures of diagnostic accuracy, study design and reporting standards in the literature. Estimates were pooled using random-effects meta-analysis. In ophthalmology, AUC's ranged between 0.933 and 1 for diagnosing diabetic retinopathy, age-related macular degeneration and glaucoma on retinal fundus photographs and optical coherence tomography. In respiratory imaging, AUC's ranged between 0.864 and 0.937 for diagnosing lung nodules or lung cancer on chest X-ray or CT scan. For breast imaging, AUC's ranged between 0.868 and 0.909 for diagnosing breast cancer on mammogram, ultrasound, MRI and digital breast tomosynthesis. Heterogeneity was high between studies and extensive variation in methodology, terminology and outcome measures was noted. This can lead to an overestimation of the diagnostic accuracy of DL algorithms on medical imaging. There is an immediate need for the development of artificial intelligence-specific EQUATOR guidelines, particularly STARD, in order to provide guidance around key issues in this field.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Translational research,Whole body imaging}
}

@article{aggarwalDiagnosticAccuracyDeep2021a,
  title = {Diagnostic Accuracy of Deep Learning in Medical Imaging: A Systematic Review and Meta-Analysis},
  shorttitle = {Diagnostic Accuracy of Deep Learning in Medical Imaging},
  author = {Aggarwal, Ravi and Sounderajah, Viknesh and Martin, Guy and Ting, Daniel S. W. and Karthikesalingam, Alan and King, Dominic and Ashrafian, Hutan and Darzi, Ara},
  year = {2021},
  month = apr,
  journal = {npj Digital Medicine},
  volume = {4},
  number = {1},
  pages = {1--23},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-021-00438-z},
  urldate = {2022-07-10},
  abstract = {Deep learning (DL) has the potential to transform medical diagnostics. However, the diagnostic accuracy of DL is uncertain. Our aim was to evaluate the diagnostic accuracy of DL algorithms to identify pathology in medical imaging. Searches were conducted in Medline and EMBASE up to January 2020. We identified 11,921 studies, of which 503 were included in the systematic review. Eighty-two studies in ophthalmology, 82 in breast disease and 115 in respiratory disease were included for meta-analysis. Two hundred twenty-four studies in other specialities were included for qualitative review. Peer-reviewed studies that reported on the diagnostic accuracy of DL algorithms to identify pathology using medical imaging were included. Primary outcomes were measures of diagnostic accuracy, study design and reporting standards in the literature. Estimates were pooled using random-effects meta-analysis. In ophthalmology, AUC's ranged between 0.933 and 1 for diagnosing diabetic retinopathy, age-related macular degeneration and glaucoma on retinal fundus photographs and optical coherence tomography. In respiratory imaging, AUC's ranged between 0.864 and 0.937 for diagnosing lung nodules or lung cancer on chest X-ray or CT scan. For breast imaging, AUC's ranged between 0.868 and 0.909 for diagnosing breast cancer on mammogram, ultrasound, MRI and digital breast tomosynthesis. Heterogeneity was high between studies and extensive variation in methodology, terminology and outcome measures was noted. This can lead to an overestimation of the diagnostic accuracy of DL algorithms on medical imaging. There is an immediate need for the development of artificial intelligence-specific EQUATOR guidelines, particularly STARD, in order to provide guidance around key issues in this field.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Translational research,Whole body imaging},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\45RTWJJK\\Aggarwal et al. - 2021 - Diagnostic accuracy of deep learning in medical im.pdf;C\:\\Users\\cleme\\Zotero\\storage\\N9RZNQP7\\aggarwal2021.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ATSVKAHR\\s41746-021-00438-z.html}
}

@article{agurtoAutomaticDetectionDiabetic2011,
  title = {Automatic {{Detection}} of {{Diabetic Retinopathy}} and {{Age-Related Macular Degeneration}} in {{Digital Fundus Images}}},
  author = {Agurto, Carla and Barriga, E. Simon and Murray, Victor and Nemeth, Sheila and Crammer, Robert and Bauman, Wendall and Zamora, Gilberto and Pattichis, Marios S. and Soliz, Peter},
  year = {2011},
  month = jul,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {52},
  number = {8},
  pages = {5862--5871},
  issn = {1552-5783},
  doi = {10.1167/iovs.10-7075},
  urldate = {2019-11-19},
  langid = {english}
}

@article{agurtoAutomaticDetectionDiabetic2011a,
  title = {Automatic {{Detection}} of {{Diabetic Retinopathy}} and {{Age-Related Macular Degeneration}} in {{Digital Fundus Images}}},
  author = {Agurto, Carla and Barriga, E. Simon and Murray, Victor and Nemeth, Sheila and Crammer, Robert and Bauman, Wendall and Zamora, Gilberto and Pattichis, Marios S. and Soliz, Peter},
  year = {2011},
  month = jul,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {52},
  number = {8},
  pages = {5862--5871},
  issn = {1552-5783},
  doi = {10.1167/iovs.10-7075},
  urldate = {2019-11-19},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\GIDFECEI\\Agurto et al. - 2011 - Automatic Detection of Diabetic Retinopathy and Ag.pdf;C\:\\Users\\cleme\\Zotero\\storage\\M2NDR8EU\\agurto2011.pdf;C\:\\Users\\cleme\\Zotero\\storage\\U7YKFTWQ\\article.html}
}

@article{ahmedt-aristizabalGraphBasedDeepLearning2021,
  title = {Graph-{{Based Deep Learning}} for {{Medical Diagnosis}} and {{Analysis}}: {{Past}}, {{Present}} and {{Future}}},
  shorttitle = {Graph-{{Based Deep Learning}} for {{Medical Diagnosis}} and {{Analysis}}},
  author = {{Ahmedt-Aristizabal}, David and Armin, Mohammad Ali and Denman, Simon and Fookes, Clinton and Petersson, Lars},
  year = {2021},
  month = jul,
  journal = {Sensors (Basel, Switzerland)},
  volume = {21},
  number = {14},
  pages = {4758},
  issn = {1424-8220},
  doi = {10.3390/s21144758},
  urldate = {2023-10-02},
  abstract = {With the advances of data-driven machine learning research, a wide variety of prediction problems have been tackled. It has become critical to explore how machine learning and specifically deep learning methods can be exploited to analyse healthcare data. A major limitation of existing methods has been the focus on grid-like data; however, the structure of physiological recordings are often irregular and unordered, which makes it difficult to conceptualise them as a matrix. As such, graph neural networks have attracted significant attention by exploiting implicit information that resides in a biological system, with interacting nodes connected by edges whose weights can be determined by either temporal associations or anatomical junctions. In this survey, we thoroughly review the different types of graph architectures and their applications in healthcare. We provide an overview of these methods in a systematic manner, organized by their domain of application including functional connectivity, anatomical structure, and electrical-based analysis. We also outline the limitations of existing techniques and discuss potential directions for future research.},
  pmcid = {PMC8309939},
  pmid = {34300498}
}

@article{ahmedt-aristizabalGraphBasedDeepLearning2021a,
  title = {Graph-{{Based Deep Learning}} for {{Medical Diagnosis}} and {{Analysis}}: {{Past}}, {{Present}} and {{Future}}},
  shorttitle = {Graph-{{Based Deep Learning}} for {{Medical Diagnosis}} and {{Analysis}}},
  author = {{Ahmedt-Aristizabal}, David and Armin, Mohammad Ali and Denman, Simon and Fookes, Clinton and Petersson, Lars},
  year = {2021},
  month = jul,
  journal = {Sensors (Basel, Switzerland)},
  volume = {21},
  number = {14},
  pages = {4758},
  issn = {1424-8220},
  doi = {10.3390/s21144758},
  urldate = {2023-10-02},
  abstract = {With the advances of data-driven machine learning research, a wide variety of prediction problems have been tackled. It has become critical to explore how machine learning and specifically deep learning methods can be exploited to analyse healthcare data. A major limitation of existing methods has been the focus on grid-like data; however, the structure of physiological recordings are often irregular and unordered, which makes it difficult to conceptualise them as a matrix. As such, graph neural networks have attracted significant attention by exploiting implicit information that resides in a biological system, with interacting nodes connected by edges whose weights can be determined by either temporal associations or anatomical junctions. In this survey, we thoroughly review the different types of graph architectures and their applications in healthcare. We provide an overview of these methods in a systematic manner, organized by their domain of application including functional connectivity, anatomical structure, and electrical-based analysis. We also outline the limitations of existing techniques and discuss potential directions for future research.},
  pmcid = {PMC8309939},
  pmid = {34300498},
  file = {C:\Users\cleme\Zotero\storage\EDCPJM57\Ahmedt-Aristizabal et al. - 2021 - Graph-Based Deep Learning for Medical Diagnosis an.pdf}
}

@article{ahmedt-aristizabalSurveyGraphbasedDeep2022,
  title = {A Survey on Graph-Based Deep Learning for Computational Histopathology},
  author = {{Ahmedt-Aristizabal}, David and Armin, Mohammad Ali and Denman, Simon and Fookes, Clinton and Petersson, Lars},
  year = {2022},
  month = jan,
  journal = {Computerized Medical Imaging and Graphics},
  volume = {95},
  pages = {102027},
  issn = {0895-6111},
  doi = {10.1016/j.compmedimag.2021.102027},
  urldate = {2023-10-02},
  abstract = {With the remarkable success of representation learning for prediction problems, we have witnessed a rapid expansion of the use of machine learning and deep learning for the analysis of digital pathology and biopsy image patches. However, learning over patch-wise features using convolutional neural networks limits the ability of the model to capture global contextual information and comprehensively model tissue composition. The phenotypical and topological distribution of constituent histological entities play a critical role in tissue diagnosis. As such, graph data representations and deep learning have attracted significant attention for encoding tissue representations, and capturing intra- and inter- entity level interactions. In this review, we provide a conceptual grounding for graph analytics in digital pathology, including entity-graph construction and graph architectures, and present their current success for tumor localization and classification, tumor invasion and staging, image retrieval, and survival prediction. We provide an overview of these methods in a systematic manner organized by the graph representation of the input image, scale, and organ on which they operate. We also outline the limitations of existing techniques, and suggest potential future research directions in this domain.},
  keywords = {Cancer classification,Cell-graph,Deep learning,Digital pathology,Graph Convolutional Networks,Hierarchical graph representation,Tissue-graph},
  file = {C:\Users\cleme\Zotero\storage\IHNBX5TT\Ahmedt-Aristizabal et al. - 2022 - A survey on graph-based deep learning for computat.pdf}
}

@inproceedings{akibaOptunaNextgenerationHyperparameter2019,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {2623--2631},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3292500.3330701},
  urldate = {2022-10-20},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  isbn = {978-1-4503-6201-6},
  keywords = {Bayesian optimization,black-box optimization,hyperparameter optimization,machine learning system}
}

@inproceedings{akibaOptunaNextgenerationHyperparameter2019a,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {2623--2631},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3292500.3330701},
  urldate = {2022-10-20},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  isbn = {978-1-4503-6201-6},
  keywords = {Bayesian optimization,black-box optimization,hyperparameter optimization,machine learning system},
  file = {C:\Users\cleme\Zotero\storage\WQEMADIM\akiba2019.pdf.pdf}
}

@article{al-jarrahNonproliferativeDiabeticRetinopathy2017,
  title = {Non-Proliferative Diabetic Retinopathy Symptoms Detection and Classification Using Neural Network},
  author = {{Al-Jarrah}, Mohammad A. and Shatnawi, Hadeel},
  year = {2017},
  month = aug,
  journal = {Journal of Medical Engineering \& Technology},
  volume = {41},
  number = {6},
  pages = {498--505},
  issn = {0309-1902},
  doi = {10.1080/03091902.2017.1358772},
  urldate = {2019-11-19},
  abstract = {Diabetic retinopathy (DR) causes blindness in the working age for people with diabetes in most countries. The increasing number of people with diabetes worldwide suggests that DR will continue to be major contributors to vision loss. Early detection of retinopathy progress in individuals with diabetes is critical for preventing visual loss. Non-proliferative DR (NPDR) is an early stage of DR. Moreover, NPDR can be classified into mild, moderate and severe. This paper proposes a novel morphology-based algorithm for detecting retinal lesions and classifying each case. First, the proposed algorithm detects the three DR lesions, namely haemorrhages, microaneurysms and exudates. Second, we defined and extracted a set of features from detected lesions. The set of selected feature emulates what physicians looked for in classifying NPDR case. Finally, we designed an artificial neural network (ANN) classifier with three layers to classify NPDR to normal, mild, moderate and severe. Bayesian regularisation and resilient backpropagation algorithms are used to train ANN. The accuracy for the proposed classifiers based on Bayesian regularisation and resilient backpropagation algorithms are 96.6 and 89.9, respectively. The obtained results are compared with results of the recent published classifier. Our proposed classifier outperforms the best in terms of sensitivity and specificity.},
  pmid = {28786703},
  keywords = {Exudate detection,haemorrhage detection,microaneurysm detection,neural network diabetic retinopathy classifier,non-proliferative diabetic retinopathy classification}
}

@article{al-jarrahNonproliferativeDiabeticRetinopathy2017a,
  title = {Non-Proliferative Diabetic Retinopathy Symptoms Detection and Classification Using Neural Network},
  author = {{Al-Jarrah}, Mohammad A. and Shatnawi, Hadeel},
  year = {2017},
  month = aug,
  journal = {Journal of Medical Engineering \& Technology},
  volume = {41},
  number = {6},
  pages = {498--505},
  issn = {0309-1902},
  doi = {10.1080/03091902.2017.1358772},
  urldate = {2019-11-19},
  abstract = {Diabetic retinopathy (DR) causes blindness in the working age for people with diabetes in most countries. The increasing number of people with diabetes worldwide suggests that DR will continue to be major contributors to vision loss. Early detection of retinopathy progress in individuals with diabetes is critical for preventing visual loss. Non-proliferative DR (NPDR) is an early stage of DR. Moreover, NPDR can be classified into mild, moderate and severe. This paper proposes a novel morphology-based algorithm for detecting retinal lesions and classifying each case. First, the proposed algorithm detects the three DR lesions, namely haemorrhages, microaneurysms and exudates. Second, we defined and extracted a set of features from detected lesions. The set of selected feature emulates what physicians looked for in classifying NPDR case. Finally, we designed an artificial neural network (ANN) classifier with three layers to classify NPDR to normal, mild, moderate and severe. Bayesian regularisation and resilient backpropagation algorithms are used to train ANN. The accuracy for the proposed classifiers based on Bayesian regularisation and resilient backpropagation algorithms are 96.6 and 89.9, respectively. The obtained results are compared with results of the recent published classifier. Our proposed classifier outperforms the best in terms of sensitivity and specificity.},
  pmid = {28786703},
  keywords = {Exudate detection,haemorrhage detection,microaneurysm detection,neural network diabetic retinopathy classifier,non-proliferative diabetic retinopathy classification},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\6H9AG9X3\\al-jarrah2017.pdf;C\:\\Users\\cleme\\Zotero\\storage\\3X6C8G6E\\03091902.2017.html}
}

@article{alainUnderstandingIntermediateLayers2016,
  title = {Understanding Intermediate Layers Using Linear Classifier Probes},
  author = {Alain, Guillaume and Bengio, Yoshua},
  year = {2016},
  month = oct,
  journal = {ArXiv},
  urldate = {2023-05-11},
  abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.}
}

@article{alainUnderstandingIntermediateLayers2016a,
  title = {Understanding Intermediate Layers Using Linear Classifier Probes},
  author = {Alain, Guillaume and Bengio, Yoshua},
  year = {2016},
  month = oct,
  journal = {ArXiv},
  urldate = {2023-05-11},
  abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself.  This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems.  We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
  file = {C:\Users\cleme\Zotero\storage\TLJUHPVC\Alain et Bengio - 2016 - Understanding intermediate layers using linear cla.pdf}
}

@book{alainUnderstandingIntermediateLayers2017,
  title = {Understanding Intermediate Layers Using Linear Classifier Probes},
  author = {Alain, Guillaume and Bengio, Y.},
  year = {2017},
  month = apr,
  abstract = {Neural network models have a reputation for being black boxes. We propose a new method to understand better the roles and dynamics of the intermediate layers. This has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics (such as the auxiliary heads in the Inception model). Our method uses linear classifiers, referred to as "probes", where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training. They allow the user to visualize the state of the model at multiple steps of training. We demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems.}
}

@book{alainUnderstandingIntermediateLayers2017a,
  title = {Understanding Intermediate Layers Using Linear Classifier Probes},
  author = {Alain, Guillaume and Bengio, Y.},
  year = {2017},
  month = apr,
  abstract = {Neural network models have a reputation for being black boxes. We propose a new method to understand better the roles and dynamics of the intermediate layers. This has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics (such as the auxiliary heads in the Inception model). Our method uses linear classifiers, referred to as "probes", where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training. They allow the user to visualize the state of the model at multiple steps of training. We demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems.}
}

@inproceedings{alainUnderstandingIntermediateLayers2017b,
  title = {Understanding Intermediate Layers Using Linear Classifier Probes},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
  author = {Alain, Guillaume and Bengio, Yoshua},
  year = {2017},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/AlainB17.bib},
  timestamp = {Thu, 04 Apr 2019 13:20:09 +0200}
}

@inproceedings{albarrak2013age,
  title = {Age-Related Macular Degeneration Identification in Volumetric Optical Coherence Tomography Using Decomposition and Local Feature Extraction},
  booktitle = {Proceedings of 2013 International Conference on Medical Image, Understanding and Analysis},
  author = {Albarrak, Abdulrahman and Coenen, Frans and Zheng, Yalin and others},
  year = {2013},
  pages = {59--64}
}

@inproceedings{albarrak2013age,
  title = {Age-Related Macular Degeneration Identification in Volumetric Optical Coherence Tomography Using Decomposition and Local Feature Extraction},
  booktitle = {Proceedings of 2013 International Conference on Medical Image, Understanding and Analysis},
  author = {Albarrak, Abdulrahman and Coenen, Frans and Zheng, Yalin and others},
  year = {2013},
  pages = {59--64}
}

@article{albarrakAgerelatedMacularDegeneration,
  title = {Age-Related {{Macular Degeneration Identification In Volumetric Optical Coherence Tomography Using Decomposition}} and {{Local Feature Extraction}}},
  author = {Albarrak, Abdulrahman and Albarrak, A and Coenen, Frans},
  abstract = {In this paper we proposed a decomposition based approach, coupled with local feature extraction, to support the analysis of Three-Dimensional (3D) Optical Coherence Tomography (OCT) images so as to determine the presence (or otherwise) of Age-related Macular Degeneration (AMD) in the retina of the human eye. AMD is one of the leading causes of vision loss in people aged over 50 years in the world. The 3D OCT imaging technique has become an indispensable diagnostic tool for the management of AMD. However, there is a lack of automated decision-making tools for analysing the large volumes of data that can be collected using OCT. In order to address this problem, a volumetric analysis technique is proposed for the automated diagnosis of AMD in 3D OCT images without the need for detecting AMD lesions. The process commences with the decomposition of a given image into sub-regions by recursively dividing a volume into sub-volumes. Then, for each sub-volume, oriented gradient local binary pattern histograms are extracted and formed into a feature vector to which classifier generation techniques can be applied. The proposed technique was evaluated using ten-fold cross validation by applying it to 140 volumetric OCT images, the results demonstrated a promising performance with a best Area Under the receiver operating Curve (AUC) value of 94.4\%.},
  langid = {english}
}

@article{albarrakAgerelatedMacularDegeneration2013,
  title = {Age-Related {{Macular Degeneration Identification In Volumetric Optical Coherence Tomography Using Decomposition}} and {{Local Feature Extraction}}},
  author = {Albarrak, Abdulrahman and Albarrak, A and Coenen, Frans and Zheng, Yalin},
  year = {2013},
  journal = {Proceedings of 2013 international conference on medical image, understanding and analysis},
  pages = {59--64},
  abstract = {In this paper we proposed a decomposition based approach, coupled with local feature extraction, to support the analysis of Three-Dimensional (3D) Optical Coherence Tomography (OCT) images so as to determine the presence (or otherwise) of Age-related Macular Degeneration (AMD) in the retina of the human eye. AMD is one of the leading causes of vision loss in people aged over 50 years in the world. The 3D OCT imaging technique has become an indispensable diagnostic tool for the management of AMD. However, there is a lack of automated decision-making tools for analysing the large volumes of data that can be collected using OCT. In order to address this problem, a volumetric analysis technique is proposed for the automated diagnosis of AMD in 3D OCT images without the need for detecting AMD lesions. The process commences with the decomposition of a given image into sub-regions by recursively dividing a volume into sub-volumes. Then, for each sub-volume, oriented gradient local binary pattern histograms are extracted and formed into a feature vector to which classifier generation techniques can be applied. The proposed technique was evaluated using ten-fold cross validation by applying it to 140 volumetric OCT images, the results demonstrated a promising performance with a best Area Under the receiver operating Curve (AUC) value of 94.4\%.},
  langid = {english}
}

@article{albarrakAgerelatedMacularDegeneration2013a,
  title = {Age-Related {{Macular Degeneration Identification In Volumetric Optical Coherence Tomography Using Decomposition}} and {{Local Feature Extraction}}},
  author = {Albarrak, Abdulrahman and Albarrak, A and Coenen, Frans and Zheng, Yalin},
  year = {2013},
  journal = {Proceedings of 2013 international conference on medical image, understanding and analysis},
  pages = {59--64},
  abstract = {In this paper we proposed a decomposition based approach, coupled with local feature extraction, to support the analysis of Three-Dimensional (3D) Optical Coherence Tomography (OCT) images so as to determine the presence (or otherwise) of Age-related Macular Degeneration (AMD) in the retina of the human eye. AMD is one of the leading causes of vision loss in people aged over 50 years in the world. The 3D OCT imaging technique has become an indispensable diagnostic tool for the management of AMD. However, there is a lack of automated decision-making tools for analysing the large volumes of data that can be collected using OCT. In order to address this problem, a volumetric analysis technique is proposed for the automated diagnosis of AMD in 3D OCT images without the need for detecting AMD lesions. The process commences with the decomposition of a given image into sub-regions by recursively dividing a volume into sub-volumes. Then, for each sub-volume, oriented gradient local binary pattern histograms are extracted and formed into a feature vector to which classifier generation techniques can be applied. The proposed technique was evaluated using ten-fold cross validation by applying it to 140 volumetric OCT images, the results demonstrated a promising performance with a best Area Under the receiver operating Curve (AUC) value of 94.4\%.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\9TPCUVB8\Albarrak et al. - Age-related Macular Degeneration Identiﬁcation In .pdf}
}

@article{albarrakAgerelatedMacularDegenerationa,
  title = {Age-Related {{Macular Degeneration Identification In Volumetric Optical Coherence Tomography Using Decomposition}} and {{Local Feature Extraction}}},
  author = {Albarrak, Abdulrahman and Albarrak, A and Coenen, Frans},
  abstract = {In this paper we proposed a decomposition based approach, coupled with local feature extraction, to support the analysis of Three-Dimensional (3D) Optical Coherence Tomography (OCT) images so as to determine the presence (or otherwise) of Age-related Macular Degeneration (AMD) in the retina of the human eye. AMD is one of the leading causes of vision loss in people aged over 50 years in the world. The 3D OCT imaging technique has become an indispensable diagnostic tool for the management of AMD. However, there is a lack of automated decision-making tools for analysing the large volumes of data that can be collected using OCT. In order to address this problem, a volumetric analysis technique is proposed for the automated diagnosis of AMD in 3D OCT images without the need for detecting AMD lesions. The process commences with the decomposition of a given image into sub-regions by recursively dividing a volume into sub-volumes. Then, for each sub-volume, oriented gradient local binary pattern histograms are extracted and formed into a feature vector to which classifier generation techniques can be applied. The proposed technique was evaluated using ten-fold cross validation by applying it to 140 volumetric OCT images, the results demonstrated a promising performance with a best Area Under the receiver operating Curve (AUC) value of 94.4\%.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\RLM8B9ZP\Albarrak et al. - Age-related Macular Degeneration Identiﬁcation In .pdf}
}

@article{albarrakVolumetricImageClassification2017,
  title = {Volumetric Image Classification Using Homogeneous Decomposition and Dictionary Learning: {{A}} Study Using Retinal Optical Coherence Tomography for Detecting Age-Related Macular Degeneration},
  shorttitle = {Volumetric Image Classification Using Homogeneous Decomposition and Dictionary Learning},
  author = {Albarrak, Abdulrahman and Coenen, Frans and Zheng, Yalin},
  year = {2017},
  month = jan,
  journal = {Computerized Medical Imaging and Graphics: The Official Journal of the Computerized Medical Imaging Society},
  volume = {55},
  pages = {113--123},
  issn = {1879-0771},
  doi = {10.1016/j.compmedimag.2016.07.007},
  abstract = {Three-dimensional (3D) (volumetric) diagnostic imaging techniques are indispensable with respect to the diagnosis and management of many medical conditions. However there is a lack of automated diagnosis techniques to facilitate such 3D image analysis (although some support tools do exist). This paper proposes a novel framework for volumetric medical image classification founded on homogeneous decomposition and dictionary learning. In the proposed framework each image (volume) is recursively decomposed until homogeneous regions are arrived at. Each region is represented using a Histogram of Oriented Gradients (HOG) which is transformed into a set of feature vectors. The Gaussian Mixture Model (GMM) is then used to generate a "dictionary" and the Improved Fisher Kernel (IFK) approach is used to encode feature vectors so as to generate a single feature vector for each volume, which can then be fed into a classifier generator. The principal advantage offered by the framework is that it does not require the detection (segmentation) of specific objects within the input data. The nature of the framework is fully described. A wide range of experiments was conducted with which to analyse the operation of the proposed framework and these are also reported fully in the paper. Although the proposed approach is generally applicable to 3D volumetric images, the focus for the work is 3D retinal Optical Coherence Tomography (OCT) images in the context of the diagnosis of Age-related Macular Degeneration (AMD). The results indicate that excellent diagnostic predictions can be produced using the proposed framework.},
  langid = {english},
  pmid = {27507326},
  keywords = {Age-related Macular Degeneration (AMD),Algorithms,Dictionary learning,Feature selection,Homogeneous decomposition,Humans,Image classification,Imaging,Macular Degeneration,Optical Coherence,Optical Coherence Tomography (OCT),Retina,Three-Dimensional,Tomography}
}

@article{albarrakVolumetricImageClassification2017a,
  title = {Volumetric Image Classification Using Homogeneous Decomposition and Dictionary Learning: {{A}} Study Using Retinal Optical Coherence Tomography for Detecting Age-Related Macular Degeneration},
  shorttitle = {Volumetric Image Classification Using Homogeneous Decomposition and Dictionary Learning},
  author = {Albarrak, Abdulrahman and Coenen, Frans and Zheng, Yalin},
  year = {2017},
  month = jan,
  journal = {Computerized Medical Imaging and Graphics: The Official Journal of the Computerized Medical Imaging Society},
  volume = {55},
  pages = {113--123},
  issn = {1879-0771},
  doi = {10.1016/j.compmedimag.2016.07.007},
  abstract = {Three-dimensional (3D) (volumetric) diagnostic imaging techniques are indispensable with respect to the diagnosis and management of many medical conditions. However there is a lack of automated diagnosis techniques to facilitate such 3D image analysis (although some support tools do exist). This paper proposes a novel framework for volumetric medical image classification founded on homogeneous decomposition and dictionary learning. In the proposed framework each image (volume) is recursively decomposed until homogeneous regions are arrived at. Each region is represented using a Histogram of Oriented Gradients (HOG) which is transformed into a set of feature vectors. The Gaussian Mixture Model (GMM) is then used to generate a "dictionary" and the Improved Fisher Kernel (IFK) approach is used to encode feature vectors so as to generate a single feature vector for each volume, which can then be fed into a classifier generator. The principal advantage offered by the framework is that it does not require the detection (segmentation) of specific objects within the input data. The nature of the framework is fully described. A wide range of experiments was conducted with which to analyse the operation of the proposed framework and these are also reported fully in the paper. Although the proposed approach is generally applicable to 3D volumetric images, the focus for the work is 3D retinal Optical Coherence Tomography (OCT) images in the context of the diagnosis of Age-related Macular Degeneration (AMD). The results indicate that excellent diagnostic predictions can be produced using the proposed framework.},
  langid = {english},
  pmid = {27507326},
  keywords = {Age-related Macular Degeneration (AMD),Algorithms,Dictionary learning,Feature selection,Homogeneous decomposition,Humans,Image classification,Imaging Three-Dimensional,Macular Degeneration,Optical Coherence Tomography (OCT),Retina,Tomography Optical Coherence}
}

@misc{AlexanderGrishin_thesisPdfGoogle,
  title = {{{AlexanderGrishin}}\_thesis.Pdf - {{Google~Drive}}},
  urldate = {2019-02-14},
  howpublished = {https://drive.google.com/file/d/1x0GOtvymCucv8AjsCO1uXF2W32swliDP/view},
  file = {C:\Users\cleme\Zotero\storage\NPV4JLMQ\view.html}
}

@misc{AlexanderGrishinThesisPdf,
  title = {{{AlexanderGrishin}}\_thesis.Pdf - {{Google Drive}}},
  urldate = {2019-02-14}
}

@inproceedings{alsaihClassificationRetinalCysts2018,
  title = {Classification of {{Retinal Cysts}} on {{SD-OCT Images Using Stacked Auto-Encoder}}},
  booktitle = {2018 {{International Conference}} on {{Intelligent}} and {{Advanced System}} ({{ICIAS}})},
  author = {Alsaih, K. and Tang, T. and M{\'e}riaudeau, F. and Lema{\^i}tre, G. and Rastgoo, M. and Sidib{\'e}, D.},
  year = {2018},
  month = aug,
  pages = {1--4},
  doi = {10.1109/ICIAS.2018.8540565},
  abstract = {Studies have shown that diabetes costs over \$770 million in USA. Diabetic retinopathy (DR) and its complication diabetic macular edema (DME) and age-related macular degeneration (AMD) are crucial diseases that might affect the retina and lead to blindness. Optical Coherence Tomography screening is one of the most effective screening methods to diagnose the retinal cysts lesions and to view the retina in 3-D volumes. This paper presents a methodology for automated detection of retinal cysts on Spectral Domain OCT (SD-OCT) volumes. The proposed method considers a generic classification pipeline that extracts the stable regions and then compares the stable regions with the optimal ground truth in order to label the potential regions. After that, the potential regions were resized and sent to the autoencoder to extract the features in an unsupervised fashion. Finally, the trained data was classified using the softmax layer in a supervised fashion, and the test data is passed through the network to validate the results. The results obtained from our pipeline are 93.0\% and 82.0\% for sensitivity (SE) and specificity (SP) respectively.},
  keywords = {3D volumes,age-related macular degeneration,automated detection,biomedical optical imaging,blindness,Data mining,Diabetes,diabetic macular edema,diabetic retinopathy,diseases,effective screening methods,eye,feature extraction,Feature extraction,generic classification pipeline,image classification,image coding,Image segmentation,medical image processing,Optical Coherence Tomography screening,optical tomography,optimal ground truth,Retina,retinal cysts lesions,SD-OCT images,Sensitivity,softmax layer,spectral domain OCT volumes,stable regions,stacked autoencoder,supervised fashion,Testing,unsupervised fashion,vision defects}
}

@inproceedings{alsaihClassificationRetinalCysts2018a,
  title = {Classification of {{Retinal Cysts}} on {{SD-OCT Images Using Stacked Auto-Encoder}}},
  booktitle = {2018 {{International Conference}} on {{Intelligent}} and {{Advanced System}} ({{ICIAS}})},
  author = {Alsaih, K. and Tang, T. and M{\'e}riaudeau, F. and Lema{\^i}tre, G. and Rastgoo, M. and Sidib{\'e}, D.},
  year = {2018},
  month = aug,
  pages = {1--4},
  doi = {10.1109/ICIAS.2018.8540565},
  abstract = {Studies have shown that diabetes costs over \$770 million in USA. Diabetic retinopathy (DR) and its complication diabetic macular edema (DME) and age-related macular degeneration (AMD) are crucial diseases that might affect the retina and lead to blindness. Optical Coherence Tomography screening is one of the most effective screening methods to diagnose the retinal cysts lesions and to view the retina in 3-D volumes. This paper presents a methodology for automated detection of retinal cysts on Spectral Domain OCT (SD-OCT) volumes. The proposed method considers a generic classification pipeline that extracts the stable regions and then compares the stable regions with the optimal ground truth in order to label the potential regions. After that, the potential regions were resized and sent to the autoencoder to extract the features in an unsupervised fashion. Finally, the trained data was classified using the softmax layer in a supervised fashion, and the test data is passed through the network to validate the results. The results obtained from our pipeline are 93.0\% and 82.0\% for sensitivity (SE) and specificity (SP) respectively.},
  keywords = {3D volumes,age-related macular degeneration,automated detection,biomedical optical imaging,blindness,Data mining,Diabetes,diabetic macular edema,diabetic retinopathy,diseases,effective screening methods,eye,feature extraction,Feature extraction,generic classification pipeline,image classification,image coding,Image segmentation,medical image processing,Optical Coherence Tomography screening,optical tomography,optimal ground truth,Retina,retinal cysts lesions,SD-OCT images,Sensitivity,softmax layer,spectral domain OCT volumes,stable regions,stacked autoencoder,supervised fashion,Testing,unsupervised fashion,vision defects},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\35HUKZ4M\\Alsaih et al. - 2018 - Classification of Retinal Cysts on SD-OCT Images U.pdf;C\:\\Users\\cleme\\Zotero\\storage\\QUMAXUN4\\8540565.html}
}

@article{alsaihMachineLearningTechniques2017,
  title = {Machine Learning Techniques for Diabetic Macular Edema ({{DME}}) Classification on {{SD-OCT}} Images},
  author = {Alsaih, Khaled and Lemaitre, Guillaume and Rastgoo, Mojdeh and Massich, Joan and Sidib{\'e}, D{\'e}sir{\'e} and Meriaudeau, Fabrice},
  year = {2017},
  month = jun,
  journal = {BioMedical Engineering OnLine},
  volume = {16},
  number = {1},
  pages = {68},
  issn = {1475-925X},
  doi = {10.1186/s12938-017-0352-9},
  urldate = {2019-07-23},
  abstract = {Spectral domain optical coherence tomography (OCT) (SD-OCT) is most widely imaging equipment used in ophthalmology to detect diabetic macular edema (DME). Indeed, it offers an accurate visualization of the morphology of the retina as well as the retina layers.}
}

@article{alsaihMachineLearningTechniques2017a,
  title = {Machine Learning Techniques for Diabetic Macular Edema ({{DME}}) Classification on {{SD-OCT}} Images},
  author = {Alsaih, Khaled and Lemaitre, Guillaume and Rastgoo, Mojdeh and Massich, Joan and Sidib{\'e}, D{\'e}sir{\'e} and Meriaudeau, Fabrice},
  year = {2017},
  month = jun,
  journal = {BioMedical Engineering OnLine},
  volume = {16},
  number = {1},
  pages = {68},
  issn = {1475-925X},
  doi = {10.1186/s12938-017-0352-9},
  urldate = {2019-11-21},
  abstract = {Spectral domain optical coherence tomography (OCT) (SD-OCT) is most widely imaging equipment used in ophthalmology to detect diabetic macular edema (DME). Indeed, it offers an accurate visualization of the morphology of the retina as well as the retina layers.}
}

@article{alsaihMachineLearningTechniques2017b,
  title = {Machine Learning Techniques for Diabetic Macular Edema ({{DME}}) Classification on {{SD-OCT}} Images},
  author = {Alsaih, Khaled and Lemaitre, Guillaume and Rastgoo, Mojdeh and Massich, Joan and Sidib{\'e}, D{\'e}sir{\'e} and Meriaudeau, Fabrice},
  year = {2017},
  month = jun,
  journal = {BioMedical Engineering OnLine},
  volume = {16},
  number = {1},
  pages = {68},
  issn = {1475-925X},
  doi = {10.1186/s12938-017-0352-9},
  urldate = {2019-07-23},
  abstract = {Spectral domain optical coherence tomography (OCT) (SD-OCT) is most widely imaging equipment used in ophthalmology to detect diabetic macular edema (DME). Indeed, it offers an accurate visualization of the morphology of the retina as well as the retina layers.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\3GP37XW9\\Alsaih et al. - 2017 - Machine learning techniques for diabetic macular e.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZEFEM2ZM\\s12938-017-0352-9.html}
}

@article{alsaihMachineLearningTechniques2017c,
  title = {Machine Learning Techniques for Diabetic Macular Edema ({{DME}}) Classification on {{SD-OCT}} Images},
  author = {Alsaih, Khaled and Lemaitre, Guillaume and Rastgoo, Mojdeh and Massich, Joan and Sidib{\'e}, D{\'e}sir{\'e} and Meriaudeau, Fabrice},
  year = {2017},
  month = jun,
  journal = {BioMedical Engineering OnLine},
  volume = {16},
  number = {1},
  pages = {68},
  issn = {1475-925X},
  doi = {10.1186/s12938-017-0352-9},
  urldate = {2019-11-21},
  abstract = {Spectral domain optical coherence tomography (OCT) (SD-OCT) is most widely imaging equipment used in ophthalmology to detect diabetic macular edema (DME). Indeed, it offers an accurate visualization of the morphology of the retina as well as the retina layers.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\VMN2ZWGH\\Alsaih et al. - 2017 - Machine learning techniques for diabetic macular e.pdf;C\:\\Users\\cleme\\Zotero\\storage\\HST3UMN4\\s12938-017-0352-9.html}
}

@article{alzubaidiReviewDeepLearning2021,
  title = {Review of Deep Learning: {{Concepts}}, {{CNN}} Architectures, Challenges, Applications, Future Directions},
  shorttitle = {Review of Deep Learning},
  author = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J. and {Al-Dujaili}, Ayad and Duan, Ye and {Al-Shamma}, Omran and Santamar{\'i}a, J. and Fadhel, Mohammed A. and {Al-Amidie}, Muthana and Farhan, Laith},
  year = {2021},
  month = mar,
  journal = {Journal of Big Data},
  volume = {8},
  number = {1},
  pages = {53},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00444-8},
  urldate = {2023-02-10},
  abstract = {In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.},
  keywords = {Convolution neural network (CNN),Deep learning,Deep learning applications,Deep neural network architectures,FPGA,GPU,Image classification,Machine learning,Medical image analysis,Supervised learning,Transfer learning}
}

@article{alzubaidiReviewDeepLearning2021a,
  title = {Review of Deep Learning: Concepts, {{CNN}} Architectures, Challenges, Applications, Future Directions},
  shorttitle = {Review of Deep Learning},
  author = {Alzubaidi, Laith and Zhang, Jinglan and Humaidi, Amjad J. and {Al-Dujaili}, Ayad and Duan, Ye and {Al-Shamma}, Omran and Santamar{\'i}a, J. and Fadhel, Mohammed A. and {Al-Amidie}, Muthana and Farhan, Laith},
  year = {2021},
  month = mar,
  journal = {Journal of Big Data},
  volume = {8},
  number = {1},
  pages = {53},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00444-8},
  urldate = {2023-02-10},
  abstract = {In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.},
  keywords = {Convolution neural network (CNN),Deep learning,Deep learning applications,Deep neural network architectures,FPGA,GPU,Image classification,Machine learning,Medical image analysis,Supervised learning,Transfer learning},
  file = {C:\Users\cleme\Zotero\storage\PXMXQXU8\Alzubaidi et al. - 2021 - Review of deep learning concepts, CNN architectur.pdf}
}

@article{amannExplainabilityArtificialIntelligence2020,
  title = {Explainability for Artificial Intelligence in Healthcare: A Multidisciplinary Perspective},
  shorttitle = {Explainability for Artificial Intelligence in Healthcare},
  author = {Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I.},
  year = {2020},
  month = nov,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {20},
  issn = {1472-6947},
  doi = {10.1186/s12911-020-01332-6},
  urldate = {2021-03-01},
  abstract = {Background Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice. Methods Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the ``Principles of Biomedical Ethics'' by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI. Results Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health. Conclusions To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.},
  pmcid = {PMC7706019},
  pmid = {null}
}

@article{amannExplainabilityArtificialIntelligence2020a,
  title = {Explainability for Artificial Intelligence in Healthcare: A Multidisciplinary Perspective},
  shorttitle = {Explainability for Artificial Intelligence in Healthcare},
  author = {Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I.},
  year = {2020},
  month = dec,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {20},
  number = {1},
  pages = {1--9},
  publisher = {BioMed Central},
  issn = {1472-6947},
  doi = {10.1186/s12911-020-01332-6},
  urldate = {2021-11-11},
  abstract = {Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice. Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the ``Principles of Biomedical Ethics'' by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI. Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health. To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.},
  copyright = {2020 The Author(s)},
  langid = {english}
}

@article{amannExplainabilityArtificialIntelligence2020b,
  title = {Explainability for Artificial Intelligence in Healthcare: A Multidisciplinary Perspective},
  shorttitle = {Explainability for Artificial Intelligence in Healthcare},
  author = {Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I.},
  year = {2020},
  month = dec,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {20},
  number = {1},
  pages = {1--9},
  publisher = {BioMed Central},
  issn = {1472-6947},
  doi = {10.1186/s12911-020-01332-6},
  urldate = {2021-11-11},
  abstract = {Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice. Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the ``Principles of Biomedical Ethics'' by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI. Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health. To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.},
  copyright = {2020 The Author(s)},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\AM5LP2JH\\Amann et al. - 2020 - Explainability for artificial intelligence in heal.pdf;C\:\\Users\\cleme\\Zotero\\storage\\T3CKRZVF\\s12911-020-01332-6.html}
}

@article{amannExplainabilityArtificialIntelligence2020c,
  title = {Explainability for Artificial Intelligence in Healthcare: A Multidisciplinary Perspective},
  shorttitle = {Explainability for Artificial Intelligence in Healthcare},
  author = {Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I.},
  year = {2020},
  month = nov,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {20},
  issn = {1472-6947},
  doi = {10.1186/s12911-020-01332-6},
  urldate = {2021-03-01},
  abstract = {Background Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice. Methods Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the ``Principles of Biomedical Ethics'' by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI. Results Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health. Conclusions To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.},
  pmcid = {PMC7706019},
  pmid = {null},
  file = {C:\Users\cleme\Zotero\storage\3TVKT3BS\Amann et al. - 2020 - Explainability for artificial intelligence in heal.pdf}
}

@article{aminReviewRecentDevelopments2016,
  title = {A {{Review}} on {{Recent Developments}} for {{Detection}} of {{Diabetic Retinopathy}}},
  author = {Amin, Javeria and Sharif, Muhammad and Yasmin, Mussarat},
  year = {2016},
  journal = {Scientifica},
  volume = {2016},
  pages = {6838976},
  issn = {2090-908X},
  doi = {10.1155/2016/6838976},
  abstract = {Diabetic retinopathy is caused by the retinal micro vasculature which may be formed as a result of diabetes mellitus. Blindness may appear as a result of unchecked and severe cases of diabetic retinopathy. Manual inspection of fundus images to check morphological changes in microaneurysms, exudates, blood vessels, hemorrhages, and macula is a very time-consuming and tedious work. It can be made easily with the help of computer-aided system and intervariability for the observer. In this paper, several techniques for detecting microaneurysms, hemorrhages, and exudates are discussed for ultimate detection of nonproliferative diabetic retinopathy. Blood vessels detection techniques are also discussed for the diagnosis of proliferative diabetic retinopathy. Furthermore, the paper elaborates a discussion on the experiments accessed by authors for the detection of diabetic retinopathy. This work will be helpful for the researchers and technical persons who want to utilize the ongoing research in this area.},
  langid = {english},
  pmcid = {PMC5061953},
  pmid = {27777811},
  file = {C:\Users\cleme\Zotero\storage\FLC49XCD\Amin et al. - 2016 - A Review on Recent Developments for Detection of D.pdf}
}

@article{andrewDeepCanonicalCorrelation,
  title = {Deep {{Canonical Correlation Analysis}}},
  author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  pages = {9},
  abstract = {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation. It can be viewed as a nonlinear extension of the linear method canonical correlation analysis (CCA). It is an alternative to the nonparametric method kernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances. In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.},
  langid = {english}
}

@article{andrewDeepCanonicalCorrelationa,
  title = {Deep {{Canonical Correlation Analysis}}},
  author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  abstract = {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation. It can be viewed as a nonlinear extension of the linear method canonical correlation analysis (CCA). It is an alternative to the nonparametric method kernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances. In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\X2YUQFHI\Andrew et al. - Deep Canonical Correlation Analysis.pdf}
}

@article{andrewDeepCanonicalCorrelationb,
  title = {Deep {{Canonical Correlation Analysis}}},
  author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  pages = {9},
  abstract = {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation. It can be viewed as a nonlinear extension of the linear method canonical correlation analysis (CCA). It is an alternative to the nonparametric method kernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances. In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\P4RWN9SI\Andrew et al. - Deep Canonical Correlation Analysis.pdf}
}

@article{anGlaucomaDiagnosisMachine2019,
  title = {Glaucoma {{Diagnosis}} with {{Machine Learning Based}} on {{Optical Coherence Tomography}} and {{Color Fundus Images}}},
  author = {An, Guangzhou and Omodaka, Kazuko and Hashimoto, Kazuki and Tsuda, Satoru and Shiga, Yukihiro and Takada, Naoko and Kikawa, Tsutomu and Yokota, Hideo and Akiba, Masahiro and Nakazawa, Toru},
  year = {2019},
  month = feb,
  journal = {Journal of Healthcare Engineering},
  volume = {2019},
  pages = {1--9},
  issn = {2040-2295, 2040-2309},
  doi = {10.1155/2019/4061313},
  urldate = {2019-07-30},
  langid = {english}
}

@misc{anGlaucomaDiagnosisMachine2019a,
  type = {Research {{Article}}},
  title = {Glaucoma {{Diagnosis}} with {{Machine Learning Based}} on {{Optical Coherence Tomography}} and {{Color Fundus Images}}},
  author = {An, Guangzhou and Omodaka, Kazuko and Hashimoto, Kazuki and Tsuda, Satoru and Shiga, Yukihiro and Takada, Naoko and Kikawa, Tsutomu and Yokota, Hideo and Akiba, Masahiro and Nakazawa, Toru},
  year = {2019},
  doi = {10.1155/2019/4061313},
  urldate = {2019-07-30},
  abstract = {This study aimed to develop a machine learning-based algorithm for glaucoma diagnosis in patients with open-angle glaucoma, based on three-dimensional optical coherence tomography (OCT) data and color fundus images. In this study, 208 glaucomatous and 149 healthy eyes were enrolled, and color fundus images and volumetric OCT data from the optic disc and macular area of these eyes were captured with a spectral-domain OCT (3D OCT-2000, Topcon). Thickness and deviation maps were created with a segmentation algorithm. Transfer learning of convolutional neural network (CNN) was used with the following types of input images: (1) fundus image of optic disc in grayscale format, (2) disc retinal nerve fiber layer (RNFL) thickness map, (3) macular ganglion cell complex (GCC) thickness map, (4) disc RNFL deviation map, and (5) macular GCC deviation map. Data augmentation and dropout were performed to train the CNN. For combining the results from each CNN model, a random forest (RF) was trained to classify the disc fundus images of healthy and glaucomatous eyes using feature vector representation of each input image, removing the second fully connected layer. The area under receiver operating characteristic curve (AUC) of a 10-fold cross validation (CV) was used to evaluate the models. The 10-fold CV AUCs of the CNNs were 0.940 for color fundus images, 0.942 for RNFL thickness maps, 0.944 for macular GCC thickness maps, 0.949 for disc RNFL deviation maps, and 0.952 for macular GCC deviation maps. The RF combining the five separate CNN models improved the 10-fold CV AUC to 0.963. Therefore, the machine learning system described here can accurately differentiate between healthy and glaucomatous subjects based on their extracted images from OCT data and color fundus images. This system should help to improve the diagnostic accuracy in glaucoma.},
  langid = {english}
}

@article{anGlaucomaDiagnosisMachine2019b,
  title = {Glaucoma {{Diagnosis}} with {{Machine Learning Based}} on {{Optical Coherence Tomography}} and {{Color Fundus Images}}},
  author = {An, Guangzhou and Omodaka, Kazuko and Hashimoto, Kazuki and Tsuda, Satoru and Shiga, Yukihiro and Takada, Naoko and Kikawa, Tsutomu and Yokota, Hideo and Akiba, Masahiro and Nakazawa, Toru},
  year = {2019},
  month = feb,
  journal = {Journal of Healthcare Engineering},
  volume = {2019},
  pages = {1--9},
  issn = {2040-2295, 2040-2309},
  doi = {10.1155/2019/4061313},
  urldate = {2019-07-30},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\2PTWC5ZD\An et al. - 2019 - Glaucoma Diagnosis with Machine Learning Based on .pdf}
}

@misc{anGlaucomaDiagnosisMachine2019c,
  type = {Research Article},
  title = {Glaucoma {{Diagnosis}} with {{Machine Learning Based}} on {{Optical Coherence Tomography}} and {{Color Fundus Images}}},
  author = {An, Guangzhou and Omodaka, Kazuko and Hashimoto, Kazuki and Tsuda, Satoru and Shiga, Yukihiro and Takada, Naoko and Kikawa, Tsutomu and Yokota, Hideo and Akiba, Masahiro and Nakazawa, Toru},
  year = {2019},
  journal = {Journal of Healthcare Engineering},
  doi = {10.1155/2019/4061313},
  urldate = {2019-07-30},
  abstract = {This study aimed to develop a machine learning-based algorithm for glaucoma diagnosis in patients with open-angle glaucoma, based on three-dimensional optical coherence tomography (OCT) data and color fundus images. In this study, 208 glaucomatous and 149 healthy eyes were enrolled, and color fundus images and volumetric OCT data from the optic disc and macular area of these eyes were captured with a spectral-domain OCT (3D OCT-2000, Topcon). Thickness and deviation maps were created with a segmentation algorithm. Transfer learning of convolutional neural network (CNN) was used with the following types of input images: (1) fundus image of optic disc in grayscale format, (2) disc retinal nerve fiber layer (RNFL) thickness map, (3) macular ganglion cell complex (GCC) thickness map, (4) disc RNFL deviation map, and (5) macular GCC deviation map. Data augmentation and dropout were performed to train the CNN. For combining the results from each CNN model, a random forest (RF) was trained to classify the disc fundus images of healthy and glaucomatous eyes using feature vector representation of each input image, removing the second fully connected layer. The area under receiver operating characteristic curve (AUC) of a 10-fold cross validation (CV) was used to evaluate the models. The 10-fold CV AUCs of the CNNs were 0.940 for color fundus images, 0.942 for RNFL thickness maps, 0.944 for macular GCC thickness maps, 0.949 for disc RNFL deviation maps, and 0.952 for macular GCC deviation maps. The RF combining the five separate CNN models improved the 10-fold CV AUC to 0.963. Therefore, the machine learning system described here can accurately differentiate between healthy and glaucomatous subjects based on their extracted images from OCT data and color fundus images. This system should help to improve the diagnostic accuracy in glaucoma.},
  howpublished = {https://www.hindawi.com/journals/jhe/2019/4061313/},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\DXX3FLVB\\An et al. - 2019 - Glaucoma Diagnosis with Machine Learning Based on .pdf;C\:\\Users\\cleme\\Zotero\\storage\\LZLVJ5KQ\\4061313.html}
}

@misc{APTOS2019Blindnessa,
  title = {{{APTOS}} 2019 {{Blindness Detection}} {\textbar} {{Kaggle}}},
  urldate = {2019-07-24},
  howpublished = {https://www.kaggle.com/c/aptos2019-blindness-detection},
  file = {C:\Users\cleme\Zotero\storage\LJB5DWST\aptos2019-blindness-detection.html}
}

@article{arcaduDeepLearningAlgorithm2019,
  title = {Deep Learning Algorithm Predicts Diabetic Retinopathy Progression in Individual Patients},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Willis, Jeff and Haskova, Zdenka and Prunotto, Marco},
  year = {2019},
  month = sep,
  journal = {npj Digital Medicine},
  volume = {2},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0172-3},
  urldate = {2023-05-24},
  abstract = {The global burden of diabetic retinopathy (DR) continues to worsen and DR remains a leading cause of vision loss worldwide. Here, we describe an algorithm to predict DR progression by means of deep learning (DL), using as input color fundus photographs (CFPs) acquired at a single visit from a patient with DR. The proposed DL models were designed to predict future DR progression, defined as 2-step worsening on the Early Treatment Diabetic Retinopathy Diabetic Retinopathy Severity Scale, and were trained against DR severity scores assessed after 6, 12, and 24 months from the baseline visit by masked, well-trained, human reading center graders. The performance of one of these models (prediction at month 12) resulted in an area under the curve equal to 0.79. Interestingly, our results highlight the importance of the predictive signal located in the peripheral retinal fields, not routinely collected for DR assessments, and the importance of microvascular abnormalities. Our findings show the feasibility of predicting future DR progression by leveraging CFPs of a patient acquired at a single visit. Upon further development on larger and more diverse datasets, such an algorithm could enable early diagnosis and referral to a retina specialist for more frequent monitoring and even consideration of early intervention. Moreover, it could also improve patient recruitment for clinical trials targeting DR.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Macular degeneration,Predictive markers,Vision disorders}
}

@article{arcaduDeepLearningAlgorithm2019a,
  title = {Deep Learning Algorithm Predicts Diabetic Retinopathy Progression in Individual Patients},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Willis, Jeff and Haskova, Zdenka and Prunotto, Marco},
  year = {2019},
  month = sep,
  journal = {npj Digital Medicine},
  volume = {2},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0172-3},
  urldate = {2023-10-05},
  abstract = {The global burden of diabetic retinopathy (DR) continues to worsen and DR remains a leading cause of vision loss worldwide. Here, we describe an algorithm to predict DR progression by means of deep learning (DL), using as input color fundus photographs (CFPs) acquired at a single visit from a patient with DR. The proposed DL models were designed to predict future DR progression, defined as 2-step worsening on the Early Treatment Diabetic Retinopathy Diabetic Retinopathy Severity Scale, and were trained against DR severity scores assessed after 6, 12, and 24 months from the baseline visit by masked, well-trained, human reading center graders. The performance of one of these models (prediction at month 12) resulted in an area under the curve equal to 0.79. Interestingly, our results highlight the importance of the predictive signal located in the peripheral retinal fields, not routinely collected for DR assessments, and the importance of microvascular abnormalities. Our findings show the feasibility of predicting future DR progression by leveraging CFPs of a patient acquired at a single visit. Upon further development on larger and more diverse datasets, such an algorithm could enable early diagnosis and referral to a retina specialist for more frequent monitoring and even consideration of early intervention. Moreover, it could also improve patient recruitment for clinical trials targeting DR.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Macular degeneration,Predictive markers,Vision disorders}
}

@article{arcaduDeepLearningAlgorithm2019b,
  title = {Deep Learning Algorithm Predicts Diabetic Retinopathy Progression in Individual Patients},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Willis, Jeff and Haskova, Zdenka and Prunotto, Marco},
  year = {2019},
  month = sep,
  journal = {npj Digital Medicine},
  volume = {2},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0172-3},
  urldate = {2023-10-05},
  abstract = {The global burden of diabetic retinopathy (DR) continues to worsen and DR remains a leading cause of vision loss worldwide. Here, we describe an algorithm to predict DR progression by means of deep learning (DL), using as input color fundus photographs (CFPs) acquired at a single visit from a patient with DR. The proposed DL models were designed to predict future DR progression, defined as 2-step worsening on the Early Treatment Diabetic Retinopathy Diabetic Retinopathy Severity Scale, and were trained against DR severity scores assessed after 6, 12, and 24 months from the baseline visit by masked, well-trained, human reading center graders. The performance of one of these models (prediction at month 12) resulted in an area under the curve equal to 0.79. Interestingly, our results highlight the importance of the predictive signal located in the peripheral retinal fields, not routinely collected for DR assessments, and the importance of microvascular abnormalities. Our findings show the feasibility of predicting future DR progression by leveraging CFPs of a patient acquired at a single visit. Upon further development on larger and more diverse datasets, such an algorithm could enable early diagnosis and referral to a retina specialist for more frequent monitoring and even consideration of early intervention. Moreover, it could also improve patient recruitment for clinical trials targeting DR.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Macular degeneration,Predictive markers,Vision disorders},
  file = {C:\Users\cleme\Zotero\storage\IANKM2MN\Arcadu et al. - 2019 - Deep learning algorithm predicts diabetic retinopa.pdf}
}

@article{arcaduDeepLearningAlgorithm2019c,
  title = {Deep Learning Algorithm Predicts Diabetic Retinopathy Progression in Individual Patients},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Willis, Jeff and Haskova, Zdenka and Prunotto, Marco},
  year = {2019},
  month = sep,
  journal = {npj Digital Medicine},
  volume = {2},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0172-3},
  urldate = {2023-05-24},
  abstract = {The global burden of diabetic retinopathy (DR) continues to worsen and DR remains a leading cause of vision loss worldwide. Here, we describe an algorithm to predict DR progression by means of deep learning (DL), using as input color fundus photographs (CFPs) acquired at a single visit from a patient with DR. The proposed DL models were designed to predict future DR progression, defined as 2-step worsening on the Early Treatment Diabetic Retinopathy Diabetic Retinopathy Severity Scale, and were trained against DR severity scores assessed after 6, 12, and 24 months from the baseline visit by masked, well-trained, human reading center graders. The performance of one of these models (prediction at month 12) resulted in an area under the curve equal to 0.79. Interestingly, our results highlight the importance of the predictive signal located in the peripheral retinal fields, not routinely collected for DR assessments, and the importance of microvascular abnormalities. Our findings show the feasibility of predicting future DR progression by leveraging CFPs of a patient acquired at a single visit. Upon further development on larger and more diverse datasets, such an algorithm could enable early diagnosis and referral to a retina specialist for more frequent monitoring and even consideration of early intervention. Moreover, it could also improve patient recruitment for clinical trials targeting DR.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Macular degeneration,Predictive markers,Vision disorders},
  file = {C:\Users\cleme\Zotero\storage\YB5898PX\Arcadu et al. - 2019 - Deep learning algorithm predicts diabetic retinopa.pdf}
}

@article{arcaduDeepLearningPredicts2019,
  title = {Deep {{Learning Predicts OCT Measures}} of {{Diabetic Macular Thickening From Color Fundus Photographs}}},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Michon, John and Haskova, Zdenka and McClintock, Dana and Adamis, Anthony P. and Willis, Jeffrey R. and Prunotto, Marco},
  year = {2019},
  month = mar,
  journal = {Investigative Opthalmology \& Visual Science},
  volume = {60},
  number = {4},
  pages = {852},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-25634},
  urldate = {2019-07-24},
  langid = {english}
}

@article{arcaduDeepLearningPredicts2019a,
  title = {Deep {{Learning Predicts OCT Measures}} of {{Diabetic Macular Thickening From Color Fundus Photographs}}},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Michon, John and Haskova, Zdenka and McClintock, Dana and Adamis, Anthony P. and Willis, Jeffrey R. and Prunotto, Marco},
  year = {2019},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {60},
  number = {4},
  pages = {852--857},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-25634},
  urldate = {2019-12-09},
  langid = {english}
}

@article{arcaduDeepLearningPredicts2019b,
  title = {Deep {{Learning Predicts OCT Measures}} of {{Diabetic Macular Thickening From Color Fundus Photographs}}},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Michon, John and Haskova, Zdenka and McClintock, Dana and Adamis, Anthony P. and Willis, Jeffrey R. and Prunotto, Marco},
  year = {2019},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {60},
  number = {4},
  pages = {852--857},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-25634},
  urldate = {2022-07-08},
  abstract = {To develop deep learning (DL) models for the automatic detection of optical coherence tomography (OCT) measures of diabetic macular thickening (MT) from color fundus photographs (CFPs). Retrospective analysis on 17,997 CFPs and their associated OCT measurements from the phase 3 RIDE/RISE diabetic macular edema (DME) studies. DL with transfer-learning cascade was applied on CFPs to predict time-domain OCT (TD-OCT)--equivalent measures of MT, including central subfield thickness (CST) and central foveal thickness (CFT). MT was defined by using two OCT cutoff points: 250 {$\mu$}m and 400 {$\mu$}m. A DL regression model was developed to directly quantify the actual CFT and CST from CFPs. The best DL model was able to predict CST {$\geq$} 250 {$\mu$}m and CFT {$\geq$} 250 {$\mu$}m with an area under the curve (AUC) of 0.97 (95\% confidence interval [CI], 0.89--1.00) and 0.91 (95\% CI, 0.76--0.99), respectively. To predict CST {$\geq$} 400 {$\mu$}m and CFT {$\geq$} 400 {$\mu$}m, the best DL model had an AUC of 0.94 (95\% CI, 0.82--1.00) and 0.96 (95\% CI, 0.88--1.00), respectively. The best deep convolutional neural network regression model to quantify CST and CFT had an R2 of 0.74 (95\% CI, 0.49--0.91) and 0.54 (95\% CI, 0.20--0.87), respectively. The performance of the DL models declined when the CFPs were of poor quality or contained laser scars. DL is capable of predicting key quantitative TD-OCT measurements related to MT from CFPs. The DL models presented here could enhance the efficiency of DME diagnosis in tele-ophthalmology programs, promoting better visual outcomes. Future research is needed to validate DL algorithms for MT in the real-world.}
}

@article{arcaduDeepLearningPredicts2019c,
  title = {Deep {{Learning Predicts OCT Measures}} of {{Diabetic Macular Thickening From Color Fundus Photographs}}},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Michon, John and Haskova, Zdenka and McClintock, Dana and Adamis, Anthony P. and Willis, Jeffrey R. and Prunotto, Marco},
  year = {2019},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {60},
  number = {4},
  pages = {852--857},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-25634},
  urldate = {2022-07-08},
  abstract = {To develop deep learning (DL) models for the automatic detection of optical coherence tomography (OCT) measures of diabetic macular thickening (MT) from color fundus photographs (CFPs). Retrospective analysis on 17,997 CFPs and their associated OCT measurements from the phase 3 RIDE/RISE diabetic macular edema (DME) studies. DL with transfer-learning cascade was applied on CFPs to predict time-domain OCT (TD-OCT)--equivalent measures of MT, including central subfield thickness (CST) and central foveal thickness (CFT). MT was defined by using two OCT cutoff points: 250 {$\mu$}m and 400 {$\mu$}m. A DL regression model was developed to directly quantify the actual CFT and CST from CFPs. The best DL model was able to predict CST {$\geq$} 250 {$\mu$}m and CFT {$\geq$} 250 {$\mu$}m with an area under the curve (AUC) of 0.97 (95\% confidence interval [CI], 0.89--1.00) and 0.91 (95\% CI, 0.76--0.99), respectively. To predict CST {$\geq$} 400 {$\mu$}m and CFT {$\geq$} 400 {$\mu$}m, the best DL model had an AUC of 0.94 (95\% CI, 0.82--1.00) and 0.96 (95\% CI, 0.88--1.00), respectively. The best deep convolutional neural network regression model to quantify CST and CFT had an R2 of 0.74 (95\% CI, 0.49--0.91) and 0.54 (95\% CI, 0.20--0.87), respectively. The performance of the DL models declined when the CFPs were of poor quality or contained laser scars. DL is capable of predicting key quantitative TD-OCT measurements related to MT from CFPs. The DL models presented here could enhance the efficiency of DME diagnosis in tele-ophthalmology programs, promoting better visual outcomes. Future research is needed to validate DL algorithms for MT in the real-world.}
}

@article{arcaduDeepLearningPredicts2019d,
  title = {Deep {{Learning Predicts OCT Measures}} of {{Diabetic Macular Thickening From Color Fundus Photographs}}},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Michon, John and Haskova, Zdenka and McClintock, Dana and Adamis, Anthony P. and Willis, Jeffrey R. and Prunotto, Marco},
  year = {2019},
  month = mar,
  journal = {Investigative Opthalmology \& Visual Science},
  volume = {60},
  number = {4},
  pages = {852},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-25634},
  urldate = {2019-07-24},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\FXGPR8SY\Arcadu et al. - 2019 - Deep Learning Predicts OCT Measures of Diabetic Ma.pdf}
}

@article{arcaduDeepLearningPredicts2019e,
  title = {Deep {{Learning Predicts OCT Measures}} of {{Diabetic Macular Thickening From Color Fundus Photographs}}},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Michon, John and Haskova, Zdenka and McClintock, Dana and Adamis, Anthony P. and Willis, Jeffrey R. and Prunotto, Marco},
  year = {2019},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {60},
  number = {4},
  pages = {852--857},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-25634},
  urldate = {2019-12-09},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HLFMCFPZ\\Arcadu et al. - 2019 - Deep Learning Predicts OCT Measures of Diabetic Ma.pdf;C\:\\Users\\cleme\\Zotero\\storage\\KENAQTG8\\arcadu2019.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ULGX89GD\\article.html}
}

@article{arcaduDeepLearningPredicts2019f,
  title = {Deep {{Learning Predicts OCT Measures}} of {{Diabetic Macular Thickening From Color Fundus Photographs}}},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Michon, John and Haskova, Zdenka and McClintock, Dana and Adamis, Anthony P. and Willis, Jeffrey R. and Prunotto, Marco},
  year = {2019},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {60},
  number = {4},
  pages = {852--857},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-25634},
  urldate = {2022-07-08},
  abstract = {To develop deep learning (DL) models for the automatic detection of optical coherence tomography (OCT) measures of diabetic macular thickening (MT) from color fundus photographs (CFPs).    Retrospective analysis on 17,997 CFPs and their associated OCT measurements from the phase 3 RIDE/RISE diabetic macular edema (DME) studies. DL with transfer-learning cascade was applied on CFPs to predict time-domain OCT (TD-OCT)--equivalent measures of MT, including central subfield thickness (CST) and central foveal thickness (CFT). MT was defined by using two OCT cutoff points: 250 {$\mu$}m and 400 {$\mu$}m. A DL regression model was developed to directly quantify the actual CFT and CST from CFPs.    The best DL model was able to predict CST {$\geq$} 250 {$\mu$}m and CFT {$\geq$} 250 {$\mu$}m with an area under the curve (AUC) of 0.97 (95\% confidence interval [CI], 0.89--1.00) and 0.91 (95\% CI, 0.76--0.99), respectively. To predict CST {$\geq$} 400 {$\mu$}m and CFT {$\geq$} 400 {$\mu$}m, the best DL model had an AUC of 0.94 (95\% CI, 0.82--1.00) and 0.96 (95\% CI, 0.88--1.00), respectively. The best deep convolutional neural network regression model to quantify CST and CFT had an R2 of 0.74 (95\% CI, 0.49--0.91) and 0.54 (95\% CI, 0.20--0.87), respectively. The performance of the DL models declined when the CFPs were of poor quality or contained laser scars.    DL is capable of predicting key quantitative TD-OCT measurements related to MT from CFPs. The DL models presented here could enhance the efficiency of DME diagnosis in tele-ophthalmology programs, promoting better visual outcomes. Future research is needed to validate DL algorithms for MT in the real-world.},
  file = {C:\Users\cleme\Zotero\storage\EXGBWDV8\article.html}
}

@article{arcaduDeepLearningPredicts2019g,
  title = {Deep {{Learning Predicts OCT Measures}} of {{Diabetic Macular Thickening From Color Fundus Photographs}}},
  author = {Arcadu, Filippo and Benmansour, Fethallah and Maunz, Andreas and Michon, John and Haskova, Zdenka and McClintock, Dana and Adamis, Anthony P. and Willis, Jeffrey R. and Prunotto, Marco},
  year = {2019},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {60},
  number = {4},
  pages = {852--857},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-25634},
  urldate = {2022-07-08},
  abstract = {To develop deep learning (DL) models for the automatic detection of optical coherence tomography (OCT) measures of diabetic macular thickening (MT) from color fundus photographs (CFPs).    Retrospective analysis on 17,997 CFPs and their associated OCT measurements from the phase 3 RIDE/RISE diabetic macular edema (DME) studies. DL with transfer-learning cascade was applied on CFPs to predict time-domain OCT (TD-OCT)--equivalent measures of MT, including central subfield thickness (CST) and central foveal thickness (CFT). MT was defined by using two OCT cutoff points: 250 {$\mu$}m and 400 {$\mu$}m. A DL regression model was developed to directly quantify the actual CFT and CST from CFPs.    The best DL model was able to predict CST {$\geq$} 250 {$\mu$}m and CFT {$\geq$} 250 {$\mu$}m with an area under the curve (AUC) of 0.97 (95\% confidence interval [CI], 0.89--1.00) and 0.91 (95\% CI, 0.76--0.99), respectively. To predict CST {$\geq$} 400 {$\mu$}m and CFT {$\geq$} 400 {$\mu$}m, the best DL model had an AUC of 0.94 (95\% CI, 0.82--1.00) and 0.96 (95\% CI, 0.88--1.00), respectively. The best deep convolutional neural network regression model to quantify CST and CFT had an R2 of 0.74 (95\% CI, 0.49--0.91) and 0.54 (95\% CI, 0.20--0.87), respectively. The performance of the DL models declined when the CFPs were of poor quality or contained laser scars.    DL is capable of predicting key quantitative TD-OCT measurements related to MT from CFPs. The DL models presented here could enhance the efficiency of DME diagnosis in tele-ophthalmology programs, promoting better visual outcomes. Future research is needed to validate DL algorithms for MT in the real-world.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\UPH8TL37\\10.1167@iovs.18-25634.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RI7KFKHI\\article.html}
}

@article{article,
  title = {Retinal Disease Detection Using Deep Learning Techniques: {{A}} Comprehensive Review},
  author = {Muchuchuti, Stewart and Viriri, Serestina},
  year = {2023},
  month = apr,
  journal = {Journal of Imaging},
  volume = {9},
  pages = {84},
  doi = {10.3390/jimaging9040084}
}

@article{article,
  title = {Retinal Disease Detection Using Deep Learning Techniques: {{A}} Comprehensive Review},
  author = {Muchuchuti, Stewart and Viriri, Serestina},
  year = {2023},
  month = apr,
  journal = {Journal of Imaging},
  volume = {9},
  pages = {84},
  doi = {10.3390/jimaging9040084}
}

@article{asaokaUsingDeepLearning2019b,
  title = {Using {{Deep Learning}} and {{Transfer Learning}} to {{Accurately Diagnose Early-Onset Glaucoma From Macular Optical Coherence Tomography Images}}},
  author = {Asaoka, Ryo and Murata, Hiroshi and Hirasawa, Kazunori and Fujino, Yuri and Matsuura, Masato and Miki, Atsuya and Kanamoto, Takashi and Ikeda, Yoko and Mori, Kazuhiko and Iwase, Aiko and Shoji, Nobuyuki and Inoue, Kenji and Yamagami, Junkichi and Araie, Makoto},
  year = {2019},
  month = feb,
  journal = {American Journal of Ophthalmology},
  volume = {198},
  pages = {136--145},
  issn = {1879-1891},
  doi = {10.1016/j.ajo.2018.10.007},
  abstract = {PURPOSE: We sought to construct and evaluate a deep learning (DL) model to diagnose early glaucoma from spectral-domain optical coherence tomography (OCT) images. DESIGN: Artificial intelligence diagnostic tool development, evaluation, and comparison. METHODS: This multi-institution study included pretraining data of 4316 OCT images (RS3000) from 1371 eyes with open angle glaucoma (OAG) regardless of the stage of glaucoma and 193 normal eyes. Training data included OCT-1000/2000 images from 94 eyes of 94 patients with early OAG (mean deviation {$>~$}-5.0 dB) and 84 eyes of 84 normal subjects. Testing data included OCT-1000/2000 from 114 eyes of 114 patients with early OAG (mean deviation {$>~$}-5.0 dB) and 82 eyes of 82 normal subjects. A DL (convolutional neural network) classifier was trained using a pretraining dataset, followed by a second round of training using an independent training dataset. The DL model input features were the 8~{\texttimes} 8 grid macular retinal nerve fiber layer thickness and ganglion cell complex layer thickness from spectral-domain OCT. Diagnostic accuracy was investigated in the testing dataset. For comparison, diagnostic accuracy was also evaluated using the random forests and support vector machine models. The primary outcome measure was the area under the receiver operating characteristic curve (AROC). RESULTS: The AROC with the DL model was 93.7\%. The AROC significantly decreased to between 76.6\% and 78.8\% without the pretraining process. Significantly smaller AROCs were obtained with random forests and support vector machine models (82.0\% and 67.4\%, respectively). CONCLUSION: A DL model for glaucoma using spectral-domain OCT offers a substantive increase in diagnostic performance.},
  langid = {english},
  pmid = {30316669},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\B68BFV6L\\Asaoka et al. - 2019 - Using Deep Learning and Transfer Learning to Accur.pdf;C\:\\Users\\cleme\\Zotero\\storage\\HK2GPGB3\\S0002939418305890.html}
}

@misc{ASCRSPredictionImminent2023a,
  title = {{{ASCRS}}: {{Prediction}} of Imminent Conversion to Neovascular {{AMD}} Using Deep Learning and {{3D OCT}} Volumes},
  shorttitle = {{{ASCRS}}},
  year = {2023},
  month = may,
  journal = {Optometry Times},
  urldate = {2023-06-20},
  abstract = {Alvin Liu, MD, sat down with Sheryl Stevenson, Group Editorial Director,~Ophthalmology Times{\textregistered}, to discuss his presentation on deep learning and 3D OCT at the ASCRS annual meeting in San Diego.},
  howpublished = {https://www.optometrytimes.com/view/ascrs-prediction-of-imminent-conversion-to-neovascular-amd-using-deep-learning-and-3d-oct-volumes},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\DVECCLE8\ascrs-prediction-of-imminent-conversion-to-neovascular-amd-using-deep-learning-and-3d-oct-volum.html}
}

@article{attikuComparisonDiabeticRetinopathy2023,
  title = {Comparison of Diabetic Retinopathy Severity Grading on {{ETDRS}} 7-Field versus Ultrawide-Field Assessment},
  author = {Attiku, Yamini and Nittala, Muneeswar Gupta and Velaga, Swetha B. and Ramachandra, Chaithanya and Bhat, Sandeep and Solanki, Kaushal and Jayadev, Chaitra and Choudhry, Netan and Orr, Samantha Miyoko Ashlyn and Jiang, Shangjun and He, Ye and Sadda, SriniVas R.},
  year = {2023},
  month = oct,
  journal = {Eye},
  volume = {37},
  number = {14},
  pages = {2946--2949},
  publisher = {Nature Publishing Group},
  issn = {1476-5454},
  doi = {10.1038/s41433-023-02445-8},
  urldate = {2024-02-22},
  abstract = {To compare the diabetic retinopathy (DR) severity level determined when considering only the ETDRS 7-field region versus the entire ultrawidefield (UWF) image.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Retinal diseases,Tomography},
  file = {C:\Users\cleme\Zotero\storage\VFTZIGDA\Attiku et al. - 2023 - Comparison of diabetic retinopathy severity gradin.pdf}
}

@misc{AutomatedAnalysisDiabetica,
  title = {Automated {{Analysis}} of {{Diabetic Retinopathy Images}}: {{Principles}}, {{Recent Developments}}, and {{Emerging Trends}} {\textbar} {{SpringerLink}}},
  urldate = {2021-10-04},
  howpublished = {https://link.springer.com/article/10.1007\%2Fs11892-013-0393-9}
}

@article{avidorCosteffectivenessDiabeticRetinopathy2020,
  title = {Cost-Effectiveness of Diabetic Retinopathy Screening Programs Using Telemedicine: A Systematic Review},
  shorttitle = {Cost-Effectiveness of Diabetic Retinopathy Screening Programs Using Telemedicine},
  author = {Avidor, Daniel and Loewenstein, Anat and Waisbourd, Michael and Nutman, Amir},
  year = {2020},
  month = apr,
  journal = {Cost Effectiveness and Resource Allocation},
  volume = {18},
  number = {1},
  pages = {16},
  issn = {1478-7547},
  doi = {10.1186/s12962-020-00211-1},
  urldate = {2021-11-12},
  abstract = {Diabetic retinopathy (DR) is a significant global public health and economic burden. DR accounts for approximately 15--17\% of all cases of total blindness in the USA and Europe. Telemedicine is a new intervention for DR screening, however, there is not enough evidence to support its cost-effectiveness. The aim of this study is to review the most recent published literature on economic evaluations of telemedicine in DR screening and summarize the evidence on the cost-effectiveness of this technology.},
  langid = {english}
}

@article{avidorCosteffectivenessDiabeticRetinopathy2020a,
  title = {Cost-Effectiveness of Diabetic Retinopathy Screening Programs Using Telemedicine: A Systematic Review},
  shorttitle = {Cost-Effectiveness of Diabetic Retinopathy Screening Programs Using Telemedicine},
  author = {Avidor, Daniel and Loewenstein, Anat and Waisbourd, Michael and Nutman, Amir},
  year = {2020},
  month = apr,
  journal = {Cost Effectiveness and Resource Allocation},
  volume = {18},
  number = {1},
  pages = {16},
  issn = {1478-7547},
  doi = {10.1186/s12962-020-00211-1},
  urldate = {2021-11-12},
  abstract = {Diabetic retinopathy (DR) is a significant global public health and economic burden. DR accounts for approximately 15--17\% of all cases of total blindness in the USA and Europe. Telemedicine is a new intervention for DR screening, however, there is not enough evidence to support its cost-effectiveness. The aim of this study is to review the most recent published literature on economic evaluations of telemedicine in DR screening and summarize the evidence on the cost-effectiveness of this technology.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\ZNWADMW7\Avidor et al. - 2020 - Cost-effectiveness of diabetic retinopathy screeni.pdf}
}

@inproceedings{awaisClassificationSDOCTImages2017,
  title = {Classification of {{SD-OCT}} Images Using a {{Deep}} Learning Approach},
  booktitle = {2017 {{IEEE International Conference}} on {{Signal}} and {{Image Processing Applications}} ({{ICSIPA}})},
  author = {Awais, Muhammad and M{\"u}ller, Henning and Tang, Tong B. and Meriaudeau, Fabrice},
  year = {2017},
  month = sep,
  pages = {489--492},
  doi = {10.1109/ICSIPA.2017.8120661},
  abstract = {Diabetic Macular Edema (DME) is one of the many eye diseases that is commonly found in diabetic patients. If it is left untreated it may cause vision loss. This paper focuses on classification of abnormal and normal OCT (Optical Coherence Tomography) image volumes using a pre-trained CNN (Convolutional Neural Network). Using VGG16 (Visual Geometry Group), features are extracted at different layers of the network, e.g. before fully connected layer and after each fully connected layer. On the basis of these features classification was performed using different classifiers and results are higher than recently published work on the same dataset with an accuracy of 87.5\%, with sensitivity and specificity being 93.5\% and 81\% respectively.},
  keywords = {Biomedical imaging,Convolution,Deep learning,Diabetic Macular Edema (DME),Feature extraction,Feature Matrices,Machine learning,Optical coherence tomography,Retina,Visual Graphic Geometry (VGG)}
}

@inproceedings{awaisClassificationSDOCTImages2017a,
  title = {Classification of {{SD-OCT}} Images Using a {{Deep}} Learning Approach},
  booktitle = {2017 {{IEEE International Conference}} on {{Signal}} and {{Image Processing Applications}} ({{ICSIPA}})},
  author = {Awais, Muhammad and M{\"u}ller, Henning and Tang, Tong B. and Meriaudeau, Fabrice},
  year = {2017},
  month = sep,
  pages = {489--492},
  doi = {10.1109/ICSIPA.2017.8120661},
  abstract = {Diabetic Macular Edema (DME) is one of the many eye diseases that is commonly found in diabetic patients. If it is left untreated it may cause vision loss. This paper focuses on classification of abnormal and normal OCT (Optical Coherence Tomography) image volumes using a pre-trained CNN (Convolutional Neural Network). Using VGG16 (Visual Geometry Group), features are extracted at different layers of the network, e.g. before fully connected layer and after each fully connected layer. On the basis of these features classification was performed using different classifiers and results are higher than recently published work on the same dataset with an accuracy of 87.5\%, with sensitivity and specificity being 93.5\% and 81\% respectively.},
  keywords = {Biomedical imaging,Convolution,Deep learning,Diabetic Macular Edema (DME),Feature extraction,Feature Matrices,Machine learning,Optical coherence tomography,Retina,Visual Graphic Geometry (VGG)}
}

@inproceedings{awaisClassificationSDOCTImages2017b,
  title = {Classification of {{SD-OCT}} Images Using a {{Deep}} Learning Approach},
  booktitle = {2017 {{IEEE International Conference}} on {{Signal}} and {{Image Processing Applications}} ({{ICSIPA}})},
  author = {Awais, Muhammad and M{\"u}ller, Henning and Tang, Tong B. and Meriaudeau, Fabrice},
  year = {2017},
  month = sep,
  pages = {489--492},
  doi = {10.1109/ICSIPA.2017.8120661},
  abstract = {Diabetic Macular Edema (DME) is one of the many eye diseases that is commonly found in diabetic patients. If it is left untreated it may cause vision loss. This paper focuses on classification of abnormal and normal OCT (Optical Coherence Tomography) image volumes using a pre-trained CNN (Convolutional Neural Network). Using VGG16 (Visual Geometry Group), features are extracted at different layers of the network, e.g. before fully connected layer and after each fully connected layer. On the basis of these features classification was performed using different classifiers and results are higher than recently published work on the same dataset with an accuracy of 87.5\%, with sensitivity and specificity being 93.5\% and 81\% respectively.},
  keywords = {Biomedical imaging,Convolution,Deep learning,Diabetic Macular Edema (DME),Feature extraction,Feature Matrices,Machine learning,Optical coherence tomography,Retina,Visual Graphic Geometry (VGG)},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\WDNCTFUX\\Awais et al. - 2017 - Classification of SD-OCT images using a Deep learn.pdf;C\:\\Users\\cleme\\Zotero\\storage\\SWW989AW\\8120661.html}
}

@inproceedings{awaisClassificationSDOCTImages2017c,
  title = {Classification of {{SD-OCT}} Images Using a {{Deep}} Learning Approach},
  booktitle = {2017 {{IEEE International Conference}} on {{Signal}} and {{Image Processing Applications}} ({{ICSIPA}})},
  author = {Awais, Muhammad and M{\"u}ller, Henning and Tang, Tong B. and Meriaudeau, Fabrice},
  year = {2017},
  month = sep,
  pages = {489--492},
  doi = {10.1109/ICSIPA.2017.8120661},
  abstract = {Diabetic Macular Edema (DME) is one of the many eye diseases that is commonly found in diabetic patients. If it is left untreated it may cause vision loss. This paper focuses on classification of abnormal and normal OCT (Optical Coherence Tomography) image volumes using a pre-trained CNN (Convolutional Neural Network). Using VGG16 (Visual Geometry Group), features are extracted at different layers of the network, e.g. before fully connected layer and after each fully connected layer. On the basis of these features classification was performed using different classifiers and results are higher than recently published work on the same dataset with an accuracy of 87.5\%, with sensitivity and specificity being 93.5\% and 81\% respectively.},
  keywords = {Biomedical imaging,Convolution,Deep learning,Diabetic Macular Edema (DME),Feature extraction,Feature Matrices,Machine learning,Optical coherence tomography,Retina,Visual Graphic Geometry (VGG)},
  file = {C:\Users\cleme\Zotero\storage\8DQXD2PK\8120661.html}
}

@misc{azadAdvancesMedicalImage2023a,
  title = {Advances in {{Medical Image Analysis}} with {{Vision Transformers}}: {{A Comprehensive Review}}},
  shorttitle = {Advances in {{Medical Image Analysis}} with {{Vision Transformers}}},
  author = {Azad, Reza and Kazerouni, Amirhossein and Heidari, Moein and Aghdam, Ehsan Khodapanah and Molaei, Amirali and Jia, Yiwei and Jose, Abin and Roy, Rijo and Merhof, Dorit},
  year = {2023},
  month = jan,
  number = {arXiv:2301.03505},
  eprint = {2301.03505},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-12},
  abstract = {The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependencies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifically, we present a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these applications, we investigate the novelty, strengths and weaknesses of the different proposed strategies and develop taxonomies highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on different datasets. Finally, we summarize key challenges and discuss different future research directions. In addition, we have provided cited papers with their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\WLKYJP43\Azad et al. - 2023 - Advances in Medical Image Analysis with Vision Tra.pdf}
}

@article{bachPixelWiseExplanationsNonLinear2015,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {10 juil. 2015},
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2021-03-03},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  langid = {english},
  keywords = {Algorithms,Coding mechanisms,Imaging techniques,Kernel functions,Neural networks,Neurons,Sensory perception,Vision}
}

@article{bachPixelWiseExplanationsNonLinear2015a,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2015},
  month = jul,
  journal = {PLoS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2021-11-16},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  pmcid = {PMC4498753},
  pmid = {26161953}
}

@article{bachPixelWiseExplanationsNonLinear2015b,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  editor = {Suarez, Oscar Deniz},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2023-06-01},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  langid = {english}
}

@article{bachPixelWiseExplanationsNonLinear2015c,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2015},
  month = jul,
  journal = {PLoS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2021-11-16},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  pmcid = {PMC4498753},
  pmid = {26161953},
  file = {C:\Users\cleme\Zotero\storage\V35WCQLP\Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf}
}

@article{bachPixelWiseExplanationsNonLinear2015d,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {10 juil. 2015},
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2021-03-03},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  langid = {english},
  keywords = {Algorithms,Coding mechanisms,Imaging techniques,Kernel functions,Neural networks,Neurons,Sensory perception,Vision},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\ZPTTDJWY\\Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf;C\:\\Users\\cleme\\Zotero\\storage\\43LULSXN\\article.html;C\:\\Users\\cleme\\Zotero\\storage\\A7SVYUIU\\article.html}
}

@article{bachPixelWiseExplanationsNonLinear2015e,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  editor = {Suarez, Oscar Deniz},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2023-06-01},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\LH5Q4MVN\Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf}
}

@article{bacmeisterProjectedChangesTropical2018,
  title = {Projected Changes in Tropical Cyclone Activity under Future Warming Scenarios Using a High-Resolution Climate Model},
  author = {Bacmeister, Julio T. and Reed, Kevin A. and Hannay, Cecile and Lawrence, Peter and Bates, Susan and Truesdale, John E. and Rosenbloom, Nan and Levy, Michael},
  year = {2018},
  month = feb,
  journal = {Climatic Change},
  volume = {146},
  number = {3},
  pages = {547--560},
  issn = {1573-1480},
  doi = {10.1007/s10584-016-1750-x},
  urldate = {2019-06-04},
  abstract = {This study examines how characteristics of tropical cyclones (TCs) that are explicitly resolved in a global atmospheric model with horizontal resolution of approximately 28 km are projected to change in a warmer climate using bias-corrected sea-surface temperatures (SSTs). The impact of mitigating from RCP8.5 to RCP4.5 is explicitly considered and is compared with uncertainties arising from SST projections. We find a reduction in overall global TC activity as climate warms. This reduction is somewhat less pronounced under RCP4.5 than under RCP8.5. By contrast, the frequency of very intense TCs is projected to increase dramatically in a warmer climate, with most of the increase concentrated in the NW Pacific basin. Extremes of storm related precipitation are also projected to become more common. Reduction in the frequency of extreme precipitation events is possible through mitigation from RCP8.5 to RCP4.5. In general more detailed basin-scale projections of future TC activity are subject to large uncertainties due to uncertainties in future SSTs. In most cases these uncertainties are larger than the effects of mitigating from RCP8.5 to RCP4.5.},
  langid = {english},
  keywords = {Climate change,High-resolution,Tropical cyclones}
}

@article{bacmeisterProjectedChangesTropical2018a,
  title = {Projected Changes in Tropical Cyclone Activity under Future Warming Scenarios Using a High-Resolution Climate Model},
  author = {Bacmeister, Julio T. and Reed, Kevin A. and Hannay, Cecile and Lawrence, Peter and Bates, Susan and Truesdale, John E. and Rosenbloom, Nan and Levy, Michael},
  year = {2018},
  month = feb,
  journal = {Climatic Change},
  volume = {146},
  number = {3},
  pages = {547--560},
  issn = {1573-1480},
  doi = {10.1007/s10584-016-1750-x},
  urldate = {2019-06-04},
  abstract = {This study examines how characteristics of tropical cyclones (TCs) that are explicitly resolved in a global atmospheric model with horizontal resolution of approximately 28 km are projected to change in a warmer climate using bias-corrected sea-surface temperatures (SSTs). The impact of mitigating from RCP8.5 to RCP4.5 is explicitly considered and is compared with uncertainties arising from SST projections. We find a reduction in overall global TC activity as climate warms. This reduction is somewhat less pronounced under RCP4.5 than under RCP8.5. By contrast, the frequency of very intense TCs is projected to increase dramatically in a warmer climate, with most of the increase concentrated in the NW Pacific basin. Extremes of storm related precipitation are also projected to become more common. Reduction in the frequency of extreme precipitation events is possible through mitigation from RCP8.5 to RCP4.5. In general more detailed basin-scale projections of future TC activity are subject to large uncertainties due to uncertainties in future SSTs. In most cases these uncertainties are larger than the effects of mitigating from RCP8.5 to RCP4.5.},
  langid = {english},
  keywords = {Climate change,High-resolution,Tropical cyclones},
  file = {C:\Users\cleme\Zotero\storage\99SLGLYN\Bacmeister et al. - 2018 - Projected changes in tropical cyclone activity und.pdf}
}

@misc{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  number = {arXiv:1607.06450},
  eprint = {1607.06450},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.06450},
  urldate = {2023-02-22},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{baLayerNormalization2016a,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  number = {arXiv:1607.06450},
  eprint = {1607.06450},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.06450},
  urldate = {2023-03-24},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{baLayerNormalization2016b,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  number = {arXiv:1607.06450},
  eprint = {1607.06450},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.06450},
  urldate = {2023-02-22},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\M5MEFRS8\\Ba et al. - 2016 - Layer Normalization.pdf;C\:\\Users\\cleme\\Zotero\\storage\\QTVP9LTF\\1607.html}
}

@misc{baLayerNormalization2016c,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  number = {arXiv:1607.06450},
  eprint = {1607.06450},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.06450},
  urldate = {2023-03-24},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\N8XPFMKN\\Ba et al. - 2016 - Layer Normalization.pdf;C\:\\Users\\cleme\\Zotero\\storage\\9V9XB6TB\\1607.html}
}

@article{barragan-monteroSafeEfficientClinical2022,
  title = {Towards a Safe and Efficient Clinical Implementation of Machine Learning in Radiation Oncology by Exploring Model Interpretability, Explainability and Data-Model Dependency},
  author = {{Barrag{\'a}n-Montero}, Ana and Bibal, Adrien and Dastarac, Margerie Huet and Draguet, Camille and Vald{\'e}s, Gilmer and Nguyen, Dan and Willems, Siri and Vandewinckele, Liesbeth and Holmstr{\"o}m, Mats and L{\"o}fman, Fredrik and Souris, Kevin and Sterpin, Edmond and Lee, John A.},
  year = {2022},
  month = may,
  journal = {Physics in Medicine \& Biology},
  volume = {67},
  number = {11},
  pages = {11TR01},
  publisher = {IOP Publishing},
  issn = {0031-9155},
  doi = {10.1088/1361-6560/ac678a},
  urldate = {2023-05-05},
  abstract = {The interest in machine learning (ML) has grown tremendously in recent years, partly due to the performance leap that occurred with new techniques of deep learning, convolutional neural networks for images, increased computational power, and wider availability of large datasets. Most fields of medicine follow that popular trend and, notably, radiation oncology is one of those that are at the forefront, with already a long tradition in using digital images and fully computerized workflows. ML models are driven by data, and in contrast with many statistical or physical models, they can be very large and complex, with countless generic parameters. This inevitably raises two questions, namely, the tight dependence between the models and the datasets that feed them, and the interpretability of the models, which scales with its complexity. Any problems in the data used to train the model will be later reflected in their performance. This, together with the low interpretability of ML models, makes their implementation into the clinical workflow particularly difficult. Building tools for risk assessment and quality assurance of ML models must involve then two main points: interpretability and data-model dependency. After a joint introduction of both radiation oncology and ML, this paper reviews the main risks and current solutions when applying the latter to workflows in the former. Risks associated with data and models, as well as their interaction, are detailed. Next, the core concepts of interpretability, explainability, and data-model dependency are formally defined and illustrated with examples. Afterwards, a broad discussion goes through key applications of ML in workflows of radiation oncology as well as vendors' perspectives for the clinical implementation of ML.},
  langid = {english}
}

@article{barragan-monteroSafeEfficientClinical2022a,
  title = {Towards a Safe and Efficient Clinical Implementation of Machine Learning in Radiation Oncology by Exploring Model Interpretability, Explainability and Data-Model Dependency},
  author = {{Barrag{\'a}n-Montero}, Ana and Bibal, Adrien and Dastarac, Margerie Huet and Draguet, Camille and Vald{\'e}s, Gilmer and Nguyen, Dan and Willems, Siri and Vandewinckele, Liesbeth and Holmstr{\"o}m, Mats and L{\"o}fman, Fredrik and Souris, Kevin and Sterpin, Edmond and Lee, John A.},
  year = {2022},
  month = may,
  journal = {Physics in Medicine \& Biology},
  volume = {67},
  number = {11},
  pages = {11TR01},
  publisher = {IOP Publishing},
  issn = {0031-9155},
  doi = {10.1088/1361-6560/ac678a},
  urldate = {2023-05-05},
  abstract = {The interest in machine learning (ML) has grown tremendously in recent years, partly due to the performance leap that occurred with new techniques of deep learning, convolutional neural networks for images, increased computational power, and wider availability of large datasets. Most fields of medicine follow that popular trend and, notably, radiation oncology is one of those that are at the forefront, with already a long tradition in using digital images and fully computerized workflows. ML models are driven by data, and in contrast with many statistical or physical models, they can be very large and complex, with countless generic parameters. This inevitably raises two questions, namely, the tight dependence between the models and the datasets that feed them, and the interpretability of the models, which scales with its complexity. Any problems in the data used to train the model will be later reflected in their performance. This, together with the low interpretability of ML models, makes their implementation into the clinical workflow particularly difficult. Building tools for risk assessment and quality assurance of ML models must involve then two main points: interpretability and data-model dependency. After a joint introduction of both radiation oncology and ML, this paper reviews the main risks and current solutions when applying the latter to workflows in the former. Risks associated with data and models, as well as their interaction, are detailed. Next, the core concepts of interpretability, explainability, and data-model dependency are formally defined and illustrated with examples. Afterwards, a broad discussion goes through key applications of ML in workflows of radiation oncology as well as vendors' perspectives for the clinical implementation of ML.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\6WJBSI5U\Barragán-Montero et al. - 2022 - Towards a safe and efficient clinical implementati.pdf}
}

@article{barredoarrietaExplainableArtificialIntelligence2020,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and {D{\'i}az-Rodr{\'i}guez}, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and {Gil-Lopez}, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  year = {2020},
  month = jun,
  journal = {Information Fusion},
  volume = {58},
  number = {C},
  pages = {82--115},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2019.12.012},
  urldate = {2023-05-04},
  abstract = {{$\bullet$} We review concepts related to the explainability of AI methods (XAI). {$\bullet$} We comprehensive analyze the XAI literature organized in two taxonomies. {$\bullet$} We identify future research directions of the XAI field. {$\bullet$} We discuss potential implications of XAI and privacy in data fusion contexts. {$\bullet$} We identify Responsible AI as a concept promoting XAI and other AI principles in practical settings. In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency}
}

@article{barredoarrietaExplainableArtificialIntelligence2020a,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and {D{\'i}az-Rodr{\'i}guez}, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and {Gil-Lopez}, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  year = {2020},
  month = jun,
  journal = {Information Fusion},
  volume = {58},
  number = {C},
  pages = {82--115},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2019.12.012},
  urldate = {2023-05-04},
  abstract = {{$\bullet$} We review concepts related to the explainability of AI methods (XAI). {$\bullet$} We comprehensive analyze the XAI literature organized in two taxonomies. {$\bullet$} We identify future research directions of the XAI field. {$\bullet$} We discuss potential implications of XAI and privacy in data fusion contexts. {$\bullet$} We identify Responsible AI as a concept promoting XAI and other AI principles in practical settings. In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
  file = {C:\Users\cleme\Zotero\storage\NMQTJZMQ\Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf}
}

@incollection{bartlettRademacherGaussianComplexities2001,
  title = {Rademacher and {{Gaussian Complexities}}: {{Risk Bounds}} and {{Structural Results}}},
  shorttitle = {Rademacher and {{Gaussian Complexities}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {Bartlett, Peter L. and Mendelson, Shahar},
  editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Helmbold, David and Williamson, Bob},
  year = {2001},
  volume = {2111},
  pages = {224--240},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44581-1_15},
  urldate = {2023-04-23},
  abstract = {We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines.},
  isbn = {978-3-540-42343-0 978-3-540-44581-4},
  langid = {english}
}

@incollection{bartlettRademacherGaussianComplexities2001a,
  title = {Rademacher and {{Gaussian Complexities}}: {{Risk Bounds}} and {{Structural Results}}},
  shorttitle = {Rademacher and {{Gaussian Complexities}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {Bartlett, Peter L. and Mendelson, Shahar},
  editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Helmbold, David and Williamson, Bob},
  year = {2001},
  volume = {2111},
  pages = {224--240},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44581-1_15},
  urldate = {2023-04-23},
  abstract = {We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines.},
  isbn = {978-3-540-42343-0 978-3-540-44581-4},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\GMQYF2MT\Bartlett et Mendelson - 2001 - Rademacher and Gaussian Complexities Risk Bounds .pdf}
}

@article{bartlettSampleComplexityPattern1998,
  title = {The Sample Complexity of Pattern Classification with Neural Networks: {{The}} Size of the Weights Is More Important than the Size of the Network},
  shorttitle = {The Sample Complexity of Pattern Classification with Neural Networks},
  author = {Bartlett, P.L.},
  year = {1998},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {44},
  number = {2},
  pages = {525--536},
  issn = {1557-9654},
  doi = {10.1109/18.661502},
  abstract = {Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples.},
  keywords = {Computer networks,Neural networks,Pattern analysis,Pattern classification,Pattern recognition,Probability distribution,Statistical learning,Training data,Upper bound,Virtual colonoscopy}
}

@article{bartlettSampleComplexityPattern1998a,
  title = {The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights Is More Important than the Size of the Network},
  shorttitle = {The Sample Complexity of Pattern Classification with Neural Networks},
  author = {Bartlett, P.L.},
  year = {1998},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {44},
  number = {2},
  pages = {525--536},
  issn = {1557-9654},
  doi = {10.1109/18.661502},
  abstract = {Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples.},
  keywords = {Computer networks,Neural networks,Pattern analysis,Pattern classification,Pattern recognition,Probability distribution,Statistical learning,Training data,Upper bound,Virtual colonoscopy},
  file = {C:\Users\cleme\Zotero\storage\WKTXU4KV\661502.html}
}

@article{bartlettVapnikChervonenkisDimensionNeural,
  title = {Vapnik-{{Chervonenkis Dimension}} of {{Neural Nets}}},
  author = {Bartlett, Peter L and Technologies, Biowulf},
  langid = {english}
}

@article{bartlettVapnikChervonenkisDimensionNeurala,
  title = {Vapnik-{{Chervonenkis Dimension}} of {{Neural Nets}}},
  author = {Bartlett, Peter L and Technologies, {\relax Bio}wulf},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\LI8VQR6W\Bartlett et Technologies - Vapnik-Chervonenkis Dimension of Neural Nets.pdf}
}

@article{bellemoArtificialIntelligenceScreening2019,
  title = {Artificial {{Intelligence Screening}} for {{Diabetic Retinopathy}}: {{The Real-World Emerging Application}}},
  shorttitle = {Artificial {{Intelligence Screening}} for {{Diabetic Retinopathy}}},
  author = {Bellemo, Valentina and Lim, Gilbert and Rim, Tyler Hyungtaek and Tan, Gavin S. W. and Cheung, Carol Y. and Sadda, SriniVas and He, Ming-guang and Tufail, Adnan and Lee, Mong Li and Hsu, Wynne and Ting, Daniel Shu Wei},
  year = {2019},
  month = jul,
  journal = {Current Diabetes Reports},
  volume = {19},
  number = {9},
  pages = {72},
  issn = {1539-0829},
  doi = {10.1007/s11892-019-1189-3},
  urldate = {2022-07-10},
  abstract = {This paper systematically reviews the recent progress in diabetic retinopathy screening. It provides an integrated overview of the current state of knowledge of emerging techniques using artificial intelligence integration in national screening programs around the world. Existing methodological approaches and research insights are evaluated. An understanding of existing gaps and future directions is created.},
  langid = {english},
  keywords = {Artificial intelligence,Deep learning,Diabetic retinopathy screening,Retinal images,Survey,Tele-medicine}
}

@article{bellemoArtificialIntelligenceScreening2019a,
  title = {Artificial {{Intelligence Screening}} for {{Diabetic Retinopathy}}: The {{Real-World Emerging Application}}},
  shorttitle = {Artificial {{Intelligence Screening}} for {{Diabetic Retinopathy}}},
  author = {Bellemo, Valentina and Lim, Gilbert and Rim, Tyler Hyungtaek and Tan, Gavin S. W. and Cheung, Carol Y. and Sadda, SriniVas and He, Ming-guang and Tufail, Adnan and Lee, Mong Li and Hsu, Wynne and Ting, Daniel Shu Wei},
  year = {2019},
  month = jul,
  journal = {Current Diabetes Reports},
  volume = {19},
  number = {9},
  pages = {72},
  issn = {1539-0829},
  doi = {10.1007/s11892-019-1189-3},
  urldate = {2022-07-10},
  abstract = {This paper systematically reviews the recent progress in diabetic retinopathy screening. It provides an integrated overview of the current state of knowledge of emerging techniques using artificial intelligence integration in national screening programs around the world. Existing methodological approaches and research insights are evaluated. An understanding of existing gaps and future directions is created.},
  langid = {english},
  keywords = {Artificial intelligence,Deep learning,Diabetic retinopathy screening,Retinal images,Survey,Tele-medicine},
  file = {C:\Users\cleme\Zotero\storage\E4IKJML2\10.1007@s11892-019-1189-3.pdf.pdf}
}

@article{bellePrinciplesPracticeExplainable2021,
  title = {Principles and {{Practice}} of {{Explainable Machine Learning}}},
  author = {Belle, Vaishak and Papantonis, Ioannis},
  year = {2021},
  journal = {Frontiers in Big Data},
  volume = {4},
  issn = {2624-909X},
  urldate = {2023-04-23},
  abstract = {Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods---machine learning (ML) and pattern recognition models in particular---so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.}
}

@article{bellePrinciplesPracticeExplainable2021a,
  title = {Principles and {{Practice}} of {{Explainable Machine Learning}}},
  author = {Belle, Vaishak and Papantonis, Ioannis},
  year = {2021},
  journal = {Frontiers in Big Data},
  volume = {4},
  issn = {2624-909X},
  urldate = {2023-04-23},
  abstract = {Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods---machine learning (ML) and pattern recognition models in particular---so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.},
  file = {C:\Users\cleme\Zotero\storage\SCJYNVXV\Belle et Papantonis - 2021 - Principles and Practice of Explainable Machine Lea.pdf}
}

@article{bergstraAlgorithmsHyperParameterOptimization,
  title = {Algorithms for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  pages = {9},
  abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y{\textbar}x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
  langid = {english}
}

@article{bergstraAlgorithmsHyperParameterOptimizationa,
  title = {Algorithms for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James S and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  pages = {9},
  abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y{\textbar}x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\75XS4AQB\Bergstra et al. - Algorithms for Hyper-Parameter Optimization.pdf}
}

@article{besencziReviewAutomaticAnalysis2016,
  title = {A Review on Automatic Analysis Techniques for Color Fundus Photographs},
  author = {Besenczi, Ren{\'a}t{\'o} and T{\'o}th, J{\'a}nos and Hajdu, Andr{\'a}s},
  year = {2016},
  month = oct,
  journal = {Computational and Structural Biotechnology Journal},
  volume = {14},
  pages = {371--384},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2016.10.001},
  urldate = {2019-09-23},
  abstract = {In this paper, we give a review on automatic image processing tools to recognize diseases causing specific distortions in the human retina. After a brief summary of the biology of the retina, we give an overview of the types of lesions that may appear as biomarkers of both eye and non-eye diseases. We present several state-of-the-art procedures to extract the anatomic components and lesions in color fundus photographs and decision support methods to help clinical diagnosis. We list publicly available databases and appropriate measurement techniques to compare quantitatively the performance of these approaches. Furthermore, we discuss on how the performance of image processing-based systems can be improved by fusing the output of individual detector algorithms. Retinal image analysis using mobile phones is also addressed as an expected future trend in this field.},
  pmcid = {PMC5072151},
  pmid = {27800125},
  file = {C:\Users\cleme\Zotero\storage\YBW4LUIZ\Besenczi et al. - 2016 - A review on automatic analysis techniques for colo.pdf}
}

@article{beslMethodRegistration3D1992,
  title = {A Method for Registration of 3-{{D}} Shapes},
  author = {Besl, P.J. and McKay, Neil D.},
  year = {1992},
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {14},
  number = {2},
  pages = {239--256},
  issn = {1939-3539},
  doi = {10.1109/34.121791},
  abstract = {The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of 'shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces.{$<>$}},
  keywords = {3D shape registration,computational geometry,convergence,Convergence,convergence of numerical methods,geometric entity,geometric model,Inspection,Iterative algorithms,iterative closest point,Iterative closest point algorithm,iterative methods,Iterative methods,mean-square distance metric,Motion estimation,optimisation,pattern recognition,picture processing,point set registration,Quaternions,Shape measurement,Solid modeling,Testing},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HUQRM53H\\Besl et McKay - 1992 - A method for registration of 3-D shapes.pdf;C\:\\Users\\cleme\\Zotero\\storage\\7W3GGY37\\121791.html}
}

@article{bhaskaranandValueAutomatedDiabetic2019,
  title = {The {{Value}} of {{Automated Diabetic Retinopathy Screening}} with the {{EyeArt System}}: {{A Study}} of {{More Than}} 100,000 {{Consecutive Encounters}} from {{People}} with {{Diabetes}}},
  shorttitle = {The {{Value}} of {{Automated Diabetic Retinopathy Screening}} with the {{EyeArt System}}},
  author = {Bhaskaranand, Malavika and Ramachandra, Chaithanya and Bhat, Sandeep and Cuadros, Jorge and Nittala, Muneeswar G. and Sadda, Srinivas R. and Solanki, Kaushal},
  year = {2019},
  month = nov,
  journal = {Diabetes Technology \& Therapeutics},
  volume = {21},
  number = {11},
  pages = {635--643},
  issn = {1557-8593},
  doi = {10.1089/dia.2019.0164},
  abstract = {Background: Current manual diabetic retinopathy (DR) screening using eye care experts cannot scale to screen the growing population of diabetes patients who are at risk for vision loss. EyeArt system is an automated, cloud-based artificial intelligence (AI) eye screening technology designed to easily detect referral-warranted DR immediately through automated analysis of patient's retinal images. Methods: This retrospective study assessed the diagnostic efficacy of the EyeArt system v2.0 analyzing 850,908 fundus images from 101,710 consecutive patient visits, collected from 404 primary care clinics. Presence or absence of referral-warranted DR (more than mild nonproliferative DR [NPDR]) was automatically detected by the EyeArt system for each patient encounter, and its performance was compared against a clinical reference standard of quality-assured grading by rigorously trained certified ophthalmologists and optometrists. Results: Of the 101,710 visits, 75.7\% were nonreferable, 19.3\% were referable to an eye care specialist, and in 5.0\%, the DR level was unknown as per the clinical reference standard. EyeArt screening had 91.3\% (95\% confidence interval [CI]: 90.9-91.7) sensitivity and 91.1\% (95\% CI: 90.9-91.3) specificity. For 5446 encounters with potentially treatable DR (more than moderate NPDR and/or diabetic macular edema), the system provided a positive "refer" output to 5363 encounters achieving sensitivity of 98.5\%. Conclusions: This study captures variations in real-world clinical practice and shows that an AI DR screening system can be safe and effective in the real world. This study demonstrates the value of this easy-to-use, automated tool for endocrinologists, diabetologists, and general practitioners to address the growing need for DR screening and monitoring.},
  langid = {english},
  pmcid = {PMC6812728},
  pmid = {31335200},
  keywords = {Artificial intelligence,Artificial Intelligence,Automation,Computer-Assisted,Diabetic retinopathy,Diabetic Retinopathy,Humans,Image Interpretation,Macular Edema,Mass Screening,Middle Aged,Observer Variation,Ophthalmology,Reference Standards,Retrospective Studies,Screening}
}

@article{bhaskaranandValueAutomatedDiabetic2019a,
  title = {The {{Value}} of {{Automated Diabetic Retinopathy Screening}} with the {{EyeArt System}}: {{A Study}} of {{More Than}} 100,000 {{Consecutive Encounters}} from {{People}} with {{Diabetes}}},
  shorttitle = {The {{Value}} of {{Automated Diabetic Retinopathy Screening}} with the {{EyeArt System}}},
  author = {Bhaskaranand, Malavika and Ramachandra, Chaithanya and Bhat, Sandeep and Cuadros, Jorge and Nittala, Muneeswar G. and Sadda, Srinivas R. and Solanki, Kaushal},
  year = {2019},
  month = nov,
  journal = {Diabetes Technology \& Therapeutics},
  volume = {21},
  number = {11},
  pages = {635--643},
  issn = {1520-9156},
  doi = {10.1089/dia.2019.0164},
  urldate = {2021-11-29},
  abstract = {Background: Current manual diabetic retinopathy (DR) screening using eye care experts cannot scale to screen the growing population of diabetes patients who are at risk for vision loss. EyeArt system is an automated, cloud-based artificial intelligence (AI) eye screening technology designed to easily detect referral-warranted DR immediately through automated analysis of patient's retinal images., Methods: This retrospective study assessed the diagnostic efficacy of the EyeArt system v2.0 analyzing 850,908 fundus images from 101,710 consecutive patient visits, collected from 404 primary care clinics. Presence or absence of referral-warranted DR (more than mild nonproliferative DR [NPDR]) was automatically detected by the EyeArt system for each patient encounter, and its performance was compared against a clinical reference standard of quality-assured grading by rigorously trained certified ophthalmologists and optometrists., Results: Of the 101,710 visits, 75.7\% were nonreferable, 19.3\% were referable to an eye care specialist, and in 5.0\%, the DR level was unknown as per the clinical reference standard. EyeArt screening had 91.3\% (95\% confidence interval [CI]: 90.9--91.7) sensitivity and 91.1\% (95\% CI: 90.9--91.3) specificity. For 5446 encounters with potentially treatable DR (more than moderate NPDR and/or diabetic macular edema), the system provided a positive ``refer'' output to 5363 encounters achieving sensitivity of 98.5\%., Conclusions: This study captures variations in real-world clinical practice and shows that an AI DR screening system can be safe and effective in the real world. This study demonstrates the value of this easy-to-use, automated tool for endocrinologists, diabetologists, and general practitioners to address the growing need for DR screening and monitoring.},
  pmcid = {PMC6812728},
  pmid = {31335200}
}

@article{bhaskaranandValueAutomatedDiabetic2019b,
  title = {The {{Value}} of {{Automated Diabetic Retinopathy Screening}} with the {{EyeArt System}}: {{A Study}} of {{More Than}} 100,000 {{Consecutive Encounters}} from {{People}} with {{Diabetes}}},
  shorttitle = {The {{Value}} of {{Automated Diabetic Retinopathy Screening}} with the {{EyeArt System}}},
  author = {Bhaskaranand, Malavika and Ramachandra, Chaithanya and Bhat, Sandeep and Cuadros, Jorge and Nittala, Muneeswar G. and Sadda, Srinivas R. and Solanki, Kaushal},
  year = {2019},
  month = nov,
  journal = {Diabetes Technology \& Therapeutics},
  volume = {21},
  number = {11},
  pages = {635--643},
  issn = {1520-9156},
  doi = {10.1089/dia.2019.0164},
  urldate = {2021-11-29},
  abstract = {Background: Current manual diabetic retinopathy (DR) screening using eye care experts cannot scale to screen the growing population of diabetes patients who are at risk for vision loss. EyeArt system is an automated, cloud-based artificial intelligence (AI) eye screening technology designed to easily detect referral-warranted DR immediately through automated analysis of patient's retinal images., Methods: This retrospective study assessed the diagnostic efficacy of the EyeArt system v2.0 analyzing 850,908 fundus images from 101,710 consecutive patient visits, collected from 404 primary care clinics. Presence or absence of referral-warranted DR (more than mild nonproliferative DR [NPDR]) was automatically detected by the EyeArt system for each patient encounter, and its performance was compared against a clinical reference standard of quality-assured grading by rigorously trained certified ophthalmologists and optometrists., Results: Of the 101,710 visits, 75.7\% were nonreferable, 19.3\% were referable to an eye care specialist, and in 5.0\%, the DR level was unknown as per the clinical reference standard. EyeArt screening had 91.3\% (95\% confidence interval [CI]: 90.9--91.7) sensitivity and 91.1\% (95\% CI: 90.9--91.3) specificity. For 5446 encounters with potentially treatable DR (more than moderate NPDR and/or diabetic macular edema), the system provided a positive ``refer'' output to 5363 encounters achieving sensitivity of 98.5\%., Conclusions: This study captures variations in real-world clinical practice and shows that an AI DR screening system can be safe and effective in the real world. This study demonstrates the value of this easy-to-use, automated tool for endocrinologists, diabetologists, and general practitioners to address the growing need for DR screening and monitoring.},
  pmcid = {PMC6812728},
  pmid = {31335200},
  file = {C:\Users\cleme\Zotero\storage\V996CDJS\Bhaskaranand et al. - 2019 - The Value of Automated Diabetic Retinopathy Screen.pdf}
}

@article{bhaskaranandValueAutomatedDiabetic2019c,
  title = {The {{Value}} of {{Automated Diabetic Retinopathy Screening}} with the {{EyeArt System}}: {{A Study}} of {{More Than}} 100,000 {{Consecutive Encounters}} from {{People}} with {{Diabetes}}},
  shorttitle = {The {{Value}} of {{Automated Diabetic Retinopathy Screening}} with the {{EyeArt System}}},
  author = {Bhaskaranand, Malavika and Ramachandra, Chaithanya and Bhat, Sandeep and Cuadros, Jorge and Nittala, Muneeswar G. and Sadda, Srinivas R. and Solanki, Kaushal},
  year = {2019},
  month = nov,
  journal = {Diabetes Technology \& Therapeutics},
  volume = {21},
  number = {11},
  pages = {635--643},
  issn = {1557-8593},
  doi = {10.1089/dia.2019.0164},
  abstract = {Background:                      Current manual diabetic retinopathy (DR) screening using eye care experts cannot scale to screen the growing population of diabetes patients who are at risk for vision loss. EyeArt system is an automated, cloud-based artificial intelligence (AI) eye screening technology designed to easily detect referral-warranted DR immediately through automated analysis of patient's retinal images.                          Methods:                      This retrospective study assessed the diagnostic efficacy of the EyeArt system v2.0 analyzing 850,908 fundus images from 101,710 consecutive patient visits, collected from 404 primary care clinics. Presence or absence of referral-warranted DR (more than mild nonproliferative DR [NPDR]) was automatically detected by the EyeArt system for each patient encounter, and its performance was compared against a clinical reference standard of quality-assured grading by rigorously trained certified ophthalmologists and optometrists.                          Results:                      Of the 101,710 visits, 75.7\% were nonreferable, 19.3\% were referable to an eye care specialist, and in 5.0\%, the DR level was unknown as per the clinical reference standard. EyeArt screening had 91.3\% (95\% confidence interval [CI]: 90.9-91.7) sensitivity and 91.1\% (95\% CI: 90.9-91.3) specificity. For 5446 encounters with potentially treatable DR (more than moderate NPDR and/or diabetic macular edema), the system provided a positive "refer" output to 5363 encounters achieving sensitivity of 98.5\%.                          Conclusions:                      This study captures variations in real-world clinical practice and shows that an AI DR screening system can be safe and effective in the real world. This study demonstrates the value of this easy-to-use, automated tool for endocrinologists, diabetologists, and general practitioners to address the growing need for DR screening and monitoring.},
  langid = {english},
  pmcid = {PMC6812728},
  pmid = {31335200},
  keywords = {Artificial intelligence,Artificial Intelligence,Automation,Diabetic retinopathy,Diabetic Retinopathy,Humans,Image Interpretation Computer-Assisted,Macular Edema,Mass Screening,Middle Aged,Observer Variation,Ophthalmology,Reference Standards,Retrospective Studies,Screening},
  file = {C:\Users\cleme\Zotero\storage\HC5585TN\Bhaskaranand et al. - 2019 - The Value of Automated Diabetic Retinopathy Screen.pdf}
}

@article{Bhat2023EffectOL,
  title = {Effect of Latent Space Distribution on the Segmentation of Images with Multiple Annotations},
  author = {Bhat, Ishaan and Pluim, Josien P. W. and Viergever, Max A. and Kuijf, Hugo J.},
  year = {2023},
  journal = {ArXiv},
  volume = {abs/2304.13476}
}

@article{bhuiyanArtificialIntelligenceStratify2020,
  title = {Artificial {{Intelligence}} to {{Stratify Severity}} of {{Age-Related Macular Degeneration}} ({{AMD}}) and {{Predict Risk}} of {{Progression}} to {{Late AMD}}},
  author = {Bhuiyan, Alauddin and Wong, Tien Yin and Ting, Daniel Shu Wei and Govindaiah, Arun and Souied, Eric H. and Smith, R. Theodore},
  year = {2020},
  month = apr,
  journal = {Translational Vision Science \& Technology},
  volume = {9},
  number = {2},
  pages = {25},
  issn = {2164-2591},
  doi = {10.1167/tvst.9.2.25},
  abstract = {PURPOSE: To build and validate artificial intelligence (AI)-based models for AMD screening and for predicting late dry and wet AMD progression within 1 and 2 years. METHODS: The dataset of the Age-related Eye Disease Study (AREDS) was used to train and validate our prediction model. External validation was performed on the Nutritional AMD Treatment-2 (NAT-2) study. FIRST STEP: An ensemble of deep learning screening methods was trained and validated on 116,875 color fundus photos from 4139 participants in the AREDS study to classify them as no, early, intermediate, or advanced AMD and further stratified them along the AREDS 12 level severity scale. Second step: the resulting AMD scores were combined with sociodemographic clinical data and other automatically extracted imaging data by a logistic model tree machine learning technique to predict risk for progression to late AMD within 1 or 2 years, with training and validation performed on 923 AREDS participants who progressed within 2 years, 901 who progressed within 1 year, and 2840 who did not progress within 2 years. For those found at risk of progression to late AMD, we further predicted the type (dry or wet) of the progression of late AMD. RESULTS: For identification of early/none vs. intermediate/late (i.e., referral level) AMD, we achieved 99.2\% accuracy. The prediction model for a 2-year incident late AMD (any) achieved 86.36\% accuracy, with 66.88\% for late dry and 67.15\% for late wet AMD. For the NAT-2 dataset, the 2-year late AMD prediction accuracy was 84\%. CONCLUSIONS: Validated color fundus photo-based models for AMD screening and risk prediction for late AMD are now ready for clinical testing and potential telemedical deployment. TRANSLATIONAL RELEVANCE: Noninvasive, highly accurate, and fast AI methods to screen for referral level AMD and to predict late AMD progression offer significant potential improvements in our care of this prevalent blinding disease.},
  langid = {english},
  pmcid = {PMC7396183},
  pmid = {32818086},
  keywords = {AMD prediction,Artificial Intelligence,deep learning,dry AMD,Fundus Oculi,Humans,Machine Learning,Severity of Illness Index,wet AMD,Wet Macular Degeneration}
}

@article{bhuiyanArtificialIntelligenceStratify2020a,
  title = {Artificial {{Intelligence}} to {{Stratify Severity}} of {{Age-Related Macular Degeneration}} ({{AMD}}) and {{Predict Risk}} of {{Progression}} to {{Late AMD}}},
  author = {Bhuiyan, Alauddin and Wong, Tien Yin and Ting, Daniel Shu Wei and Govindaiah, Arun and Souied, Eric H. and Smith, R. Theodore},
  year = {2020},
  month = apr,
  journal = {Translational Vision Science \& Technology},
  volume = {9},
  number = {2},
  pages = {25},
  issn = {2164-2591},
  doi = {10.1167/tvst.9.2.25},
  abstract = {PURPOSE: To build and validate artificial intelligence (AI)-based models for AMD screening and for predicting late dry and wet AMD progression within 1 and 2 years. METHODS: The dataset of the Age-related Eye Disease Study (AREDS) was used to train and validate our prediction model. External validation was performed on the Nutritional AMD Treatment-2 (NAT-2) study. FIRST STEP: An ensemble of deep learning screening methods was trained and validated on 116,875 color fundus photos from 4139 participants in the AREDS study to classify them as no, early, intermediate, or advanced AMD and further stratified them along the AREDS 12 level severity scale. Second step: the resulting AMD scores were combined with sociodemographic clinical data and other automatically extracted imaging data by a logistic model tree machine learning technique to predict risk for progression to late AMD within 1 or 2 years, with training and validation performed on 923 AREDS participants who progressed within 2 years, 901 who progressed within 1 year, and 2840 who did not progress within 2 years. For those found at risk of progression to late AMD, we further predicted the type (dry or wet) of the progression of late AMD. RESULTS: For identification of early/none vs. intermediate/late (i.e., referral level) AMD, we achieved 99.2\% accuracy. The prediction model for a 2-year incident late AMD (any) achieved 86.36\% accuracy, with 66.88\% for late dry and 67.15\% for late wet AMD. For the NAT-2 dataset, the 2-year late AMD prediction accuracy was 84\%. CONCLUSIONS: Validated color fundus photo-based models for AMD screening and risk prediction for late AMD are now ready for clinical testing and potential telemedical deployment. TRANSLATIONAL RELEVANCE: Noninvasive, highly accurate, and fast AI methods to screen for referral level AMD and to predict late AMD progression offer significant potential improvements in our care of this prevalent blinding disease.},
  langid = {english},
  pmcid = {PMC7396183},
  pmid = {32818086},
  keywords = {AMD prediction,Artificial Intelligence,deep learning,dry AMD,Fundus Oculi,Humans,Machine Learning,Severity of Illness Index,wet AMD,Wet Macular Degeneration},
  file = {C:\Users\cleme\Zotero\storage\AQ6VHGUT\Bhuiyan et al. - 2020 - Artificial Intelligence to Stratify Severity of Ag.pdf}
}

@article{bilalSurveyRecentDevelopments2021,
  title = {Survey on Recent Developments in Automatic Detection of Diabetic Retinopathy},
  author = {Bilal, A. and Sun, G. and Mazhar, S.},
  year = {2021},
  month = mar,
  journal = {Journal Francais D'ophtalmologie},
  volume = {44},
  number = {3},
  pages = {420--440},
  issn = {1773-0597},
  doi = {10.1016/j.jfo.2020.08.009},
  abstract = {Diabetic retinopathy (DR) is a disease facilitated by the rapid spread of diabetes worldwide. DR can blind diabetic individuals. Early detection of DR is essential to restoring vision and providing timely treatment. DR can be detected manually by an ophthalmologist, examining the retinal and fundus images to analyze the macula, morphological changes in blood vessels, hemorrhage, exudates, and/or microaneurysms. This is a time consuming, costly, and challenging task. An automated system can easily perform this function by using artificial intelligence, especially in screening for early DR. Recently, much state-of-the-art research relevant to the identification of DR has been reported. This article describes the current methods of detecting non-proliferative diabetic retinopathy, exudates, hemorrhage, and microaneurysms. In addition, the authors point out future directions in overcoming current challenges in the field of DR research.},
  langid = {english},
  pmid = {33526268},
  keywords = {Artificial intelligence,Artificial Intelligence,Deep learning,Diabetes Mellitus,Diabetic retinopathy,Diabetic Retinopathy,Fundus images,Fundus Oculi,Humans,Images du fond d'\oeil,Intelligence artificielle,Machine learning,Microaneurysm,Ophtalmologie,Ophthalmology,R\'etinopathie diab\'etique,Retina}
}

@article{bilalSurveyRecentDevelopments2021a,
  title = {Survey on Recent Developments in Automatic Detection of Diabetic Retinopathy},
  author = {Bilal, A. and Sun, G. and Mazhar, S.},
  year = {2021},
  month = mar,
  journal = {Journal Francais D'ophtalmologie},
  volume = {44},
  number = {3},
  pages = {420--440},
  issn = {1773-0597},
  doi = {10.1016/j.jfo.2020.08.009},
  abstract = {Diabetic retinopathy (DR) is a disease facilitated by the rapid spread of diabetes worldwide. DR can blind diabetic individuals. Early detection of DR is essential to restoring vision and providing timely treatment. DR can be detected manually by an ophthalmologist, examining the retinal and fundus images to analyze the macula, morphological changes in blood vessels, hemorrhage, exudates, and/or microaneurysms. This is a time consuming, costly, and challenging task. An automated system can easily perform this function by using artificial intelligence, especially in screening for early DR. Recently, much state-of-the-art research relevant to the identification of DR has been reported. This article describes the current methods of detecting non-proliferative diabetic retinopathy, exudates, hemorrhage, and microaneurysms. In addition, the authors point out future directions in overcoming current challenges in the field of DR research.},
  langid = {english},
  pmid = {33526268},
  keywords = {Artificial intelligence,Artificial Intelligence,Deep learning,Diabetes Mellitus,Diabetic retinopathy,Diabetic Retinopathy,Fundus images,Fundus Oculi,Humans,Images du fond d'oeil,Intelligence artificielle,Machine learning,Microaneurysm,Ophtalmologie,Ophthalmology,Retina,Retinopathie diabetique}
}

@article{Biousse167,
  title = {Ophthalmoscopy in the 21st Century},
  author = {Biousse, Val{\'e}rie and Bruce, Beau B. and Newman, Nancy J.},
  year = {2018},
  journal = {Neurology},
  volume = {90},
  number = {4},
  eprint = {https://n.neurology.org/content/90/4/167.full.pdf},
  pages = {167--175},
  publisher = {Wolters Kluwer Health, Inc. on behalf of the American Academy of Neurology},
  issn = {0028-3878},
  doi = {10.1212/WNL.0000000000004868},
  abstract = {Although the usefulness of viewing the ocular fundus is well-recognized, ophthalmoscopy is infrequently and poorly performed by most nonophthalmologist physicians, including neurologists. Barriers to the practice of ophthalmoscopy by nonophthalmologists include not only the technical difficulty related to direct ophthalmoscopy, but also lack of adequate training and discouragement by preceptors. Recent studies have shown that digital retinal fundus photographs with electronic transmission and remote interpretation of images by an ophthalmologist are an efficient and reliable way to allow examination of the ocular fundus in patients with systemic disorders such as diabetes mellitus. Ocular fundus photographs obtained without pharmacologic dilation of the pupil using nonmydriatic fundus cameras could be of great value in emergency departments (EDs) and neurologic settings. The Fundus Photography vs Ophthalmoscopy Trial Outcomes in the Emergency Department (FOTO-ED) study showed that ED providers consistently failed to correctly identify relevant ocular funduscopic findings using the direct ophthalmoscope, and that nonmydriatic fundus photography was an effective alternate way of providing access to the ocular fundus in the ED. Extrapolating these results to headache clinics, outpatient neurology clinics, and adult and pediatric primary care settings seems self-evident. As technology advances, nonmydriatic ocular fundus imaging systems will be of higher quality and more portable and affordable, thereby circumventing the need to master the use of the ophthalmoscope. Visualizing the ocular fundus is more important than the method used. Ocular fundus photography facilitates nonophthalmologists performance of this essential part of the physical examination, thus helping to reestablish the value of doing so.ACEP=American College of Emergency Physicians; ACGME=Accreditation Council for Graduate Medical Education; CI=confidence interval; ED=emergency department; FOTO-ED=Fundus Photography vs Ophthalmoscopy Trial Outcomes in the Emergency Department; IIH=idiopathic intracranial hypertension},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\XQ9BIV65\\biousse2017.pdf;C\:\\Users\\cleme\\Zotero\\storage\\YF33MLT2\\biousse2017.pdf}
}

@article{Biousse167,
  title = {Ophthalmoscopy in the 21st Century},
  author = {Biousse, Val{\'e}rie and Bruce, Beau B. and Newman, Nancy J.},
  year = {2018},
  journal = {Neurology},
  volume = {90},
  number = {4},
  eprint = {https://n.neurology.org/content/90/4/167.full.pdf},
  pages = {167--175},
  publisher = {Wolters Kluwer Health, Inc. on behalf of the American Academy of Neurology},
  issn = {0028-3878},
  doi = {10.1212/WNL.0000000000004868},
  abstract = {Although the usefulness of viewing the ocular fundus is well-recognized, ophthalmoscopy is infrequently and poorly performed by most nonophthalmologist physicians, including neurologists. Barriers to the practice of ophthalmoscopy by nonophthalmologists include not only the technical difficulty related to direct ophthalmoscopy, but also lack of adequate training and discouragement by preceptors. Recent studies have shown that digital retinal fundus photographs with electronic transmission and remote interpretation of images by an ophthalmologist are an efficient and reliable way to allow examination of the ocular fundus in patients with systemic disorders such as diabetes mellitus. Ocular fundus photographs obtained without pharmacologic dilation of the pupil using nonmydriatic fundus cameras could be of great value in emergency departments (EDs) and neurologic settings. The Fundus Photography vs Ophthalmoscopy Trial Outcomes in the Emergency Department (FOTO-ED) study showed that ED providers consistently failed to correctly identify relevant ocular funduscopic findings using the direct ophthalmoscope, and that nonmydriatic fundus photography was an effective alternate way of providing access to the ocular fundus in the ED. Extrapolating these results to headache clinics, outpatient neurology clinics, and adult and pediatric primary care settings seems self-evident. As technology advances, nonmydriatic ocular fundus imaging systems will be of higher quality and more portable and affordable, thereby circumventing the need to master the use of the ophthalmoscope. Visualizing the ocular fundus is more important than the method used. Ocular fundus photography facilitates nonophthalmologists performance of this essential part of the physical examination, thus helping to reestablish the value of doing so.ACEP=American College of Emergency Physicians; ACGME=Accreditation Council for Graduate Medical Education; CI=confidence interval; ED=emergency department; FOTO-ED=Fundus Photography vs Ophthalmoscopy Trial Outcomes in the Emergency Department; IIH=idiopathic intracranial hypertension}
}

@article{blumerLearnabilityVapnikChervonenkisDimension1989,
  title = {Learnability and the {{Vapnik-Chervonenkis}} Dimension},
  author = {Blumer, Anselm and Ehrenfeucht, A. and Haussler, David and Warmuth, Manfred K.},
  year = {1989},
  month = oct,
  journal = {Journal of the ACM},
  volume = {36},
  number = {4},
  pages = {929--965},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/76359.76371},
  urldate = {2023-04-22},
  abstract = {Valiant's learnability model is extended to learning classesof concepts defined by regions in Euclidean space E''. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the VapnikChervonenkis dimension, a simple combinatorial parameter of the classof concepts to be learned. Using this parameter, the complexity and closure properties of learnable classesare analyzed, and the necessary and sufftcient conditions are provided for feasible learnability.},
  langid = {english}
}

@article{blumerLearnabilityVapnikChervonenkisDimension1989a,
  title = {Learnability and the {{Vapnik-Chervonenkis}} Dimension},
  author = {Blumer, Anselm and Ehrenfeucht, A. and Haussler, David and Warmuth, Manfred K.},
  year = {1989},
  month = oct,
  journal = {Journal of the ACM},
  volume = {36},
  number = {4},
  pages = {929--965},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/76359.76371},
  urldate = {2023-04-22},
  abstract = {Valiant's learnability model is extended to learning classesof concepts defined by regions in Euclidean space E''. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the VapnikChervonenkis dimension, a simple combinatorial parameter of the classof concepts to be learned. Using this parameter, the complexity and closure properties of learnable classesare analyzed, and the necessary and sufftcient conditions are provided for feasible learnability.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\3Q2V4FU6\Blumer et al. - 1989 - Learnability and the Vapnik-Chervonenkis dimension.pdf}
}

@article{bockGlaucomaRiskIndex2010,
  title = {Glaucoma Risk Index:{{Automated}} Glaucoma Detection from Color Fundus Images},
  shorttitle = {Glaucoma Risk Index},
  author = {Bock, R{\"u}diger and Meier, J{\"o}rg and Ny{\'u}l, L{\'a}szl{\'o} G. and Hornegger, Joachim and Michelson, Georg},
  year = {2010},
  month = jun,
  journal = {Medical Image Analysis},
  volume = {14},
  number = {3},
  pages = {471--481},
  issn = {1361-8415},
  doi = {10.1016/j.media.2009.12.006},
  urldate = {2019-11-27},
  abstract = {Glaucoma as a neurodegeneration of the optic nerve is one of the most common causes of blindness. Because revitalization of the degenerated nerve fibers of the optic nerve is impossible early detection of the disease is essential. This can be supported by a robust and automated mass-screening. We propose a novel automated glaucoma detection system that operates on inexpensive to acquire and widely used digital color fundus images. After a glaucoma specific preprocessing, different generic feature types are compressed by an appearance-based dimension reduction technique. Subsequently, a probabilistic two-stage classification scheme combines these features types to extract the novel Glaucoma Risk Index (GRI) that shows a reasonable glaucoma detection performance. On a sample set of 575 fundus images a classification accuracy of 80\% has been achieved in a 5-fold cross-validation setup. The GRI gains a competitive area under ROC (AUC) of 88\% compared to the established topography-based glaucoma probability score of scanning laser tomography with AUC of 87\%. The proposed color fundus image-based GRI achieves a competitive and reliable detection performance on a low-priced modality by the statistical analysis of entire images of the optic nerve head.},
  langid = {english},
  keywords = {Appearance-based image analysis,Computer aided diagnosis,Glaucoma,Linear principal component analysis,Optic disk}
}

@article{bockGlaucomaRiskIndex2010a,
  title = {Glaucoma Risk Index:{{Automated}} Glaucoma Detection from Color Fundus Images},
  shorttitle = {Glaucoma Risk Index},
  author = {Bock, R{\"u}diger and Meier, J{\"o}rg and Ny{\'u}l, L{\'a}szl{\'o} G. and Hornegger, Joachim and Michelson, Georg},
  year = {2010},
  month = jun,
  journal = {Medical Image Analysis},
  volume = {14},
  number = {3},
  pages = {471--481},
  issn = {1361-8415},
  doi = {10.1016/j.media.2009.12.006},
  urldate = {2019-11-27},
  abstract = {Glaucoma as a neurodegeneration of the optic nerve is one of the most common causes of blindness. Because revitalization of the degenerated nerve fibers of the optic nerve is impossible early detection of the disease is essential. This can be supported by a robust and automated mass-screening. We propose a novel automated glaucoma detection system that operates on inexpensive to acquire and widely used digital color fundus images. After a glaucoma specific preprocessing, different generic feature types are compressed by an appearance-based dimension reduction technique. Subsequently, a probabilistic two-stage classification scheme combines these features types to extract the novel Glaucoma Risk Index (GRI) that shows a reasonable glaucoma detection performance. On a sample set of 575 fundus images a classification accuracy of 80\% has been achieved in a 5-fold cross-validation setup. The GRI gains a competitive area under ROC (AUC) of 88\% compared to the established topography-based glaucoma probability score of scanning laser tomography with AUC of 87\%. The proposed color fundus image-based GRI achieves a competitive and reliable detection performance on a low-priced modality by the statistical analysis of entire images of the optic nerve head.},
  langid = {english},
  keywords = {Appearance-based image analysis,Computer aided diagnosis,Glaucoma,Linear principal component analysis,Optic disk},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\6NQPCCZL\\bock2010.pdf;C\:\\Users\\cleme\\Zotero\\storage\\PWPNYM7A\\Bock et al. - 2010 - Glaucoma risk indexAutomated glaucoma detection f.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6TJ3RQZF\\S1361841509001509.html}
}

@article{boraPredictingRiskDeveloping2021,
  title = {Predicting the Risk of Developing Diabetic Retinopathy Using Deep Learning},
  author = {Bora, Ashish and Balasubramanian, Siva and Babenko, Boris and Virmani, Sunny and Venugopalan, Subhashini and Mitani, Akinori and Marinho, Guilherme de Oliveira and Cuadros, Jorge and Ruamviboonsuk, Paisan and Corrado, Greg S. and Peng, Lily and Webster, Dale R. and Varadarajan, Avinash V. and Hammel, Naama and Liu, Yun and Bavishi, Pinal},
  year = {2021},
  month = jan,
  journal = {The Lancet Digital Health},
  volume = {3},
  number = {1},
  pages = {e10-e19},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(20)30250-8},
  urldate = {2023-10-05},
  langid = {english}
}

@article{boraPredictingRiskDeveloping2021a,
  title = {Predicting the Risk of Developing Diabetic Retinopathy Using Deep Learning},
  author = {Bora, Ashish and Balasubramanian, Siva and Babenko, Boris and Virmani, Sunny and Venugopalan, Subhashini and Mitani, Akinori and Marinho, Guilherme de Oliveira and Cuadros, Jorge and Ruamviboonsuk, Paisan and Corrado, Greg S. and Peng, Lily and Webster, Dale R. and Varadarajan, Avinash V. and Hammel, Naama and Liu, Yun and Bavishi, Pinal},
  year = {2021},
  month = jan,
  journal = {The Lancet Digital Health},
  volume = {3},
  number = {1},
  pages = {e10-e19},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(20)30250-8},
  urldate = {2023-10-05},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\UPCM82ZJ\Bora et al. - 2021 - Predicting the risk of developing diabetic retinop.pdf}
}

@article{boucherEvidencebasedCanadianGuidelines2020,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: {{Recommendations}} from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V. and {Steering Committee for Tele-Ophthalmology Screening, Canadian Retina Research Network}},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology. Journal Canadien D'ophtalmologie},
  volume = {55},
  number = {1S1},
  pages = {14--24},
  issn = {1715-3360},
  doi = {10.1016/j.jcjo.2020.01.001},
  abstract = {OBJECTIVE: The purpose of this report is to develop a consensus for Canadian national guidelines specific to a tele-medicine approach to screening for diabetic retinopathy (DR) using evidence-based and clinical data. METHODS: Canadian Tele-Screening Grading Scales for DR and diabetic macular edema (DME) were created primarily based on severity grading scales outlined by the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDR) and the Scottish DR Grading Scheme 2007. Other grading scales used in international screening programs and the clinical expertise of the Canadian Retina Research Network members and retina specialists nationwide were also used in the creation of the guidelines. RESULTS: National Tele-Screening Guidelines for DR and DME with and without optical coherence tomography (OCT) images are proposed. These outline a diagnosis and management algorithm for patients presenting with different stages of DR and/or DME. General guidelines detailing the requirements for imaged retina fields, image quality, quality control, and follow-up care and the role of visual acuity, pupil dilation, OCT, ultra-wide-field imaging, and artificial intelligence are discussed. CONCLUSIONS: Tele-retina screening can help to address the need for timely and effective screening for DR, whose prevalence continues to rise. A standardized and evidence-based national approach to DR tele-screening has been proposed, based on DR/DME grading using two 45{$^\circ$} image fields or a single widefield or ultra-wide-field image, preferable use of OCT imaging, and a focus on local quality control measures.},
  langid = {english},
  pmid = {32089161}
}

@article{boucherEvidencebasedCanadianGuidelines2020a,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: {{Recommendations}} from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V.},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology},
  volume = {55},
  number = {1, Supplement 1},
  pages = {14--24},
  issn = {0008-4182},
  doi = {10.1016/j.jcjo.2020.01.001},
  urldate = {2020-04-23},
  abstract = {Objective The purpose of this report is to develop a consensus for Canadian national guidelines specific to a tele-medicine approach to screening for diabetic retinopathy (DR) using evidence-based and clinical data. Methods Canadian Tele-Screening Grading Scales for DR and diabetic macular edema (DME) were created primarily based on severity grading scales outlined by the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDR) and the Scottish DR Grading Scheme 2007. Other grading scales used in international screening programs and the clinical expertise of the Canadian Retina Research Network members and retina specialists nationwide were also used in the creation of the guidelines. Results National Tele-Screening Guidelines for DR and DME with and without optical coherence tomography (OCT) images are proposed. These outline a diagnosis and management algorithm for patients presenting with different stages of DR and/or DME. General guidelines detailing the requirements for imaged retina fields, image quality, quality control, and follow-up care and the role of visual acuity, pupil dilation, OCT, ultra-wide-field imaging, and artificial intelligence are discussed. Conclusions Tele-retina screening can help to address the need for timely and effective screening for DR, whose prevalence continues to rise. A standardized and evidence-based national approach to DR tele-screening has been proposed, based on DR/DME grading using two 45{$^\circ$} image fields or a single widefield or ultra-wide-field image, preferable use of OCT imaging, and a focus on local quality control measures.},
  langid = {english}
}

@article{boucherEvidencebasedCanadianGuidelines2020b,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: {{Recommendations}} from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V.},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology},
  series = {Tele-Retina {{Screening}} for {{Diabetic Retinopathy}}},
  volume = {55},
  number = {1, Supplement 1},
  pages = {14--24},
  issn = {0008-4182},
  doi = {10.1016/j.jcjo.2020.01.001},
  urldate = {2021-04-12},
  abstract = {Objective The purpose of this report is to develop a consensus for Canadian national guidelines specific to a tele-medicine approach to screening for diabetic retinopathy (DR) using evidence-based and clinical data. Methods Canadian Tele-Screening Grading Scales for DR and diabetic macular edema (DME) were created primarily based on severity grading scales outlined by the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDR) and the Scottish DR Grading Scheme 2007. Other grading scales used in international screening programs and the clinical expertise of the Canadian Retina Research Network members and retina specialists nationwide were also used in the creation of the guidelines. Results National Tele-Screening Guidelines for DR and DME with and without optical coherence tomography (OCT) images are proposed. These outline a diagnosis and management algorithm for patients presenting with different stages of DR and/or DME. General guidelines detailing the requirements for imaged retina fields, image quality, quality control, and follow-up care and the role of visual acuity, pupil dilation, OCT, ultra-wide-field imaging, and artificial intelligence are discussed. Conclusions Tele-retina screening can help to address the need for timely and effective screening for DR, whose prevalence continues to rise. A standardized and evidence-based national approach to DR tele-screening has been proposed, based on DR/DME grading using two 45{$^\circ$} image fields or a single widefield or ultra-wide-field image, preferable use of OCT imaging, and a focus on local quality control measures.},
  langid = {english}
}

@article{boucherEvidencebasedCanadianGuidelines2020c,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: {{Recommendations}} from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V. and {Steering Committee for Tele-Ophthalmology Screening, Canadian Retina Research Network}},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology. Journal Canadien D'ophtalmologie},
  volume = {55},
  number = {1 Suppl 1},
  pages = {14--24},
  issn = {1715-3360},
  doi = {10.1016/j.jcjo.2020.01.001},
  abstract = {OBJECTIVE: The purpose of this report is to develop a consensus for Canadian national guidelines specific to a tele-medicine approach to screening for diabetic retinopathy (DR) using evidence-based and clinical data. METHODS: Canadian Tele-Screening Grading Scales for DR and diabetic macular edema (DME) were created primarily based on severity grading scales outlined by the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDR) and the Scottish DR Grading Scheme 2007. Other grading scales used in international screening programs and the clinical expertise of the Canadian Retina Research Network members and retina specialists nationwide were also used in the creation of the guidelines. RESULTS: National Tele-Screening Guidelines for DR and DME with and without optical coherence tomography (OCT) images are proposed. These outline a diagnosis and management algorithm for patients presenting with different stages of DR and/or DME. General guidelines detailing the requirements for imaged retina fields, image quality, quality control, and follow-up care and the role of visual acuity, pupil dilation, OCT, ultra-wide-field imaging, and artificial intelligence are discussed. CONCLUSIONS: Tele-retina screening can help to address the need for timely and effective screening for DR, whose prevalence continues to rise. A standardized and evidence-based national approach to DR tele-screening has been proposed, based on DR/DME grading using two 45{$^\circ$} image fields or a single widefield or ultra-wide-field image, preferable use of OCT imaging, and a focus on local quality control measures.},
  langid = {english},
  pmid = {32089161},
  keywords = {Artificial Intelligence,Canada,Diabetes Mellitus,Diabetic Retinopathy,Guidelines as Topic,Humans,Macular Edema,Optical Coherence,Retina,Telemedicine,Tomography,Type 2}
}

@article{boucherEvidencebasedCanadianGuidelines2020d,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: {{Recommendations}} from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V.},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology},
  volume = {55},
  number = {1},
  pages = {14--24},
  publisher = {Elsevier},
  issn = {0008-4182, 1715-3360},
  doi = {10.1016/j.jcjo.2020.01.001},
  urldate = {2022-07-09},
  langid = {english},
  pmid = {32089161}
}

@article{boucherEvidencebasedCanadianGuidelines2020e,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: Recommendations from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V. and {Steering Committee for Tele-Ophthalmology Screening, Canadian Retina Research Network}},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology. Journal Canadien D'ophtalmologie},
  volume = {55},
  number = {1 Suppl 1},
  pages = {14--24},
  issn = {1715-3360},
  doi = {10.1016/j.jcjo.2020.01.001},
  abstract = {OBJECTIVE: The purpose of this report is to develop a consensus for Canadian national guidelines specific to a tele-medicine approach to screening for diabetic retinopathy (DR) using evidence-based and clinical data. METHODS: Canadian Tele-Screening Grading Scales for DR and diabetic macular edema (DME) were created primarily based on severity grading scales outlined by the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDR) and the Scottish DR Grading Scheme 2007. Other grading scales used in international screening programs and the clinical expertise of the Canadian Retina Research Network members and retina specialists nationwide were also used in the creation of the guidelines. RESULTS: National Tele-Screening Guidelines for DR and DME with and without optical coherence tomography (OCT) images are proposed. These outline a diagnosis and management algorithm for patients presenting with different stages of DR and/or DME. General guidelines detailing the requirements for imaged retina fields, image quality, quality control, and follow-up care and the role of visual acuity, pupil dilation, OCT, ultra-wide-field imaging, and artificial intelligence are discussed. CONCLUSIONS: Tele-retina screening can help to address the need for timely and effective screening for DR, whose prevalence continues to rise. A standardized and evidence-based national approach to DR tele-screening has been proposed, based on DR/DME grading using two 45{$^\circ$} image fields or a single widefield or ultra-wide-field image, preferable use of OCT imaging, and a focus on local quality control measures.},
  langid = {english},
  pmid = {32089161},
  keywords = {Artificial Intelligence,Canada,Diabetes Mellitus Type 2,Diabetic Retinopathy,Guidelines as Topic,Humans,Macular Edema,Retina,Telemedicine,Tomography Optical Coherence}
}

@article{boucherEvidencebasedCanadianGuidelines2020f,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: Recommendations from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V.},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology},
  series = {Tele-Retina {{Screening}} for {{Diabetic Retinopathy}}},
  volume = {55},
  number = {1, Supplement 1},
  pages = {14--24},
  issn = {0008-4182},
  doi = {10.1016/j.jcjo.2020.01.001},
  urldate = {2021-04-12},
  abstract = {Objective The purpose of this report is to develop a consensus for Canadian national guidelines specific to a tele-medicine approach to screening for diabetic retinopathy (DR) using evidence-based and clinical data. Methods Canadian Tele-Screening Grading Scales for DR and diabetic macular edema (DME) were created primarily based on severity grading scales outlined by the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDR) and the Scottish DR Grading Scheme 2007. Other grading scales used in international screening programs and the clinical expertise of the Canadian Retina Research Network members and retina specialists nationwide were also used in the creation of the guidelines. Results National Tele-Screening Guidelines for DR and DME with and without optical coherence tomography (OCT) images are proposed. These outline a diagnosis and management algorithm for patients presenting with different stages of DR and/or DME. General guidelines detailing the requirements for imaged retina fields, image quality, quality control, and follow-up care and the role of visual acuity, pupil dilation, OCT, ultra-wide-field imaging, and artificial intelligence are discussed. Conclusions Tele-retina screening can help to address the need for timely and effective screening for DR, whose prevalence continues to rise. A standardized and evidence-based national approach to DR tele-screening has been proposed, based on DR/DME grading using two 45{$^\circ$} image fields or a single widefield or ultra-wide-field image, preferable use of OCT imaging, and a focus on local quality control measures.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\YI9EA5LX\S0008418219312980.html}
}

@article{boucherEvidencebasedCanadianGuidelines2020g,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: Recommendations from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V.},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology},
  volume = {55},
  number = {1, Supplement 1},
  pages = {14--24},
  issn = {0008-4182},
  doi = {10.1016/j.jcjo.2020.01.001},
  urldate = {2020-04-23},
  abstract = {Objective The purpose of this report is to develop a consensus for Canadian national guidelines specific to a tele-medicine approach to screening for diabetic retinopathy (DR) using evidence-based and clinical data. Methods Canadian Tele-Screening Grading Scales for DR and diabetic macular edema (DME) were created primarily based on severity grading scales outlined by the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDR) and the Scottish DR Grading Scheme 2007. Other grading scales used in international screening programs and the clinical expertise of the Canadian Retina Research Network members and retina specialists nationwide were also used in the creation of the guidelines. Results National Tele-Screening Guidelines for DR and DME with and without optical coherence tomography (OCT) images are proposed. These outline a diagnosis and management algorithm for patients presenting with different stages of DR and/or DME. General guidelines detailing the requirements for imaged retina fields, image quality, quality control, and follow-up care and the role of visual acuity, pupil dilation, OCT, ultra-wide-field imaging, and artificial intelligence are discussed. Conclusions Tele-retina screening can help to address the need for timely and effective screening for DR, whose prevalence continues to rise. A standardized and evidence-based national approach to DR tele-screening has been proposed, based on DR/DME grading using two 45{$^\circ$} image fields or a single widefield or ultra-wide-field image, preferable use of OCT imaging, and a focus on local quality control measures.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\WGYHAZ95\\Boucher et al. - 2020 - Evidence-based Canadian guidelines for tele-retina.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RRIB2BWS\\S0008418219312980.html}
}

@article{boucherEvidencebasedCanadianGuidelines2020h,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: Recommendations from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V. and {Steering Committee for Tele-Ophthalmology Screening, Canadian Retina Research Network}},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology. Journal Canadien D'ophtalmologie},
  volume = {55},
  number = {1S1},
  pages = {14--24},
  issn = {1715-3360},
  doi = {10.1016/j.jcjo.2020.01.001},
  abstract = {OBJECTIVE: The purpose of this report is to develop a consensus for Canadian national guidelines specific to a tele-medicine approach to screening for diabetic retinopathy (DR) using evidence-based and clinical data. METHODS: Canadian Tele-Screening Grading Scales for DR and diabetic macular edema (DME) were created primarily based on severity grading scales outlined by the International Clinical Diabetic Retinopathy Disease Severity Scale (ICDR) and the Scottish DR Grading Scheme 2007. Other grading scales used in international screening programs and the clinical expertise of the Canadian Retina Research Network members and retina specialists nationwide were also used in the creation of the guidelines. RESULTS: National Tele-Screening Guidelines for DR and DME with and without optical coherence tomography (OCT) images are proposed. These outline a diagnosis and management algorithm for patients presenting with different stages of DR and/or DME. General guidelines detailing the requirements for imaged retina fields, image quality, quality control, and follow-up care and the role of visual acuity, pupil dilation, OCT, ultra-wide-field imaging, and artificial intelligence are discussed. CONCLUSIONS: Tele-retina screening can help to address the need for timely and effective screening for DR, whose prevalence continues to rise. A standardized and evidence-based national approach to DR tele-screening has been proposed, based on DR/DME grading using two 45{$^\circ$} image fields or a single widefield or ultra-wide-field image, preferable use of OCT imaging, and a focus on local quality control measures.},
  langid = {english},
  pmid = {32089161}
}

@article{boucherEvidencebasedCanadianGuidelines2020i,
  title = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy: Recommendations from the {{Canadian Retina Research Network}} ({{CR2N}}) {{Tele-Retina Steering Committee}}},
  shorttitle = {Evidence-Based {{Canadian}} Guidelines for Tele-Retina Screening for Diabetic Retinopathy},
  author = {Boucher, M. C. and Qian, J. and Brent, M. H. and Wong, D. T. and Sheidow, T. and Duval, R. and Kherani, A. and Dookeran, R. and Maberley, D. and Samad, A. and Chaudhary, V.},
  year = {2020},
  month = feb,
  journal = {Canadian Journal of Ophthalmology},
  volume = {55},
  number = {1},
  pages = {14--24},
  publisher = {Elsevier},
  issn = {0008-4182, 1715-3360},
  doi = {10.1016/j.jcjo.2020.01.001},
  urldate = {2022-07-09},
  langid = {english},
  pmid = {32089161},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\SVBGE7NG\\10.1016@j.jcjo.2020.01.001.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\QL4AVZE9\\fulltext.html}
}

@article{breenEmergencyDepartmentEvaluation2008,
  title = {Emergency Department Evaluation of Sudden, Severe Headache},
  author = {Breen, D. P. and Duncan, C. W. and Pope, A. E. and Gray, A. J. and {Al-Shahi Salman}, R.},
  year = {2008},
  month = jun,
  journal = {QJM: An International Journal of Medicine},
  volume = {101},
  number = {6},
  pages = {435--443},
  issn = {1460-2725},
  doi = {10.1093/qjmed/hcn036},
  urldate = {2019-11-12},
  abstract = {Abstract. Aim: To assess the clinical management of adults presenting with sudden, severe headache.Methods: We retrospectively reviewed the medical records of},
  langid = {english}
}

@article{breenEmergencyDepartmentEvaluation2008a,
  title = {Emergency Department Evaluation of Sudden, Severe Headache},
  author = {Breen, D. P. and Duncan, C. W. and Pope, A. E. and Gray, A. J. and {Al-Shahi Salman}, R.},
  year = {2008},
  month = jun,
  journal = {QJM: An International Journal of Medicine},
  volume = {101},
  number = {6},
  pages = {435--443},
  issn = {1460-2725},
  doi = {10.1093/qjmed/hcn036},
  urldate = {2019-11-12},
  abstract = {Abstract.  Aim: To assess the clinical management of adults presenting with sudden, severe headache.Methods: We retrospectively reviewed the medical records of},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\586S7W9T\\Breen et al. - 2008 - Emergency department evaluation of sudden, severe .pdf;C\:\\Users\\cleme\\Zotero\\storage\\DECJMUXS\\1549287.html}
}

@inproceedings{brody2022how,
  title = {How Attentive Are Graph Attention Networks?},
  booktitle = {International Conference on Learning Representations},
  author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
  year = {2022}
}

@inproceedings{brody2022how,
  title = {How Attentive Are Graph Attention Networks?},
  booktitle = {International Conference on Learning Representations},
  author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
  year = {2022}
}

@article{bruceDiagnosticAccuracyUse2013,
  title = {Diagnostic Accuracy and Use of Nonmydriatic Ocular Fundus Photography by Emergency Physicians: {{Phase II}} of the {{FOTO-ED}} Study},
  shorttitle = {Diagnostic Accuracy and Use of Nonmydriatic Ocular Fundus Photography by Emergency Physicians},
  author = {Bruce, Beau B. and Thulasi, Praneetha and Fraser, Clare L. and Keadey, Matthew T. and Ward, Antoinette and Heilpern, Katherine L. and Wright, David W. and Newman, Nancy J. and Biousse, Val{\'e}rie},
  year = {2013},
  month = jul,
  journal = {Annals of Emergency Medicine},
  volume = {62},
  number = {1},
  pages = {28-33.e1},
  issn = {1097-6760},
  doi = {10.1016/j.annemergmed.2013.01.010},
  abstract = {STUDY OBJECTIVE: During the first phase of the Fundus Photography vs Ophthalmoscopy Trial Outcomes in the Emergency Department study, 13\% (44/350; 95\% confidence interval [CI] 9\% to 17\%) of patients had an ocular fundus finding, such as papilledema, relevant to their emergency department (ED) management found by nonmydriatic ocular fundus photography reviewed by neuro-ophthalmologists. All of these findings were missed by emergency physicians, who examined only 14\% of enrolled patients by direct ophthalmoscopy. In the present study, we evaluate the sensitivity of nonmydriatic ocular fundus photography, an alternative to direct ophthalmoscopy, for relevant findings when photographs are made available for use by emergency physicians during routine clinical care. METHODS: Three hundred fifty-four patients presenting to our ED with headache, focal neurologic deficit, visual change, or diastolic blood pressure greater than or equal to 120 mm Hg had nonmydriatic fundus photography obtained (Kowa nonmydriatic {$\alpha$}-D). Photographs were placed on the electronic medical record for emergency physician review. Identification of relevant findings on photographs by emergency physicians was compared with a reference standard of neuro-ophthalmologist review. RESULTS: Emergency physicians reviewed photographs of 239 patients (68\%). Thirty-five patients (10\%; 95\% CI 7\% to 13\%) had relevant findings identified by neuro-ophthalmologist review (6 disc edema, 6 grade III/IV hypertensive retinopathy, 7 isolated hemorrhages, 15 optic disc pallor, and 1 retinal vascular occlusion). Emergency physicians identified 16 of 35 relevant findings (sensitivity 46\%; 95\% CI 29\% to 63\%) and also identified 289 of 319 normal findings (specificity 91\%; 95\% CI 87\% to 94\%). Emergency physicians reported that photographs were helpful for 125 patients (35\%). CONCLUSION: Emergency physicians used nonmydriatic fundus photographs more frequently than they performed direct ophthalmoscopy, and their detection of relevant abnormalities improved. Ocular fundus photography often assisted ED care even when results were normal. Nonmydriatic ocular fundus photography offers a promising alternative to direct ophthalmoscopy.},
  langid = {english},
  pmcid = {PMC3722897},
  pmid = {23433654},
  keywords = {Adult,Cohort Studies,Electronic Health Records,Emergency Medicine,Emergency Service,Female,Fundus Oculi,Hospital,Humans,Male,Middle Aged,Mydriatics,Ophthalmoscopy,Photography,Prospective Studies,Retinal Diseases,Sensitivity and Specificity}
}

@article{bruceDiagnosticAccuracyUse2013a,
  title = {Diagnostic Accuracy and Use of Nonmydriatic Ocular Fundus Photography by Emergency Physicians: Phase {{II}} of the {{FOTO-ED}} Study},
  shorttitle = {Diagnostic Accuracy and Use of Nonmydriatic Ocular Fundus Photography by Emergency Physicians},
  author = {Bruce, Beau B. and Thulasi, Praneetha and Fraser, Clare L. and Keadey, Matthew T. and Ward, Antoinette and Heilpern, Katherine L. and Wright, David W. and Newman, Nancy J. and Biousse, Val{\'e}rie},
  year = {2013},
  month = jul,
  journal = {Annals of Emergency Medicine},
  volume = {62},
  number = {1},
  pages = {28-33.e1},
  issn = {1097-6760},
  doi = {10.1016/j.annemergmed.2013.01.010},
  abstract = {STUDY OBJECTIVE: During the first phase of the Fundus Photography vs Ophthalmoscopy Trial Outcomes in the Emergency Department study, 13\% (44/350; 95\% confidence interval [CI] 9\% to 17\%) of patients had an ocular fundus finding, such as papilledema, relevant to their emergency department (ED) management found by nonmydriatic ocular fundus photography reviewed by neuro-ophthalmologists. All of these findings were missed by emergency physicians, who examined only 14\% of enrolled patients by direct ophthalmoscopy. In the present study, we evaluate the sensitivity of nonmydriatic ocular fundus photography, an alternative to direct ophthalmoscopy, for relevant findings when photographs are made available for use by emergency physicians during routine clinical care. METHODS: Three hundred fifty-four patients presenting to our ED with headache, focal neurologic deficit, visual change, or diastolic blood pressure greater than or equal to 120 mm Hg had nonmydriatic fundus photography obtained (Kowa nonmydriatic {$\alpha$}-D). Photographs were placed on the electronic medical record for emergency physician review. Identification of relevant findings on photographs by emergency physicians was compared with a reference standard of neuro-ophthalmologist review. RESULTS: Emergency physicians reviewed photographs of 239 patients (68\%). Thirty-five patients (10\%; 95\% CI 7\% to 13\%) had relevant findings identified by neuro-ophthalmologist review (6 disc edema, 6 grade III/IV hypertensive retinopathy, 7 isolated hemorrhages, 15 optic disc pallor, and 1 retinal vascular occlusion). Emergency physicians identified 16 of 35 relevant findings (sensitivity 46\%; 95\% CI 29\% to 63\%) and also identified 289 of 319 normal findings (specificity 91\%; 95\% CI 87\% to 94\%). Emergency physicians reported that photographs were helpful for 125 patients (35\%). CONCLUSION: Emergency physicians used nonmydriatic fundus photographs more frequently than they performed direct ophthalmoscopy, and their detection of relevant abnormalities improved. Ocular fundus photography often assisted ED care even when results were normal. Nonmydriatic ocular fundus photography offers a promising alternative to direct ophthalmoscopy.},
  langid = {english},
  pmcid = {PMC3722897},
  pmid = {23433654},
  keywords = {Adult,Cohort Studies,Electronic Health Records,Emergency Medicine,Emergency Service Hospital,Female,Fundus Oculi,Humans,Male,Middle Aged,Mydriatics,Ophthalmoscopy,Photography,Prospective Studies,Retinal Diseases,Sensitivity and Specificity},
  file = {C:\Users\cleme\Zotero\storage\G3GU4H3V\Bruce et al. - 2013 - Diagnostic accuracy and use of nonmydriatic ocular.pdf}
}

@article{bruceFeasibilityNonmydriaticOcular2011,
  title = {Feasibility of Nonmydriatic Ocular Fundus Photography in the Emergency Department: {{Phase I}} of the {{FOTO-ED}} Study},
  shorttitle = {Feasibility of Nonmydriatic Ocular Fundus Photography in the Emergency Department},
  author = {Bruce, Beau B. and Lamirel, C{\'e}dric and Biousse, Val{\'e}rie and Ward, Antionette and Heilpern, Katherine L. and Newman, Nancy J. and Wright, David W.},
  year = {2011},
  month = sep,
  journal = {Academic Emergency Medicine: Official Journal of the Society for Academic Emergency Medicine},
  volume = {18},
  number = {9},
  pages = {928--933},
  issn = {1553-2712},
  doi = {10.1111/j.1553-2712.2011.01147.x},
  abstract = {OBJECTIVES: Examination of the ocular fundus is imperative in many acute medical and neurologic conditions, but direct ophthalmoscopy by nonophthalmologists is underutilized, poorly performed, and difficult without pharmacologic pupillary dilation. The objective was to examine the feasibility of nonmydriatic fundus photography as a clinical alternative to direct ophthalmoscopy by emergency physicians (EPs). METHODS: Adult patients presenting to the emergency department (ED) with headache, acute focal neurologic deficit, diastolic blood pressure {$\geq$} 120 mm Hg, or acute visual change had ocular fundus photographs taken by nurse practitioners using a nonmydriatic fundus camera. Photographs were reviewed by a neuroophthalmologist within 24 hours for findings relevant to acute ED patient care. Nurse practitioners and patients rated ease, comfort, and speed of nonmydriatic fundus photography on a 10-point Likert scale (10 best). Timing of visit and photography were recorded by automated electronic systems. RESULTS: A total of 350 patients were enrolled. There were 1,734 photographs taken during 230 nurse practitioner shifts. Eighty-three percent of the 350 patients had at least one eye with a high-quality photograph, while only 3\% of patients had no photographs of diagnostic value. Mean ratings were {$\geq$} 8.7 (standard deviation [SD] {$\leq$} 1.9) for all measures. The median photography session lasted 1.9 minutes (interquartile range [IQR] = 1.3 to 2.9 minutes), typically accounting for less that 0.5\% of the patient's total ED visit. CONCLUSIONS: Nonmydriatic fundus photography taken by nurse practitioners is a feasible alternative to direct ophthalmoscopy in the ED. It is performed well by nonphysician staff, is well-received by staff and patients, and requires a trivial amount of time to perform.},
  langid = {english},
  pmcid = {PMC3172688},
  pmid = {21906202},
  keywords = {Adult,Emergency Service,Eye Diseases,Female,Fundus Oculi,Hospital,Humans,Male,Middle Aged,Mydriatics,Nurse Practitioners,Ophthalmoscopy,Outcome and Process Assessment (Health Care),Photography,Physicians,Prospective Studies}
}

@article{bruceFeasibilityNonmydriaticOcular2011a,
  title = {Feasibility of Nonmydriatic Ocular Fundus Photography in the Emergency Department: {{Phase I}} of the {{FOTO-ED}} Study},
  shorttitle = {Feasibility of Nonmydriatic Ocular Fundus Photography in the Emergency Department},
  author = {Bruce, Beau B. and Lamirel, C{\'e}dric and Biousse, Val{\'e}rie and Ward, Antionette and Heilpern, Katherine L. and Newman, Nancy J. and Wright, David W.},
  year = {2011},
  month = sep,
  journal = {Academic Emergency Medicine: Official Journal of the Society for Academic Emergency Medicine},
  volume = {18},
  number = {9},
  pages = {928--933},
  issn = {1553-2712},
  doi = {10.1111/j.1553-2712.2011.01147.x},
  abstract = {OBJECTIVES: Examination of the ocular fundus is imperative in many acute medical and neurologic conditions, but direct ophthalmoscopy by nonophthalmologists is underutilized, poorly performed, and difficult without pharmacologic pupillary dilation. The objective was to examine the feasibility of nonmydriatic fundus photography as a clinical alternative to direct ophthalmoscopy by emergency physicians (EPs). METHODS: Adult patients presenting to the emergency department (ED) with headache, acute focal neurologic deficit, diastolic blood pressure {$\geq$} 120 mm Hg, or acute visual change had ocular fundus photographs taken by nurse practitioners using a nonmydriatic fundus camera. Photographs were reviewed by a neuroophthalmologist within 24 hours for findings relevant to acute ED patient care. Nurse practitioners and patients rated ease, comfort, and speed of nonmydriatic fundus photography on a 10-point Likert scale (10 best). Timing of visit and photography were recorded by automated electronic systems. RESULTS: A total of 350 patients were enrolled. There were 1,734 photographs taken during 230 nurse practitioner shifts. Eighty-three percent of the 350 patients had at least one eye with a high-quality photograph, while only 3\% of patients had no photographs of diagnostic value. Mean ratings were {$\geq$} 8.7 (standard deviation [SD] {$\leq$} 1.9) for all measures. The median photography session lasted 1.9 minutes (interquartile range [IQR] = 1.3 to 2.9 minutes), typically accounting for less that 0.5\% of the patient's total ED visit. CONCLUSIONS: Nonmydriatic fundus photography taken by nurse practitioners is a feasible alternative to direct ophthalmoscopy in the ED. It is performed well by nonphysician staff, is well-received by staff and patients, and requires a trivial amount of time to perform.},
  langid = {english},
  pmcid = {PMC3172688},
  pmid = {21906202},
  keywords = {Adult,Emergency Service Hospital,Eye Diseases,Female,Fundus Oculi,Humans,Male,Middle Aged,Mydriatics,Nurse Practitioners,Ophthalmoscopy,Outcome and Process Assessment (Health Care),Photography,Physicians,Prospective Studies},
  file = {C:\Users\cleme\Zotero\storage\35XHHF93\Bruce et al. - 2011 - Feasibility of nonmydriatic ocular fundus photogra.pdf}
}

@article{bruceFundusPhotographyVs2018,
  title = {Fundus {{Photography}} vs. {{Ophthalmoscopy Outcomes}} in the {{Emergency Department}} ({{FOTO-ED}}) {{Phase III}}: {{Web-based}}, {{In-service Training}} of {{Emergency Providers}}},
  shorttitle = {Fundus {{Photography}} vs. {{Ophthalmoscopy Outcomes}} in the {{Emergency Department}} ({{FOTO-ED}}) {{Phase III}}},
  author = {Bruce, Beau B. and Bidot, Samuel and Hage, Rabih and Clough, Lindsay C. and {Fajoles-Vasseneix}, Caroline and Melomed, Mikhail and Keadey, Matthew T. and Wright, David W. and Newman, Nancy J. and Biousse, Val{\'e}rie},
  year = {2018},
  month = jan,
  journal = {Neuro-Ophthalmology},
  volume = {42},
  number = {5},
  pages = {269--274},
  issn = {0165-8107},
  doi = {10.1080/01658107.2017.1419368},
  urldate = {2019-11-12},
  abstract = {We evaluated a web-based training aimed at improving the review of fundus photography by emergency providers. 587 patients were included, 12.6\% with relevant abnormalities. Emergency providers spent 31 minutes (median) training and evaluated 359 patients. Median post-test score improvement was 6 percentage points (IQR: 2--14; p = 0.06). Pre- vs. post-training, the emergency providers reviewed 45\% vs. 43\% of photographs; correctly identified abnormals in 67\% vs. 57\% of cases; and correctly identified normals in 80\% vs. 84\%. The Fundus photography vs. Ophthalmoscopy Trial Outcomes in the Emergency Department studies have demonstrated that emergency providers perform substantially better with fundus photography than direct ophthalmoscopy, but our web-based, in-service training did not result in further improvements at our institution.},
  pmcid = {PMC6152496},
  pmid = {30258471}
}

@article{bruceFundusPhotographyVs2018a,
  title = {Fundus {{Photography}} vs. {{Ophthalmoscopy Outcomes}} in the {{Emergency Department}} ({{FOTO-ED}}) {{Phase III}}: {{Web-based}}, {{In-service Training}} of {{Emergency Providers}}},
  shorttitle = {Fundus {{Photography}} vs. {{Ophthalmoscopy Outcomes}} in the {{Emergency Department}} ({{FOTO-ED}}) {{Phase III}}},
  author = {Bruce, Beau B. and Bidot, Samuel and Hage, Rabih and Clough, Lindsay C. and {Fajoles-Vasseneix}, Caroline and Melomed, Mikhail and Keadey, Matthew T. and Wright, David W. and Newman, Nancy J. and Biousse, Val{\'e}rie},
  year = {2018},
  month = jan,
  journal = {Neuro-Ophthalmology},
  volume = {42},
  number = {5},
  pages = {269--274},
  issn = {0165-8107},
  doi = {10.1080/01658107.2017.1419368},
  urldate = {2019-11-12},
  abstract = {We evaluated a web-based training aimed at improving the review of fundus photography by emergency providers. 587 patients were included, 12.6\% with relevant abnormalities. Emergency providers spent 31~minutes (median) training and evaluated 359 patients. Median post-test score improvement was 6 percentage points (IQR: 2--14; p~=~0.06). Pre- vs. post-training, the emergency providers reviewed 45\% vs. 43\% of photographs; correctly identified abnormals in 67\% vs. 57\% of cases; and correctly identified normals in 80\% vs. 84\%. The Fundus photography vs. Ophthalmoscopy Trial Outcomes in the Emergency Department studies have demonstrated that emergency providers perform substantially better with fundus photography than direct ophthalmoscopy, but our web-based, in-service training did not result in further improvements at our institution.},
  pmcid = {PMC6152496},
  pmid = {30258471},
  file = {C:\Users\cleme\Zotero\storage\7DTVDGWF\Bruce et al. - 2018 - Fundus Photography vs. Ophthalmoscopy Outcomes in .pdf}
}

@article{brunaIntriguingPropertiesNeural2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Bruna, Joan and Szegedy, Christian and Sutskever, Ilya and Goodfellow, Ian and Zaremba, Wojciech and Fergus, Rob and Erhan, Dumitru},
  year = {2013},
  month = dec,
  urldate = {2023-11-30},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  langid = {english}
}

@article{brunaIntriguingPropertiesNeural2013a,
  title = {Intriguing Properties of Neural Networks},
  author = {Bruna, Joan and Szegedy, Christian and Sutskever, Ilya and Goodfellow, Ian and Zaremba, Wojciech and Fergus, Rob and Erhan, Dumitru},
  year = {2013},
  month = dec,
  urldate = {2023-11-30},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. Specifically, we find that we can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  langid = {english}
}

@article{brunaSpectralNetworksLocally2014,
  title = {Spectral {{Networks}} and {{Locally Connected Networks}} on {{Graphs}}},
  author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
  year = {2014},
  month = may,
  journal = {arXiv:1312.6203 [cs]},
  eprint = {1312.6203},
  primaryclass = {cs},
  urldate = {2019-11-28},
  abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{brunaSpectralNetworksLocally2014a,
  title = {Spectral {{Networks}} and {{Locally Connected Networks}} on {{Graphs}}},
  author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
  year = {2014},
  month = may,
  journal = {arXiv:1312.6203 [cs]},
  eprint = {1312.6203},
  primaryclass = {cs},
  urldate = {2019-11-28},
  abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for lowdimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\cleme\Zotero\storage\UIJVHI6U\Bruna et al. - 2014 - Spectral Networks and Locally Connected Networks o.pdf}
}

@article{burkartSurveyExplainabilitySupervised2021a,
  title = {A {{Survey}} on the {{Explainability}} of {{Supervised Machine Learning}}},
  author = {Burkart, Nadia and Huber, Marco F.},
  year = {2021},
  month = jan,
  journal = {Journal of Artificial Intelligence Research},
  volume = {70},
  pages = {245--317},
  issn = {1076-9757},
  doi = {10.1613/jair.1.12228},
  urldate = {2023-04-23},
  abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {knowledge discovery,machine learning,neural networks,rule learning},
  file = {C:\Users\cleme\Zotero\storage\WYIHDIKU\Burkart et Huber - 2021 - A Survey on the Explainability of Supervised Machi.pdf}
}

@article{burnieTelemedecineTeleophtalmologie2003,
  title = {La T{\'e}l{\'e}m{\'e}decine et La T{\'e}l{\'e}-Ophtalmologie},
  author = {Burnie, Miguel N.},
  year = {2003},
  month = aug,
  journal = {Canadian Journal of Ophthalmology},
  volume = {38},
  number = {5},
  pages = {345},
  issn = {0008-4182, 1715-3360},
  doi = {10.1016/S0008-4182(03)80043-1},
  urldate = {2019-12-24},
  langid = {english}
}

@article{burnieTelemedecineTeleophtalmologie2003a,
  title = {La T{\'e}l{\'e}m{\'e}decine et La T{\'e}l{\'e}-Ophtalmologie},
  author = {Burnie, Miguel N.},
  year = {2003},
  month = aug,
  journal = {Canadian Journal of Ophthalmology},
  volume = {38},
  number = {5},
  pages = {345},
  issn = {0008-4182, 1715-3360},
  doi = {10.1016/S0008-4182(03)80043-1},
  urldate = {2019-12-24},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\BWTLZN56\\burnie2003.pdf;C\:\\Users\\cleme\\Zotero\\storage\\A7BCZIFV\\pdf.html}
}

@article{caiAchieveMinimumWidth2023,
  title = {Achieve the {{Minimum Width}} of {{Neural Net-}} Works for {{Universal Approximation}}},
  author = {Cai, Yongqiang},
  year = {2023},
  abstract = {The universal approximation property (UAP) of neural networks is fundamental for deep learning, and it is well known that wide neural networks are universal approximators of continuous functions within both the Lp norm and the continuous/uniform norm. However, the exact minimum width, wmin, for the UAP has not been studied thoroughly. Recently, using a decoder-memorizer-encoder scheme, Park et al. (2021) found that wmin = max(dx + 1, dy) for both the Lp-UAP of ReLU networks and the C-UAP of ReLU+STEP networks, where dx, dy are the input and output dimensions, respectively. In this paper, we consider neural networks with an arbitrary set of activation functions. We prove that both C-UAP and Lp-UAP for functions on compact domains share a universal lower bound of the minimal width; that is, wm\_{$\ast$} in = max(dx, dy). In particular, the critical width, wm\_{$\ast$} in, for Lp-UAP can be achieved by leaky-ReLU networks, provided that the input or output dimension is larger than one. Our construction is based on the approximation power of neural ordinary differential equations and the ability to approximate flow maps by neural networks. The nonmonotone or discontinuous activation functions case and the one-dimensional case are also discussed.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\V4CWBWG9\Cai - 2023 - Achieve the Minimum Width of Neural Net- works for.pdf}
}

@misc{caiAchieveMinimumWidth2023b,
  title = {Achieve the {{Minimum Width}} of {{Neural Networks}} for {{Universal Approximation}}},
  author = {Cai, Yongqiang},
  year = {2023},
  month = feb,
  number = {arXiv:2209.11395},
  eprint = {2209.11395},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.11395},
  urldate = {2023-06-19},
  abstract = {The universal approximation property (UAP) of neural networks is fundamental for deep learning, and it is well known that wide neural networks are universal approximators of continuous functions within both the \$L{\textasciicircum}p\$ norm and the continuous/uniform norm. However, the exact minimum width, \$w\_\{{\textbackslash}min\}\$, for the UAP has not been studied thoroughly. Recently, using a decoder-memorizer-encoder scheme, {\textbackslash}citet\{Park2021Minimum\} found that \$w\_\{{\textbackslash}min\} = {\textbackslash}max(d\_x+1,d\_y)\$ for both the \$L{\textasciicircum}p\$-UAP of ReLU networks and the \$C\$-UAP of ReLU+STEP networks, where \$d\_x,d\_y\$ are the input and output dimensions, respectively. In this paper, we consider neural networks with an arbitrary set of activation functions. We prove that both \$C\$-UAP and \$L{\textasciicircum}p\$-UAP for functions on compact domains share a universal lower bound of the minimal width; that is, \$w{\textasciicircum}*\_\{{\textbackslash}min\} = {\textbackslash}max(d\_x,d\_y)\$. In particular, the critical width, \$w{\textasciicircum}*\_\{{\textbackslash}min\}\$, for \$L{\textasciicircum}p\$-UAP can be achieved by leaky-ReLU networks, provided that the input or output dimension is larger than one. Our construction is based on the approximation power of neural ordinary differential equations and the ability to approximate flow maps by neural networks. The nonmonotone or discontinuous activation functions case and the one-dimensional case are also discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\Q372EY6Y\\Cai - 2023 - Achieve the Minimum Width of Neural Networks for U.pdf;C\:\\Users\\cleme\\Zotero\\storage\\TLLT2GZ7\\2209.html}
}

@misc{caiEfficientViTEnhancedLinear2022,
  title = {{{EfficientViT}}: {{Enhanced Linear Attention}} for {{High-Resolution Low-Computation Visual Recognition}}},
  shorttitle = {{{EfficientViT}}},
  author = {Cai, Han and Gan, Chuang and Hu, Muyan and Han, Song},
  year = {2022},
  month = sep,
  number = {arXiv:2205.14756},
  eprint = {2205.14756},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-19},
  abstract = {Vision Transformer (ViT) has achieved remarkable performance in many vision tasks. However, ViT is inferior to convolutional neural networks (CNNs) when targeting high-resolution mobile vision applications. The key computational bottleneck of ViT is the softmax attention module which has quadratic computational complexity with the input resolution. It is essential to reduce the cost of ViT to deploy it on edge devices. Existing methods (e.g., Swin, PVT) restrict the softmax attention within local windows or reduce the resolution of key/value tensors to reduce the cost, which sacrifices ViT's core advantages on global feature extractions. In this work, we present EfficientViT, an efficient ViT architecture for high-resolution low-computation visual recognition. Instead of restricting the softmax attention, we propose to replace softmax attention with linear attention while enhancing its local feature extraction ability with depthwise convolution. EfficientViT maintains global and local feature extraction capability while enjoying linear computational complexity. Extensive experiments on COCO object detection and Cityscapes semantic segmentation demonstrate the effectiveness of our method. On the COCO dataset, EfficientViT achieves 42.6 AP with 4.4G MACs, surpassing EfficientDet-D1 by 2.4 AP while having 27.9\% fewer MACs. On Cityscapes, EfficientViT reaches 78.7 mIoU with 19.1G MACs, outperforming SegFormer by 2.5 mIoU while requiring less than 1/3 the computational cost. On Qualcomm Snapdragon 855 CPU, EfficientViT is 3{\texttimes} faster than EfficientNet while achieving higher ImageNet accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{caiEfficientViTEnhancedLinear2022a,
  title = {{{EfficientViT}}: {{Enhanced Linear Attention}} for {{High-Resolution Low-Computation Visual Recognition}}},
  shorttitle = {{{EfficientViT}}},
  author = {Cai, Han and Gan, Chuang and Hu, Muyan and Han, Song},
  year = {2022},
  month = sep,
  number = {arXiv:2205.14756},
  eprint = {2205.14756},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-19},
  abstract = {Vision Transformer (ViT) has achieved remarkable performance in many vision tasks. However, ViT is inferior to convolutional neural networks (CNNs) when targeting high-resolution mobile vision applications. The key computational bottleneck of ViT is the softmax attention module which has quadratic computational complexity with the input resolution. It is essential to reduce the cost of ViT to deploy it on edge devices. Existing methods (e.g., Swin, PVT) restrict the softmax attention within local windows or reduce the resolution of key/value tensors to reduce the cost, which sacrifices ViT's core advantages on global feature extractions. In this work, we present EfficientViT, an efficient ViT architecture for high-resolution low-computation visual recognition. Instead of restricting the softmax attention, we propose to replace softmax attention with linear attention while enhancing its local feature extraction ability with depthwise convolution. EfficientViT maintains global and local feature extraction capability while enjoying linear computational complexity. Extensive experiments on COCO object detection and Cityscapes semantic segmentation demonstrate the effectiveness of our method. On the COCO dataset, EfficientViT achieves 42.6 AP with 4.4G MACs, surpassing EfficientDet-D1 by 2.4 AP while having 27.9\% fewer MACs. On Cityscapes, EfficientViT reaches 78.7 mIoU with 19.1G MACs, outperforming SegFormer by 2.5 mIoU while requiring less than 1/3 the computational cost. On Qualcomm Snapdragon 855 CPU, EfficientViT is 3{\texttimes} faster than EfficientNet while achieving higher ImageNet accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\XQSDXMA5\Cai et al. - 2022 - EfficientViT Enhanced Linear Attention for High-R.pdf}
}

@article{caoDeepLearningBased2018,
  title = {Deep {{Learning}} Based {{Inter-Modality Image Registration Supervised}} by {{Intra-Modality Similarity}}},
  author = {Cao, Xiaohuan and Yang, Jianhua and Wang, Li and Xue, Zhong and Wang, Qian and Shen, Dinggang},
  year = {2018},
  month = sep,
  journal = {Machine learning in medical imaging. MLMI (Workshop)},
  volume = {11046},
  pages = {55--63},
  doi = {10.1007/978-3-030-00919-9_7},
  urldate = {2019-07-25},
  abstract = {Non-rigid inter-modality registration can facilitate accurate information fusion from different modalities, but it is challenging due to the very different image appearances across modalities. In this paper, we propose to train a non-rigid inter-modality image registration network, which can directly predict the transformation field from the input multimodal images, such as CT and MR images. In particular, the training of our inter-modality registration network is supervised by intra-modality similarity metric based on the available paired data, which is derived from a pre-aligned CT and MR dataset. Specifically, in the training stage, to register the input CT and MR images, their similarity is evaluated on the warped MR image and the MR image that is paired with the input CT. So that, the intra-modality similarity metric can be directly applied to measure whether the input CT and MR images are well registered. Moreover, we use the idea of dual-modality fashion, in which we measure the similarity on both CT modality and MR modality. In this way, the complementary anatomies in both modalities can be jointly considered to more accurately train the inter-modality registration network. In the testing stage, the trained inter-modality registration network can be directly applied to register the new multimodal images without any paired data. Experimental results have shown that, the proposed method can achieve promising accuracy and efficiency for the challenging non-rigid inter-modality registration task and also outperforms the state-of-the-art approaches.},
  pmcid = {PMC6516490},
  pmid = {31098597}
}

@article{caoDeepLearningBased2018a,
  title = {Deep {{Learning}} Based {{Inter-Modality Image Registration Supervised}} by {{Intra-Modality Similarity}}},
  author = {Cao, Xiaohuan and Yang, Jianhua and Wang, Li and Xue, Zhong and Wang, Qian and Shen, Dinggang},
  year = {2018},
  month = sep,
  journal = {Machine learning in medical imaging. MLMI (Workshop)},
  volume = {11046},
  pages = {55--63},
  doi = {10.1007/978-3-030-00919-9_7},
  urldate = {2019-07-25},
  abstract = {Non-rigid inter-modality registration can facilitate accurate information fusion from different modalities, but it is challenging due to the very different image appearances across modalities. In this paper, we propose to train a non-rigid inter-modality image registration network, which can directly predict the transformation field from the input multimodal images, such as CT and MR images. In particular, the training of our inter-modality registration network is supervised by intra-modality similarity metric based on the available paired data, which is derived from a pre-aligned CT and MR dataset. Specifically, in the training stage, to register the input CT and MR images, their similarity is evaluated on the warped MR image and the MR image that is paired with the input CT. So that, the intra-modality similarity metric can be directly applied to measure whether the input CT and MR images are well registered. Moreover, we use the idea of dual-modality fashion, in which we measure the similarity on both CT modality and MR modality. In this way, the complementary anatomies in both modalities can be jointly considered to more accurately train the inter-modality registration network. In the testing stage, the trained inter-modality registration network can be directly applied to register the new multimodal images without any paired data. Experimental results have shown that, the proposed method can achieve promising accuracy and efficiency for the challenging non-rigid inter-modality registration task and also outperforms the state-of-the-art approaches.},
  pmcid = {PMC6516490},
  pmid = {31098597},
  file = {C:\Users\cleme\Zotero\storage\NG2QYFLG\Cao et al. - 2018 - Deep Learning based Inter-Modality Image Registrat.pdf}
}

@inproceedings{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {213--229},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58452-8_13},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  isbn = {978-3-030-58452-8},
  langid = {english}
}

@inproceedings{carionEndtoEndObjectDetection2020a,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {213--229},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58452-8_13},
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  isbn = {978-3-030-58452-8},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\QVLKE3AR\Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf}
}

@article{carmonaIdentificationOpticNerve2008,
  title = {Identification of the Optic Nerve Head with Genetic Algorithms},
  author = {Carmona, Enrique J. and Rinc{\'o}n, Mariano and {Garc{\'i}a-Feijo{\'o}}, Juli{\'a}n and {Mart{\'i}nez-de-la-Casa}, Jos{\'e} M.},
  year = {2008},
  month = jul,
  journal = {Artificial Intelligence in Medicine},
  volume = {43},
  number = {3},
  pages = {243--259},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2008.04.005},
  abstract = {OBJECTIVE: This work proposes creating an automatic system to locate and segment the optic nerve head (ONH) in eye fundus photographic images using genetic algorithms. METHODS AND MATERIAL: Domain knowledge is used to create a set of heuristics that guide the various steps involved in the process. Initially, using an eye fundus colour image as input, a set of hypothesis points was obtained that exhibited geometric properties and intensity levels similar to the ONH contour pixels. Next, a genetic algorithm was used to find an ellipse containing the maximum number of hypothesis points in an offset of its perimeter, considering some constraints. The ellipse thus obtained is the approximation to the ONH. The segmentation method is tested in a sample of 110 eye fundus images, belonging to 55 patients with glaucoma (23.1\%) and eye hypertension (76.9\%) and random selected from an eye fundus image base belonging to the Ophthalmology Service at Miguel Servet Hospital, Saragossa (Spain). RESULTS AND CONCLUSIONS: The results obtained are competitive with those in the literature. The method's generalization capability is reinforced when it is applied to a different image base from the one used in our study and a discrepancy curve is obtained very similar to the one obtained in our image base. In addition, the robustness of the method proposed can be seen in the high percentage of images obtained with a discrepancy delta{$<$}5 (96\% and 99\% in our and a different image base, respectively). The results also confirm the hypothesis that the ONH contour can be properly approached with a non-deformable ellipse. Another important aspect of the method is that it directly provides the parameters characterising the shape of the papilla: lengths of its major and minor axes, its centre of location and its orientation with regard to the horizontal position.},
  langid = {english},
  pmid = {18534830},
  keywords = {Algorithms,Artificial Intelligence,Cataract,Computer-Assisted,Genetics,Glaucoma,Humans,Image Interpretation,Ocular Hypertension,Optic Disk,Population,Reproducibility of Results}
}

@article{carmonaIdentificationOpticNerve2008a,
  title = {Identification of the Optic Nerve Head with Genetic Algorithms},
  author = {Carmona, Enrique J. and Rinc{\'o}n, Mariano and {Garc{\'i}a-Feijo{\'o}}, Juli{\'a}n and {Mart{\'i}nez-de-la-Casa}, Jos{\'e} M.},
  year = {2008},
  month = jul,
  journal = {Artificial Intelligence in Medicine},
  volume = {43},
  number = {3},
  pages = {243--259},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2008.04.005},
  abstract = {OBJECTIVE: This work proposes creating an automatic system to locate and segment the optic nerve head (ONH) in eye fundus photographic images using genetic algorithms. METHODS AND MATERIAL: Domain knowledge is used to create a set of heuristics that guide the various steps involved in the process. Initially, using an eye fundus colour image as input, a set of hypothesis points was obtained that exhibited geometric properties and intensity levels similar to the ONH contour pixels. Next, a genetic algorithm was used to find an ellipse containing the maximum number of hypothesis points in an offset of its perimeter, considering some constraints. The ellipse thus obtained is the approximation to the ONH. The segmentation method is tested in a sample of 110 eye fundus images, belonging to 55 patients with glaucoma (23.1\%) and eye hypertension (76.9\%) and random selected from an eye fundus image base belonging to the Ophthalmology Service at Miguel Servet Hospital, Saragossa (Spain). RESULTS AND CONCLUSIONS: The results obtained are competitive with those in the literature. The method's generalization capability is reinforced when it is applied to a different image base from the one used in our study and a discrepancy curve is obtained very similar to the one obtained in our image base. In addition, the robustness of the method proposed can be seen in the high percentage of images obtained with a discrepancy delta{$<$}5 (96\% and 99\% in our and a different image base, respectively). The results also confirm the hypothesis that the ONH contour can be properly approached with a non-deformable ellipse. Another important aspect of the method is that it directly provides the parameters characterising the shape of the papilla: lengths of its major and minor axes, its centre of location and its orientation with regard to the horizontal position.},
  langid = {english},
  pmid = {18534830},
  keywords = {Algorithms,Artificial Intelligence,Cataract,Genetics,Glaucoma,Humans,Image Interpretation Computer-Assisted,Ocular Hypertension,Optic Disk,Population,Reproducibility of Results},
  file = {C:\Users\cleme\Zotero\storage\JWYN8UJS\carmona2008.html}
}

@article{carrClinicalApplicabilityDeep2022,
  title = {Clinical Applicability of Deep Learning to Predict Retinal Disease from Small {{OCT}} Datasets},
  author = {Carr, Thomas and Broadway, David and Sanderson, Julie and Sami, Saber},
  year = {2022},
  month = jun,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {63},
  number = {7},
  pages = {730 -- F0458},
  issn = {1552-5783},
  abstract = {Interpretable staged transfer learning (iSTL), a pipeline for improving image classification with small sample sizes, was trained to carry out classification of optical coherence tomography (OCT) images. Model attention was visualised using SHAP maps, allowing interpretation of its behaviour. A disease classification task using the Inception-v4 convolutional neural network architecture and iSTL training pipeline was carried out. Target OCT images are available under licence (Gholami et al). Normal (n=206), macular hole (MH: n=104), diabetic retinopathy (DR; n=109), and central serous retinopathy (CSR; n=104) scans were used. 50 images were used for data augmentation and training, the remainder were used for validation. The model was pre-trained on ImageNet, given randomly initialised output layers; the early layers frozen. The model was then trained on an intermediate bridge dataset (Kermany et al), output layers replaced, and final training carried out on the target dataset. 10 models were trained, the best performing selected for SHAP attention maps visualisation. SHAP maps showed both image and disease specific features were allocated importance during prediction. High importance areas have a large impact on the model's prediction, low importance areas have less impact. Classifying CSR, high importance was allocated to areas surrounding posterior epithelial detachment and not to areas of subretinal fluid. With DR, retinal microaneurysms and intraretinal oedema were highlighted but subretinal fluid was not. MH scans saw strong regional importance allocated to the vertical edge of full-thickness holes and surrounding intraretinal cysts when present. Normal images typically presented with medium to high importance on the retinal pigment epithelium adjacent to the macula as well as the inner limiting membrane surface. Many images saw moderate attention within areas of the choroid and vitreous that had no apparent clinical importance. When present in the scan field the optic nerve head was not allocated high importance. Confounding factors are a concern when training deep learning models, particularly with small datasets. The iSTL model appeared to predominantly use clinically applicable features to make predictions. Further work is needed to determine whether some features used to make predictions are confounds or genuine clinical features related to disease biomarkers. This abstract was presented at the 2022 ARVO Annual Meeting, held in Denver, CO, May 1-4, 2022, and virtually.}
}

@article{carrClinicalApplicabilityDeep2022a,
  title = {Clinical Applicability of Deep Learning to Predict Retinal Disease from Small {{OCT}} Datasets},
  author = {Carr, Thomas and Broadway, David and Sanderson, Julie and Sami, Saber},
  year = {2022},
  month = jun,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {63},
  number = {7},
  pages = {730 --  F0458},
  issn = {1552-5783},
  abstract = {Interpretable staged transfer learning (iSTL), a pipeline for improving image classification with small sample sizes, was trained to carry out classification of optical coherence tomography (OCT) images. Model attention was visualised using SHAP maps, allowing interpretation of its behaviour.    A disease classification task using the Inception-v4 convolutional neural network architecture and iSTL training pipeline was carried out. Target OCT images are available under licence (Gholami et al). Normal (n=206), macular hole (MH: n=104), diabetic retinopathy (DR; n=109), and central serous retinopathy (CSR; n=104) scans were used. 50 images were used for data augmentation and training, the remainder were used for validation. The model was pre-trained on ImageNet, given randomly initialised output layers; the early layers frozen. The model was then trained on an intermediate bridge dataset (Kermany et al), output layers replaced, and final training carried out on the target dataset. 10 models were trained, the best performing selected for SHAP attention maps visualisation.    SHAP maps showed both image and disease specific features were allocated importance during prediction. High importance areas have a large impact on the model's prediction, low importance areas have less impact. Classifying CSR, high importance was allocated to areas surrounding posterior epithelial detachment and not to areas of subretinal fluid. With DR, retinal microaneurysms and intraretinal oedema were highlighted but subretinal fluid was not. MH scans saw strong regional importance allocated to the vertical edge of full-thickness holes and surrounding intraretinal cysts when present. Normal images typically presented with medium to high importance on the retinal pigment epithelium adjacent to the macula as well as the inner limiting membrane surface. Many images saw moderate attention within areas of the choroid and vitreous that had no apparent clinical importance. When present in the scan field the optic nerve head was not allocated high importance.    Confounding factors are a concern when training deep learning models, particularly with small datasets. The iSTL model appeared to predominantly use clinically applicable features to make predictions. Further work is needed to determine whether some features used to make predictions are confounds or genuine clinical features related to disease biomarkers.  This abstract was presented at the 2022 ARVO Annual Meeting, held in Denver, CO, May 1-4, 2022, and virtually.},
  file = {C:\Users\cleme\Zotero\storage\7P2YJUNB\article.html}
}

@misc{CATARACTSGrandChallengea,
  title = {{{CATARACTS}} - {{Grand Challenge}}},
  journal = {grand-challenge.org},
  urldate = {2020-05-06},
  abstract = {The Challenge on Automatic Tool Annotation for cataRACT Surgery aims at evaluating image-based tool detection algorithms in the context of the most common surgical procedure in the world.},
  howpublished = {https://cataracts.grand-challenge.org/Data/},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\C5CNDRYJ\Data.html}
}

@article{cheferTransformerInterpretabilityAttention2020,
  title = {Transformer {{Interpretability Beyond Attention Visualization}}},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.09838 [cs]},
  eprint = {2012.09838},
  primaryclass = {cs},
  urldate = {2021-03-03},
  abstract = {Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps, or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the deep Taylor decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila- chefer/ Transformer- Explainability .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{cheferTransformerInterpretabilityAttention2020a,
  title = {Transformer {{Interpretability Beyond Attention Visualization}}},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.09838 [cs]},
  eprint = {2012.09838},
  primaryclass = {cs},
  urldate = {2021-03-03},
  abstract = {Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps, or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the deep Taylor decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila- chefer/ Transformer- Explainability .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{cheferTransformerInterpretabilityAttention2020b,
  title = {Transformer {{Interpretability Beyond Attention Visualization}}},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.09838 [cs]},
  eprint = {2012.09838},
  primaryclass = {cs},
  urldate = {2021-03-03},
  abstract = {Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps, or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the deep Taylor decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila- chefer/ Transformer- Explainability .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\DLCYIJQS\Chefer et al. - 2020 - Transformer Interpretability Beyond Attention Visu.pdf}
}

@article{cheferTransformerInterpretabilityAttention2020c,
  title = {Transformer {{Interpretability Beyond Attention Visualization}}},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.09838 [cs]},
  eprint = {2012.09838},
  primaryclass = {cs},
  urldate = {2021-03-03},
  abstract = {Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps, or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the deep Taylor decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila- chefer/ Transformer- Explainability .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\REUNUK4T\Chefer et al. - 2020 - Transformer Interpretability Beyond Attention Visu.pdf}
}

@inproceedings{cheferTransformerInterpretabilityAttention2021,
  title = {Transformer {{Interpretability Beyond Attention Visualization}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = {2021},
  month = jun,
  pages = {782--791},
  publisher = {IEEE Computer Society},
  doi = {10.1109/CVPR46437.2021.00084},
  urldate = {2023-06-05},
  abstract = {Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila-chefer/Transformer-Explainability.},
  isbn = {978-1-66544-509-2},
  langid = {english}
}

@inproceedings{cheferTransformerInterpretabilityAttention2021a,
  title = {Transformer {{Interpretability Beyond Attention Visualization}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  year = {2021},
  month = jun,
  pages = {782--791},
  publisher = {IEEE Computer Society},
  doi = {10.1109/CVPR46437.2021.00084},
  urldate = {2023-06-05},
  abstract = {Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila-chefer/Transformer-Explainability.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\SIIX4X9A\Chefer et al. - 2021 - Transformer Interpretability Beyond Attention Visu.pdf}
}

@inproceedings{chen2021visformer,
  title = {Visformer: {{The}} Vision-Friendly Transformer},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Chen, Zhengsu and Xie, Lingxi and Niu, Jianwei and Liu, Xuefeng and Wei, Longhui and Tian, Qi},
  year = {2021},
  pages = {589--598}
}

@inproceedings{chen2021visformer,
  title = {Visformer: {{The}} Vision-Friendly Transformer},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Chen, Zhengsu and Xie, Lingxi and Niu, Jianwei and Liu, Xuefeng and Wei, Longhui and Tian, Qi},
  year = {2021},
  pages = {589--598}
}

@article{chenDeepLabSemanticImage2018,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  shorttitle = {{{DeepLab}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = {2018},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {40},
  number = {4},
  pages = {834--848},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2017.2699184},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed ``DeepLab'' system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  keywords = {atrous convolution,Computational modeling,conditional random fields,Context,Convolution,Convolutional neural networks,Image resolution,Image segmentation,Neural networks,semantic segmentation,Semantics}
}

@article{chenDeepLabSemanticImage2018a,
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  shorttitle = {{{DeepLab}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = {2018},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {40},
  number = {4},
  pages = {834--848},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2017.2699184},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed ``DeepLab'' system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  keywords = {atrous convolution,Computational modeling,conditional random fields,Context,Convolution,Convolutional neural networks,Image resolution,Image segmentation,Neural networks,semantic segmentation,Semantics},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\VNZA5FQU\\Chen et al. - 2018 - DeepLab Semantic Image Segmentation with Deep Con.pdf;C\:\\Users\\cleme\\Zotero\\storage\\C7HH355Z\\7913730.html}
}

@article{chenGenerativePretrainingPixels,
  title = {Generative {{Pretraining}} from {{Pixels}}},
  author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
  pages = {12},
  abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
  langid = {english}
}

@article{chenGenerativePretrainingPixelsa,
  title = {Generative {{Pretraining}} from {{Pixels}}},
  author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
  pages = {12},
  abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\KIMXI658\Chen et al. - Generative Pretraining from Pixels.pdf}
}

@misc{chenRethinkingAtrousConvolution2017,
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2017},
  month = dec,
  number = {arXiv:1706.05587},
  eprint = {1706.05587},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-03},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{chenRethinkingAtrousConvolution2017a,
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2017},
  month = dec,
  number = {arXiv:1706.05587},
  eprint = {1706.05587},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-03},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\SB3DVUNC\Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image S.pdf}
}

@inproceedings{choiSelfEnsemblingGANBasedData2019,
  title = {Self-{{Ensembling With GAN-Based Data Augmentation}} for {{Domain Adaptation}} in {{Semantic Segmentation}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Choi, Jaehoon and Kim, Taekyung and Kim, Changick},
  year = {2019},
  month = oct,
  pages = {6829--6839},
  publisher = {IEEE},
  address = {Seoul, Korea (South)},
  doi = {10.1109/ICCV.2019.00693},
  urldate = {2023-06-15},
  abstract = {Deep learning-based semantic segmentation methods have an intrinsic limitation that training a model requires a large amount of data with pixel-level annotations. To address this challenging issue, many researchers give attention to unsupervised domain adaptation for semantic segmentation. Unsupervised domain adaptation seeks to adapt the model trained on the source domain to the target domain. In this paper, we introduce a self-ensembling technique, one of the successful methods for domain adaptation in classification. However, applying self-ensembling to semantic segmentation is very difficult because heavily-tuned manual data augmentation used in self-ensembling is not useful to reduce the large domain gap in the semantic segmentation. To overcome this limitation, we propose a novel framework consisting of two components, which are complementary to each other. First, we present a data augmentation method based on Generative Adversarial Networks (GANs), which is computationally efficient and effective to facilitate domain alignment. Given those augmented images, we apply self-ensembling to enhance the performance of the segmentation network on the target domain. The proposed method outperforms state-of-the-art semantic segmentation methods on unsupervised domain adaptation benchmarks.},
  isbn = {978-1-72814-803-8},
  langid = {english}
}

@inproceedings{choiSelfEnsemblingGANBasedData2019a,
  title = {Self-{{Ensembling With GAN-Based Data Augmentation}} for {{Domain Adaptation}} in {{Semantic Segmentation}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Choi, Jaehoon and Kim, Taekyung and Kim, Changick},
  year = {2019},
  month = oct,
  pages = {6829--6839},
  publisher = {IEEE},
  address = {Seoul, Korea (South)},
  doi = {10.1109/ICCV.2019.00693},
  urldate = {2023-06-15},
  abstract = {Deep learning-based semantic segmentation methods have an intrinsic limitation that training a model requires a large amount of data with pixel-level annotations. To address this challenging issue, many researchers give attention to unsupervised domain adaptation for semantic segmentation. Unsupervised domain adaptation seeks to adapt the model trained on the source domain to the target domain. In this paper, we introduce a self-ensembling technique, one of the successful methods for domain adaptation in classification. However, applying self-ensembling to semantic segmentation is very difficult because heavily-tuned manual data augmentation used in self-ensembling is not useful to reduce the large domain gap in the semantic segmentation. To overcome this limitation, we propose a novel framework consisting of two components, which are complementary to each other. First, we present a data augmentation method based on Generative Adversarial Networks (GANs), which is computationally efficient and effective to facilitate domain alignment. Given those augmented images, we apply self-ensembling to enhance the performance of the segmentation network on the target domain. The proposed method outperforms state-of-the-art semantic segmentation methods on unsupervised domain adaptation benchmarks.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\SNI7QLPX\Choi et al. - 2019 - Self-Ensembling With GAN-Based Data Augmentation f.pdf}
}

@inproceedings{choiSurgicaltoolsDetectionBased2017,
  title = {Surgical-Tools Detection Based on {{Convolutional Neural Network}} in Laparoscopic Robot-Assisted Surgery},
  booktitle = {2017 39th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Choi, Bareum and Jo, Kyungmin and Choi, Songe and Choi, Jaesoon},
  year = {2017},
  month = jul,
  pages = {1756--1759},
  issn = {1558-4615},
  doi = {10.1109/EMBC.2017.8037183},
  abstract = {Laparoscopic surgery, a type of minimally invasive surgery, is used in a variety of clinical surgeries because it has a faster recovery rate and causes less pain. However, in general, the robotic system used in laparoscopic surgery can cause damage to the surgical instruments, organs, or tissues during surgery due to a narrow field of view and operating space, and insufficient tactile feedback. This study proposes real-time models for the detection of surgical instruments during laparoscopic surgery by using a CNN(Convolutional Neural Network). A dataset included information of the 7 surgical tools is used for learning CNN. To track surgical instruments in real time, unified architecture of YOLO apply to the models. So as to evaluate performance of the suggested models, degree of recall and precision is calculated and compared. Finally, we achieve 72.26\% mean average precision over our dataset.},
  keywords = {clinical surgery,CNN learning,convolutional neural network,feedforward neural nets,Instruments,Laparoscopes,laparoscopic robot-assisted surgery,Laparoscopy,learning (artificial intelligence),Machine Learning,medical computing,medical robotics,minimally invasive surgery,Minimally invasive surgery,Neural Networks (Computer),real-time models,Real-time systems,Robotic Surgical Procedures,Robots,surgery,surgical instruments,surgical-tool detection,Tools}
}

@inproceedings{choiSurgicaltoolsDetectionBased2017a,
  title = {Surgical-Tools Detection Based on {{Convolutional Neural Network}} in Laparoscopic Robot-Assisted Surgery},
  booktitle = {2017 39th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Choi, Bareum and Jo, Kyungmin and Choi, Songe and Choi, Jaesoon},
  year = {2017},
  month = jul,
  pages = {1756--1759},
  issn = {1558-4615},
  doi = {10.1109/EMBC.2017.8037183},
  abstract = {Laparoscopic surgery, a type of minimally invasive surgery, is used in a variety of clinical surgeries because it has a faster recovery rate and causes less pain. However, in general, the robotic system used in laparoscopic surgery can cause damage to the surgical instruments, organs, or tissues during surgery due to a narrow field of view and operating space, and insufficient tactile feedback. This study proposes real-time models for the detection of surgical instruments during laparoscopic surgery by using a CNN(Convolutional Neural Network). A dataset included information of the 7 surgical tools is used for learning CNN. To track surgical instruments in real time, unified architecture of YOLO apply to the models. So as to evaluate performance of the suggested models, degree of recall and precision is calculated and compared. Finally, we achieve 72.26\% mean average precision over our dataset.},
  keywords = {clinical surgery,CNN learning,convolutional neural network,feedforward neural nets,Instruments,Laparoscopes,laparoscopic robot-assisted surgery,Laparoscopy,learning (artificial intelligence),Machine Learning,medical computing,medical robotics,minimally invasive surgery,Minimally invasive surgery,Neural Networks (Computer),real-time models,Real-time systems,Robotic Surgical Procedures,Robots,surgery,surgical instruments,surgical-tool detection,Tools},
  file = {C:\Users\cleme\Zotero\storage\4ZXMGSPT\8037183.html}
}

@inproceedings{choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}--{{Decoder}} for {{Statistical Machine Translation}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Cho, Kyunghyun and {van Merri{\"e}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = oct,
  pages = {1724--1734},
  doi = {10.3115/v1/D14-1179},
  urldate = {2019-06-14},
  langid = {american}
}

@inproceedings{choLearningPhraseRepresentations2014a,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}--{{Decoder}} for {{Statistical Machine Translation}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = oct,
  pages = {1724--1734},
  doi = {10.3115/v1/D14-1179},
  urldate = {2019-06-14},
  langid = {american},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\PH94IDWK\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder–.pdf;C\:\\Users\\cleme\\Zotero\\storage\\SUT2ZKGI\\D14-1179.html}
}

@article{chowriappaEnsembleSelectionFeaturebased2013,
  title = {Ensemble Selection for Feature-Based Classification of Diabetic Maculopathy Images},
  author = {Chowriappa, Pradeep and Dua, Sumeet and Rajendra Acharya, U. and Muthu Rama Krishnan, M.},
  year = {2013},
  month = dec,
  journal = {Computers in Biology and Medicine},
  volume = {43},
  number = {12},
  pages = {2156--2162},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2013.10.003},
  urldate = {2019-11-19},
  abstract = {As diabetic maculopathy (DM) is a prevalent cause of blindness in the world, it is increasingly important to use automated techniques for the early detection of the disease. In this paper, we propose a decision system to classify DM fundus images into normal, clinically significant macular edema (CMSE), and non-clinically significant macular edema (non-CMSE) classes. The objective of the proposed decision system is three fold namely, to automatically extract textural features (both region specific and global), to effectively choose subset of discriminatory features, and to classify DM fundus images to their corresponding class of disease severity. The system uses a gamut of textural features and an ensemble classifier derived from four popular classifiers such as the hidden na{\"i}ve Bayes, na{\"i}ve Bayes, sequential minimal optimization (SMO), and the tree-based J48 classifiers. We achieved an average classification accuracy of 96.7\% using five-fold cross validation.},
  langid = {english},
  keywords = {Decision system,Diabetic retinopathy,Ensemble classifier,Feature extraction,Fundus imaging,Image texture}
}

@article{chowriappaEnsembleSelectionFeaturebased2013a,
  title = {Ensemble Selection for Feature-Based Classification of Diabetic Maculopathy Images},
  author = {Chowriappa, Pradeep and Dua, Sumeet and Rajendra Acharya, U. and Muthu Rama Krishnan, M.},
  year = {2013},
  month = dec,
  journal = {Computers in Biology and Medicine},
  volume = {43},
  number = {12},
  pages = {2156--2162},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2013.10.003},
  urldate = {2019-11-19},
  abstract = {As diabetic maculopathy (DM) is a prevalent cause of blindness in the world, it is increasingly important to use automated techniques for the early detection of the disease. In this paper, we propose a decision system to classify DM fundus images into normal, clinically significant macular edema (CMSE), and non-clinically significant macular edema (non-CMSE) classes. The objective of the proposed decision system is three fold namely, to automatically extract textural features (both region specific and global), to effectively choose subset of discriminatory features, and to classify DM fundus images to their corresponding class of disease severity. The system uses a gamut of textural features and an ensemble classifier derived from four popular classifiers such as the hidden na{\"i}ve Bayes, na{\"i}ve Bayes, sequential minimal optimization (SMO), and the tree-based J48 classifiers. We achieved an average classification accuracy of 96.7\% using five-fold cross validation.},
  langid = {english},
  keywords = {Decision system,Diabetic retinopathy,Ensemble classifier,Feature extraction,Fundus imaging,Image texture},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\EPMT9I3K\\Chowriappa et al. - 2013 - Ensemble selection for feature-based classificatio.pdf;C\:\\Users\\cleme\\Zotero\\storage\\X62IXRMF\\chowriappa2013.pdf;C\:\\Users\\cleme\\Zotero\\storage\\GRDRGXI3\\S0010482513002813.html}
}

@article{christodoulidisMultiscaleTensorVoting2016,
  title = {A Multi-Scale Tensor Voting Approach for Small Retinal Vessel Segmentation in High Resolution Fundus Images},
  author = {Christodoulidis, Argyrios and Hurtut, Thomas and Tahar, Houssem Ben and Cheriet, Farida},
  year = {2016},
  month = sep,
  journal = {Computerized Medical Imaging and Graphics: The Official Journal of the Computerized Medical Imaging Society},
  volume = {52},
  pages = {28--43},
  issn = {1879-0771},
  doi = {10.1016/j.compmedimag.2016.06.001},
  abstract = {Segmenting the retinal vessels from fundus images is a prerequisite for many CAD systems for the automatic detection of diabetic retinopathy lesions. So far, research efforts have concentrated mainly on the accurate localization of the large to medium diameter vessels. However, failure to detect the smallest vessels at the segmentation step can lead to false positive lesion detection counts in a subsequent lesion analysis stage. In this study, a new hybrid method for the segmentation of the smallest vessels is proposed. Line detection and perceptual organization techniques are combined in a multi-scale scheme. Small vessels are reconstructed from the perceptual-based approach via tracking and pixel painting. The segmentation was validated in a high resolution fundus image database including healthy and diabetic subjects using pixel-based as well as perceptual-based measures. The proposed method achieves 85.06\% sensitivity rate, while the original multi-scale line detection method achieves 81.06\% sensitivity rate for the corresponding images (p{$<$}0.05). The improvement in the sensitivity rate for the database is 6.47\% when only the smallest vessels are considered (p{$<$}0.05). For the perceptual-based measure, the proposed method improves the detection of the vasculature by 7.8\% against the original multi-scale line detection method (p{$<$}0.05).},
  langid = {english},
  pmid = {27341026},
  keywords = {Algorithms,Automated,Diabetic retinopathy,Diabetic Retinopathy,Diagnostic Imaging,Fundus imaging,Fundus Oculi,Humans,Multi-scale line detection,Pattern Recognition,Pattern Recognition Automated,Perceptual organization,Retinal blood vessel segmentation,Retinal Vessels,Sensitivity and Specificity}
}

@inproceedings{chudzikExudateSegmentationUsing2018,
  title = {Exudate Segmentation Using Fully Convolutional Neural Networks and Inception Modules},
  booktitle = {Medical {{Imaging}} 2018: {{Image Processing}}},
  author = {Chudzik, Piotr and Majumdar, Somshubra and Caliva, Francesco and {Al-Diri}, Bashir and Hunter, Andrew},
  year = {2018},
  month = mar,
  volume = {10574},
  pages = {785--792},
  publisher = {SPIE},
  doi = {10.1117/12.2293549},
  urldate = {2023-02-27},
  abstract = {Diabetic retinopathy is an eye disease associated with diabetes mellitus and also it is the leading cause of preventable blindness in working-age population. Early detection and treatment of DR is essential to prevent vision loss. Exudates are one of the earliest signs of diabetic retinopathy. This paper proposes an automatic method for the detection and segmentation of exudates in fundus photographies. A novel fully convolutional neural network architecture with Inception modules is proposed. Compared to other methods it does not require the removal of other anatomical structures. Furthermore, a transfer learning approach is applied between small datasets of different modalities from the same domain. To the best of authors' knowledge, it is the first time that such approach has been used in the exudate segmentation domain. The proposed method was evaluated using publicly available E-Ophtha datasets. It achieved better results than the state-of-the-art methods in terms of sensitivity and specificity metrics. The proposed algorithm accomplished better results using a diseased/not diseased evaluation scenario which indicates its applicability for screening purposes. Simplicity, performance, efficiency and robustness of the proposed method demonstrate its suitability for diabetic retinopathy screening applications.}
}

@inproceedings{chudzikExudateSegmentationUsing2018a,
  title = {Exudate Segmentation Using Fully Convolutional Neural Networks and Inception Modules},
  booktitle = {Medical {{Imaging}} 2018: {{Image Processing}}},
  author = {Chudzik, Piotr and Majumdar, Somshubra and Caliva, Francesco and {Al-Diri}, Bashir and Hunter, Andrew},
  year = {2018},
  month = mar,
  volume = {10574},
  pages = {785--792},
  publisher = {SPIE},
  doi = {10.1117/12.2293549},
  urldate = {2023-02-27},
  abstract = {Diabetic retinopathy is an eye disease associated with diabetes mellitus and also it is the leading cause of preventable blindness in working-age population. Early detection and treatment of DR is essential to prevent vision loss. Exudates are one of the earliest signs of diabetic retinopathy. This paper proposes an automatic method for the detection and segmentation of exudates in fundus photographies. A novel fully convolutional neural network architecture with Inception modules is proposed. Compared to other methods it does not require the removal of other anatomical structures. Furthermore, a transfer learning approach is applied between small datasets of different modalities from the same domain. To the best of authors' knowledge, it is the first time that such approach has been used in the exudate segmentation domain. The proposed method was evaluated using publicly available E-Ophtha datasets. It achieved better results than the state-of-the-art methods in terms of sensitivity and specificity metrics. The proposed algorithm accomplished better results using a diseased/not diseased evaluation scenario which indicates its applicability for screening purposes. Simplicity, performance, efficiency and robustness of the proposed method demonstrate its suitability for diabetic retinopathy screening applications.},
  file = {C:\Users\cleme\Zotero\storage\ACWJ4PJL\Chudzik et al. - 2018 - Exudate segmentation using fully convolutional neu.pdf}
}

@inproceedings{chudzikExudatesSegmentationUsing2018,
  title = {Exudates {{Segmentation}} Using {{Fully Convolutional Neural Network}} and {{Auxiliary Codebook}}},
  booktitle = {2018 40th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Chudzik, P. and {Al-Diri}, B. and Caliv{\'a}, F. and Ometto, G. and Hunter, A.},
  year = {2018},
  month = jul,
  pages = {770--773},
  doi = {10.1109/EMBC.2018.8512354},
  abstract = {Diabetic retinopathy (DR) is an asymptotic complication of diabetes and the leading cause of preventable blindness in the working-age population. Early detection and treatment of DR is critical to avoid vision loss. Exudates are one of the earliest and most prevalent signs of DR. In this work, we propose a novel two-stage method for the detection and segmentation of exudates in fundus photographs. In the first stage, a fully convolutional neural network architecture is trained to segment exudates using small image patches. Next, an auxilary codebook is built from network's intermediate layer output using incremental principal component analysis. Finally, outputs of both systems are combined to produce final result. Compared to other methods, the proposed algorithm does not require computation of candidate regions or removal of other anatomical structures. Furthermore, a transfer learning approach was applied to improve the performance of the system. The proposed method was evaluated using publicly available E-Ophtha datasets. It achieved better results than the state-of-the-art methods in terms of sensitivity and specificity metrics. The proposed method accomplished better results using a diseased//not diseased evaluation scenario which indicates its applicability for screening purposes. Simplicity, performance, efficiency and robustness of the proposed method demonstrate its suitability for diabetic retinopathy screening applications.},
  keywords = {anatomical structures,auxilary codebook,auxiliary codebook,Computer architecture,convolution,Diabetes,diabetic retinopathy screening applications,diseases,DR,eye,feedforward neural nets,fully convolutional neural network architecture,fundus photographs,image classification,image patches,image segmentation,Image segmentation,learning (artificial intelligence),Measurement,medical image processing,object detection,preventable blindness,principal component analysis,publicly available E-Ophtha datasets,Retinopathy,segment exudates,Training,transfer learning approach,vision defects,vision loss,working-age population}
}

@inproceedings{chudzikExudatesSegmentationUsing2018a,
  title = {Exudates {{Segmentation}} Using {{Fully Convolutional Neural Network}} and {{Auxiliary Codebook}}},
  booktitle = {2018 40th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Chudzik, P. and {Al-Diri}, B. and Caliv{\'a}, F. and Ometto, G. and Hunter, A.},
  year = {2018},
  month = jul,
  pages = {770--773},
  doi = {10.1109/EMBC.2018.8512354},
  abstract = {Diabetic retinopathy (DR) is an asymptotic complication of diabetes and the leading cause of preventable blindness in the working-age population. Early detection and treatment of DR is critical to avoid vision loss. Exudates are one of the earliest and most prevalent signs of DR. In this work, we propose a novel two-stage method for the detection and segmentation of exudates in fundus photographs. In the first stage, a fully convolutional neural network architecture is trained to segment exudates using small image patches. Next, an auxilary codebook is built from network's intermediate layer output using incremental principal component analysis. Finally, outputs of both systems are combined to produce final result. Compared to other methods, the proposed algorithm does not require computation of candidate regions or removal of other anatomical structures. Furthermore, a transfer learning approach was applied to improve the performance of the system. The proposed method was evaluated using publicly available E-Ophtha datasets. It achieved better results than the state-of-the-art methods in terms of sensitivity and specificity metrics. The proposed method accomplished better results using a diseased//not diseased evaluation scenario which indicates its applicability for screening purposes. Simplicity, performance, efficiency and robustness of the proposed method demonstrate its suitability for diabetic retinopathy screening applications.},
  keywords = {anatomical structures,auxilary codebook,auxiliary codebook,Computer architecture,convolution,Diabetes,diabetic retinopathy screening applications,diseases,DR,eye,feedforward neural nets,fully convolutional neural network architecture,fundus photographs,image classification,image patches,image segmentation,Image segmentation,learning (artificial intelligence),Measurement,medical image processing,object detection,preventable blindness,principal component analysis,publicly available E-Ophtha datasets,Retinopathy,segment exudates,Training,transfer learning approach,vision defects,vision loss,working-age population},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\6WCHUDH4\\Chudzik et al. - 2018 - Exudates Segmentation using Fully Convolutional Ne.pdf;C\:\\Users\\cleme\\Zotero\\storage\\WKMI73CH\\8512354.html}
}

@article{cohenWeightedKappaNominal1968,
  title = {Weighted Kappa: {{Nominal}} Scale Agreement Provision for Scaled Disagreement or Partial Credit},
  shorttitle = {Weighted Kappa},
  author = {Cohen, Jacob},
  year = {1968},
  journal = {Psychological Bulletin},
  volume = {70},
  pages = {213--220},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/h0026256},
  abstract = {A previously described coefficient of agreement for nominal scales, kappa, treats all disagreements equally. A generalization to weighted kappa (Kw) is presented. The Kw provides for the incorpation of ratio-scaled degrees of disagreement (or agreement) to each of the cells of the k * k table of joint nominal scale assignments such that disagreements of varying gravity (or agreements of varying degree) are weighted accordingly. Although providing for partial credit, Kw is fully chance corrected. Its sampling characteristics and procedures for hypothesis testing and setting confidence limits are given. Under certain conditions, Kw equals product-moment r. The use of unequal weights for symmetrical cells makes Kw suitable as a measure of validity. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Mathematics (Concepts),Scaling (Testing),Statistical Analysis,Testing Methods}
}

@article{cohenWeightedKappaNominal1968a,
  title = {Weighted Kappa: {{Nominal}} Scale Agreement Provision for Scaled Disagreement or Partial Credit},
  shorttitle = {Weighted Kappa},
  author = {Cohen, Jacob},
  year = {1968},
  journal = {Psychological Bulletin},
  volume = {70},
  pages = {213--220},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/h0026256},
  abstract = {A previously described coefficient of agreement for nominal scales, kappa, treats all disagreements equally. A generalization to weighted kappa (Kw) is presented. The Kw provides for the incorpation of ratio-scaled degrees of disagreement (or agreement) to each of the cells of the k * k table of joint nominal scale assignments such that disagreements of varying gravity (or agreements of varying degree) are weighted accordingly. Although providing for partial credit, Kw is fully chance corrected. Its sampling characteristics and procedures for hypothesis testing and setting confidence limits are given. Under certain conditions, Kw equals product-moment r. The use of unequal weights for symmetrical cells makes Kw suitable as a measure of validity. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Mathematics (Concepts),Scaling (Testing),Statistical Analysis,Testing Methods},
  file = {C:\Users\cleme\Zotero\storage\CVKRSAQZ\doiLanding.html}
}

@inproceedings{combinidoConvolutionalNeuralNetwork2018,
  title = {A {{Convolutional Neural Network Approach}} for {{Estimating Tropical Cyclone Intensity Using Satellite-based Infrared Images}}},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Combinido, J. S. and Mendoza, J. R. and Aborot, J.},
  year = {2018},
  month = aug,
  pages = {1474--1480},
  doi = {10.1109/ICPR.2018.8545593},
  abstract = {Existing techniques for satellite-based tropical cyclone (TC) intensity estimation involve an explicit feature extraction step to model TC intensity on a set of relevant TC features or patterns such as eye formation and cloud organization. However, crafting such a feature set is often time-consuming and requires expert knowledge. In this paper, a convolutional neural network (CNN) approach, which eliminates explicit feature extraction, for estimating the intensity of tropical cyclones is proposed. Utilizing a Visual Geometry Group 19-1ayer CNN (VGG19) model pre-trained on ImageNet, transfer learning experiments were performed using grayscale IR images of TCs obtained from various geostationary satellites in the Western North Pacific region (1996 - 2016) to estimate TC intensity. The model re-trained on TC images achieved a root-mean-square error (RMSE) of 13.23 knots - a performance comparable to existing feature-based approaches (RMSE ranging from 12 to 20 knots). Moreover, the model was able to learn generic TC features that were previously identified in feature-based approaches as important indicators of TC intensity.},
  keywords = {atmospheric techniques,cloud organization,Clouds,convolutional neural network approach,Estimation,explicit feature extraction step,eye formation,feature extraction,Feature extraction,feature-based approach,generic TC features,geophysical image processing,geostationary satellites,image classification,infrared imaging,learning (artificial intelligence),neural nets,Organizations,satellite-based infrared images,satellite-based tropical cyclone intensity estimation,Satellites,storms,Task analysis,TC images,TC intensity,Training,tropical cyclones,Visual Geometry Group 19-1ayer CNN},
  file = {C:\Users\cleme\Zotero\storage\556AAXBM\8545593.html}
}

@misc{commissionerFDAPermitsMarketing2020,
  title = {{{FDA}} Permits Marketing of Artificial Intelligence-Based Device to Detect Certain Diabetes-Related Eye Problems},
  author = {{of the Commissioner}, Office},
  year = {Tue, 03/24/2020 - 22:17},
  publisher = {FDA},
  urldate = {2021-04-12},
  abstract = {FDA permits marketing of first medical device to use artificial intelligence to detect greater than a mild level of diabetic retinopathy in the eye of adults who have diabetes.},
  langid = {english}
}

@misc{commissionerFDAPermitsMarketing2020a,
  title = {{{FDA}} Permits Marketing of Artificial Intelligence-Based Device to Detect Certain Diabetes-Related Eye Problems},
  author = {of the Commissioner, Office},
  year = {Tue, 03/24/2020 - 22:17},
  journal = {FDA},
  publisher = {FDA},
  urldate = {2021-04-12},
  abstract = {FDA permits marketing of first medical device to use artificial intelligence to detect greater than a mild level of diabetic retinopathy in the eye of adults who have diabetes.},
  howpublished = {https://www.fda.gov/news-events/press-announcements/fda-permits-marketing-artificial-intelligence-based-device-detect-certain-diabetes-related-eye},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\T6HE77VZ\fda-permits-marketing-artificial-intelligence-based-device-detect-certain-diabetes-related-eye.html}
}

@article{congerIntegrationGeneralizationKappas1980,
  title = {Integration and Generalization of Kappas for Multiple Raters},
  author = {Conger, Anthony J.},
  year = {1980},
  journal = {Psychological Bulletin},
  volume = {88},
  pages = {322--328},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.88.2.322},
  abstract = {J. A. Cohen's kappa (1960) for measuring agreement between 2 raters, using a nominal scale, has been extended for use with multiple raters by R. J. Light (1971) and J. L. Fleiss (1971). In the present article, these indices are analyzed and reformulated in terms of agreement statistics based on all pairs of raters. It has been argued that simultaneous agreement among all raters could provide an alternative basis for measuring multiple-rater agreement; however, agreement among raters can actually be considered to be an arbitrary choice along a continuum ranging from agreement for a pair of raters to agreement among all raters. Using this generalized concept of g-wise agreement, multiple-rater kappas are extended, interrelated, and illustrated. (4 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Rating,Statistical Correlation}
}

@article{congerIntegrationGeneralizationKappas1980a,
  title = {Integration and Generalization of Kappas for Multiple Raters},
  author = {Conger, Anthony J.},
  year = {1980},
  journal = {Psychological Bulletin},
  volume = {88},
  pages = {322--328},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.88.2.322},
  abstract = {J. A. Cohen's kappa (1960) for measuring agreement between 2 raters, using a nominal scale, has been extended for use with multiple raters by R. J. Light (1971) and J. L. Fleiss (1971). In the present article, these indices are analyzed and reformulated in terms of agreement statistics based on all pairs of raters. It has been argued that simultaneous agreement among all raters could provide an alternative basis for measuring multiple-rater agreement; however, agreement among raters can actually be considered to be an arbitrary choice along a continuum ranging from agreement for a pair of raters to agreement among all raters. Using this generalized concept of g-wise agreement, multiple-rater kappas are extended, interrelated, and illustrated. (4 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Rating,Statistical Correlation},
  file = {C:\Users\cleme\Zotero\storage\7NWUAVK7\1980-29309-001.html}
}

@misc{ConsortiumNeurologyClerkship,
  title = {Consortium of {{Neurology Clerkship Directors}}},
  urldate = {2019-11-12},
  abstract = {Exchange views on teaching clinical neurology, find information about neurology clerkships at American medical schools and develop educational materials.},
  langid = {english}
}

@misc{ConsortiumNeurologyClerkshipa,
  title = {Consortium of {{Neurology Clerkship Directors}}},
  urldate = {2019-11-12},
  abstract = {Exchange views on teaching clinical neurology, find information about neurology clerkships at American medical schools and develop educational materials.},
  howpublished = {https://www.aan.com/residents-and-fellows/clerkship-and-course-director-resources/consortium-of-neurology-clerkship-directors/},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\VNTUUQ4V\consortium-of-neurology-clerkship-directors.html}
}

@inproceedings{cordonnierRelationshipSelfAttentionConvolutional2019,
  title = {On the {{Relationship}} between {{Self-Attention}} and {{Convolutional Layers}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  year = {2019},
  month = sep,
  urldate = {2021-03-01},
  abstract = {A self-attention layer can perform convolution and often learns to do so in practice.},
  langid = {english}
}

@inproceedings{cordonnierRelationshipSelfAttentionConvolutional2019a,
  title = {On the {{Relationship}} between {{Self-Attention}} and {{Convolutional Layers}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  year = {2019},
  month = sep,
  urldate = {2021-03-01},
  abstract = {A self-attention layer can perform convolution and often learns to do so in practice.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\RK37MMMF\\Cordonnier et al. - 2019 - On the Relationship between Self-Attention and Con.pdf;C\:\\Users\\cleme\\Zotero\\storage\\28YZECWH\\forum.html}
}

@misc{croceRobustSemanticSegmentation2023,
  title = {Robust {{Semantic Segmentation}}: {{Strong Adversarial Attacks}} and {{Fast Training}} of {{Robust Models}}},
  shorttitle = {Robust {{Semantic Segmentation}}},
  author = {Croce, Francesco and Singh, Naman D. and Hein, Matthias},
  year = {2023},
  month = jun,
  number = {arXiv:2306.12941},
  eprint = {2306.12941},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.12941},
  urldate = {2024-06-10},
  abstract = {While a large amount of work has focused on designing adversarial attacks against image classifiers, only a few methods exist to attack semantic segmentation models. We show that attacking segmentation models presents task-specific challenges, for which we propose novel solutions. Our final evaluation protocol outperforms existing methods, and shows that those can overestimate the robustness of the models. Additionally, so far adversarial training, the most successful way for obtaining robust image classifiers, could not be successfully applied to semantic segmentation. We argue that this is because the task to be learned is more challenging, and requires significantly higher computational effort than for image classification. As a remedy, we show that by taking advantage of recent advances in robust ImageNet classifiers, one can train adversarially robust segmentation models at limited computational cost by fine-tuning robust backbones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\BWZLGA55\Croce et al. - 2023 - Robust Semantic Segmentation Strong Adversarial A.pdf}
}

@article{cuadros_bresnick_2009,
  title = {{{EyePACS}}: {{An}} Adaptable Telemedicine System for Diabetic Retinopathy Screening},
  author = {Cuadros, Jorge and Bresnick, George},
  year = {2009},
  journal = {Journal of Diabetes Science and Technology},
  volume = {3},
  number = {3},
  pages = {509--516},
  doi = {10.1177/193229680900300315}
}

@article{cuadros_bresnick_2009,
  title = {{{EyePACS}}: {{An}} Adaptable Telemedicine System for Diabetic Retinopathy Screening},
  author = {Cuadros, Jorge and Bresnick, George},
  year = {2009},
  journal = {Journal of Diabetes Science and Technology},
  volume = {3},
  number = {3},
  pages = {509--516},
  doi = {10.1177/193229680900300315}
}

@article{cuiDeepLearningPerformance2023,
  title = {Deep {{Learning Performance}} of {{Ultra-Widefield Fundus Imaging}} for {{Screening Retinal Lesions}} in {{Rural Locales}}},
  author = {Cui, Tingxin and Lin, Duoru and Yu, Shanshan and Zhao, Xinyu and Lin, Zhenzhe and Zhao, Lanqin and Xu, Fabao and Yun, Dongyuan and Pang, Jianyu and Li, Ruiyang and Xie, Liqiong and Zhu, Pengzhi and Huang, Yuzhe and Huang, Hongxin and Hu, Changming and Huang, Wenyong and Liang, Xiaoling and Lin, Haotian},
  year = {2023},
  month = nov,
  journal = {JAMA Ophthalmology},
  volume = {141},
  number = {11},
  pages = {1045--1051},
  issn = {2168-6165},
  doi = {10.1001/jamaophthalmol.2023.4650},
  urldate = {2024-02-22},
  abstract = {Retinal diseases are the leading cause of irreversible blindness worldwide, and timely detection contributes to prevention of permanent vision loss, especially for patients in rural areas with limited medical resources. Deep learning systems (DLSs) based on fundus images with a 45{$^\circ$} field of view have been extensively applied in population screening, while the feasibility of using ultra-widefield (UWF) fundus image--based DLSs to detect retinal lesions in patients in rural areas warrants exploration.To explore the performance of a DLS for multiple retinal lesion screening using UWF fundus images from patients in rural areas.In this diagnostic study, a previously developed DLS based on UWF fundus images was used to screen for 5 retinal lesions (retinal exudates or drusen, glaucomatous optic neuropathy, retinal hemorrhage, lattice degeneration or retinal breaks, and retinal detachment) in 24 villages of Yangxi County, China, between November 17, 2020, and March 30, 2021.The captured images were analyzed by the DLS and ophthalmologists.The performance of the DLS in rural screening was compared with that of the internal validation in the previous model development stage. The image quality, lesion proportion, and complexity of lesion composition were compared between the model development stage and the rural screening stage.A total of 6222 eyes in 3149 participants (1685 women [53.5\%]; mean [SD] age, 70.9 [9.1] years) were screened. The DLS achieved a mean (SD) area under the receiver operating characteristic curve (AUC) of 0.918 (0.021) (95\% CI, 0.892-0.944) for detecting 5 retinal lesions in the entire data set when applied for patients in rural areas, which was lower than that reported at the model development stage (AUC, 0.998 [0.002] [95\% CI, 0.995-1.000]; P\,\&lt;\,.001). Compared with the fundus images in the model development stage, the fundus images in this rural screening study had an increased frequency of poor quality (13.8\% [860 of 6222] vs 0\%), increased variation in lesion proportions (0.1\% [6 of 6222]-36.5\% [2271 of 6222] vs 14.0\% [2793 of 19\,891]-21.3\% [3433 of 16\,138]), and an increased complexity of lesion composition.This diagnostic study suggests that the DLS exhibited excellent performance using UWF fundus images as a screening tool for 5 retinal lesions in patients in a rural setting. However, poor image quality, diverse lesion proportions, and a complex set of lesions may have reduced the performance of the DLS; these factors in targeted screening scenarios should be taken into consideration in the model development stage to ensure good performance.}
}

@misc{CVPR2018Opena,
  title = {{{CVPR}} 2018 {{Open Access Repository}}},
  urldate = {2019-12-30},
  howpublished = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Fey\_SplineCNN\_Fast\_Geometric\_CVPR\_2018\_paper.html},
  file = {C:\Users\cleme\Zotero\storage\CUWBTXNF\Fey_SplineCNN_Fast_Geometric_CVPR_2018_paper.html}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  urldate = {2023-06-19},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english},
  keywords = {Approximation,Completeness,Neural networks}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989a,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  urldate = {2023-06-19},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english},
  keywords = {Approximation,Completeness,Neural networks},
  file = {C:\Users\cleme\Zotero\storage\XKHQETP4\Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf}
}

@article{daiDeepLearningSystem2021,
  title = {A Deep Learning System for Detecting Diabetic Retinopathy across the Disease Spectrum},
  author = {Dai, Ling and Wu, Liang and Li, Huating and Cai, Chun and Wu, Qiang and Kong, Hongyu and Liu, Ruhan and Wang, Xiangning and Hou, Xuhong and Liu, Yuexing and Long, Xiaoxue and Wen, Yang and Lu, Lina and Shen, Yaxin and Chen, Yan and Shen, Dinggang and Yang, Xiaokang and Zou, Haidong and Sheng, Bin and Jia, Weiping},
  year = {2021},
  month = may,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {3242},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-23458-5},
  urldate = {2023-02-22},
  abstract = {Retinal screening contributes to early detection of diabetic retinopathy and timely treatment. To facilitate the screening process, we develop a deep learning system, named DeepDR, that can detect early-to-late stages of diabetic retinopathy. DeepDR is trained for real-time image quality assessment, lesion detection and grading using 466,247 fundus images from 121,342 patients with diabetes. Evaluation is performed on a local dataset with 200,136 fundus images from 52,004 patients and three external datasets with a total of 209,322 images. The area under the receiver operating characteristic curves for detecting microaneurysms, cotton-wool spots, hard exudates and hemorrhages are 0.901, 0.941, 0.954 and 0.967, respectively. The grading of diabetic retinopathy as mild, moderate, severe and proliferative achieves area under the curves of 0.943, 0.955, 0.960 and 0.972, respectively. In external validations, the area under the curves for grading range from 0.916 to 0.970, which further supports the system is efficient for diabetic retinopathy grading.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Diabetes complications,Retinal diseases},
  file = {C:\Users\cleme\Zotero\storage\R57GS8DJ\Dai et al. - 2021 - A deep learning system for detecting diabetic reti.pdf}
}

@article{daiDeepLearningSystem2024,
  title = {A Deep Learning System for Predicting Time to Progression of Diabetic Retinopathy},
  author = {Dai, Ling and Sheng, Bin and Chen, Tingli and Wu, Qiang and Liu, Ruhan and Cai, Chun and Wu, Liang and Yang, Dawei and Hamzah, Haslina and Liu, Yuexing and Wang, Xiangning and Guan, Zhouyu and Yu, Shujie and Li, Tingyao and Tang, Ziqi and Ran, Anran and Che, Haoxuan and Chen, Hao and Zheng, Yingfeng and Shu, Jia and Huang, Shan and Wu, Chan and Lin, Shiqun and Liu, Dan and Li, Jiajia and Wang, Zheyuan and Meng, Ziyao and Shen, Jie and Hou, Xuhong and Deng, Chenxin and Ruan, Lei and Lu, Feng and Chee, Miaoli and Quek, Ten Cheer and Srinivasan, Ramyaa and Raman, Rajiv and Sun, Xiaodong and Wang, Ya Xing and Wu, Jiarui and Jin, Hai and Dai, Rongping and Shen, Dinggang and Yang, Xiaokang and Guo, Minyi and Zhang, Cuntai and Cheung, Carol Y. and Tan, Gavin Siew Wei and Tham, Yih-Chung and Cheng, Ching-Yu and Li, Huating and Wong, Tien Yin and Jia, Weiping},
  year = {2024},
  month = feb,
  journal = {Nature Medicine},
  volume = {30},
  number = {2},
  pages = {584--594},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-023-02702-z},
  urldate = {2024-06-10},
  abstract = {Diabetic retinopathy (DR) is the leading cause of preventable blindness worldwide. The risk of DR progression is highly variable among different individuals, making it difficult to predict risk and personalize screening intervals. We developed and validated a deep learning system (DeepDR Plus) to predict time to DR progression within 5 years solely from fundus images. First, we used 717,308 fundus images from 179,327 participants with diabetes to pretrain the system. Subsequently, we trained and validated the system with a multiethnic dataset comprising 118,868 images from 29,868 participants with diabetes. For predicting time to DR progression, the system achieved concordance indexes of 0.754--0.846 and integrated Brier scores of 0.153--0.241 for all times up to 5 years. Furthermore, we validated the system in real-world cohorts of participants with diabetes. The integration with clinical workflow could potentially extend the mean screening interval from 12 months to 31.97 months, and the percentage of participants recommended to be screened at 1--5 years was 30.62\%, 20.00\%, 19.63\%, 11.85\% and 17.89\%, respectively, while delayed detection of progression to vision-threatening DR was 0.18\%. Altogether, the DeepDR Plus system could predict individualized risk and time to DR progression over 5 years, potentially allowing personalized screening intervals.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Diabetes complications,Machine learning,Predictive markers},
  file = {C:\Users\cleme\Zotero\storage\QFB7SP29\Dai et al. - 2024 - A deep learning system for predicting time to prog.pdf}
}

@article{daiMicrostructureParapapillaryAtrophy2013,
  title = {Microstructure of {{Parapapillary Atrophy}}: {{Beta Zone}} and {{Gamma Zone}}},
  shorttitle = {Microstructure of {{Parapapillary Atrophy}}},
  author = {Dai, Yi and Jonas, Jost B. and Huang, Haili and Wang, Min and Sun, Xinghuai},
  year = {2013},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {54},
  number = {3},
  pages = {2013--2018},
  issn = {1552-5783},
  doi = {10.1167/iovs.12-11255},
  urldate = {2019-11-15},
  langid = {english}
}

@article{daiMicrostructureParapapillaryAtrophy2013a,
  title = {Microstructure of {{Parapapillary Atrophy}}: {{Beta Zone}} and {{Gamma Zone}}},
  shorttitle = {Microstructure of {{Parapapillary Atrophy}}},
  author = {Dai, Yi and Jonas, Jost B. and Huang, Haili and Wang, Min and Sun, Xinghuai},
  year = {2013},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {54},
  number = {3},
  pages = {2013--2018},
  issn = {1552-5783},
  doi = {10.1167/iovs.12-11255},
  urldate = {2019-11-15},
  langid = {english}
}

@article{daiMicrostructureParapapillaryAtrophy2013b,
  title = {Microstructure of {{Parapapillary Atrophy}}: {{Beta Zone}} and {{Gamma Zone}}},
  shorttitle = {Microstructure of {{Parapapillary Atrophy}}},
  author = {Dai, Yi and Jonas, Jost B. and Huang, Haili and Wang, Min and Sun, Xinghuai},
  year = {2013},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {54},
  number = {3},
  pages = {2013--2018},
  issn = {1552-5783},
  doi = {10.1167/iovs.12-11255},
  urldate = {2019-11-15},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\CBQ9CH22\\Dai et al. - 2013 - Microstructure of Parapapillary Atrophy Beta Zone.pdf;C\:\\Users\\cleme\\Zotero\\storage\\FSPAQ7YY\\article.html}
}

@article{daiMicrostructureParapapillaryAtrophy2013c,
  title = {Microstructure of {{Parapapillary Atrophy}}: {{Beta Zone}} and {{Gamma Zone}}},
  shorttitle = {Microstructure of {{Parapapillary Atrophy}}},
  author = {Dai, Yi and Jonas, Jost B. and Huang, Haili and Wang, Min and Sun, Xinghuai},
  year = {2013},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {54},
  number = {3},
  pages = {2013--2018},
  issn = {1552-5783},
  doi = {10.1167/iovs.12-11255},
  urldate = {2019-11-15},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\P6HV9968\\Dai et al. - 2013 - Microstructure of Parapapillary Atrophy Beta Zone.pdf;C\:\\Users\\cleme\\Zotero\\storage\\U7CC7TIA\\article.html}
}

@misc{Danlan_report_2PdfGoogle,
  title = {Danlan\_report\_2.0.Pdf - {{Google~Drive}}},
  urldate = {2019-02-14},
  howpublished = {https://drive.google.com/file/d/1m7BKn92KKsbhcjEqPKsQe6EscuAUxeM1/view},
  file = {C:\Users\cleme\Zotero\storage\34B4IHBT\view.html}
}

@misc{DanlanReportPdf,
  title = {Danlan\_report\_2.0.Pdf - {{Google Drive}}},
  urldate = {2019-02-14}
}

@article{dasDataEfficientApproachAutomated2020,
  title = {A {{Data-Efficient Approach}} for {{Automated Classification}} of {{OCT Images Using Generative Adversarial Network}}},
  author = {Das, Vineeta and Dandapat, Samarendra and Bora, Prabin Kumar},
  year = {2020},
  month = jan,
  journal = {IEEE Sensors Letters},
  volume = {4},
  number = {1},
  pages = {1--4},
  issn = {2475-1472},
  doi = {10.1109/LSENS.2019.2963712},
  abstract = {Deep learning algorithms can offer a reliable automated interpretation of retinal optical coherence tomography (OCT) images to assist clinicians in disease diagnosis and management. However, retinal image processing presents pertinent obstacles such as the struggle of large-scale data acquisition and high cost of annotation. To address this, we have developed a data-efficient semisupervised generative adversarial network based classifier for automated diagnosis with limited labeled data. The framework consists of a generator and a discriminator. The adversarial learning between them assists in building a generalizable classifier to predict progressive retinal diseases like age-related macular degeneration and diabetic macular edema. Experimental results on clinical-grade OCT images show an overall improvement of more than 10\% in accuracy, compared to the state-of-the-art methods.},
  keywords = {classification,Diseases,Gallium nitride,generative adversarial network (GAN),Generative adversarial networks,Generators,Imaging,optical coherence tomography,Retina,semisupervised learning,Sensor signal processing,sensor signals processing,Training}
}

@article{dashtbozorgAutomaticGraphBasedApproach2014,
  title = {An {{Automatic Graph-Based Approach}} for {{Artery}}/{{Vein Classification}} in {{Retinal Images}}},
  author = {Dashtbozorg, Behdad and Mendon{\c c}a, Ana Maria and Campilho, Aur{\'e}lio},
  year = {2014},
  month = mar,
  journal = {IEEE Transactions on Image Processing},
  volume = {23},
  number = {3},
  pages = {1073--1083},
  issn = {1941-0042},
  doi = {10.1109/TIP.2013.2263809},
  abstract = {The classification of retinal vessels into artery/vein (A/V) is an important phase for automating the detection of vascular changes, and for the calculation of characteristic signs associated with several systemic diseases such as diabetes, hypertension, and other cardiovascular conditions. This paper presents an automatic approach for A/V classification based on the analysis of a graph extracted from the retinal vasculature. The proposed method classifies the entire vascular tree deciding on the type of each intersection point (graph nodes) and assigning one of two labels to each vessel segment (graph links). Final classification of a vessel segment as A/V is performed through the combination of the graph-based labeling results with a set of intensity features. The results of this proposed method are compared with manual labeling for three public databases. Accuracy values of 88.3\%, 87.4\%, and 89.8\% are obtained for the images of the INSPIRE-AVR, DRIVE, and VICAVR databases, respectively. These results demonstrate that our method outperforms recent approaches for A/V classification.},
  keywords = {A-V classification,Algorithms,artery-vein classification,Artery/vein classification,Artificial Intelligence,Automated,automatic graph-based approach,blood vessels,cardiovascular conditions,cardiovascular system,Computer-Assisted,diabetes,diseases,DRIVE databases,eye,graph,graph extraction,graph links,graph nodes,graph theory,graph-based labeling,Humans,hypertension,image classification,Image Enhancement,Image Interpretation,image segmentation,INSPIRE-AVR databases,intersection point,medical image processing,Pattern Recognition,Reproducibility of Results,Retinal Artery,Retinal Diseases,retinal images,Retinal Vein,retinal vessel classification,Retinoscopy,Sensitivity and Specificity,systemic diseases,vascular changes,vascular tree,vessel segment,vessel segmentation,VICAVR databases}
}

@article{dashtbozorgAutomaticGraphBasedApproach2014a,
  title = {An {{Automatic Graph-Based Approach}} for {{Artery}}/{{Vein Classification}} in {{Retinal Images}}},
  author = {Dashtbozorg, Behdad and Mendon{\c c}a, Ana Maria and Campilho, Aur{\'e}lio},
  year = {2014},
  month = mar,
  journal = {IEEE Transactions on Image Processing},
  volume = {23},
  number = {3},
  pages = {1073--1083},
  issn = {1941-0042},
  doi = {10.1109/TIP.2013.2263809},
  abstract = {The classification of retinal vessels into artery/vein (A/V) is an important phase for automating the detection of vascular changes, and for the calculation of characteristic signs associated with several systemic diseases such as diabetes, hypertension, and other cardiovascular conditions. This paper presents an automatic approach for A/V classification based on the analysis of a graph extracted from the retinal vasculature. The proposed method classifies the entire vascular tree deciding on the type of each intersection point (graph nodes) and assigning one of two labels to each vessel segment (graph links). Final classification of a vessel segment as A/V is performed through the combination of the graph-based labeling results with a set of intensity features. The results of this proposed method are compared with manual labeling for three public databases. Accuracy values of 88.3\%, 87.4\%, and 89.8\% are obtained for the images of the INSPIRE-AVR, DRIVE, and VICAVR databases, respectively. These results demonstrate that our method outperforms recent approaches for A/V classification.},
  keywords = {A-V classification,Algorithms,artery-vein classification,Artery/vein classification,Artificial Intelligence,automatic graph-based approach,blood vessels,cardiovascular conditions,cardiovascular system,diabetes,diseases,DRIVE databases,eye,graph,graph extraction,graph links,graph nodes,graph theory,graph-based labeling,Humans,hypertension,image classification,Image Enhancement,Image Interpretation Computer-Assisted,image segmentation,INSPIRE-AVR databases,intersection point,medical image processing,Pattern Recognition Automated,Reproducibility of Results,Retinal Artery,Retinal Diseases,retinal images,Retinal Vein,retinal vessel classification,Retinoscopy,Sensitivity and Specificity,systemic diseases,vascular changes,vascular tree,vessel segment,vessel segmentation,VICAVR databases},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\2EDHUFNZ\\dashtbozorg2014.pdf;C\:\\Users\\cleme\\Zotero\\storage\\DATK76GE\\Dashtbozorg et al. - 2014 - An Automatic Graph-Based Approach for ArteryVein .pdf;C\:\\Users\\cleme\\Zotero\\storage\\JJK3RKQQ\\6517259.html}
}

@inproceedings{daumeiiiFrustratinglyEasyDomain2007,
  title = {Frustratingly {{Easy Domain Adaptation}}},
  booktitle = {Proceedings of the 45th {{Annual Meeting}} of the {{Association}} of {{Computational Linguistics}}},
  author = {Daum{\'e} III, Hal},
  year = {2007},
  month = jun,
  pages = {256--263},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic},
  urldate = {2023-06-15}
}

@inproceedings{daumeiiiFrustratinglyEasyDomain2007a,
  title = {Frustratingly {{Easy Domain Adaptation}}},
  booktitle = {Proceedings of the 45th {{Annual Meeting}} of the {{Association}} of {{Computational Linguistics}}},
  author = {Daum{\'e} III, Hal},
  year = {2007},
  month = jun,
  pages = {256--263},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic},
  urldate = {2023-06-15},
  file = {C:\Users\cleme\Zotero\storage\95NS7D4A\Daumé III - 2007 - Frustratingly Easy Domain Adaptation.pdf}
}

@misc{daumeiiiFrustratinglyEasyDomain2009,
  title = {Frustratingly {{Easy Domain Adaptation}}},
  author = {Daum{\'e} III, Hal},
  year = {2009},
  month = jul,
  number = {arXiv:0907.1815},
  eprint = {0907.1815},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-15},
  abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms stateof-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multidomain adaptation problem, where one has data from a variety of different domains.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{daumeiiiFrustratinglyEasyDomain2009a,
  title = {Frustratingly {{Easy Domain Adaptation}}},
  author = {Daum{\'e} III, Hal},
  year = {2009},
  month = jul,
  number = {arXiv:0907.1815},
  eprint = {0907.1815},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-15},
  abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms stateof-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multidomain adaptation problem, where one has data from a variety of different domains.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\HY74NDIK\Daumé III - 2009 - Frustratingly Easy Domain Adaptation.pdf}
}

@article{davisAgeRelatedEyeDisease2005,
  title = {The {{Age-Related Eye Disease Study}} Severity Scale for Age-Related Macular Degeneration: {{AREDS Report No}}. 17},
  shorttitle = {The {{Age-Related Eye Disease Study}} Severity Scale for Age-Related Macular Degeneration},
  author = {Davis, Matthew D. and Gangnon, Ronald E. and Lee, Li-Yin and Hubbard, Larry D. and Klein, Barbara E. K. and Klein, Ronald and Ferris, Frederick L. and Bressler, Susan B. and Milton, Roy C. and {Age-Related Eye Disease Study Group}},
  year = {2005},
  month = nov,
  journal = {Archives of Ophthalmology (Chicago, Ill.: 1960)},
  volume = {123},
  number = {11},
  pages = {1484--1498},
  issn = {0003-9950},
  doi = {10.1001/archopht.123.11.1484},
  abstract = {OBJECTIVE: To develop a fundus photographic severity scale for age-related macular degeneration (AMD). METHODS: In the Age-Related Eye Disease Study, stereoscopic color fundus photographs were taken at baseline, at the 2-year follow-up visit, and annually thereafter. Photographs were graded for drusen characteristics (size, type, area), pigmentary abnormalities (increased pigment, depigmentation, geographic atrophy), and presence of abnormalities characteristic of neovascular AMD (retinal pigment epithelial detachment, serous or hemorrhagic sensory retinal detachment, subretinal or sub-retinal pigment epithelial hemorrhage, subretinal fibrous tissue). Advanced AMD was defined as presence of 1 or more neovascular AMD abnormalities, photocoagulation for AMD, or geographic atrophy involving the center of the macula. We explored associations among right eyes of 3212 participants between severity of drusen characteristics and pigmentary abnormalities at baseline and development of advanced AMD within 5 years of follow-up. RESULTS: A 9-step severity scale that combines a 6-step drusen area scale with a 5-step pigmentary abnormality scale was developed, on which the 5-year risk of advanced AMD increased progressively from less than 1\% in step 1 to about 50\% in step 9. Among the 334 eyes that had at least a 3-step progression on the scale between the baseline and 5-year visits, almost half showed stepwise progression through intervening severity levels at intervening visits. Replicate gradings showed agreement within 1 step on the scale in 87\% of eyes. CONCLUSIONS: The scale provides convenient risk categories and has acceptable reproducibility. Progression along it may prove to be useful as a surrogate for progression to advanced AMD.},
  langid = {english},
  pmcid = {PMC1472813},
  pmid = {16286610},
  keywords = {Atrophy,Disease Progression,Humans,Macular Degeneration,Photography,Randomized Controlled Trials as Topic,Reproducibility of Results,Retina,Retinal Drusen,Retinitis Pigmentosa,Severity of Illness Index}
}

@article{davisAgeRelatedEyeDisease2005a,
  title = {The {{Age-Related Eye Disease Study}} Severity Scale for Age-Related Macular Degeneration: {{AREDS Report No}}. 17},
  shorttitle = {The {{Age-Related Eye Disease Study}} Severity Scale for Age-Related Macular Degeneration},
  author = {Davis, Matthew D. and Gangnon, Ronald E. and Lee, Li-Yin and Hubbard, Larry D. and Klein, Barbara E. K. and Klein, Ronald and Ferris, Frederick L. and Bressler, Susan B. and Milton, Roy C. and {Age-Related Eye Disease Study Group}},
  year = {2005},
  month = nov,
  journal = {Archives of Ophthalmology (Chicago, Ill.: 1960)},
  volume = {123},
  number = {11},
  pages = {1484--1498},
  issn = {0003-9950},
  doi = {10.1001/archopht.123.11.1484},
  abstract = {OBJECTIVE: To develop a fundus photographic severity scale for age-related macular degeneration (AMD). METHODS: In the Age-Related Eye Disease Study, stereoscopic color fundus photographs were taken at baseline, at the 2-year follow-up visit, and annually thereafter. Photographs were graded for drusen characteristics (size, type, area), pigmentary abnormalities (increased pigment, depigmentation, geographic atrophy), and presence of abnormalities characteristic of neovascular AMD (retinal pigment epithelial detachment, serous or hemorrhagic sensory retinal detachment, subretinal or sub-retinal pigment epithelial hemorrhage, subretinal fibrous tissue). Advanced AMD was defined as presence of 1 or more neovascular AMD abnormalities, photocoagulation for AMD, or geographic atrophy involving the center of the macula. We explored associations among right eyes of 3212 participants between severity of drusen characteristics and pigmentary abnormalities at baseline and development of advanced AMD within 5 years of follow-up. RESULTS: A 9-step severity scale that combines a 6-step drusen area scale with a 5-step pigmentary abnormality scale was developed, on which the 5-year risk of advanced AMD increased progressively from less than 1\% in step 1 to about 50\% in step 9. Among the 334 eyes that had at least a 3-step progression on the scale between the baseline and 5-year visits, almost half showed stepwise progression through intervening severity levels at intervening visits. Replicate gradings showed agreement within 1 step on the scale in 87\% of eyes. CONCLUSIONS: The scale provides convenient risk categories and has acceptable reproducibility. Progression along it may prove to be useful as a surrogate for progression to advanced AMD.},
  langid = {english},
  pmcid = {PMC1472813},
  pmid = {16286610},
  keywords = {Atrophy,Disease Progression,Humans,Macular Degeneration,Photography,Randomized Controlled Trials as Topic,Reproducibility of Results,Retina,Retinal Drusen,Retinitis Pigmentosa,Severity of Illness Index},
  file = {C:\Users\cleme\Zotero\storage\9E34U7E4\Davis et al. - 2005 - The Age-Related Eye Disease Study severity scale f.pdf}
}

@inproceedings{DB15a,
  title = {Striving for Simplicity: {{The}} All Convolutional Net},
  booktitle = {{{ICLR}} (Workshop Track)},
  author = {Springenberg, J.T. and Dosovitskiy, A. and Brox, T. and Riedmiller, M.},
  year = {2015}
}

@inproceedings{DB15a,
  title = {Striving for Simplicity: {{The}} All Convolutional Net},
  booktitle = {{{ICLR}} (Workshop Track)},
  author = {Springenberg, J.T. and Dosovitskiy, A. and Brox, T. and Riedmiller, M.},
  year = {2015}
}

@inproceedings{DB15a,
  title = {Striving for Simplicity: {{The}} All Convolutional Net},
  booktitle = {{{ICLR}} (Workshop Track)},
  author = {Springenberg, J.T. and Dosovitskiy, A. and Brox, T. and Riedmiller, M.},
  year = {2015}
}

@inproceedings{DB15a,
  title = {Striving for Simplicity: {{The}} All Convolutional Net},
  booktitle = {{{ICLR}} (Workshop Track)},
  author = {Springenberg, J.T. and Dosovitskiy, A. and Brox, T. and Riedmiller, M.},
  year = {2015}
}

@inproceedings{DBLP:conf/iclr/MadryMSTV18,
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  booktitle = {6th International Conference on Learning Representations, {{ICLR}} 2018, Vancouver, {{BC}}, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2018},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/MadryMSTV18.bib},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200}
}

@inproceedings{DBLP:conf/iclr/MadryMSTV18,
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  booktitle = {6th International Conference on Learning Representations, {{ICLR}} 2018, Vancouver, {{BC}}, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2018},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200}
}

@inproceedings{DBLP:conf/isbi/LepetitAimonBDC23,
  title = {Steered Convolutional Neurons to Better Learn the Classification of Retinal Vessels},
  booktitle = {20th {{IEEE}} International Symposium on Biomedical Imaging, {{ISBI}} 2023, Cartagena, Colombia, April 18-21, 2023},
  author = {{Lepetit-Aimon}, Gabriel and Boucher, Marie-Carole and Duval, Renaud and Cheriet, Farida},
  year = {2023},
  pages = {1--5},
  publisher = {IEEE},
  doi = {10.1109/ISBI53787.2023.10230840},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/isbi/LepetitAimonBDC23.bib},
  timestamp = {Wed, 13 Sep 2023 08:15:09 +0200}
}

@inproceedings{DBLP:conf/isbi/LepetitAimonBDC23,
  title = {Steered Convolutional Neurons to Better Learn the Classification of Retinal Vessels},
  booktitle = {20th {{IEEE}} International Symposium on Biomedical Imaging, {{ISBI}} 2023, Cartagena, Colombia, April 18-21, 2023},
  author = {{Lepetit-Aimon}, Gabriel and Boucher, Marie-Carole and Duval, Renaud and Cheriet, Farida},
  year = {2023},
  pages = {1--5},
  publisher = {IEEE},
  doi = {10.1109/ISBI53787.2023.10230840},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Wed, 13 Sep 2023 08:15:09 +0200}
}

@inproceedings{DBLP:journals/corr/SimonyanVZ13,
  title = {Deep inside Convolutional Networks: {{Visualising}} Image Classification Models and Saliency Maps},
  booktitle = {2nd International Conference on Learning Representations, {{ICLR}} 2014, Banff, {{AB}}, Canada, April 14-16, 2014, Workshop Track Proceedings},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2014},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/SimonyanVZ13.bib},
  timestamp = {Thu, 25 Jul 2019 14:36:46 +0200}
}

@inproceedings{DBLP:journals/corr/SimonyanVZ13,
  title = {Deep inside Convolutional Networks: {{Visualising}} Image Classification Models and Saliency Maps},
  booktitle = {2nd International Conference on Learning Representations, {{ICLR}} 2014, Banff, {{AB}}, Canada, April 14-16, 2014, Workshop Track Proceedings},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2014},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Thu, 25 Jul 2019 14:36:46 +0200}
}

@article{debarrosgarciaDiabeticRetinopathyOCT2017,
  title = {Diabetic Retinopathy and {{OCT}} Angiography: {{Clinical}} Findings and Future Perspectives},
  shorttitle = {Diabetic Retinopathy and {{OCT}} Angiography},
  author = {{de Barros Garcia}, Jose Mauricio Botto and Isaac, David Leonardo Cruvinel and Avila, Marcos},
  year = {2017},
  month = mar,
  journal = {International Journal of Retina and Vitreous},
  volume = {3},
  number = {1},
  pages = {14},
  issn = {2056-9920},
  doi = {10.1186/s40942-017-0062-2},
  urldate = {2022-06-25},
  abstract = {In diabetic retinopathy (DR), macular involvement can present as either macular edema or ischemia. Fluorescein angiography remains the gold standard in the evaluation of retinal vascular perfusion and diagnosis of macular ischemia. However, it is a costly, time-consuming technique, it requires venipuncture, and reports of anaphylaxis and death related to fluorescein injections have been documented, despite their rarity. Optical coherence tomography (OCT) provides a fast and non-invasive method to assess retinal structures at a microscopic level. OCT angiography permits the noninvasive study of retinal and choroid circulation via motion contrast imaging. Split-spectrum amplitude decorrelation angiography combined with OCT angiography has furthered the understanding of retinal and choroidal vascular diseases, allowing the evaluation of retinal microvasculature and identification of subsequent disorders, including DR. Previous studies using OCT angiography have demonstrated that it may demonstrate DR findings such as microaneurysms, arteriolar wall staining, retinal neovascularization, and intraretinal microvascular abnormalities. The purpose of this article is to describe and discuss different concepts regarding OCT angiography, as well as its role in the diagnosis of DR and maculopathy.},
  keywords = {Imaging,Macula,Optical coherence tomography,Retina}
}

@article{debarrosgarciaDiabeticRetinopathyOCT2017a,
  title = {Diabetic Retinopathy and {{OCT}} Angiography: Clinical Findings and Future Perspectives},
  shorttitle = {Diabetic Retinopathy and {{OCT}} Angiography},
  author = {{de Barros Garcia}, Jose Mauricio Botto and Isaac, David Leonardo Cruvinel and Avila, Marcos},
  year = {2017},
  month = mar,
  journal = {International Journal of Retina and Vitreous},
  volume = {3},
  number = {1},
  pages = {14},
  issn = {2056-9920},
  doi = {10.1186/s40942-017-0062-2},
  urldate = {2022-06-25},
  abstract = {In diabetic retinopathy (DR), macular involvement can present as either macular edema or ischemia. Fluorescein angiography remains the gold standard in the evaluation of retinal vascular perfusion and diagnosis of macular ischemia. However, it is a costly, time-consuming technique, it requires venipuncture, and reports of anaphylaxis and death related to fluorescein injections have been documented, despite their rarity. Optical coherence tomography (OCT) provides a fast and non-invasive method to assess retinal structures at a microscopic level. OCT angiography permits the noninvasive study of retinal and choroid circulation via motion contrast imaging. Split-spectrum amplitude decorrelation angiography combined with OCT angiography has furthered the understanding of retinal and choroidal vascular diseases, allowing the evaluation of retinal microvasculature and identification of subsequent disorders, including DR. Previous studies using OCT angiography have demonstrated that it may demonstrate DR findings such as microaneurysms, arteriolar wall staining, retinal neovascularization, and intraretinal microvascular abnormalities. The purpose of this article is to describe and discuss different concepts regarding OCT angiography, as well as its role in the diagnosis of DR and maculopathy.},
  keywords = {Imaging,Macula,Optical coherence tomography,Retina},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\R58V8NXE\\de Barros Garcia et al. - 2017 - Diabetic retinopathy and OCT angiography clinical.pdf;C\:\\Users\\cleme\\Zotero\\storage\\NV3UF9S4\\s40942-017-0062-2.html}
}

@article{decenciereFEEDBACKPUBLICLYDISTRIBUTED2014,
  title = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}: {{THE MESSIDOR DATABASE}}},
  shorttitle = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}},
  author = {Decenci{\`e}re, Etienne and Zhang, Xiwei and Cazuguel, Guy and Lay, Bruno and Cochener, B{\'e}atrice and Trone, Caroline and Gain, Philippe and Ordonez, Richard and Massin, Pascale and Erginay, Ali and Charton, B{\'e}atrice and Klein, Jean-Claude},
  year = {2014},
  month = aug,
  journal = {Image Analysis \& Stereology},
  volume = {33},
  number = {3},
  pages = {231--234},
  issn = {1854-5165},
  doi = {10.5566/ias.1155},
  urldate = {2019-11-22},
  abstract = {The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.},
  copyright = {Copyright (c) 2014 Image Analysis \& Stereology},
  langid = {english},
  keywords = {diabetic retinopathy,image database,image processing,Messidor}
}

@article{decenciereFEEDBACKPUBLICLYDISTRIBUTED2014a,
  title = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}: {{THE MESSIDOR DATABASE}}},
  shorttitle = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}},
  author = {Decenci{\`e}re, Etienne and Zhang, Xiwei and Cazuguel, Guy and Lay, Bruno and Cochener, B{\'e}atrice and Trone, Caroline and Gain, Philippe and Ordonez, Richard and Massin, Pascale and Erginay, Ali and Charton, B{\'e}atrice and Klein, Jean-Claude},
  year = {2014},
  month = aug,
  journal = {Image Analysis \& Stereology},
  volume = {33},
  number = {3},
  pages = {231--234},
  issn = {1854-5165},
  doi = {10.5566/ias.1155},
  urldate = {2020-02-18},
  abstract = {The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.},
  copyright = {Copyright (c) 2014 Image Analysis \& Stereology},
  langid = {english},
  keywords = {diabetic retinopathy,image database,image processing,Messidor}
}

@article{decenciereFEEDBACKPUBLICLYDISTRIBUTED2014b,
  title = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}: {{THE MESSIDOR DATABASE}}},
  shorttitle = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}},
  author = {Decenci{\`e}re, Etienne and Zhang, Xiwei and Cazuguel, Guy and Lay, Bruno and Cochener, B{\'e}atrice and Trone, Caroline and Gain, Philippe and Ordonez, Richard and Massin, Pascale and Erginay, Ali and Charton, B{\'e}atrice and Klein, Jean-Claude},
  year = {2014},
  month = aug,
  journal = {Image Analysis \& Stereology},
  volume = {33},
  number = {3},
  pages = {231--234},
  issn = {1854-5165},
  doi = {10.5566/ias.1155},
  urldate = {2019-12-26},
  abstract = {The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.},
  copyright = {Copyright (c) 2014 Image Analysis \& Stereology},
  langid = {english},
  keywords = {diabetic retinopathy,image database,image processing,Messidor}
}

@article{decenciereFEEDBACKPUBLICLYDISTRIBUTED2014c,
  title = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}: {{THE MESSIDOR DATABASE}}},
  shorttitle = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}},
  author = {Decenci{\`e}re, Etienne and Zhang, Xiwei and Cazuguel, Guy and Lay, Bruno and Cochener, B{\'e}atrice and Trone, Caroline and Gain, Philippe and Ordonez, Richard and Massin, Pascale and Erginay, Ali and Charton, B{\'e}atrice and Klein, Jean-Claude},
  year = {2014},
  month = aug,
  journal = {Image Analysis \& Stereology},
  volume = {33},
  number = {3},
  pages = {231--234},
  issn = {1854-5165},
  doi = {10.5566/ias.1155},
  urldate = {2022-10-25},
  abstract = {The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.},
  copyright = {Copyright (c) 2014 Image Analysis \& Stereology},
  langid = {english},
  keywords = {diabetic retinopathy,image database,image processing,Messidor}
}

@article{decenciereFEEDBACKPUBLICLYDISTRIBUTED2014d,
  title = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}: {{THE MESSIDOR DATABASE}}},
  shorttitle = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}},
  author = {Decenci{\`e}re, Etienne and Zhang, Xiwei and Cazuguel, Guy and Lay, Bruno and Cochener, B{\'e}atrice and Trone, Caroline and Gain, Philippe and Ordonez, Richard and Massin, Pascale and Erginay, Ali and Charton, B{\'e}atrice and Klein, Jean-Claude},
  year = {2014},
  month = aug,
  journal = {Image Analysis \& Stereology},
  volume = {33},
  number = {3},
  pages = {231--234},
  issn = {1854-5165},
  doi = {10.5566/ias.1155},
  urldate = {2019-12-26},
  abstract = {The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.},
  copyright = {Copyright (c) 2014 Image Analysis \& Stereology},
  langid = {english},
  keywords = {diabetic retinopathy,image database,image processing,Messidor},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\BEPJ9PPL\\Decencière et al. - 2014 - FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6MAHSVPS\\1155.html;C\:\\Users\\cleme\\Zotero\\storage\\6U2HE7MN\\10.5566@ias.1155.html}
}

@article{decenciereFEEDBACKPUBLICLYDISTRIBUTED2014e,
  title = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}: {{THE MESSIDOR DATABASE}}},
  shorttitle = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}},
  author = {Decenci{\`e}re, Etienne and Zhang, Xiwei and Cazuguel, Guy and Lay, Bruno and Cochener, B{\'e}atrice and Trone, Caroline and Gain, Philippe and Ordonez, Richard and Massin, Pascale and Erginay, Ali and Charton, B{\'e}atrice and Klein, Jean-Claude},
  year = {2014},
  month = aug,
  journal = {Image Analysis \& Stereology},
  volume = {33},
  number = {3},
  pages = {231--234},
  issn = {1854-5165},
  doi = {10.5566/ias.1155},
  urldate = {2019-11-22},
  abstract = {The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.},
  copyright = {Copyright (c) 2014 Image Analysis \& Stereology},
  langid = {english},
  keywords = {diabetic retinopathy,image database,image processing,Messidor},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5A4LI4CY\\10.5566@ias.1155.pdf;C\:\\Users\\cleme\\Zotero\\storage\\8PER8GUP\\Decencière et al. - 2014 - FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE.pdf;C\:\\Users\\cleme\\Zotero\\storage\\B2ZHLX6P\\10.5566@ias.1155.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6ID3SC4R\\1155.html}
}

@article{decenciereFEEDBACKPUBLICLYDISTRIBUTED2014f,
  title = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}: {{THE MESSIDOR DATABASE}}},
  shorttitle = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}},
  author = {Decenci{\`e}re, Etienne and Zhang, Xiwei and Cazuguel, Guy and Lay, Bruno and Cochener, B{\'e}atrice and Trone, Caroline and Gain, Philippe and Ordonez, Richard and Massin, Pascale and Erginay, Ali and Charton, B{\'e}atrice and Klein, Jean-Claude},
  year = {2014},
  month = aug,
  journal = {Image Analysis \& Stereology},
  volume = {33},
  number = {3},
  pages = {231--234},
  issn = {1854-5165},
  doi = {10.5566/ias.1155},
  urldate = {2020-02-18},
  abstract = {The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.},
  copyright = {Copyright (c) 2014 Image Analysis \& Stereology},
  langid = {english},
  keywords = {diabetic retinopathy,image database,image processing,Messidor},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\L7JQJLNY\\10.5566@ias.1155.pdf;C\:\\Users\\cleme\\Zotero\\storage\\YT37CIPL\\Decencière et al. - 2014 - FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE.pdf;C\:\\Users\\cleme\\Zotero\\storage\\33ZJRLR4\\1155.html}
}

@article{decenciereFEEDBACKPUBLICLYDISTRIBUTED2014g,
  title = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}: {{THE MESSIDOR DATABASE}}},
  shorttitle = {{{FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE}}},
  author = {Decenci{\`e}re, Etienne and Zhang, Xiwei and Cazuguel, Guy and Lay, Bruno and Cochener, B{\'e}atrice and Trone, Caroline and Gain, Philippe and Ordonez, Richard and Massin, Pascale and Erginay, Ali and Charton, B{\'e}atrice and Klein, Jean-Claude},
  year = {2014},
  month = aug,
  journal = {Image Analysis \& Stereology},
  volume = {33},
  number = {3},
  pages = {231--234},
  issn = {1854-5165},
  doi = {10.5566/ias.1155},
  urldate = {2022-10-25},
  abstract = {The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.},
  copyright = {Copyright (c) 2014 Image Analysis \& Stereology},
  langid = {english},
  keywords = {diabetic retinopathy,image database,image processing,Messidor},
  file = {C:\Users\cleme\Zotero\storage\GJJL8QKC\Decencière et al. - 2014 - FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE.pdf}
}

@article{decenciereTeleOphtaMachineLearning2013,
  title = {{{TeleOphta}}: {{Machine}} Learning and Image Processing Methods for Teleophthalmology},
  shorttitle = {{{TeleOphta}}},
  author = {Decenci{\`e}re, E. and Cazuguel, G. and Zhang, X. and Thibault, G. and Klein, J. -C. and Meyer, F. and Marcotegui, B. and Quellec, G. and Lamard, M. and Danno, R. and Elie, D. and Massin, P. and Viktor, Z. and Erginay, A. and La{\"y}, B. and Chabouis, A.},
  year = {2013},
  month = apr,
  journal = {IRBM},
  series = {Special Issue : {{ANR TECSAN}} : {{Technologies}} for {{Health}} and {{Autonomy}}},
  volume = {34},
  number = {2},
  pages = {196--203},
  issn = {1959-0318},
  doi = {10.1016/j.irbm.2013.01.010},
  urldate = {2020-02-17},
  abstract = {A complete prototype for the automatic detection of normal examinations on a teleophthalmology network for diabetic retinopathy screening is presented. The system combines pathological pattern mining methods, with specific lesion detection methods, to extract information from the images. This information, plus patient and other contextual data, is used by a classifier to compute an abnormality risk. Such a system should reduce the burden on readers on teleophthalmology networks.},
  langid = {english}
}

@article{decenciereTeleOphtaMachineLearning2013a,
  title = {{TeleOphta: Machine learning and image processing methods for teleophthalmology}},
  shorttitle = {{TeleOphta}},
  author = {Decenci{\`e}re, E. and Cazuguel, G. and Zhang, X. and Thibault, Guillaume and Klein, J. C. and Meyer, F. and Marcotegui, B. and Quellec, G. and Lamard, M. and Danno, R. and Elie, D. and Massin, P. and Viktor, Z. and Erginay, A. and La{\"y}, B. and Chabouis, A.},
  year = {2013},
  month = apr,
  journal = {IRBM},
  volume = {34},
  number = {2},
  pages = {196--203},
  issn = {1959-0318},
  doi = {10.1016/j.irbm.2013.01.010},
  urldate = {2019-12-26},
  langid = {English (US)}
}

@article{decenciereTeleOphtaMachineLearning2013b,
  title = {{TeleOphta: Machine learning and image processing methods for teleophthalmology}},
  shorttitle = {{TeleOphta}},
  author = {Decenci{\`e}re, E. and Cazuguel, G. and Zhang, X. and Thibault, Guillaume and Klein, J. C. and Meyer, F. and Marcotegui, B. and Quellec, G. and Lamard, M. and Danno, R. and Elie, D. and Massin, P. and Viktor, Z. and Erginay, A. and La{\"y}, B. and Chabouis, A.},
  year = {2013},
  month = apr,
  journal = {IRBM},
  volume = {34},
  number = {2},
  pages = {196--203},
  issn = {1959-0318},
  doi = {10.1016/j.irbm.2013.01.010},
  urldate = {2019-12-26},
  langid = {English (US)},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\E5RBRDXC\\teleophta-machine-learning-and-image-processing-methods-for-teleo.html;C\:\\Users\\cleme\\Zotero\\storage\\RMJEJASL\\10.1016@j.irbm.2013.01.010.html}
}

@article{decenciereTeleOphtaMachineLearning2013c,
  title = {{{TeleOphta}}: {{Machine}} Learning and Image Processing Methods for Teleophthalmology},
  shorttitle = {{{TeleOphta}}},
  author = {Decenci{\`e}re, E. and Cazuguel, G. and Zhang, X. and Thibault, G. and Klein, J. -C. and Meyer, F. and Marcotegui, B. and Quellec, G. and Lamard, M. and Danno, R. and Elie, D. and Massin, P. and Viktor, Z. and Erginay, A. and La{\"y}, B. and Chabouis, A.},
  year = {2013},
  month = apr,
  journal = {IRBM},
  series = {Special Issue : {{ANR TECSAN}} : {{Technologies}} for {{Health}} and {{Autonomy}}},
  volume = {34},
  number = {2},
  pages = {196--203},
  issn = {1959-0318},
  doi = {10.1016/j.irbm.2013.01.010},
  urldate = {2020-02-17},
  abstract = {A complete prototype for the automatic detection of normal examinations on a teleophthalmology network for diabetic retinopathy screening is presented. The system combines pathological pattern mining methods, with specific lesion detection methods, to extract information from the images. This information, plus patient and other contextual data, is used by a classifier to compute an abnormality risk. Such a system should reduce the burden on readers on teleophthalmology networks.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\YZMZBGVS\\Decencière et al. - 2013 - TeleOphta Machine learning and image processing m.pdf;C\:\\Users\\cleme\\Zotero\\storage\\DJS8P9PN\\S1959031813000237.html;C\:\\Users\\cleme\\Zotero\\storage\\SB9GR6KX\\10.1016@j.irbm.2013.01.010.html}
}

@misc{DeepLearning,
  title = {Deep {{Learning}}},
  urldate = {2023-04-05}
}

@misc{DeepLearninga,
  title = {Deep {{Learning}}},
  urldate = {2023-04-05},
  howpublished = {https://www.deeplearningbook.org/},
  file = {C:\Users\cleme\Zotero\storage\H2NZN3GN\www.deeplearningbook.org.html}
}

@misc{DeepLearningAlgorithms,
  title = {Deep Learning Algorithms for Detection of Diabetic Retinopathy in Retinal Fundus Photographs: {{A}} Systematic Review and Meta-Analysis - {{PubMed}}},
  urldate = {2021-10-04}
}

@misc{DeepLearningAlgorithmsa,
  title = {Deep Learning Algorithms for Detection of Diabetic Retinopathy in Retinal Fundus Photographs: {{A}} Systematic Review and Meta-Analysis - {{PubMed}}},
  urldate = {2021-10-04},
  howpublished = {https://pubmed.ncbi.nlm.nih.gov/32088490/}
}

@misc{DeepLearningBased,
  title = {(4) {{Deep Learning}} Based {{Inter-Modality Image Registration Supervised}} by {{Intra-Modality Similarity}} {\textbar} {{Xiaohuan Cao}} {\textbar} {{Request PDF}}},
  urldate = {2019-07-25},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  langid = {english}
}

@article{DeepLearningBased2019,
  title = {Deep Learning Based Early Stage Diabetic Retinopathy Detection Using Optical Coherence Tomography},
  year = {2019},
  month = dec,
  journal = {Neurocomputing},
  volume = {369},
  pages = {134--144},
  publisher = {Elsevier},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.08.079},
  urldate = {2022-07-10},
  abstract = {Diabetic retinopathy (DR) is one of the leading causes of preventable blindness globally. Performing retinal examinations on all diabetic patients is {\dots}},
  langid = {english}
}

@article{DeepLearningBased2019a,
  title = {Deep Learning Based Early Stage Diabetic Retinopathy Detection Using Optical Coherence Tomography},
  year = {2019},
  month = dec,
  journal = {Neurocomputing},
  volume = {369},
  pages = {134--144},
  publisher = {Elsevier},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.08.079},
  urldate = {2022-07-10},
  abstract = {Diabetic retinopathy (DR) is one of the leading causes of preventable blindness globally. Performing retinal examinations on all diabetic patients is {\dots}},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\HW5P2BXP\li2019.pdf.pdf}
}

@misc{DeepLearningBaseda,
  title = {(4) {{Deep Learning}} Based {{Inter-Modality Image Registration Supervised}} by {{Intra-Modality Similarity}} {\textbar} {{Xiaohuan Cao}} {\textbar} {{Request PDF}}},
  journal = {ResearchGate},
  urldate = {2019-07-25},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  howpublished = {https://www.researchgate.net/publication/327674877\_Deep\_Learning\_based\_Inter-Modality\_Image\_Registration\_Supervised\_by\_Intra-Modality\_Similarity},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\5ZT5CYX2\327674877_Deep_Learning_based_Inter-Modality_Image_Registration_Supervised_by_Intra-Modality_Si.html}
}

@misc{DeepLearningDetection,
  title = {Deep {{Learning}} for {{Detection}} of {{Diabetic Eye Disease}}},
  urldate = {2019-12-10},
  abstract = {Posted by Lily Peng MD PhD, Product Manager and Varun Gulshan PhD, Research Engineer Diabetic retinopathy (DR) is the fastest growing cause...},
  langid = {english}
}

@misc{DeepLearningDetectiona,
  title = {Deep {{Learning}} for {{Detection}} of {{Diabetic Eye Disease}}},
  journal = {Google AI Blog},
  urldate = {2019-12-10},
  abstract = {Posted by Lily Peng MD PhD, Product Manager and Varun Gulshan PhD, Research Engineer Diabetic retinopathy  (DR) is the fastest growing cause...},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\U8XWTXX7\deep-learning-for-detection-of-diabetic.html}
}

@misc{DeepLearningDiabetic,
  title = {Deep Learning for Diabetic Retinopathy Detection and Classification Based on Fundus Images: {{A}} Review {\textbar} {{Elsevier Enhanced Reader}}},
  shorttitle = {Deep Learning for Diabetic Retinopathy Detection and Classification Based on Fundus Images},
  doi = {10.1016/j.compbiomed.2021.104599},
  urldate = {2021-09-14},
  langid = {english}
}

@misc{DeepLearningDiabetica,
  title = {Deep Learning for Diabetic Retinopathy Detection and Classification Based on Fundus Images: {{A}} Review {\textbar} {{Elsevier Enhanced Reader}}},
  shorttitle = {Deep Learning for Diabetic Retinopathy Detection and Classification Based on Fundus Images},
  doi = {10.1016/j.compbiomed.2021.104599},
  urldate = {2021-09-14},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0010482521003930?token=5E66094EC87F3E9A592C1ACEE2BF66888D2800F6AC94356E1B7DB349EAE271D41141B90370ECD28BB4C808EB4CE7EDDF\&originRegion=eu-west-1\&originCreation=20210914085744},
  langid = {english}
}

@article{defauwClinicallyApplicableDeep2018,
  title = {Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease},
  author = {De Fauw, Jeffrey and Ledsam, Joseph R. and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O'Donoghue, Brendan and Visentin, Daniel and {van den Driessche}, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, C{\'i}an O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
  year = {2018},
  month = sep,
  journal = {Nature Medicine},
  volume = {24},
  number = {9},
  pages = {1342--1350},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0107-6},
  urldate = {2022-07-10},
  abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Diagnosis,Eye manifestations,Machine learning,Three-dimensional imaging}
}

@article{defauwClinicallyApplicableDeep2018a,
  title = {Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease},
  author = {De Fauw, Jeffrey and Ledsam, Joseph R. and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O'Donoghue, Brendan and Visentin, Daniel and {van den Driessche}, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, C{\'i}an O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
  year = {2018},
  month = sep,
  journal = {Nature Medicine},
  volume = {24},
  number = {9},
  pages = {1342--1350},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0107-6},
  urldate = {2022-07-10},
  abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Diagnosis,Eye manifestations,Machine learning,Three-dimensional imaging},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\UP8E9SDW\\De Fauw et al. - 2018 - Clinically applicable deep learning for diagnosis .pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZBVPUA84\\defauw2018.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\LHDNM8Y6\\s41591-018-0107-6.html}
}

@article{defferrardConvolutionalNeuralNetworks,
  title = {Convolutional {{Neural Networks}} on {{Graphs}} with {{Fast Localized Spectral Filtering}}},
  author = {Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  pages = {9},
  abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
  langid = {english}
}

@article{defferrardConvolutionalNeuralNetworksa,
  title = {Convolutional {{Neural Networks}} on {{Graphs}} with {{Fast Localized Spectral Filtering}}},
  author = {Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  pages = {9},
  abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\NJ8CDXK5\Defferrard et al. - Convolutional Neural Networks on Graphs with Fast .pdf}
}

@article{dengGraphConvolutionalAdversarial2022,
  title = {Graph {{Convolutional Adversarial Networks}} for {{Spatiotemporal Anomaly Detection}}},
  author = {Deng, Leyan and Lian, Defu and Huang, Zhenya and Chen, Enhong},
  year = {2022},
  month = jun,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {6},
  pages = {2416--2428},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3136171},
  urldate = {2023-10-05},
  abstract = {Traffic anomalies, such as traffic accidents and unexpected crowd gathering, may endanger public safety if not handled timely. Detecting traffic anomalies in their early stage can benefit citizens' quality of life and city planning. However, traffic anomaly detection faces two main challenges. First, it is challenging to model traffic dynamics due to the complex spatiotemporal characteristics of traffic data. Second, the criteria of traffic anomalies may vary with locations and times. In this article, we propose a spatiotemporal graph convolutional adversarial network (STGAN) to address these above challenges. More specifically, we devise a spatiotemporal generator to predict the normal traffic dynamics and a spatiotemporal discriminator to determine whether an input sequence is real or not. There are high correlations between neighboring data points in the spatial and temporal dimensions. Therefore, we propose a recent module and leverage graph convolutional gated recurrent unit (GCGRU) to help the generator and discriminator learn the spatiotemporal features of traffic dynamics and traffic anomalies, respectively. After adversarial training, the generator and discriminator can be used as detectors independently, where the generator models the normal traffic dynamics patterns and the discriminator provides detection criteria varying with spatiotemporal features. We then design a novel anomaly score combining the abilities of two detectors, which considers the misleading of unpredictable traffic dynamics to the discriminator. We evaluate our method on two real-world datasets from New York City and California. The experimental results show that the proposed method detects various traffic anomalies effectively and outperforms the state-of-the-art methods. Furthermore, the devised anomaly score achieves more robust detection performances than the general score.}
}

@article{dengGraphConvolutionalAdversarial2022a,
  title = {Graph {{Convolutional Adversarial Networks}} for {{Spatiotemporal Anomaly Detection}}},
  author = {Deng, Leyan and Lian, Defu and Huang, Zhenya and Chen, Enhong},
  year = {2022},
  month = jun,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {6},
  pages = {2416--2428},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3136171},
  urldate = {2023-10-05},
  abstract = {Traffic anomalies, such as traffic accidents and unexpected crowd gathering, may endanger public safety if not handled timely. Detecting traffic anomalies in their early stage can benefit citizens' quality of life and city planning. However, traffic anomaly detection faces two main challenges. First, it is challenging to model traffic dynamics due to the complex spatiotemporal characteristics of traffic data. Second, the criteria of traffic anomalies may vary with locations and times. In this article, we propose a spatiotemporal graph convolutional adversarial network (STGAN) to address these above challenges. More specifically, we devise a spatiotemporal generator to predict the normal traffic dynamics and a spatiotemporal discriminator to determine whether an input sequence is real or not. There are high correlations between neighboring data points in the spatial and temporal dimensions. Therefore, we propose a recent module and leverage graph convolutional gated recurrent unit (GCGRU) to help the generator and discriminator learn the spatiotemporal features of traffic dynamics and traffic anomalies, respectively. After adversarial training, the generator and discriminator can be used as detectors independently, where the generator models the normal traffic dynamics patterns and the discriminator provides detection criteria varying with spatiotemporal features. We then design a novel anomaly score combining the abilities of two detectors, which considers the misleading of unpredictable traffic dynamics to the discriminator. We evaluate our method on two real-world datasets from New York City and California. The experimental results show that the proposed method detects various traffic anomalies effectively and outperforms the state-of-the-art methods. Furthermore, the devised anomaly score achieves more robust detection performances than the general score.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\MZM94DJW\\Deng et al. - 2022 - Graph Convolutional Adversarial Networks for Spati.pdf;C\:\\Users\\cleme\\Zotero\\storage\\YHXM9W8M\\9669110.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500--1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine}
}

@inproceedings{dengImageNetLargescaleHierarchical2009a,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500--1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {C:\Users\cleme\Zotero\storage\D7M3P3YX\5206848.html}
}

@misc{DiabeticRetinopathyDetection,
  title = {Diabetic {{Retinopathy Detection}}},
  urldate = {2019-07-24},
  abstract = {Identify signs of diabetic retinopathy in eye images},
  langid = {english}
}

@misc{DiabeticRetinopathyDetectiona,
  title = {Diabetic {{Retinopathy Detection}}},
  urldate = {2019-07-24},
  abstract = {Identify signs of diabetic retinopathy in eye images},
  howpublished = {https://kaggle.com/c/diabetic-retinopathy-detection},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\2NQAESUS\diabetic-retinopathy-detection.html}
}

@article{diakogiannisResUNetaDeepLearning2020,
  title = {{{ResUNet-a}}: {{A}} Deep Learning Framework for Semantic Segmentation of Remotely Sensed Data},
  shorttitle = {{{ResUNet-a}}},
  author = {Diakogiannis, Foivos I. and Waldner, Fran{\c c}ois and Caccetta, Peter and Wu, Chen},
  year = {2020},
  month = apr,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {162},
  pages = {94--114},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2020.01.013},
  urldate = {2023-05-18},
  abstract = {Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes. The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9\% over all classes for our best model.},
  langid = {english},
  keywords = {Architecture,Convolutional neural network,Data augmentation,Loss function,Very high spatial resolution}
}

@article{diakogiannisResUNetaDeepLearning2020a,
  title = {{{ResUNet-a}}: {{A}} Deep Learning Framework for Semantic Segmentation of Remotely Sensed Data},
  shorttitle = {{{ResUNet-a}}},
  author = {Diakogiannis, Foivos I. and Waldner, Fran{\c c}ois and Caccetta, Peter and Wu, Chen},
  year = {2020},
  month = apr,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {162},
  pages = {94--114},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2020.01.013},
  urldate = {2023-05-18},
  abstract = {Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes. The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9\% over all classes for our best model.},
  langid = {english},
  keywords = {Architecture,Convolutional neural network,Data augmentation,Loss function,Very high spatial resolution},
  file = {C:\Users\cleme\Zotero\storage\TQUK838R\Diakogiannis et al. - 2020 - ResUNet-a A deep learning framework for semantic .pdf}
}

@misc{Doi101016,
  title = {Doi:10.1016/{{S0002-9394}}(03)00792-{{X}} {\textbar} {{Elsevier Enhanced Reader}}},
  shorttitle = {Doi},
  doi = {10.1016/S0002-9394(03)00792-X},
  urldate = {2019-10-30},
  langid = {english}
}

@misc{Doi101016a,
  title = {Doi:10.1016/{{S0002-9394}}(03)00792-{{X}} {\textbar} {{Elsevier Enhanced Reader}}},
  shorttitle = {Doi},
  doi = {10.1016/S0002-9394(03)00792-X},
  urldate = {2019-10-30},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S000293940300792X?token=691F73E060400EB24500B3FE8F20BFB9E87B86ABBBED7A42F92BB73720D55D0D67FCDA44A330468CE0CFF6EFA3EB5CBC},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\RCGZ2V3X\S000293940300792X.html}
}

@incollection{doshi-velezConsiderationsEvaluationGeneralization2018,
  title = {Considerations for {{Evaluation}} and {{Generalization}} in {{Interpretable Machine Learning}}},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar{\'o}, Xavier and G{\"u}{\c c}l{\"u}t{\"u}rk, Ya{\u g}mur and G{\"u}{\c c}l{\"u}, Umut},
  year = {2018},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  pages = {3--17},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-98131-4_1},
  urldate = {2023-05-04},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is little consensus on what interpretable machine learning is and how it should be measured and evaluated. In this paper, we discuss a definitions of interpretability and describe when interpretability is needed (and when it is not). Finally, we talk about a taxonomy for rigorous evaluation, and recommendations for researchers. We will end with discussing open questions and concrete problems for new researchers.},
  isbn = {978-3-319-98131-4},
  langid = {english},
  keywords = {Accountability,Interpretability,Machine learning,Transparency}
}

@incollection{doshi-velezConsiderationsEvaluationGeneralization2018a,
  title = {Considerations for {{Evaluation}} and {{Generalization}} in {{Interpretable Machine Learning}}},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar{\'o}, Xavier and G{\"u}{\c c}l{\"u}t{\"u}rk, Ya{\u g}mur and G{\"u}{\c c}l{\"u}, Umut},
  year = {2018},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  pages = {3--17},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-98131-4_1},
  urldate = {2023-05-04},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is little consensus on what interpretable machine learning is and how it should be measured and evaluated. In this paper, we discuss a definitions of interpretability and describe when interpretability is needed (and when it is not). Finally, we talk about a taxonomy for rigorous evaluation, and recommendations for researchers. We will end with discussing open questions and concrete problems for new researchers.},
  isbn = {978-3-319-98131-4},
  langid = {english},
  keywords = {Accountability,Interpretability,Machine learning,Transparency}
}

@article{doshi-velezRigorousScienceInterpretable2017,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = {2017},
  month = mar,
  journal = {arXiv:1702.08608 [cs, stat]},
  eprint = {1702.08608},
  primaryclass = {cs, stat},
  urldate = {2019-12-06},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{doshi-velezRigorousScienceInterpretable2017a,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = {2017},
  month = mar,
  journal = {arXiv:1702.08608 [cs, stat]},
  eprint = {1702.08608},
  primaryclass = {cs, stat},
  urldate = {2019-12-06},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\KL7PN28J\Doshi-Velez et Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf}
}

@incollection{dosovitskiyGeneratingImagesPerceptual2016,
  title = {Generating {{Images}} with {{Perceptual Similarity Metrics}} Based on {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Dosovitskiy, Alexey and Brox, Thomas},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {658--666},
  publisher = {Curran Associates, Inc.},
  urldate = {2019-06-13}
}

@incollection{dosovitskiyGeneratingImagesPerceptual2016a,
  title = {Generating {{Images}} with {{Perceptual Similarity Metrics}} Based on {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Dosovitskiy, Alexey and Brox, Thomas},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {658--666},
  publisher = {Curran Associates, Inc.},
  urldate = {2019-06-13},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\Z5UXU5PK\\Dosovitskiy et Brox - 2016 - Generating Images with Perceptual Similarity Metri.pdf;C\:\\Users\\cleme\\Zotero\\storage\\FBQGIQ3P\\6158-generating-images-with-perceptual-similarity-metrics-based-on-deep-networks.html}
}

@inproceedings{dosovitskiyImageWorth16x162020,
  title = {An {{Image Is Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image Is Worth}} 16x16 {{Words}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2020},
  month = sep,
  urldate = {2021-02-28},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied...},
  langid = {english}
}

@inproceedings{dosovitskiyImageWorth16x162020a,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2020},
  month = sep,
  urldate = {2021-02-28},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied...},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\STTLPUAM\\Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf;C\:\\Users\\cleme\\Zotero\\storage\\LSGVLAAY\\forum.html}
}

@article{downsLaminaCribrosaGlaucoma2017,
  title = {Lamina {{Cribrosa}} in {{Glaucoma}}},
  author = {Downs, J. Crawford and Girkin, Christopher A.},
  year = {2017},
  month = mar,
  journal = {Current opinion in ophthalmology},
  volume = {28},
  number = {2},
  pages = {113--119},
  issn = {1040-8738},
  doi = {10.1097/ICU.0000000000000354},
  urldate = {2019-11-15},
  abstract = {Purpose of Review To present, summarize and interpret most recent advances in the study and understanding of the lamina cribrosa (LC) in glaucoma, in the context of previous work. Recent Findings The lamina is an active living structure that responds to strain and changes morphology at the micro- and macro-scales in glaucoma. Changes in LC morphology in glaucoma include posteriorization of the laminar insertion into the sclera, increased cupping or depth of the LC, and the development of focal LC defects. These LC changes are associated with disk hemorrhages and visual field damage, and are detectable with clinical imaging techniques such as optical coherence tomography (OCT). Glaucomatous changes in the LC are driven by cellular processes mediated by focal cyclical mechanical strain. Strain is eye specific and mediated by IOP, cerebrospinal fluid pressure (CSFP), and scleral and LC morphology and structural stiffness; deleterious LC strains can occur at all levels of mean IOP. Summary Laminar morphology is ever changing in health and disease, and recent studies have identified several promising morphological changes that are indicative of glaucoma susceptibility, onset and progression.},
  pmcid = {PMC5480216},
  pmid = {27898470}
}

@article{downsLaminaCribrosaGlaucoma2017a,
  title = {Lamina {{Cribrosa}} in {{Glaucoma}}},
  author = {Downs, J. Crawford and Girkin, Christopher A.},
  year = {2017},
  month = mar,
  journal = {Current opinion in ophthalmology},
  volume = {28},
  number = {2},
  pages = {113--119},
  issn = {1040-8738},
  doi = {10.1097/ICU.0000000000000354},
  urldate = {2019-11-15},
  abstract = {Purpose of Review To present, summarize and interpret most recent advances in the study and understanding of the lamina cribrosa (LC) in glaucoma, in the context of previous work. Recent Findings The lamina is an active living structure that responds to strain and changes morphology at the micro- and macro-scales in glaucoma. Changes in LC morphology in glaucoma include posteriorization of the laminar insertion into the sclera, increased cupping or depth of the LC, and the development of focal LC defects. These LC changes are associated with disk hemorrhages and visual field damage, and are detectable with clinical imaging techniques such as optical coherence tomography (OCT). Glaucomatous changes in the LC are driven by cellular processes mediated by focal cyclical mechanical strain. Strain is eye specific and mediated by IOP, cerebrospinal fluid pressure (CSFP), and scleral and LC morphology and structural stiffness; deleterious LC strains can occur at all levels of mean IOP. Summary Laminar morphology is ever changing in health and disease, and recent studies have identified several promising morphological changes that are indicative of glaucoma susceptibility, onset and progression.},
  pmcid = {PMC5480216},
  pmid = {27898470},
  file = {C:\Users\cleme\Zotero\storage\7G2VY7WR\Downs et Girkin - 2017 - Lamina Cribrosa in Glaucoma.pdf}
}

@article{dreoRobustRigidRegistration2006,
  title = {Robust Rigid Registration of Retinal Angiograms through Optimization},
  author = {Dr{\'e}o, Johann and Nunes, Jean-Claude and Siarry, Patrick},
  year = {2006},
  month = dec,
  journal = {Computerized Medical Imaging and Graphics},
  volume = {30},
  number = {8},
  pages = {453--463},
  issn = {0895-6111},
  doi = {10.1016/j.compmedimag.2006.07.004},
  urldate = {2019-12-10},
  abstract = {Retinal fundus photographs are employed as standard diagnostic tools in ophthalmology. Serial photographs of the flow of fluorescein and indocyanine green (ICG) dye are used to determine the areas of the retinal lesions. For objective measurements of features, the registration of the images is a necessity. In this paper, we employ optimization techniques for registration with the help of 2-parameter translational motion model of retinal angiograms, based on non-linear pre-processing (Wiener filtering and morphological gradient) and computation of the similarity criteria for the alignment of the two gradient images for any given rigid transformation. The optimization methods are effectively employed to minimize the similarity criterion. The presence of noise, the variations in the background and the temporal variation of the fluorescence level pose serious problems in obtaining a robust registration of the retinal images. Moreover, local search strategies are not robust in the case of ICG angiograms, even if one uses a multiresolution approach. The present work makes a systematic comparison of different optimization techniques, namely the minimization method derived from the optical flow formulation, the Nelder-Mead local search and the HCIAC ant colony metaheuristic, each optimizing a similarity criterion for the gradient images. The impact of the resolution and median filtering of gradient image is studied and the robustness of the approaches is tested through experimental studies, performed on macular fluorescein and ICG angiographies. Our proposed optimization techniques have shown interesting results especially for high resolution difficult registration problems. Moreover, this approach seems promising for affine (6-parameter motion model) or elastical registrations.},
  langid = {english},
  keywords = {Global search,Image registration,Local search,Mathematical morphology,Metaheuristic,Optimization,Retinal angiography}
}

@article{dreoRobustRigidRegistration2006a,
  title = {Robust Rigid Registration of Retinal Angiograms through Optimization},
  author = {Dr{\'e}o, Johann and Nunes, Jean-Claude and Siarry, Patrick},
  year = {2006},
  month = dec,
  journal = {Computerized Medical Imaging and Graphics},
  volume = {30},
  number = {8},
  pages = {453--463},
  issn = {0895-6111},
  doi = {10.1016/j.compmedimag.2006.07.004},
  urldate = {2019-12-10},
  abstract = {Retinal fundus photographs are employed as standard diagnostic tools in ophthalmology. Serial photographs of the flow of fluorescein and indocyanine green (ICG) dye are used to determine the areas of the retinal lesions. For objective measurements of features, the registration of the images is a necessity. In this paper, we employ optimization techniques for registration with the help of 2-parameter translational motion model of retinal angiograms, based on non-linear pre-processing (Wiener filtering and morphological gradient) and computation of the similarity criteria for the alignment of the two gradient images for any given rigid transformation. The optimization methods are effectively employed to minimize the similarity criterion. The presence of noise, the variations in the background and the temporal variation of the fluorescence level pose serious problems in obtaining a robust registration of the retinal images. Moreover, local search strategies are not robust in the case of ICG angiograms, even if one uses a multiresolution approach. The present work makes a systematic comparison of different optimization techniques, namely the minimization method derived from the optical flow formulation, the Nelder-Mead local search and the HCIAC ant colony metaheuristic, each optimizing a similarity criterion for the gradient images. The impact of the resolution and median filtering of gradient image is studied and the robustness of the approaches is tested through experimental studies, performed on macular fluorescein and ICG angiographies. Our proposed optimization techniques have shown interesting results especially for high resolution difficult registration problems. Moreover, this approach seems promising for affine (6-parameter motion model) or elastical registrations.},
  langid = {english},
  keywords = {Global search,Image registration,Local search,Mathematical morphology,Metaheuristic,Optimization,Retinal angiography},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\EY9EEKCF\\Dréo et al. - 2006 - Robust rigid registration of retinal angiograms th.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RJVWB6YM\\10.1016@j.compmedimag.2006.07.004.pdf;C\:\\Users\\cleme\\Zotero\\storage\\HAZC6AWX\\S0895611106000760.html}
}

@article{drexlerStateoftheartRetinalOptical2008,
  title = {State-of-the-Art Retinal Optical Coherence Tomography},
  author = {Drexler, Wolfgang and Fujimoto, James G.},
  year = {2008},
  month = jan,
  journal = {Progress in Retinal and Eye Research},
  volume = {27},
  number = {1},
  pages = {45--88},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2007.07.005},
  urldate = {2019-11-14},
  abstract = {OCT functions as a type of optical biopsy, providing information on retinal pathology in situ and in real time, with resolutions approaching that of excisional biopsy and histopathology. The development of ultrabroad-bandwidth and tunable light sources, as well as high-speed Fourier detection techniques, has enabled a significant improvement in ophthalmic optical coherence tomography (OCT) imaging performance. Three-dimensional, ultrahigh-resolution OCT (UHR OCT) can provide information on intraretinal morphology that is not available from any other non-invasive diagnostic. High-speed imaging facilitates the acquisition of three-dimensional data sets (3D-OCT), thus enabling volumetric rendering and the generation of OCT fundus images that precisely and reproducibly register OCT images to fundus features. The development of broadband light sources emitting at new wavelengths, e.g., {$\sim$}1050nm, has enabled not only 3D-OCT imaging with enhanced choroidal visualization, but also reduced scattering losses and improved OCT performance in cataract patients. Adaptive optics using high-stroke, deformable mirror technology to correct higher order aberrations in the human eye, in combination with specially designed optics to compensate chromatic aberration along with three-dimensional UHR OCT, has recently enabled in vivo cellular-resolution retinal imaging. In addition, extensions of OCT have been developed to enhance image contrast and to enable non-invasive depth-resolved functional imaging of the retina, thus providing blood flow, spectroscopic, polarization-sensitive and physiological information. Functional OCT promises to enable the differentiation of retinal pathologies via localized, functional retinal response or metabolic properties. These advances promise to have a powerful impact on fundamental as well as clinical studies.},
  langid = {english}
}

@article{drexlerStateoftheartRetinalOptical2008a,
  title = {State-of-the-Art Retinal Optical Coherence Tomography},
  author = {Drexler, Wolfgang and Fujimoto, James G.},
  year = {2008},
  month = jan,
  journal = {Progress in Retinal and Eye Research},
  volume = {27},
  number = {1},
  pages = {45--88},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2007.07.005},
  urldate = {2019-11-14},
  abstract = {OCT functions as a type of optical biopsy, providing information on retinal pathology in situ and in real time, with resolutions approaching that of excisional biopsy and histopathology. The development of ultrabroad-bandwidth and tunable light sources, as well as high-speed Fourier detection techniques, has enabled a significant improvement in ophthalmic optical coherence tomography (OCT) imaging performance. Three-dimensional, ultrahigh-resolution OCT (UHR OCT) can provide information on intraretinal morphology that is not available from any other non-invasive diagnostic. High-speed imaging facilitates the acquisition of three-dimensional data sets (3D-OCT), thus enabling volumetric rendering and the generation of OCT fundus images that precisely and reproducibly register OCT images to fundus features. The development of broadband light sources emitting at new wavelengths, e.g., {$\sim$}1050nm, has enabled not only 3D-OCT imaging with enhanced choroidal visualization, but also reduced scattering losses and improved OCT performance in cataract patients. Adaptive optics using high-stroke, deformable mirror technology to correct higher order aberrations in the human eye, in combination with specially designed optics to compensate chromatic aberration along with three-dimensional UHR OCT, has recently enabled in vivo cellular-resolution retinal imaging. In addition, extensions of OCT have been developed to enhance image contrast and to enable non-invasive depth-resolved functional imaging of the retina, thus providing blood flow, spectroscopic, polarization-sensitive and physiological information. Functional OCT promises to enable the differentiation of retinal pathologies via localized, functional retinal response or metabolic properties. These advances promise to have a powerful impact on fundamental as well as clinical studies.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\ENRMLPX3\\Drexler et Fujimoto - 2008 - State-of-the-art retinal optical coherence tomogra.pdf;C\:\\Users\\cleme\\Zotero\\storage\\WS8TF6UG\\S1350946207000444.html}
}

@article{duboisHighresolutionFullfieldOptical2002,
  title = {High-Resolution Full-Field Optical Coherence Tomography with a {{Linnik}} Microscope},
  author = {Dubois, Arnaud and Vabre, Laurent and Boccara, Albert-Claude and Beaurepaire, Emmanuel},
  year = {2002},
  month = feb,
  journal = {Applied Optics},
  volume = {41},
  number = {4},
  pages = {805--812},
  issn = {2155-3165},
  doi = {10.1364/AO.41.000805},
  urldate = {2019-11-14},
  abstract = {We describe an original microscope for high-resolution optical coherence tomography applications. Our system is based on a Linnik interference microscope with high-numerical-aperture objectives. Lock-in detection of the interference signal is achieved in parallel on a CCD by use of a photoelastic birefringence modulator and full-field stroboscopic illumination with an infrared LED. Transverse cross-section (en-face, or XY) images can be obtained in real time with better than 1-{\textmu}m axial (Z) resolution and 0.5-{\textmu}m transverse (XY) resolution. A sensitivity of {$\sim$}80 dB is reached at a 1-image/s acquisition rate, which allows tomography in scattering media such as biological tissues.},
  copyright = {\&\#169; 2002 Optical Society of America},
  langid = {english},
  keywords = {Full field optical coherence tomography,Imaging systems,Phase modulation,Spatial frequency,Three dimensional imaging,Tomography}
}

@article{duboisHighresolutionFullfieldOptical2002a,
  title = {High-Resolution Full-Field Optical Coherence Tomography with a {{Linnik}} Microscope},
  author = {Dubois, Arnaud and Vabre, Laurent and Boccara, Albert-Claude and Beaurepaire, Emmanuel},
  year = {2002},
  month = feb,
  journal = {Applied Optics},
  volume = {41},
  number = {4},
  pages = {805--812},
  issn = {2155-3165},
  doi = {10.1364/AO.41.000805},
  urldate = {2019-11-14},
  abstract = {We describe an original microscope for high-resolution optical coherence tomography applications. Our system is based on a Linnik interference microscope with high-numerical-aperture objectives. Lock-in detection of the interference signal is achieved in parallel on a CCD by use of a photoelastic birefringence modulator and full-field stroboscopic illumination with an infrared LED. Transverse cross-section (en-face, or XY) images can be obtained in real time with better than 1-{\textmu}m axial (Z) resolution and 0.5-{\textmu}m transverse (XY) resolution. A sensitivity of {$\sim$}80 dB is reached at a 1-image/s acquisition rate, which allows tomography in scattering media such as biological tissues.},
  copyright = {\&\#169; 2002 Optical Society of America},
  langid = {english},
  keywords = {Full field optical coherence tomography,Imaging systems,Phase modulation,Spatial frequency,Three dimensional imaging,Tomography},
  file = {C:\Users\cleme\Zotero\storage\SEEGW68T\abstract.html}
}

@article{duboisUltrahighresolutionFullfieldOptical2004,
  title = {Ultrahigh-Resolution Full-Field Optical Coherence Tomography},
  author = {Dubois, Arnaud and Grieve, Kate and Moneron, Gael and Lecaque, Romain and Vabre, Laurent and Boccara, Claude},
  year = {2004},
  month = may,
  journal = {Applied Optics},
  volume = {43},
  number = {14},
  pages = {2874--2883},
  issn = {2155-3165},
  doi = {10.1364/AO.43.002874},
  urldate = {2019-11-14},
  abstract = {We have developed a white-light interference microscope for ultrahigh-resolution full-field optical coherence tomography of biological media. The experimental setup is based on a Linnik-type interferometer illuminated by a tungsten halogen lamp. En face tomographic images are calculated by a combination of interferometric images recorded by a high-speed CCD camera. Spatial resolution of 1.8 {$\mu$}m {\texttimes} 0.9 {$\mu$}m (transverse {\texttimes} axial) is achieved owing to the extremely short coherence length of the source, the compensation of dispersion mismatch in the interferometer arms, and the use of relatively high-numerical-aperture microscope objectives. A shot-noise-limited detection sensitivity of 90 dB is obtained in an acquisition time per image of 4 s. Subcellular-level images of plant, animal, and human tissues are presented.},
  copyright = {\&\#169; 2004 Optical Society of America},
  langid = {english},
  keywords = {Extended depth of field,Full field optical coherence tomography,Image quality,Imaging systems,Imaging techniques,Optical imaging}
}

@article{duboisUltrahighresolutionFullfieldOptical2004a,
  title = {Ultrahigh-Resolution Full-Field Optical Coherence Tomography},
  author = {Dubois, Arnaud and Grieve, Kate and Moneron, Gael and Lecaque, Romain and Vabre, Laurent and Boccara, Claude},
  year = {2004},
  month = may,
  journal = {Applied Optics},
  volume = {43},
  number = {14},
  pages = {2874--2883},
  issn = {2155-3165},
  doi = {10.1364/AO.43.002874},
  urldate = {2019-11-14},
  abstract = {We have developed a white-light interference microscope for ultrahigh-resolution full-field optical coherence tomography of biological media. The experimental setup is based on a Linnik-type interferometer illuminated by a tungsten halogen lamp. En face tomographic images are calculated by a combination of interferometric images recorded by a high-speed CCD camera. Spatial resolution of 1.8 {$\mu$}m {\texttimes} 0.9 {$\mu$}m (transverse {\texttimes} axial) is achieved owing to the extremely short coherence length of the source, the compensation of dispersion mismatch in the interferometer arms, and the use of relatively high-numerical-aperture microscope objectives. A shot-noise-limited detection sensitivity of 90 dB is obtained in an acquisition time per image of 4 s. Subcellular-level images of plant, animal, and human tissues are presented.},
  copyright = {\&\#169; 2004 Optical Society of America},
  langid = {english},
  keywords = {Extended depth of field,Full field optical coherence tomography,Image quality,Imaging systems,Imaging techniques,Optical imaging},
  file = {C:\Users\cleme\Zotero\storage\MP9N95E7\abstract.html}
}

@article{dvorakTropicalCycloneIntensity1975,
  title = {Tropical {{Cyclone Intensity Analysis}} and {{Forecasting}} from {{Satellite Imagery}}},
  author = {Dvorak, Vernon F.},
  year = {1975},
  month = may,
  journal = {Monthly Weather Review},
  volume = {103},
  number = {5},
  pages = {420--430},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1975)103<0420:TCIAAF>2.0.CO;2},
  urldate = {2019-06-10},
  abstract = {A technique for using satellite pictures to analyse and forecast tropical cyclone intensifies is described. The cloud features used to estimate the cyclone's intensity and its future change of intensity are described. Procedures for interpreting cloud characteristics and their day-by-day changes within the guidance and constraints of an empirical model of tropical cyclone changes are outlined.}
}

@article{dvorakTropicalCycloneIntensity1975a,
  title = {Tropical {{Cyclone Intensity Analysis}} and {{Forecasting}} from {{Satellite Imagery}}},
  author = {Dvorak, Vernon F.},
  year = {1975},
  month = may,
  journal = {Monthly Weather Review},
  volume = {103},
  number = {5},
  pages = {420--430},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1975)103<0420:TCIAAF>2.0.CO;2},
  urldate = {2019-06-10},
  abstract = {A technique for using satellite pictures to analyse and forecast tropical cyclone intensifies is described. The cloud features used to estimate the cyclone's intensity and its future change of intensity are described. Procedures for interpreting cloud characteristics and their day-by-day changes within the guidance and constraints of an empirical model of tropical cyclone changes are outlined.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\YRT2Z3Z8\\Dvorak - 1975 - Tropical Cyclone Intensity Analysis and Forecastin.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UBFXNY4K\\1520-0493(1975)1030420TCIAAF2.0.html}
}

@article{dwivediExplainableAIXAI2023,
  title = {Explainable {{AI}} ({{XAI}}): {{Core Ideas}}, {{Techniques}}, and {{Solutions}}},
  shorttitle = {Explainable {{AI}} ({{XAI}})},
  author = {Dwivedi, Rudresh and Dave, Devam and Naik, Het and Singhal, Smiti and Omer, Rana and Patel, Pankesh and Qian, Bin and Wen, Zhenyu and Shah, Tejal and Morgan, Graham and Ranjan, Rajiv},
  year = {2023},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {9},
  pages = {194:1--194:33},
  issn = {0360-0300},
  doi = {10.1145/3561048},
  urldate = {2023-05-04},
  abstract = {As our dependence on intelligent machines continues to grow, so does the demand for more transparent and interpretable models. In addition, the ability to explain the model generally is now the gold standard for building trust and deployment of artificial intelligence systems in critical domains. Explainable artificial intelligence (XAI) aims to provide a suite of machine learning techniques that enable human users to understand, appropriately trust, and produce more explainable models. Selecting an appropriate approach for building an XAI-enabled application requires a clear understanding of the core ideas within XAI and the associated programming frameworks. We survey state-of-the-art programming techniques for XAI and present the different phases of XAI in a typical machine learning development process. We classify the various XAI approaches and, using this taxonomy, discuss the key differences among the existing XAI techniques. Furthermore, concrete examples are used to describe these techniques that are mapped to programming frameworks and software toolkits. It is the intention that this survey will help stakeholders in selecting the appropriate approaches, programming frameworks, and software toolkits by comparing them through the lens of the presented taxonomy.},
  keywords = {Explainable artificial intelligence,interpretable AI,programming framework,software toolkits}
}

@article{dwivediExplainableAIXAI2023a,
  title = {Explainable {{AI}} ({{XAI}}): {{Core Ideas}}, {{Techniques}}, and {{Solutions}}},
  shorttitle = {Explainable {{AI}} ({{XAI}})},
  author = {Dwivedi, Rudresh and Dave, Devam and Naik, Het and Singhal, Smiti and Omer, Rana and Patel, Pankesh and Qian, Bin and Wen, Zhenyu and Shah, Tejal and Morgan, Graham and Ranjan, Rajiv},
  year = {2023},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {9},
  pages = {194:1--194:33},
  issn = {0360-0300},
  doi = {10.1145/3561048},
  urldate = {2023-05-05},
  abstract = {As our dependence on intelligent machines continues to grow, so does the demand for more transparent and interpretable models. In addition, the ability to explain the model generally is now the gold standard for building trust and deployment of artificial intelligence systems in critical domains. Explainable artificial intelligence (XAI) aims to provide a suite of machine learning techniques that enable human users to understand, appropriately trust, and produce more explainable models. Selecting an appropriate approach for building an XAI-enabled application requires a clear understanding of the core ideas within XAI and the associated programming frameworks. We survey state-of-the-art programming techniques for XAI and present the different phases of XAI in a typical machine learning development process. We classify the various XAI approaches and, using this taxonomy, discuss the key differences among the existing XAI techniques. Furthermore, concrete examples are used to describe these techniques that are mapped to programming frameworks and software toolkits. It is the intention that this survey will help stakeholders in selecting the appropriate approaches, programming frameworks, and software toolkits by comparing them through the lens of the presented taxonomy.},
  keywords = {Explainable artificial intelligence,interpretable AI,programming framework,software toolkits}
}

@article{dwivediExplainableAIXAI2023b,
  title = {Explainable {{AI}} ({{XAI}}): {{Core Ideas}}, {{Techniques}}, and {{Solutions}}},
  shorttitle = {Explainable {{AI}} ({{XAI}})},
  author = {Dwivedi, Rudresh and Dave, Devam and Naik, Het and Singhal, Smiti and Omer, Rana and Patel, Pankesh and Qian, Bin and Wen, Zhenyu and Shah, Tejal and Morgan, Graham and Ranjan, Rajiv},
  year = {2023},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {9},
  pages = {194:1--194:33},
  issn = {0360-0300},
  doi = {10.1145/3561048},
  urldate = {2023-05-05},
  abstract = {As our dependence on intelligent machines continues to grow, so does the demand for more transparent and interpretable models. In addition, the ability to explain the model generally is now the gold standard for building trust and deployment of artificial intelligence systems in critical domains. Explainable artificial intelligence (XAI) aims to provide a suite of machine learning techniques that enable human users to understand, appropriately trust, and produce more explainable models. Selecting an appropriate approach for building an XAI-enabled application requires a clear understanding of the core ideas within XAI and the associated programming frameworks. We survey state-of-the-art programming techniques for XAI and present the different phases of XAI in a typical machine learning development process. We classify the various XAI approaches and, using this taxonomy, discuss the key differences among the existing XAI techniques. Furthermore, concrete examples are used to describe these techniques that are mapped to programming frameworks and software toolkits. It is the intention that this survey will help stakeholders in selecting the appropriate approaches, programming frameworks, and software toolkits by comparing them through the lens of the presented taxonomy.},
  keywords = {Explainable artificial intelligence,interpretable AI,programming framework,software toolkits},
  file = {C:\Users\cleme\Zotero\storage\ZB6I6DB9\Dwivedi et al. - 2023 - Explainable AI (XAI) Core Ideas, Techniques, and .pdf}
}

@article{dwivediExplainableAIXAI2023c,
  title = {Explainable {{AI}} ({{XAI}}): {{Core Ideas}}, {{Techniques}}, and {{Solutions}}},
  shorttitle = {Explainable {{AI}} ({{XAI}})},
  author = {Dwivedi, Rudresh and Dave, Devam and Naik, Het and Singhal, Smiti and Omer, Rana and Patel, Pankesh and Qian, Bin and Wen, Zhenyu and Shah, Tejal and Morgan, Graham and Ranjan, Rajiv},
  year = {2023},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {9},
  pages = {194:1--194:33},
  issn = {0360-0300},
  doi = {10.1145/3561048},
  urldate = {2023-05-04},
  abstract = {As our dependence on intelligent machines continues to grow, so does the demand for more transparent and interpretable models. In addition, the ability to explain the model generally is now the gold standard for building trust and deployment of artificial intelligence systems in critical domains. Explainable artificial intelligence (XAI) aims to provide a suite of machine learning techniques that enable human users to understand, appropriately trust, and produce more explainable models. Selecting an appropriate approach for building an XAI-enabled application requires a clear understanding of the core ideas within XAI and the associated programming frameworks. We survey state-of-the-art programming techniques for XAI and present the different phases of XAI in a typical machine learning development process. We classify the various XAI approaches and, using this taxonomy, discuss the key differences among the existing XAI techniques. Furthermore, concrete examples are used to describe these techniques that are mapped to programming frameworks and software toolkits. It is the intention that this survey will help stakeholders in selecting the appropriate approaches, programming frameworks, and software toolkits by comparing them through the lens of the presented taxonomy.},
  keywords = {Explainable artificial intelligence,interpretable AI,programming framework,software toolkits},
  file = {C:\Users\cleme\Zotero\storage\A57X5YTP\Dwivedi et al. - 2023 - Explainable AI (XAI) Core Ideas, Techniques, and .pdf}
}

@misc{E53SolutioProblematis,
  title = {E53 -- {{Solutio}} Problematis Ad Geometriam Situs Pertinentis},
  urldate = {2019-11-24}
}

@misc{E53SolutioProblematisa,
  title = {E53 -- {{Solutio}} Problematis Ad Geometriam Situs Pertinentis},
  urldate = {2019-11-24},
  howpublished = {http://eulerarchive.maa.org//pages/E053.html},
  file = {C:\Users\cleme\Zotero\storage\SRYE38I2\E053.html}
}

@misc{EarlyTreatmentDiabetic,
  title = {Early {{Treatment Diabetic Retinopathy Study}} ({{ETDRS}}) - {{Full Text View}} - {{ClinicalTrials}}.Gov},
  urldate = {2019-11-19},
  abstract = {Early Treatment Diabetic Retinopathy Study (ETDRS) - Full Text View.},
  langid = {english}
}

@article{EarlyTreatmentDiabetic1991,
  title = {Early {{Treatment Diabetic Retinopathy Study Design}} and {{Baseline Patient Characteristics}}: {{ETDRS Report Number}} 7},
  shorttitle = {Early {{Treatment Diabetic Retinopathy Study Design}} and {{Baseline Patient Characteristics}}},
  year = {1991},
  month = may,
  journal = {Ophthalmology},
  volume = {98},
  number = {5, Supplement},
  pages = {741--756},
  issn = {0161-6420},
  doi = {10.1016/S0161-6420(13)38009-9},
  urldate = {2019-11-19},
  abstract = {The Early Treatment Diabetic Retinopathy Study (ETDRS), a multicenter collaborative clinical trial supported by the National Eye Institute, was designed to assess whether argon laser photocoagulation or aspirin treatment can reduce the risk of visual loss or slow the progression of diabetic retinopathy in patients with mild-to-severe nonproliferative or early proliferative diabetic retinopathy. The 3711 patients enrolled in the ETDRS were assigned randomly to either aspirin (650 mg per day) or placebo. One eye of each patient was assigned randomly to early argon laser photocoagulation and the other to deferral of photocoagulation. Both eyes were to be examined at least every 4 months and photocoagulation was to be initiated in eyes assigned to deferral as soon as high-risk proliferative retinopathy was detected. Examination of a large number of baseline ocular and patient characteristics indicated that there were no important differences between randomized treatment groups at baseline.},
  langid = {english}
}

@article{EarlyTreatmentDiabetic1991a,
  title = {Early {{Treatment Diabetic Retinopathy Study Design}} and {{Baseline Patient Characteristics}}: {{ETDRS Report Number}} 7},
  shorttitle = {Early {{Treatment Diabetic Retinopathy Study Design}} and {{Baseline Patient Characteristics}}},
  year = {1991},
  month = may,
  journal = {Ophthalmology},
  volume = {98},
  number = {5, Supplement},
  pages = {741--756},
  issn = {0161-6420},
  doi = {10.1016/S0161-6420(13)38009-9},
  urldate = {2019-11-19},
  abstract = {The Early Treatment Diabetic Retinopathy Study (ETDRS), a multicenter collaborative clinical trial supported by the National Eye Institute, was designed to assess whether argon laser photocoagulation or aspirin treatment can reduce the risk of visual loss or slow the progression of diabetic retinopathy in patients with mild-to-severe nonproliferative or early proliferative diabetic retinopathy. The 3711 patients enrolled in the ETDRS were assigned randomly to either aspirin (650 mg per day) or placebo. One eye of each patient was assigned randomly to early argon laser photocoagulation and the other to deferral of photocoagulation. Both eyes were to be examined at least every 4 months and photocoagulation was to be initiated in eyes assigned to deferral as soon as high-risk proliferative retinopathy was detected. Examination of a large number of baseline ocular and patient characteristics indicated that there were no important differences between randomized treatment groups at baseline.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\MHAU4UP6\S0161642013380099.html}
}

@misc{EarlyTreatmentDiabetica,
  title = {Early {{Treatment Diabetic Retinopathy Study}} ({{ETDRS}}) - {{Full Text View}} - {{ClinicalTrials}}.Gov},
  urldate = {2019-11-19},
  abstract = {Early Treatment Diabetic Retinopathy Study (ETDRS) - Full Text View.},
  howpublished = {https://clinicaltrials.gov/ct2/show/NCT00000151},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\5BBV9QW4\NCT00000151.html}
}

@inproceedings{eitelTestingRobustnessAttribution2019,
  title = {Testing the {{Robustness}} of {{Attribution Methods}} for {{Convolutional Neural Networks}} in {{MRI-Based Alzheimer}}'s {{Disease Classification}}},
  booktitle = {Interpretability of {{Machine Intelligence}} in {{Medical Image Computing}} and {{Multimodal Learning}} for {{Clinical Decision Support}}},
  author = {Eitel, Fabian and Ritter, Kerstin},
  editor = {Suzuki, Kenji and Reyes, Mauricio and {Syeda-Mahmood}, Tanveer and Konukoglu, Ender and Glocker, Ben and Wiest, Roland and Gur, Yaniv and Greenspan, Hayit and Madabhushi, Anant},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--11},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-33850-3_1},
  abstract = {Attribution methods are an easy to use tool for investigating and validating machine learning models. Multiple methods have been suggested in the literature and it is not yet clear which method is most suitable for a given task. In this study, we tested the robustness of four attribution methods, namely gradient * input, guided backpropagation, layer-wise relevance propagation and occlusion, for the task of Alzheimer's disease classification. We have repeatedly trained a convolutional neural network (CNN) with identical training settings in order to separate structural MRI data of patients with Alzheimer's disease and healthy controls. Afterwards, we produced attribution maps for each subject in the test data and quantitatively compared them across models and attribution methods. We show that visual comparison is not sufficient and that some widely used attribution methods produce highly inconsistent outcomes.},
  isbn = {978-3-030-33850-3},
  langid = {english},
  keywords = {Alzheimer's disease,Attribution methods,Convolutional neural networks,Explainability,Machine learning,MRI,Robustness}
}

@inproceedings{eitelTestingRobustnessAttribution2019a,
  title = {Testing the {{Robustness}} of {{Attribution Methods}} for {{Convolutional Neural Networks}} in {{MRI-Based Alzheimer}}'s {{Disease Classification}}},
  booktitle = {Interpretability of {{Machine Intelligence}} in {{Medical Image Computing}} and {{Multimodal Learning}} for {{Clinical Decision Support}}},
  author = {Eitel, Fabian and Ritter, Kerstin},
  editor = {Suzuki, Kenji and Reyes, Mauricio and {Syeda-Mahmood}, Tanveer and Konukoglu, Ender and Glocker, Ben and Wiest, Roland and Gur, Yaniv and Greenspan, Hayit and Madabhushi, Anant},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--11},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-33850-3_1},
  abstract = {Attribution methods are an easy to use tool for investigating and validating machine learning models. Multiple methods have been suggested in the literature and it is not yet clear which method is most suitable for a given task. In this study, we tested the robustness of four attribution methods, namely gradient * input, guided backpropagation, layer-wise relevance propagation and occlusion, for the task of Alzheimer's disease classification. We have repeatedly trained a convolutional neural network (CNN) with identical training settings in order to separate structural MRI data of patients with Alzheimer's disease and healthy controls. Afterwards, we produced attribution maps for each subject in the test data and quantitatively compared them across models and attribution methods. We show that visual comparison is not sufficient and that some widely used attribution methods produce highly inconsistent outcomes.},
  isbn = {978-3-030-33850-3},
  langid = {english},
  keywords = {Alzheimer's disease,Attribution methods,Convolutional neural networks,Explainability,Machine learning,MRI,Robustness}
}

@article{eltanbolyComputeraidedDiagnosticSystem2017,
  title = {A Computer-Aided Diagnostic System for Detecting Diabetic Retinopathy in Optical Coherence Tomography Images},
  author = {El Tanboly, A. and Ismail, M. and Shalaby, A. and Switala, A. and {El-Baz}, A. and Schaal, S. and Gimel'farb, G. and {El-Azab}, M.},
  year = {2017},
  month = mar,
  journal = {Medical Physics},
  series = {Med. {{Phys}}. ({{USA}})},
  volume = {44},
  number = {3},
  pages = {914--23},
  issn = {0094-2405},
  doi = {10.1002/mp.12071},
  abstract = {Purpose: Detection (diagnosis) of diabetic retinopathy (DR) in optical coherence tomography (OCT) images for patients with type 2 diabetes, but almost clinically normal retina appearances. Methods: The proposed computer-aided diagnostic (CAD) system detects the DR in three steps: (a) localizing and segmenting 12 distinct retinal layers on the OCT image; (b) deriving features of the segmented layers, and (c) learning most discriminative features and classifying each subject as normal or diabetic. To localise and segment the retinal layers, signals (intensities) of the OCT image are described with a joint Markov-Gibbs random field (MGRF) model of intensities and shape descriptors. Each segmented layer is characterized with cumulative probability distribution functions (CDF) of its locally extracted features, such as reflectivity, curvature, and thickness. A multistage deep fusion classification network (DFCN) with a stack of non-negativity-constrained autoencoders (NCAE) is trained to select the most discriminative retinal layers' features and use their CDFs for detecting the DR. A training atlas was built using the OCT scans for 12 normal subjects and their maps of layers hand-drawn by retina experts. Results: Preliminary experiments on 52 clinical OCT scans (26 normal and 26 with early-stage DR, balanced between 40-79 yr old males and females; 40 training and 12 test subjects) gave the DR detection accuracy, sensitivity, and specificity of 92\%; 83\%, and 100\%, respectively. The 100\% accuracy, sensitivity, and specificity have been obtained in the leave-one-out cross-validation test for all the 52 subjects. Conclusion: Both the quantitative and visual assessments confirmed the high accuracy of the proposed computer-assisted diagnostic system for early DR detection using the OCT retinal images.},
  keywords = {biomedical optical imaging,diabetic retinopathy (DR),diseases,eye,feature extraction,image classification,image coding,joint image-region-map model,Markov-Gibbs random field (MGRF),Markov\textendashGibbs random field (MGRF),medical image processing,non-negativity-constrained autoencoder (NCAE),optical coherence tomography (OCT),optical tomography,probability},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\Y338D5N8\\ElTanboly et al. - 2017 - A computer-aided diagnostic system for detecting d.pdf;C\:\\Users\\cleme\\Zotero\\storage\\7SB5NKPF\\mp.html}
}

@inproceedings{er-huzhangAutomaticRetinalImage2002,
  title = {Automatic Retinal Image Registration Based on Blood Vessels Feature Point},
  booktitle = {Proceedings. {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  author = {{Er-Hu Zhang} and {Yan Zhang} and {Tian-Xu Zhang}},
  year = {2002},
  month = nov,
  volume = {4},
  pages = {2010-2015 vol.4},
  doi = {10.1109/ICMLC.2002.1175389},
  abstract = {Retinal image registration is commonly required in order to combine the complementary information in different retinal modalities images, which is very useful for clinic diagnoses and laser treatment. In the paper, an automatic scheme to register retinal images based on blood vessel feature point extraction is presented. Firstly, the crossover and branching points of the RF (red-free) retinal image and the FA (fluoroscein angiography) retinal image are extracted using a multi-direction exploratory method after the blood vessel visibility is enhanced. Considering the number of feature points extracted on different images are not equal and their location does not correspond between the retinal images, the scheme described in the paper combined the closest interval criterion between the corresponding feature points with a SVD algorithm to accurately register retinal images. Our method compared with previously known ways is an efficient and automatic scheme of retinal image registration. The experimental results indicate the registration of the fluoroscein angiography image with the corresponding red-free retinal image and a series of the fluoroscein angiography images can be also successful using our method.},
  keywords = {Anatomy,Angiography,automatic scheme,Biomedical imaging,biomedical optical imaging,blood vessels,Blood vessels,blood vessels feature point,branching points,clinic diagnoses,closest interval criterion,complementary information,crossover points,Educational technology,eye,feature extraction,Feature extraction,fluoroscein angiography retinal image,image enhancement,image registration,Image registration,laser treatment,Medical diagnostic imaging,medical image processing,multi-direction exploratory method,Radio frequency,red-free retinal image,Retina,retinal image registration,singular value decomposition,SVD algorithm}
}

@inproceedings{er-huzhangAutomaticRetinalImage2002a,
  title = {Automatic Retinal Image Registration Based on Blood Vessels Feature Point},
  booktitle = {Proceedings. {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  author = {{Er-Hu Zhang} and {Yan Zhang} and {Tian-Xu Zhang}},
  year = {2002},
  month = nov,
  volume = {4},
  pages = {2010-2015 vol.4},
  doi = {10.1109/ICMLC.2002.1175389},
  abstract = {Retinal image registration is commonly required in order to combine the complementary information in different retinal modalities images, which is very useful for clinic diagnoses and laser treatment. In the paper, an automatic scheme to register retinal images based on blood vessel feature point extraction is presented. Firstly, the crossover and branching points of the RF (red-free) retinal image and the FA (fluoroscein angiography) retinal image are extracted using a multi-direction exploratory method after the blood vessel visibility is enhanced. Considering the number of feature points extracted on different images are not equal and their location does not correspond between the retinal images, the scheme described in the paper combined the closest interval criterion between the corresponding feature points with a SVD algorithm to accurately register retinal images. Our method compared with previously known ways is an efficient and automatic scheme of retinal image registration. The experimental results indicate the registration of the fluoroscein angiography image with the corresponding red-free retinal image and a series of the fluoroscein angiography images can be also successful using our method.},
  keywords = {Anatomy,Angiography,automatic scheme,Biomedical imaging,biomedical optical imaging,blood vessels,Blood vessels,blood vessels feature point,branching points,clinic diagnoses,closest interval criterion,complementary information,crossover points,Educational technology,eye,feature extraction,Feature extraction,fluoroscein angiography retinal image,image enhancement,image registration,Image registration,laser treatment,Medical diagnostic imaging,medical image processing,multi-direction exploratory method,Radio frequency,red-free retinal image,Retina,retinal image registration,singular value decomposition,SVD algorithm},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\42PN5VDJ\\Er-Hu Zhang et al. - 2002 - Automatic retinal image registration based on bloo.pdf;C\:\\Users\\cleme\\Zotero\\storage\\2J4689QL\\1175389.html}
}

@book{escalanteExplainableInterpretableModels2018,
  title = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar{\'o}, Xavier and G{\"u}{\c c}l{\"u}t{\"u}rk, Ya{\u g}mur and G{\"u}{\c c}l{\"u}, Umut and {van Gerven}, Marcel},
  year = {2018},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-98131-4},
  urldate = {2023-05-04},
  isbn = {978-3-319-98130-7 978-3-319-98131-4},
  langid = {english},
  keywords = {Benchmarking of explainable and interpretable models,Chalearn looking at people challenges,Explainable and interpretable decision support systems,Explainable learning machines,Explainable models in computer vision,Explaining first impressions,Explaining human behavior from data,Explaining Looking at people,Interpretable models,Interpreting human behavior analysis models,Job candidate screening,Multimodal analysis of human behavior}
}

@book{escalanteExplainableInterpretableModels2018a,
  title = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar{\'o}, Xavier and G{\"u}{\c c}l{\"u}t{\"u}rk, Ya{\u g}mur and G{\"u}{\c c}l{\"u}, Umut and {van Gerven}, Marcel},
  year = {2018},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-98131-4},
  urldate = {2023-05-04},
  isbn = {978-3-319-98130-7 978-3-319-98131-4},
  langid = {english},
  keywords = {Benchmarking of explainable and interpretable models,Chalearn looking at people challenges,Explainable and interpretable decision support systems,Explainable learning machines,Explainable models in computer vision,Explaining first impressions,Explaining human behavior from data,Explaining Looking at people,Interpretable models,Interpreting human behavior analysis models,Job candidate screening,Multimodal analysis of human behavior},
  file = {C:\Users\cleme\Zotero\storage\QFQHQJR6\Escalante et al. - 2018 - Explainable and Interpretable Models in Computer V.pdf}
}

@article{estradaRetinalArteryVeinClassification2015,
  title = {Retinal {{Artery-Vein Classification}} via {{Topology Estimation}}},
  author = {Estrada, Rolando and Allingham, Michael J. and Mettu, Priyatham S. and Cousins, Scott W. and Tomasi, Carlo and Farsiu, Sina},
  year = {2015},
  month = dec,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {34},
  number = {12},
  pages = {2518--2534},
  issn = {1558-254X},
  doi = {10.1109/TMI.2015.2443117},
  abstract = {We propose a novel, graph-theoretic framework for distinguishing arteries from veins in a fundus image. We make use of the underlying vessel topology to better classify small and midsized vessels. We extend our previously proposed tree topology estimation framework by incorporating expert, domain-specific features to construct a simple, yet powerful global likelihood model. We efficiently maximize this model by iteratively exploring the space of possible solutions consistent with the projected vessels. We tested our method on four retinal datasets and achieved classification accuracies of 91.0\%, 93.5\%, 91.7\%, and 90.9\%, outperforming existing methods. Our results show the effectiveness of our approach, which is capable of analyzing the entire vasculature, including peripheral vessels, in wide field-of-view fundus photographs. This topology-based method is a potentially important tool for diagnosing diseases with retinal vascular manifestation.},
  keywords = {Algorithms,Arteries,Artery-vein classification,biomedical optical imaging,blood vessels,Computer-Assisted,Databases,Diagnostic Techniques,eye,Factual,fundus image,graph theory,graph-theoretic framework,Humans,image analysis,image classification,Image color analysis,Image Processing,Labeling,likelihood model,medical image processing,medical imaging,Ophthalmological,Retina,Retinal Artery,retinal artery-vein classification,Retinal Vein,Space exploration,Topology,tree topology,tree topology estimation framework,trees (mathematics),Veins}
}

@article{estradaRetinalArteryVeinClassification2015a,
  title = {Retinal {{Artery-Vein Classification}} via {{Topology Estimation}}},
  author = {Estrada, Rolando and Allingham, Michael J. and Mettu, Priyatham S. and Cousins, Scott W. and Tomasi, Carlo and Farsiu, Sina},
  year = {2015},
  month = dec,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {34},
  number = {12},
  pages = {2518--2534},
  issn = {1558-254X},
  doi = {10.1109/TMI.2015.2443117},
  abstract = {We propose a novel, graph-theoretic framework for distinguishing arteries from veins in a fundus image. We make use of the underlying vessel topology to better classify small and midsized vessels. We extend our previously proposed tree topology estimation framework by incorporating expert, domain-specific features to construct a simple, yet powerful global likelihood model. We efficiently maximize this model by iteratively exploring the space of possible solutions consistent with the projected vessels. We tested our method on four retinal datasets and achieved classification accuracies of 91.0\%, 93.5\%, 91.7\%, and 90.9\%, outperforming existing methods. Our results show the effectiveness of our approach, which is capable of analyzing the entire vasculature, including peripheral vessels, in wide field-of-view fundus photographs. This topology-based method is a potentially important tool for diagnosing diseases with retinal vascular manifestation.},
  keywords = {Algorithms,Arteries,Artery-vein classification,biomedical optical imaging,blood vessels,Databases Factual,Diagnostic Techniques Ophthalmological,eye,fundus image,graph theory,graph-theoretic framework,Humans,image analysis,image classification,Image color analysis,Image Processing Computer-Assisted,Labeling,likelihood model,medical image processing,medical imaging,Retina,Retinal Artery,retinal artery-vein classification,Retinal Vein,Space exploration,Topology,tree topology,tree topology estimation framework,trees (mathematics),Veins},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\LFHA4N7S\\Estrada et al. - 2015 - Retinal Artery-Vein Classification via Topology Es.pdf;C\:\\Users\\cleme\\Zotero\\storage\\P82JBPXK\\estrada2015.pdf;C\:\\Users\\cleme\\Zotero\\storage\\55XLAZY9\\7120990.html}
}

@inproceedings{evo-vit,
  title = {Evo-Vit: {{Slow-fast}} Token Evolution for Dynamic Vision Transformer},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing},
  year = {2022},
  volume = {36},
  pages = {2964--2972}
}

@inproceedings{evo-vit,
  title = {Evo-Vit: {{Slow-fast}} Token Evolution for Dynamic Vision Transformer},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing},
  year = {2022},
  volume = {36},
  pages = {2964--2972}
}

@misc{ExplainabilityGraphNeural,
  title = {Explainability in {{Graph Neural Networks}}: {{A Taxonomic Survey}}},
  urldate = {2023-10-03}
}

@misc{ExplainabilityGraphNeurala,
  title = {Explainability in {{Graph Neural Networks}}: {{A Taxonomic Survey}}},
  urldate = {2023-10-03},
  howpublished = {https://www.computer.org/csdl/journal/tp/2023/05/09875989/1GqajxgkWcM},
  file = {C:\Users\cleme\Zotero\storage\P6SXWV2M\1GqajxgkWcM.html}
}

@misc{ExplainableAIMedical,
  title = {Explainable {{AI}} for Medical Imaging: {{Deep-learning CNN}} Ensemble for Classification of Estrogen Receptor Status from Breast {{MRI}}},
  urldate = {2021-11-16}
}

@misc{ExplainableAIMedicala,
  title = {Explainable {{AI}} for Medical Imaging: Deep-Learning {{CNN}} Ensemble for Classification of Estrogen Receptor Status from Breast {{MRI}}},
  urldate = {2021-11-16},
  howpublished = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11314/2549298/Explainable-AI-for-medical-imaging--deep-learning-CNN-ensemble/10.1117/12.2549298.short?SSO=1},
  file = {C:\Users\cleme\Zotero\storage\MH7SQZES\12.2549298.html}
}

@misc{EyenukAnnouncesFDA2020,
  title = {Eyenuk {{Announces FDA Clearance}} for {{EyeArt Autonomous AI System}} for {{Diabetic Retinopathy Screening}}},
  year = {2020},
  month = aug,
  urldate = {2023-06-26},
  abstract = {Eyenuk, Inc., a global artificial intelligence (AI) medical technology and services company and the leader in real-world applications for AI Eye Scree},
  langid = {english}
}

@misc{EyenukAnnouncesFDA2020a,
  title = {Eyenuk {{Announces FDA Clearance}} for {{EyeArt Autonomous AI System}} for {{Diabetic Retinopathy Screening}}},
  year = {2020},
  month = aug,
  urldate = {2023-06-26},
  abstract = {Eyenuk, Inc., a global artificial intelligence (AI) medical technology and services company and the leader in real-world applications for AI Eye Scree},
  howpublished = {https://www.businesswire.com/news/home/20200805005495/en/Eyenuk-Announces-FDA-Clearance-for-EyeArt-Autonomous-AI-System-for-Diabetic-Retinopathy-Screening},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\B392NCCL\Eyenuk-Announces-FDA-Clearance-EyeArt-Autonomous-AI.html}
}

@article{fangAttentionLesionLesionAware2019,
  title = {Attention to {{Lesion}}: {{Lesion-Aware Convolutional Neural Network}} for {{Retinal Optical Coherence Tomography Image Classification}}},
  shorttitle = {Attention to {{Lesion}}},
  author = {Fang, L. and Wang, C. and Li, S. and Rabbani, H. and Chen, X. and Liu, Z.},
  year = {2019},
  journal = {IEEE Transactions on Medical Imaging},
  pages = {1--1},
  issn = {0278-0062},
  doi = {10.1109/TMI.2019.2898414},
  abstract = {Automatic and accurate classification of retinal optical coherence tomography (OCT) images is essential to assist ophthalmologist in the diagnosis and grading of macular diseases. Clinically, ophthalmologists usually diagnose macular diseases according to the structures of macular lesions, whose morphologies, size, and numbers are important criteria. In this paper, we propose a novel lesion-aware convolutional neural network (LACNN) method for retinal OCT image classification, in which retinal lesions within OCT images are utilized to guide the CNN to achieve more accurate classification. The LACNN simulates the ophthalmologists' diagnosis that focuses on local lesion-related regions when analyzing the OCT image. Specifically, we firstly design a lesion detection network (LDN) to generate a soft attention map from the whole OCT image. The attention map is then incorporated into a classification network to weight the contributions of local convolutional representations. Guided by the lesion attention map, the classification network can utilize the information from local lesion-related regions to further accelerate the network training process and improve the OCT classification. Our experimental results on two clinically acquired OCT datasets demonstrate the effectiveness and efficiency of the proposed LACNN method for retinal OCT image classification.},
  keywords = {attention network,Convolution,convolutional neural network,Diseases,Feature extraction,image classification,Image classification,Kernel,Lesions,optical coherence tomography,Retina,retinal lesion}
}

@article{fangAttentionLesionLesionAware2019a,
  title = {Attention to {{Lesion}}: {{Lesion-Aware Convolutional Neural Network}} for {{Retinal Optical Coherence Tomography Image Classification}}},
  shorttitle = {Attention to {{Lesion}}},
  author = {Fang, Leyuan and Wang, Chong and Li, Shutao and Rabbani, Hossein and Chen, Xiangdong and Liu, Zhimin},
  year = {2019},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {38},
  number = {8},
  pages = {1959--1970},
  issn = {1558-254X},
  doi = {10.1109/TMI.2019.2898414},
  abstract = {Automatic and accurate classification of retinal optical coherence tomography (OCT) images is essential to assist ophthalmologist in the diagnosis and grading of macular diseases. Clinically, ophthalmologists usually diagnose macular diseases according to the structures of macular lesions, whose morphologies, size, and numbers are important criteria. In this paper, we propose a novel lesion-aware convolutional neural network (LACNN) method for retinal OCT image classification, in which retinal lesions within OCT images are utilized to guide the CNN to achieve more accurate classification. The LACNN simulates the ophthalmologists' diagnosis that focuses on local lesion-related regions when analyzing the OCT image. Specifically, we first design a lesion detection network to generate a soft attention map from the whole OCT image. The attention map is then incorporated into a classification network to weight the contributions of local convolutional representations. Guided by the lesion attention map, the classification network can utilize the information from local lesion-related regions to further accelerate the network training process and improve the OCT classification. Our experimental results on two clinically acquired OCT datasets demonstrate the effectiveness and efficiency of the proposed LACNN method for retinal OCT image classification.},
  keywords = {attention network,Convolution,convolutional neural network,Diseases,Feature extraction,image classification,Image classification,Kernel,Lesions,Optical coherence tomography,Retina,retinal lesion}
}

@article{fangAttentionLesionLesionAware2019b,
  title = {Attention to {{Lesion}}: {{Lesion-Aware Convolutional Neural Network}} for {{Retinal Optical Coherence Tomography Image Classification}}},
  shorttitle = {Attention to {{Lesion}}},
  author = {Fang, L. and Wang, C. and Li, S. and Rabbani, H. and Chen, X. and Liu, Z.},
  year = {2019},
  journal = {IEEE Transactions on Medical Imaging},
  pages = {1--1},
  issn = {0278-0062},
  doi = {10.1109/TMI.2019.2898414},
  abstract = {Automatic and accurate classification of retinal optical coherence tomography (OCT) images is essential to assist ophthalmologist in the diagnosis and grading of macular diseases. Clinically, ophthalmologists usually diagnose macular diseases according to the structures of macular lesions, whose morphologies, size, and numbers are important criteria. In this paper, we propose a novel lesion-aware convolutional neural network (LACNN) method for retinal OCT image classification, in which retinal lesions within OCT images are utilized to guide the CNN to achieve more accurate classification. The LACNN simulates the ophthalmologists' diagnosis that focuses on local lesion-related regions when analyzing the OCT image. Specifically, we firstly design a lesion detection network (LDN) to generate a soft attention map from the whole OCT image. The attention map is then incorporated into a classification network to weight the contributions of local convolutional representations. Guided by the lesion attention map, the classification network can utilize the information from local lesion-related regions to further accelerate the network training process and improve the OCT classification. Our experimental results on two clinically acquired OCT datasets demonstrate the effectiveness and efficiency of the proposed LACNN method for retinal OCT image classification.},
  keywords = {attention network,Convolution,convolutional neural network,Diseases,Feature extraction,image classification,Image classification,Kernel,Lesions,optical coherence tomography,Retina,retinal lesion},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\6CSI2HER\\Fang et al. - 2019 - Attention to Lesion Lesion-Aware Convolutional Ne.pdf;C\:\\Users\\cleme\\Zotero\\storage\\DLRADXJ8\\8637959.html}
}

@article{fangAttentionLesionLesionAware2019c,
  title = {Attention to {{Lesion}}: {{Lesion-Aware Convolutional Neural Network}} for {{Retinal Optical Coherence Tomography Image Classification}}},
  shorttitle = {Attention to {{Lesion}}},
  author = {Fang, Leyuan and Wang, Chong and Li, Shutao and Rabbani, Hossein and Chen, Xiangdong and Liu, Zhimin},
  year = {2019},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {38},
  number = {8},
  pages = {1959--1970},
  issn = {1558-254X},
  doi = {10.1109/TMI.2019.2898414},
  abstract = {Automatic and accurate classification of retinal optical coherence tomography (OCT) images is essential to assist ophthalmologist in the diagnosis and grading of macular diseases. Clinically, ophthalmologists usually diagnose macular diseases according to the structures of macular lesions, whose morphologies, size, and numbers are important criteria. In this paper, we propose a novel lesion-aware convolutional neural network (LACNN) method for retinal OCT image classification, in which retinal lesions within OCT images are utilized to guide the CNN to achieve more accurate classification. The LACNN simulates the ophthalmologists' diagnosis that focuses on local lesion-related regions when analyzing the OCT image. Specifically, we first design a lesion detection network to generate a soft attention map from the whole OCT image. The attention map is then incorporated into a classification network to weight the contributions of local convolutional representations. Guided by the lesion attention map, the classification network can utilize the information from local lesion-related regions to further accelerate the network training process and improve the OCT classification. Our experimental results on two clinically acquired OCT datasets demonstrate the effectiveness and efficiency of the proposed LACNN method for retinal OCT image classification.},
  keywords = {attention network,Convolution,convolutional neural network,Diseases,Feature extraction,image classification,Image classification,Kernel,Lesions,Optical coherence tomography,Retina,retinal lesion},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\36FXGLXL\\10.1109@TMI.2019.2898414.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\3MV45BK9\\8637959.html}
}

@article{fangAutomaticSegmentationNine2017,
  title = {Automatic Segmentation of Nine Retinal Layer Boundaries in {{OCT}} Images of Non-Exudative {{AMD}} Patients Using Deep Learning and Graph Search},
  author = {Fang, Leyuan and Cunefare, David and Wang, Chong and Guymer, Robyn H. and Li, Shutao and Farsiu, Sina},
  year = {2017},
  month = may,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {5},
  pages = {2732--2744},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.002732},
  urldate = {2022-07-08},
  abstract = {We present a novel framework combining convolutional neural networks (CNN) and graph search methods (termed as CNN-GS) for the automatic segmentation of nine layer boundaries on retinal optical coherence tomography (OCT) images. CNN-GS first utilizes a CNN to extract features of specific retinal layer boundaries and train a corresponding classifier to delineate a pilot estimate of the eight layers. Next, a graph search method uses the probability maps created from the CNN to find the final boundaries. We validated our proposed method on 60 volumes (2915 B-scans) from 20 human eyes with non-exudative age-related macular degeneration (AMD), which attested to effectiveness of our proposed technique.},
  copyright = {\&\#169; 2017 Optical Society of America},
  langid = {english}
}

@article{fangAutomaticSegmentationNine2017a,
  title = {Automatic Segmentation of Nine Retinal Layer Boundaries in {{OCT}} Images of Non-Exudative {{AMD}} Patients Using Deep Learning and Graph Search},
  author = {Fang, Leyuan and Cunefare, David and Wang, Chong and Guymer, Robyn H. and Li, Shutao and Farsiu, Sina},
  year = {2017},
  month = may,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {5},
  pages = {2732--2744},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.002732},
  urldate = {2022-07-08},
  abstract = {We present a novel framework combining convolutional neural networks (CNN) and graph search methods (termed as CNN-GS) for the automatic segmentation of nine layer boundaries on retinal optical coherence tomography (OCT) images. CNN-GS first utilizes a CNN to extract features of specific retinal layer boundaries and train a corresponding classifier to delineate a pilot estimate of the eight layers. Next, a graph search method uses the probability maps created from the CNN to find the final boundaries. We validated our proposed method on 60 volumes (2915 B-scans) from 20 human eyes with non-exudative age-related macular degeneration (AMD), which attested to effectiveness of our proposed technique.},
  copyright = {\&\#169; 2017 Optical Society of America},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\MF8U59UT\\Fang et al. - 2017 - Automatic segmentation of nine retinal layer bound.pdf;C\:\\Users\\cleme\\Zotero\\storage\\C8JL9DRL\\fulltext.html}
}

@article{farabetLearningHierarchicalFeatures2013,
  title = {Learning {{Hierarchical Features}} for {{Scene Labeling}}},
  author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {1915--1929},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.231},
  abstract = {Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320{\texttimes}240 image labeling in less than a second, including feature extraction.},
  keywords = {Accuracy,Context,Convolutional networks,deep learning,Feature extraction,image classification,Image edge detection,image segmentation,Image segmentation,Labeling,scene parsing,Vectors}
}

@article{farabetLearningHierarchicalFeatures2013a,
  title = {Learning {{Hierarchical Features}} for {{Scene Labeling}}},
  author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {1915--1929},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.231},
  abstract = {Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320{\texttimes}240 image labeling in less than a second, including feature extraction.},
  keywords = {Accuracy,Context,Convolutional networks,deep learning,Feature extraction,image classification,Image edge detection,image segmentation,Image segmentation,Labeling,scene parsing,Vectors},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\JVPE6FGJ\\Farabet et al. - 2013 - Learning Hierarchical Features for Scene Labeling.pdf;C\:\\Users\\cleme\\Zotero\\storage\\4GJ7QZNA\\6338939.html}
}

@article{farsiuQuantitativeClassificationEyes2014,
  title = {Quantitative Classification of Eyes with and without Intermediate Age-Related Macular Degeneration Using Optical Coherence Tomography},
  author = {Farsiu, Sina and Chiu, Stephanie J. and O'Connell, Rachelle V. and Folgar, Francisco A. and Yuan, Eric and Izatt, Joseph A. and Toth, Cynthia A. and {Age-Related Eye Disease Study 2 Ancillary Spectral Domain Optical Coherence Tomography Study Group}},
  year = {2014},
  month = jan,
  journal = {Ophthalmology},
  volume = {121},
  number = {1},
  pages = {162--172},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2013.07.013},
  abstract = {OBJECTIVE: To define quantitative indicators for the presence of intermediate age-related macular degeneration (AMD) via spectral-domain optical coherence tomography (SD-OCT) imaging of older adults. DESIGN: Evaluation of diagnostic test and technology. PARTICIPANTS AND CONTROLS: One eye from 115 elderly subjects without AMD and 269 subjects with intermediate AMD from the Age-Related Eye Disease Study 2 (AREDS2) Ancillary SD-OCT Study. METHODS: We semiautomatically delineated the retinal pigment epithelium (RPE) and RPE drusen complex (RPEDC, the axial distance from the apex of the drusen and RPE layer to Bruch's membrane) and total retina (TR, the axial distance between the inner limiting and Bruch's membranes) boundaries. We registered and averaged the thickness maps from control subjects to generate a map of "normal" non-AMD thickness. We considered RPEDC thicknesses larger or smaller than 3 standard deviations from the mean as abnormal, indicating drusen or geographic atrophy (GA), respectively. We measured TR volumes, RPEDC volumes, and abnormal RPEDC thickening and thinning volumes for each subject. By using different combinations of these 4 disease indicators, we designed 5 automated classifiers for the presence of AMD on the basis of the generalized linear model regression framework. We trained and evaluated the performance of these classifiers using the leave-one-out method. MAIN OUTCOME MEASURES: The range and topographic distribution of the RPEDC and TR thicknesses in a 5-mm diameter cylinder centered at the fovea. RESULTS: The most efficient method for separating AMD and control eyes required all 4 disease indicators. The area under the curve (AUC) of the receiver operating characteristic (ROC) for this classifier was {$>$}0.99. Overall neurosensory retinal thickening in eyes with AMD versus control eyes in our study contrasts with previous smaller studies. CONCLUSIONS: We identified and validated efficient biometrics to distinguish AMD from normal eyes by analyzing the topographic distribution of normal and abnormal RPEDC thicknesses across a large atlas of eyes. We created an online atlas to share the 38\,400 SD-OCT images in this study, their corresponding segmentations, and quantitative measurements.},
  langid = {english},
  pmcid = {PMC3901571},
  pmid = {23993787},
  keywords = {80 and over,Aged,Area Under Curve,Biometry,Bruch Membrane,Humans,Macular Degeneration,Middle Aged,Optical Coherence,Retina,Retinal Drusen,Retinal Pigment Epithelium,ROC Curve,Tomography}
}

@article{farsiuQuantitativeClassificationEyes2014a,
  title = {Quantitative Classification of Eyes with and without Intermediate Age-Related Macular Degeneration Using Optical Coherence Tomography},
  author = {Farsiu, Sina and Chiu, Stephanie J. and O'Connell, Rachelle V. and Folgar, Francisco A. and Yuan, Eric and Izatt, Joseph A. and Toth, Cynthia A. and {Age-Related Eye Disease Study 2 Ancillary Spectral Domain Optical Coherence Tomography Study Group}},
  year = {2014},
  month = jan,
  journal = {Ophthalmology},
  volume = {121},
  number = {1},
  pages = {162--172},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2013.07.013},
  abstract = {OBJECTIVE: To define quantitative indicators for the presence of intermediate age-related macular degeneration (AMD) via spectral-domain optical coherence tomography (SD-OCT) imaging of older adults. DESIGN: Evaluation of diagnostic test and technology. PARTICIPANTS AND CONTROLS: One eye from 115 elderly subjects without AMD and 269 subjects with intermediate AMD from the Age-Related Eye Disease Study 2 (AREDS2) Ancillary SD-OCT Study. METHODS: We semiautomatically delineated the retinal pigment epithelium (RPE) and RPE drusen complex (RPEDC, the axial distance from the apex of the drusen and RPE layer to Bruch's membrane) and total retina (TR, the axial distance between the inner limiting and Bruch's membranes) boundaries. We registered and averaged the thickness maps from control subjects to generate a map of "normal" non-AMD thickness. We considered RPEDC thicknesses larger or smaller than 3 standard deviations from the mean as abnormal, indicating drusen or geographic atrophy (GA), respectively. We measured TR volumes, RPEDC volumes, and abnormal RPEDC thickening and thinning volumes for each subject. By using different combinations of these 4 disease indicators, we designed 5 automated classifiers for the presence of AMD on the basis of the generalized linear model regression framework. We trained and evaluated the performance of these classifiers using the leave-one-out method. MAIN OUTCOME MEASURES: The range and topographic distribution of the RPEDC and TR thicknesses in a 5-mm diameter cylinder centered at the fovea. RESULTS: The most efficient method for separating AMD and control eyes required all 4 disease indicators. The area under the curve (AUC) of the receiver operating characteristic (ROC) for this classifier was {$>$}0.99. Overall neurosensory retinal thickening in eyes with AMD versus control eyes in our study contrasts with previous smaller studies. CONCLUSIONS: We identified and validated efficient biometrics to distinguish AMD from normal eyes by analyzing the topographic distribution of normal and abnormal RPEDC thicknesses across a large atlas of eyes. We created an online atlas to share the 38\,400 SD-OCT images in this study, their corresponding segmentations, and quantitative measurements.},
  langid = {english},
  pmcid = {PMC3901571},
  pmid = {23993787},
  keywords = {Aged,Aged 80 and over,Area Under Curve,Biometry,Bruch Membrane,Humans,Macular Degeneration,Middle Aged,Retina,Retinal Drusen,Retinal Pigment Epithelium,ROC Curve,Tomography Optical Coherence},
  file = {C:\Users\cleme\Zotero\storage\8TMWTDWY\Farsiu et al. - 2014 - Quantitative classification of eyes with and witho.pdf}
}

@article{faustAlgorithmsAutomatedDetection2012,
  title = {Algorithms for the {{Automated Detection}} of {{Diabetic Retinopathy Using Digital Fundus Images}}: {{A Review}}},
  shorttitle = {Algorithms for the {{Automated Detection}} of {{Diabetic Retinopathy Using Digital Fundus Images}}},
  author = {Faust, Oliver and Acharya U., Rajendra and Ng, E. Y. K. and Ng, Kwan-Hoong and Suri, Jasjit S.},
  year = {2012},
  month = feb,
  journal = {Journal of Medical Systems},
  volume = {36},
  number = {1},
  pages = {145--157},
  issn = {0148-5598, 1573-689X},
  doi = {10.1007/s10916-010-9454-7},
  urldate = {2019-09-23},
  langid = {english}
}

@article{faustAlgorithmsAutomatedDetection2012a,
  title = {Algorithms for the {{Automated Detection}} of {{Diabetic Retinopathy Using Digital Fundus Images}}: {{A Review}}},
  shorttitle = {Algorithms for the {{Automated Detection}} of {{Diabetic Retinopathy Using Digital Fundus Images}}},
  author = {Faust, Oliver and Acharya U., Rajendra and Ng, E. Y. K. and Ng, Kwan-Hoong and Suri, Jasjit S.},
  year = {2012},
  month = feb,
  journal = {Journal of Medical Systems},
  volume = {36},
  number = {1},
  pages = {145--157},
  issn = {0148-5598, 1573-689X},
  doi = {10.1007/s10916-010-9454-7},
  urldate = {2019-09-23},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\X3V555A8\Faust et al. - 2012 - Algorithms for the Automated Detection of Diabetic.pdf}
}

@article{fauwClinicallyApplicableDeep2018,
  title = {Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease},
  author = {Fauw, Jeffrey De and Ledsam, Joseph R. and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O'Donoghue, Brendan and Visentin, Daniel and {van den Driessche}, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, C{\'i}an O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
  year = {2018},
  month = sep,
  journal = {Nature Medicine},
  volume = {24},
  number = {9},
  pages = {1342--1350},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0107-6},
  urldate = {2019-12-09},
  abstract = {A novel deep learning architecture performs device-independent tissue segmentation of clinical 3D retinal images followed by separate diagnostic classification that meets or exceeds human expert clinical diagnoses of retinal disease.},
  copyright = {2018 The Author(s)},
  langid = {english}
}

@article{fauwClinicallyApplicableDeep2018a,
  title = {Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease},
  author = {Fauw, Jeffrey De and Ledsam, Joseph R. and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O'Donoghue, Brendan and Visentin, Daniel and {van den Driessche}, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, C{\'i}an O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
  year = {2018},
  month = sep,
  journal = {Nature Medicine},
  volume = {24},
  number = {9},
  pages = {1342--1350},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0107-6},
  urldate = {2021-04-13},
  abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.},
  copyright = {2018 The Author(s)},
  langid = {english}
}

@article{fauwClinicallyApplicableDeep2018b,
  title = {Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease},
  author = {Fauw, Jeffrey De and Ledsam, Joseph R. and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O'Donoghue, Brendan and Visentin, Daniel and van den Driessche, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, C{\'i}an O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
  year = {2018},
  month = sep,
  journal = {Nature Medicine},
  volume = {24},
  number = {9},
  pages = {1342--1350},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0107-6},
  urldate = {2019-12-09},
  abstract = {A novel deep learning architecture performs device-independent tissue segmentation of clinical 3D retinal images followed by separate diagnostic classification that meets or exceeds human expert clinical diagnoses of retinal disease.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\4LTDH36P\\Fauw et al. - 2018 - Clinically applicable deep learning for diagnosis .pdf;C\:\\Users\\cleme\\Zotero\\storage\\DBUVAJX3\\defauw2018.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RRALAKMM\\s41591-018-0107-6.html}
}

@article{fauwClinicallyApplicableDeep2018c,
  title = {Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease},
  author = {Fauw, Jeffrey De and Ledsam, Joseph R. and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O'Donoghue, Brendan and Visentin, Daniel and van den Driessche, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, C{\'i}an O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
  year = {2018},
  month = sep,
  journal = {Nature Medicine},
  volume = {24},
  number = {9},
  pages = {1342--1350},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0107-6},
  urldate = {2021-04-13},
  abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HCIDWZ6A\\Fauw et al. - 2018 - Clinically applicable deep learning for diagnosis .pdf;C\:\\Users\\cleme\\Zotero\\storage\\GY32LGAA\\s41591-018-0107-6.html}
}

@article{felzenszwalbEfficientGraphBasedImage2004,
  title = {Efficient {{Graph-Based Image Segmentation}}},
  author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
  year = {2004},
  month = sep,
  journal = {International Journal of Computer Vision},
  volume = {59},
  number = {2},
  pages = {167--181},
  issn = {0920-5691},
  doi = {10.1023/B:VISI.0000022288.19776.77},
  urldate = {2020-08-17},
  abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
  langid = {english}
}

@article{felzenszwalbEfficientGraphBasedImage2004a,
  title = {Efficient {{Graph-Based Image Segmentation}}},
  author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
  year = {2004},
  month = sep,
  journal = {International Journal of Computer Vision},
  volume = {59},
  number = {2},
  pages = {167--181},
  issn = {0920-5691},
  doi = {10.1023/B:VISI.0000022288.19776.77},
  urldate = {2020-08-17},
  abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\WEQ2QF2A\Felzenszwalb et Huttenlocher - 2004 - Efficient Graph-Based Image Segmentation.pdf}
}

@article{fercherEyelengthMeasurementInterferometry1988,
  title = {Eye-Length Measurement by Interferometry with Partially Coherent Light},
  author = {Fercher, A. F. and Mengedoht, K. and Werner, W.},
  year = {1988},
  month = mar,
  journal = {Optics Letters},
  volume = {13},
  number = {3},
  pages = {186--188},
  issn = {1539-4794},
  doi = {10.1364/OL.13.000186},
  urldate = {2019-11-14},
  abstract = {With a multimode semiconductor laser we have been able to measure the optical length of the eye within a precision of 0.03 mm. A first series of in{$\upsilon$}i{$\upsilon$}o measurements of several human beings shows good correlation with the acoustically determined eye length.},
  copyright = {\&\#169; 1988 Optical Society of America},
  langid = {english},
  keywords = {Diode lasers,Laser beams,Laser light,Light beams,Refractive index,Semiconductor lasers}
}

@article{fercherEyelengthMeasurementInterferometry1988a,
  title = {Eye-Length Measurement by Interferometry with Partially Coherent Light},
  author = {Fercher, A. F. and Mengedoht, K. and Werner, W.},
  year = {1988},
  month = mar,
  journal = {Optics Letters},
  volume = {13},
  number = {3},
  pages = {186--188},
  issn = {1539-4794},
  doi = {10.1364/OL.13.000186},
  urldate = {2019-11-14},
  abstract = {With a multimode semiconductor laser we have been able to measure the optical length of the eye within a precision of 0.03 mm. A first series of in{$\upsilon$}i{$\upsilon$}o measurements of several human beings shows good correlation with the acoustically determined eye length.},
  copyright = {\&\#169; 1988 Optical Society of America},
  langid = {english},
  keywords = {Diode lasers,Laser beams,Laser light,Light beams,Refractive index,Semiconductor lasers},
  file = {C:\Users\cleme\Zotero\storage\NLTXS6XK\abstract.html}
}

@article{fercherVivoOpticalCoherence1993,
  title = {In {{Vivo Optical Coherence Tomography}}},
  author = {Fercher, Adolf F. and Hitzenberger, Christoph K. and Drexler, Wolfgang and Kamp, Gerhard and Sattmann, Harald},
  year = {1993},
  month = jul,
  journal = {American Journal of Ophthalmology},
  volume = {116},
  number = {1},
  pages = {113--114},
  issn = {0002-9394},
  doi = {10.1016/S0002-9394(14)71762-3},
  urldate = {2019-11-14},
  langid = {english}
}

@article{fercherVivoOpticalCoherence1993a,
  title = {In {{Vivo Optical Coherence Tomography}}},
  author = {Fercher, Adolf F. and Hitzenberger, Christoph K. and Drexler, Wolfgang and Kamp, Gerhard and Sattmann, Harald},
  year = {1993},
  month = jul,
  journal = {American Journal of Ophthalmology},
  volume = {116},
  number = {1},
  pages = {113--114},
  issn = {0002-9394},
  doi = {10.1016/S0002-9394(14)71762-3},
  urldate = {2019-11-14},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\U6SVCFKS\S0002939414717623.html}
}

@article{fernandezDelineatingFluidfilledRegion2005,
  title = {Delineating Fluid-Filled Region Boundaries in Optical Coherence Tomography Images of the Retina},
  author = {Fernandez, D. C.},
  year = {2005},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {24},
  number = {8},
  pages = {929--945},
  issn = {0278-0062},
  doi = {10.1109/TMI.2005.848655},
  abstract = {We evaluate the ability of a deformable model to yield accurate shape descriptions of fluid-filled regions associated with age-related macular degeneration. Calculation of retinal thickness and volume by the current optical coherence tomography (OCT) system includes fluid-filled regions or lesions along with actual retinal tissue. In order to quantify these lesions independently from the retinal tissue, they must be outlined. A deformable model was applied to OCT images of retinas demonstrating cystoids and subretinal fluid spaces. Several implementation issues were addressed in order to choose appropriate parameters. The use of a nonlinear anisotropic diffusion filter to suppress speckle noise while at the same time preserving the edges of the original image was explored. Once the contours of the lesions were outlined, quantitative analysis of the surface area and volume of the lesions was performed. The deformable model could accurately outline fluid-filled regions within the retina. The detection method tested proved effective in capturing the complexity of fluid-filled regions in OCT images. Deformable models combined with nonlinear anisotropic diffusion filtering show promise in the detection of retinal features of interest for diagnosis in clinical OCT images. Thus, fluid-filled region detection may significantly aid in analysis of treatments and diagnosis.},
  keywords = {Active contour models,Adaptive optics,age-related macular degeneration,Algorithms,Anisotropic magnetoresistance,Artificial Intelligence,Automated,Biological,biological tissues,biomedical optical imaging,Body Fluids,Coherence,Computer Simulation,Computer-Assisted,cystoids,deformable model,deformable models,Deformable models,edge detection,eye,fluid-filled region boundaries,Humans,Image Enhancement,Image Interpretation,lesion volume,lesions,Lesions,Macular Degeneration,Models,nonlinear anisotropic diffusion filter,Nonlinear optics,ophthalmology,Optical Coherence,optical coherence tomography,Optical filters,optical tomography,patient diagnosis,patient treatment,Pattern Recognition,Reproducibility of Results,retina,Retina,retinal feature detection,retinal thickness,retinal tissue,retinal volume,Sensitivity and Specificity,Shape,snakes,speckle noise,subretinal fluid spaces,Tomography}
}

@article{fernandezDelineatingFluidfilledRegion2005a,
  title = {Delineating Fluid-Filled Region Boundaries in Optical Coherence Tomography Images of the Retina},
  author = {Fernandez, D. C.},
  year = {2005},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {24},
  number = {8},
  pages = {929--945},
  issn = {0278-0062},
  doi = {10.1109/TMI.2005.848655},
  abstract = {We evaluate the ability of a deformable model to yield accurate shape descriptions of fluid-filled regions associated with age-related macular degeneration. Calculation of retinal thickness and volume by the current optical coherence tomography (OCT) system includes fluid-filled regions or lesions along with actual retinal tissue. In order to quantify these lesions independently from the retinal tissue, they must be outlined. A deformable model was applied to OCT images of retinas demonstrating cystoids and subretinal fluid spaces. Several implementation issues were addressed in order to choose appropriate parameters. The use of a nonlinear anisotropic diffusion filter to suppress speckle noise while at the same time preserving the edges of the original image was explored. Once the contours of the lesions were outlined, quantitative analysis of the surface area and volume of the lesions was performed. The deformable model could accurately outline fluid-filled regions within the retina. The detection method tested proved effective in capturing the complexity of fluid-filled regions in OCT images. Deformable models combined with nonlinear anisotropic diffusion filtering show promise in the detection of retinal features of interest for diagnosis in clinical OCT images. Thus, fluid-filled region detection may significantly aid in analysis of treatments and diagnosis.},
  keywords = {Active contour models,Adaptive optics,age-related macular degeneration,Algorithms,Anisotropic magnetoresistance,Artificial Intelligence,biological tissues,biomedical optical imaging,Body Fluids,Coherence,Computer Simulation,cystoids,deformable model,deformable models,Deformable models,edge detection,eye,fluid-filled region boundaries,Humans,Image Enhancement,Image Interpretation Computer-Assisted,lesion volume,lesions,Lesions,Macular Degeneration,Models Biological,nonlinear anisotropic diffusion filter,Nonlinear optics,ophthalmology,optical coherence tomography,Optical filters,optical tomography,patient diagnosis,patient treatment,Pattern Recognition Automated,Reproducibility of Results,retina,Retina,retinal feature detection,retinal thickness,retinal tissue,retinal volume,Sensitivity and Specificity,Shape,snakes,speckle noise,subretinal fluid spaces,Tomography,Tomography Optical Coherence},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\VS4EGP3S\\Fernandez - 2005 - Delineating fluid-filled region boundaries in opti.pdf;C\:\\Users\\cleme\\Zotero\\storage\\BYTKVJ7G\\1490663.html}
}

@article{ferrisClinicalClassificationAgerelated2013,
  title = {Clinical {{Classification}} of {{Age-related Macular Degeneration}}},
  author = {Ferris, Frederick L. and Wilkinson, C. P. and Bird, Alan and Chakravarthy, Usha and Chew, Emily and Csaky, Karl and Sadda, SriniVas R.},
  year = {2013},
  month = apr,
  journal = {Ophthalmology},
  volume = {120},
  number = {4},
  pages = {844--851},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2012.10.036},
  urldate = {2019-11-27},
  abstract = {Objective To develop a clinical classification system for age-related macular degeneration (AMD). Design Evidence-based investigation, using a modified Delphi process. Participants Twenty-six AMD experts, 1 neuro-ophthalmologist, 2 committee chairmen, and 1 methodologist. Methods Each committee member completed an online assessment of statements summarizing current AMD classification criteria, indicating agreement or disagreement with each statement on a 9-step scale. The group met, reviewed the survey results, discussed the important components of a clinical classification system, and defined new data analyses needed to refine a classification system. After the meeting, additional data analyses from large studies were provided to the committee to provide risk estimates related to the presence of various AMD lesions. Main Outcome Measures Delphi review of the 9-item set of statements resulting from the meeting. Results Consensus was achieved in generating a basic clinical classification system based on fundus lesions assessed within 2 disc diameters of the fovea in persons older than 55 years. The committee agreed that a single term, age-related macular degeneration, should be used for the disease. Persons with no visible drusen or pigmentary abnormalities should be considered to have no signs of AMD. Persons with small drusen ({$<$}63 {$\mu$}m), also termed drupelets, should be considered to have normal aging changes with no clinically relevant increased risk of late AMD developing. Persons with medium drusen ({$\geq$}63--{$<$}125 {$\mu$}m), but without pigmentary abnormalities thought to be related to AMD, should be considered to have early AMD. Persons with large drusen or with pigmentary abnormalities associated with at least medium drusen should be considered to have intermediate AMD. Persons with lesions associated with neovascular AMD or geographic atrophy should be considered to have late AMD. Five-year risks of progressing to late AMD are estimated to increase approximately 100 fold, ranging from a 0.5\% 5-year risk for normal aging changes to a 50\% risk for the highest intermediate AMD risk group. Conclusions The proposed basic clinical classification scale seems to be of value in predicting the risk of late AMD. Incorporating consistent nomenclature into the practice patterns of all eye care providers may improve communication and patient care. Financial Disclosure(s) The author(s) have no proprietary or commercial interest in any materials discussed in this article.},
  langid = {english}
}

@article{ferrisClinicalClassificationAgerelated2013a,
  title = {Clinical Classification of Age-Related Macular Degeneration},
  author = {Ferris, Frederick L. and Wilkinson, C. P. and Bird, Alan and Chakravarthy, Usha and Chew, Emily and Csaky, Karl and Sadda, SriniVas R. and {Beckman Initiative for Macular Research Classification Committee}},
  year = {2013},
  month = apr,
  journal = {Ophthalmology},
  volume = {120},
  number = {4},
  pages = {844--851},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2012.10.036},
  abstract = {OBJECTIVE: To develop a clinical classification system for age-related macular degeneration (AMD). DESIGN: Evidence-based investigation, using a modified Delphi process. PARTICIPANTS: Twenty-six AMD experts, 1 neuro-ophthalmologist, 2 committee chairmen, and 1 methodologist. METHODS: Each committee member completed an online assessment of statements summarizing current AMD classification criteria, indicating agreement or disagreement with each statement on a 9-step scale. The group met, reviewed the survey results, discussed the important components of a clinical classification system, and defined new data analyses needed to refine a classification system. After the meeting, additional data analyses from large studies were provided to the committee to provide risk estimates related to the presence of various AMD lesions. MAIN OUTCOME MEASURES: Delphi review of the 9-item set of statements resulting from the meeting. RESULTS: Consensus was achieved in generating a basic clinical classification system based on fundus lesions assessed within 2 disc diameters of the fovea in persons older than 55 years. The committee agreed that a single term, age-related macular degeneration, should be used for the disease. Persons with no visible drusen or pigmentary abnormalities should be considered to have no signs of AMD. Persons with small drusen ({$<$}63 {$\mu$}m), also termed drupelets, should be considered to have normal aging changes with no clinically relevant increased risk of late AMD developing. Persons with medium drusen ({$\geq$} 63-{$<$}125 {$\mu$}m), but without pigmentary abnormalities thought to be related to AMD, should be considered to have early AMD. Persons with large drusen or with pigmentary abnormalities associated with at least medium drusen should be considered to have intermediate AMD. Persons with lesions associated with neovascular AMD or geographic atrophy should be considered to have late AMD. Five-year risks of progressing to late AMD are estimated to increase approximately 100 fold, ranging from a 0.5\% 5-year risk for normal aging changes to a 50\% risk for the highest intermediate AMD risk group. CONCLUSIONS: The proposed basic clinical classification scale seems to be of value in predicting the risk of late AMD. Incorporating consistent nomenclature into the practice patterns of all eye care providers may improve communication and patient care.},
  langid = {english},
  pmid = {23332590},
  keywords = {Fluorescein Angiography,Fundus Oculi,Humans,Macular Degeneration,Photography,Retina,Severity of Illness Index}
}

@article{ferrisClinicalClassificationAgerelated2013b,
  title = {Clinical {{Classification}} of {{Age-related Macular Degeneration}}},
  author = {Ferris, Frederick L. and Wilkinson, C. P. and Bird, Alan and Chakravarthy, Usha and Chew, Emily and Csaky, Karl and Sadda, SriniVas R.},
  year = {2013},
  month = apr,
  journal = {Ophthalmology},
  volume = {120},
  number = {4},
  pages = {844--851},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2012.10.036},
  urldate = {2019-11-27},
  abstract = {Objective To develop a clinical classification system for age-related macular degeneration (AMD). Design Evidence-based investigation, using a modified Delphi process. Participants Twenty-six AMD experts, 1 neuro-ophthalmologist, 2 committee chairmen, and 1 methodologist. Methods Each committee member completed an online assessment of statements summarizing current AMD classification criteria, indicating agreement or disagreement with each statement on a 9-step scale. The group met, reviewed the survey results, discussed the important components of a clinical classification system, and defined new data analyses needed to refine a classification system. After the meeting, additional data analyses from large studies were provided to the committee to provide risk estimates related to the presence of various AMD lesions. Main Outcome Measures Delphi review of the 9-item set of statements resulting from the meeting. Results Consensus was achieved in generating a basic clinical classification system based on fundus lesions assessed within 2 disc diameters of the fovea in persons older than 55 years. The committee agreed that a single term, age-related macular degeneration, should be used for the disease. Persons with no visible drusen or pigmentary abnormalities should be considered to have no signs of AMD. Persons with small drusen ({$<$}63 {$\mu$}m), also termed drupelets, should be considered to have normal aging changes with no clinically relevant increased risk of late AMD developing. Persons with medium drusen ({$\geq$}63--{$<$}125 {$\mu$}m), but without pigmentary abnormalities thought to be related to AMD, should be considered to have early AMD. Persons with large drusen or with pigmentary abnormalities associated with at least medium drusen should be considered to have intermediate AMD. Persons with lesions associated with neovascular AMD or geographic atrophy should be considered to have late AMD. Five-year risks of progressing to late AMD are estimated to increase approximately 100 fold, ranging from a 0.5\% 5-year risk for normal aging changes to a 50\% risk for the highest intermediate AMD risk group. Conclusions The proposed basic clinical classification scale seems to be of value in predicting the risk of late AMD. Incorporating consistent nomenclature into the practice patterns of all eye care providers may improve communication and patient care. Financial Disclosure(s) The author(s) have no proprietary or commercial interest in any materials discussed in this article.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\FPFATZLA\\Ferris et al. - 2013 - Clinical Classification of Age-related Macular Deg.pdf;C\:\\Users\\cleme\\Zotero\\storage\\9DEQFB9S\\S016164201201055X.html}
}

@article{ferrisClinicalClassificationAgerelated2013c,
  title = {Clinical Classification of Age-Related Macular Degeneration},
  author = {Ferris, Frederick L. and Wilkinson, C. P. and Bird, Alan and Chakravarthy, Usha and Chew, Emily and Csaky, Karl and Sadda, SriniVas R. and {Beckman Initiative for Macular Research Classification Committee}},
  year = {2013},
  month = apr,
  journal = {Ophthalmology},
  volume = {120},
  number = {4},
  pages = {844--851},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2012.10.036},
  abstract = {OBJECTIVE: To develop a clinical classification system for age-related macular degeneration (AMD). DESIGN: Evidence-based investigation, using a modified Delphi process. PARTICIPANTS: Twenty-six AMD experts, 1 neuro-ophthalmologist, 2 committee chairmen, and 1 methodologist. METHODS: Each committee member completed an online assessment of statements summarizing current AMD classification criteria, indicating agreement or disagreement with each statement on a 9-step scale. The group met, reviewed the survey results, discussed the important components of a clinical classification system, and defined new data analyses needed to refine a classification system. After the meeting, additional data analyses from large studies were provided to the committee to provide risk estimates related to the presence of various AMD lesions. MAIN OUTCOME MEASURES: Delphi review of the 9-item set of statements resulting from the meeting. RESULTS: Consensus was achieved in generating a basic clinical classification system based on fundus lesions assessed within 2 disc diameters of the fovea in persons older than 55 years. The committee agreed that a single term, age-related macular degeneration, should be used for the disease. Persons with no visible drusen or pigmentary abnormalities should be considered to have no signs of AMD. Persons with small drusen ({$<$}63 {$\mu$}m), also termed drupelets, should be considered to have normal aging changes with no clinically relevant increased risk of late AMD developing. Persons with medium drusen ({$\geq$} 63-{$<$}125 {$\mu$}m), but without pigmentary abnormalities thought to be related to AMD, should be considered to have early AMD. Persons with large drusen or with pigmentary abnormalities associated with at least medium drusen should be considered to have intermediate AMD. Persons with lesions associated with neovascular AMD or geographic atrophy should be considered to have late AMD. Five-year risks of progressing to late AMD are estimated to increase approximately 100 fold, ranging from a 0.5\% 5-year risk for normal aging changes to a 50\% risk for the highest intermediate AMD risk group. CONCLUSIONS: The proposed basic clinical classification scale seems to be of value in predicting the risk of late AMD. Incorporating consistent nomenclature into the practice patterns of all eye care providers may improve communication and patient care.},
  langid = {english},
  pmid = {23332590},
  keywords = {Fluorescein Angiography,Fundus Oculi,Humans,Macular Degeneration,Photography,Retina,Severity of Illness Index}
}

@article{feyJUSTJUMPDYNAMIC2019,
  title = {{{JUST JUMP}}: {{DYNAMIC NEIGHBORHOOD AGGREGATION IN GRAPH NEURAL NETWORKS}}},
  author = {Fey, Matthias},
  year = {2019},
  pages = {7},
  abstract = {We propose a dynamic neighborhood aggregation (DNA) procedure guided by (multi-head) attention for representation learning on graphs. In contrast to current graph neural networks which follow a simple neighborhood aggregation scheme, our DNA procedure allows for a selective and node-adaptive aggregation of neighboring embeddings of potentially differing locality. In order to avoid overfitting, we propose to control the channel-wise connections between input and output by making use of grouped linear projections. In a number of transductive nodeclassification experiments, we demonstrate the effectiveness of our approach.},
  langid = {english}
}

@article{feyJustJumpDynamic2019,
  title = {Just {{Jump}}: {{Dynamic Neighborhood Aggregation}} in {{Graph Neural Networks}}},
  shorttitle = {Just {{Jump}}},
  author = {Fey, Matthias},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.04849 [cs, stat]},
  eprint = {1904.04849},
  primaryclass = {cs, stat},
  urldate = {2019-12-30},
  abstract = {We propose a dynamic neighborhood aggregation (DNA) procedure guided by (multi-head) attention for representation learning on graphs. In contrast to current graph neural networks which follow a simple neighborhood aggregation scheme, our DNA procedure allows for a selective and node-adaptive aggregation of neighboring embeddings of potentially differing locality. In order to avoid overfitting, we propose to control the channel-wise connections between input and output by making use of grouped linear projections. In a number of transductive node-classification experiments, we demonstrate the effectiveness of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{feyJustJumpDynamic2019a,
  title = {Just {{Jump}}: {{Dynamic Neighborhood Aggregation}} in {{Graph Neural Networks}}},
  shorttitle = {Just {{Jump}}},
  author = {Fey, Matthias},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.04849 [cs, stat]},
  eprint = {1904.04849},
  primaryclass = {cs, stat},
  urldate = {2019-12-30},
  abstract = {We propose a dynamic neighborhood aggregation (DNA) procedure guided by (multi-head) attention for representation learning on graphs. In contrast to current graph neural networks which follow a simple neighborhood aggregation scheme, our DNA procedure allows for a selective and node-adaptive aggregation of neighboring embeddings of potentially differing locality. In order to avoid overfitting, we propose to control the channel-wise connections between input and output by making use of grouped linear projections. In a number of transductive node-classification experiments, we demonstrate the effectiveness of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\T3CJDGEV\\Fey - 2019 - Just Jump Dynamic Neighborhood Aggregation in Gra.pdf;C\:\\Users\\cleme\\Zotero\\storage\\PYAQL45Q\\1904.html}
}

@article{feyJUSTJUMPDYNAMIC2019b,
  title = {{{JUST JUMP}}: {{DYNAMIC NEIGHBORHOOD AGGREGATION IN GRAPH NEURAL NETWORKS}}},
  author = {Fey, Matthias},
  year = {2019},
  pages = {7},
  abstract = {We propose a dynamic neighborhood aggregation (DNA) procedure guided by (multi-head) attention for representation learning on graphs. In contrast to current graph neural networks which follow a simple neighborhood aggregation scheme, our DNA procedure allows for a selective and node-adaptive aggregation of neighboring embeddings of potentially differing locality. In order to avoid overfitting, we propose to control the channel-wise connections between input and output by making use of grouped linear projections. In a number of transductive nodeclassification experiments, we demonstrate the effectiveness of our approach.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\WV986YQB\Fey - 2019 - JUST JUMP DYNAMIC NEIGHBORHOOD AGGREGATION IN GRA.pdf}
}

@inproceedings{feySplineCNNFastGeometric2018,
  title = {{{SplineCNN}}: {{Fast Geometric Deep Learning}} with {{Continuous B-Spline Kernels}}},
  shorttitle = {{{SplineCNN}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Fey, Matthias and Lenssen, Jan Eric and Weichert, Frank and Muller, Heinrich},
  year = {2018},
  month = jun,
  pages = {869--877},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00097},
  urldate = {2019-12-30},
  abstract = {We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors.},
  isbn = {978-1-5386-6420-9},
  langid = {english}
}

@inproceedings{feySplineCNNFastGeometric2018a,
  title = {{{SplineCNN}}: {{Fast Geometric Deep Learning}} with {{Continuous B-Spline Kernels}}},
  shorttitle = {{{SplineCNN}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Fey, Matthias and Lenssen, Jan Eric and Weichert, Frank and Muller, Heinrich},
  year = {2018},
  month = jun,
  pages = {869--877},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00097},
  urldate = {2019-12-30},
  abstract = {We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\CCQWKTQQ\Fey et al. - 2018 - SplineCNN Fast Geometric Deep Learning with Conti.pdf}
}

@article{flaxelDiabeticRetinopathyPreferred2020,
  title = {Diabetic {{Retinopathy Preferred Practice Pattern}}{\textregistered}},
  author = {Flaxel, Christina J. and Adelman, Ron A. and Bailey, Steven T. and Fawzi, Amani and Lim, Jennifer I. and Vemulakonda, G. Atma and Ying, Gui-shuang},
  year = {2020},
  month = jan,
  journal = {Ophthalmology},
  volume = {127},
  number = {1},
  pages = {P66-P145},
  issn = {01616420},
  doi = {10.1016/j.ophtha.2019.09.025},
  urldate = {2022-10-20},
  langid = {english}
}

@article{flaxelDiabeticRetinopathyPreferred2020a,
  title = {Diabetic {{Retinopathy Preferred Practice Pattern}}{\textregistered}},
  author = {Flaxel, Christina J. and Adelman, Ron A. and Bailey, Steven T. and Fawzi, Amani and Lim, Jennifer I. and Vemulakonda, G. Atma and Ying, Gui-shuang},
  year = {2020},
  month = jan,
  journal = {Ophthalmology},
  volume = {127},
  number = {1},
  pages = {P66-P145},
  issn = {01616420},
  doi = {10.1016/j.ophtha.2019.09.025},
  urldate = {2022-10-20},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\MQ8SAHYZ\\Flaxel et al. - 2020 - Diabetic Retinopathy Preferred Practice Pattern®.pdf;C\:\\Users\\cleme\\Zotero\\storage\\YY5TNRHI\\flaxel2019.pdf.pdf}
}

@misc{FoundationsMachineLearning,
  title = {Foundations of {{Machine Learning}}, {{Second Edition}} by {{Rostamizadeh}}, {{Talwalkar}}, {{Mohri}}},
  urldate = {2023-04-23}
}

@misc{FoundationsMachineLearninga,
  title = {Foundations of {{Machine Learning}}, {{Second Edition}} by {{Rostamizadeh}}, {{Talwalkar}}, {{Mohri}}},
  urldate = {2023-04-23},
  howpublished = {https://mitpress.ublish.com/ebook/foundations-of-machine-learning--2-preview/7093/35},
  file = {C:\Users\cleme\Zotero\storage\B9SXT7VI\35.html}
}

@misc{FrontiersDiabeticRetinopathy,
  title = {Frontiers {\textbar} {{Diabetic Retinopathy Grading}} by {{Deep Graph Correlation Network}} on {{Retinal Images Without Manual Annotations}}},
  urldate = {2023-09-30}
}

@misc{FrontiersDiabeticRetinopathya,
  title = {Frontiers {\textbar} {{Diabetic Retinopathy Grading}} by {{Deep Graph Correlation Network}} on {{Retinal Images Without Manual Annotations}}},
  urldate = {2023-09-30},
  howpublished = {https://www.frontiersin.org/articles/10.3389/fmed.2022.872214/full},
  file = {C:\Users\cleme\Zotero\storage\E37GN6QU\full.html}
}

@inproceedings{fuEvaluationRetinalImage2019,
  title = {Evaluation of {{Retinal Image Quality Assessment Networks}} in {{Different Color-Spaces}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2019: 22nd {{International Conference}}, {{Shenzhen}}, {{China}}, {{October}} 13--17, 2019, {{Proceedings}}, {{Part I}}},
  author = {Fu, Huazhu and Wang, Boyang and Shen, Jianbing and Cui, Shanshan and Xu, Yanwu and Liu, Jiang and Shao, Ling},
  year = {2019},
  month = oct,
  pages = {48--56},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-32239-7_6},
  urldate = {2022-10-20},
  abstract = {Retinal image quality assessment (RIQA) is essential for controlling the quality of retinal imaging and guaranteeing the reliability of diagnoses by ophthalmologists or automated analysis systems. Existing RIQA methods focus on the RGB color-space and are developed based on small datasets with binary quality labels (i.e., `Accept' and `Reject'). In this paper, we first re-annotate an Eye-Quality (EyeQ) dataset with 28,792 retinal images from the EyePACS dataset, based on a three-level quality grading system (i.e., `Good', `Usable' and `Reject') for evaluating RIQA methods. Our RIQA dataset is characterized by its large-scale size, multi-level grading, and multi-modality. Then, we analyze the influences on RIQA of different color-spaces, and propose a simple yet efficient deep network, named Multiple Color-space Fusion Network (MCF-Net), which integrates the different color-space representations at both a feature-level and prediction-level to predict image quality grades. Experiments on our EyeQ dataset show that our MCF-Net obtains a state-of-the-art performance, outperforming the other deep learning methods. Furthermore, we also evaluate diabetic retinopathy (DR) detection methods on images of different quality, and demonstrate that the performances of automated diagnostic systems are highly dependent on image quality.},
  isbn = {978-3-030-32238-0},
  keywords = {Deep learning,Quality assessment,Retinal image}
}

@inproceedings{fuEvaluationRetinalImage2019a,
  title = {Evaluation of {{Retinal Image Quality Assessment Networks}} in {{Different Color-Spaces}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2019: 22nd {{International Conference}}, {{Shenzhen}}, {{China}}, {{October}} 13--17, 2019, {{Proceedings}}, {{Part I}}},
  author = {Fu, Huazhu and Wang, Boyang and Shen, Jianbing and Cui, Shanshan and Xu, Yanwu and Liu, Jiang and Shao, Ling},
  year = {2019},
  month = oct,
  pages = {48--56},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-32239-7_6},
  urldate = {2022-10-20},
  abstract = {Retinal image quality assessment (RIQA) is essential for controlling the quality of retinal imaging and guaranteeing the reliability of diagnoses by ophthalmologists or automated analysis systems. Existing RIQA methods focus on the RGB color-space and are developed based on small datasets with binary quality labels (i.e., `Accept' and `Reject'). In this paper, we first re-annotate an Eye-Quality (EyeQ) dataset with 28,792 retinal images from the EyePACS dataset, based on a three-level quality grading system (i.e., `Good', `Usable' and `Reject') for evaluating RIQA methods. Our RIQA dataset is characterized by its large-scale size, multi-level grading, and multi-modality. Then, we analyze the influences on RIQA of different color-spaces, and propose a simple yet efficient deep network, named Multiple Color-space Fusion Network (MCF-Net), which integrates the different color-space representations at both a feature-level and prediction-level to predict image quality grades. Experiments on our EyeQ dataset show that our MCF-Net obtains a state-of-the-art performance, outperforming the other deep learning methods. Furthermore, we also evaluate diabetic retinopathy (DR) detection methods on images of different quality, and demonstrate that the performances of automated diagnostic systems are highly dependent on image quality.},
  isbn = {978-3-030-32238-0},
  keywords = {Deep learning,Quality assessment,Retinal image},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\EEJRQL7R\\Fu et al. - 2019 - Evaluation of Retinal Image Quality Assessment Net.pdf;C\:\\Users\\cleme\\Zotero\\storage\\HW3X47ZY\\10.1007@978-3-030-32239-7.pdf.pdf}
}

@inproceedings{fumeroRIMONEOpenRetinal2011,
  title = {{{RIM-ONE}}: {{An}} Open Retinal Image Database for Optic Nerve Evaluation},
  shorttitle = {{{RIM-ONE}}},
  booktitle = {2011 24th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author = {Fumero, F. and Alayon, S. and Sanchez, J. L. and Sigut, J. and {Gonzalez-Hernandez}, M.},
  year = {2011},
  month = jun,
  pages = {1--6},
  issn = {1063-7125},
  doi = {10.1109/CBMS.2011.5999143},
  abstract = {Automated diagnosis of glaucoma disease has been studied for years. A great amount of research work in this field has been focused on the analysis of retinal fundus images to localize, detect and evaluate the optic disc. An open fundus image database with accurate gold standards of the optic nerve head has been implemented. A variability measurement by zones of the optic disc is also proposed. The relevance of this work is to provide accurate ONH segmentations and a segmentation assessment procedure to allow the design of computerized methods for glaucoma detection.},
  keywords = {automated diagnosis,Biomedical optical imaging,Databases,glaucoma detection,glaucoma disease,Gold,Hospitals,image recognition,image segmentation,Image segmentation,medical image processing,ONH segmentations,open fundus image database,open retinal image database,optic nerve evaluation,Optical imaging,patient diagnosis,Retina,RIM-ONE,visual databases}
}

@inproceedings{fumeroRIMONEOpenRetinal2011a,
  title = {{{RIM-ONE}}: {{An}} Open Retinal Image Database for Optic Nerve Evaluation},
  shorttitle = {{{RIM-ONE}}},
  booktitle = {2011 24th {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author = {Fumero, F. and Alayon, S. and Sanchez, J. L. and Sigut, J. and {Gonzalez-Hernandez}, M.},
  year = {2011},
  month = jun,
  pages = {1--6},
  issn = {1063-7125},
  doi = {10.1109/CBMS.2011.5999143},
  abstract = {Automated diagnosis of glaucoma disease has been studied for years. A great amount of research work in this field has been focused on the analysis of retinal fundus images to localize, detect and evaluate the optic disc. An open fundus image database with accurate gold standards of the optic nerve head has been implemented. A variability measurement by zones of the optic disc is also proposed. The relevance of this work is to provide accurate ONH segmentations and a segmentation assessment procedure to allow the design of computerized methods for glaucoma detection.},
  keywords = {automated diagnosis,Biomedical optical imaging,Databases,glaucoma detection,glaucoma disease,Gold,Hospitals,image recognition,image segmentation,Image segmentation,medical image processing,ONH segmentations,open fundus image database,open retinal image database,optic nerve evaluation,Optical imaging,patient diagnosis,Retina,RIM-ONE,visual databases},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\2TGMRIU3\\5999143.html;C\:\\Users\\cleme\\Zotero\\storage\\GUI2VLAK\\fumero2011.html}
}

@article{fuRetinalStatusAnalysis2016,
  title = {Retinal Status Analysis Method Based on Feature Extraction and Quantitative Grading in {{OCT}} Images},
  author = {Fu, Dongmei and Tong, Hejun and Zheng, Shuang and Luo, Ling and Gao, Fulin and Minar, Jiri},
  year = {2016},
  month = jul,
  journal = {BioMedical Engineering OnLine},
  volume = {15},
  number = {1},
  pages = {87},
  issn = {1475-925X},
  doi = {10.1186/s12938-016-0206-x},
  urldate = {2019-07-23},
  abstract = {Optical coherence tomography (OCT) is widely used in ophthalmology for viewing the morphology of the retina, which is important for disease detection and assessing therapeutic effect. The diagnosis of retinal diseases is based primarily on the subjective analysis of OCT images by trained ophthalmologists. This paper describes an OCT images automatic analysis method for computer-aided disease diagnosis and it is a critical part of the eye fundus diagnosis.}
}

@article{fuRetinalStatusAnalysis2016a,
  title = {Retinal Status Analysis Method Based on Feature Extraction and Quantitative Grading in {{OCT}} Images},
  author = {Fu, Dongmei and Tong, Hejun and Zheng, Shuang and Luo, Ling and Gao, Fulin and Minar, Jiri},
  year = {2016},
  month = jul,
  journal = {BioMedical Engineering OnLine},
  volume = {15},
  number = {1},
  pages = {87},
  issn = {1475-925X},
  doi = {10.1186/s12938-016-0206-x},
  urldate = {2019-07-23},
  abstract = {Optical coherence tomography (OCT) is widely used in ophthalmology for viewing the morphology of the retina, which is important for disease detection and assessing therapeutic effect. The diagnosis of retinal diseases is based primarily on the subjective analysis of OCT images by trained ophthalmologists. This paper describes an OCT images automatic analysis method for computer-aided disease diagnosis and it is a critical part of the eye fundus diagnosis.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\D29Y32N6\\Fu et al. - 2016 - Retinal status analysis method based on feature ex.pdf;C\:\\Users\\cleme\\Zotero\\storage\\DII6KDXB\\s12938-016-0206-x.html}
}

@article{galDropoutBayesianApproximation,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs --extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english}
}

@inproceedings{galDropoutBayesianApproximation2015,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  booktitle = {Deep {{Learning Workshop}}, {{ICML}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2015},
  volume = {1},
  pages = {2},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs --extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english}
}

@inproceedings{galDropoutBayesianApproximation2015a,
  title = {Dropout as a {{Bayesian Approximation}}:  {{Representing Model Uncertainty}} in {{Deep Learning}}},
  booktitle = {Deep {{Learning Workshop}}, {{ICML}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2015},
  volume = {1},
  pages = {2},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs --extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\LQCJR3KW\Gal et Ghahramani - Dropout as a Bayesian Approximation  Representing.pdf}
}

@inproceedings{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian}} Approximation: {{Representing}} Model Uncertainty in Deep Learning},
  shorttitle = {Dropout as a {{Bayesian}} Approximation},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 48},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  series = {{{ICML}}'16},
  pages = {1050--1059},
  publisher = {JMLR.org},
  address = {New York, NY, USA},
  urldate = {2023-09-06},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs - extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.}
}

@inproceedings{galDropoutBayesianApproximation2016a,
  title = {Dropout as a {{Bayesian}} Approximation: Representing Model Uncertainty in Deep Learning},
  shorttitle = {Dropout as a {{Bayesian}} Approximation},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 48},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  series = {{{ICML}}'16},
  pages = {1050--1059},
  publisher = {JMLR.org},
  address = {New York, NY, USA},
  urldate = {2023-09-06},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs - extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.}
}

@article{galDropoutBayesianApproximationa,
  title = {Dropout as a {{Bayesian Approximation}}:  {{Representing Model Uncertainty}} in {{Deep Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs --extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\7MXWM27U\Gal et Ghahramani - Dropout as a Bayesian Approximation  Representing.pdf}
}

@article{gallegoClusteringbasedKnearestNeighbor2018,
  title = {Clustering-Based k-{{Nearest}} Neighbor Classification for Large-Scale Data with Neural Codes Representation},
  author = {Gallego, Antonio-Javier and {Calvo-Zaragoza}, Jorge and {Valero-Mas}, Jose J. and {Rico-Juan}, Juan R.},
  year = {2018},
  month = feb,
  journal = {Pattern Recognition},
  volume = {74},
  pages = {531--543},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2017.09.038},
  urldate = {2023-05-22},
  abstract = {While standing as one of the most widely considered and successful supervised classification algorithms, the k-nearest Neighbor (kNN) classifier generally depicts a poor efficiency due to being an instance-based method. In this sense, Approximated Similarity Search (ASS) stands as a possible alternative to improve those efficiency issues at the expense of typically lowering the performance of the classifier. In this paper we take as initial point an ASS strategy based on clustering. We then improve its performance by solving issues related to instances located close to the cluster boundaries by enlarging their size and considering the use of Deep Neural Networks for learning a suitable representation for the classification task at issue. Results using a collection of eight different datasets show that the combined use of these two strategies entails a significant improvement in the accuracy performance, with a considerable reduction in the number of distances needed to classify a sample in comparison to the basic kNN rule.},
  langid = {english},
  keywords = {Clustering,Deep neural networks,Efficient NN classification}
}

@article{gallegoClusteringbasedKnearestNeighbor2018a,
  title = {Clustering-Based k-Nearest Neighbor Classification for Large-Scale Data with Neural Codes Representation},
  author = {Gallego, Antonio-Javier and {Calvo-Zaragoza}, Jorge and {Valero-Mas}, Jose J. and {Rico-Juan}, Juan R.},
  year = {2018},
  month = feb,
  journal = {Pattern Recognition},
  volume = {74},
  pages = {531--543},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2017.09.038},
  urldate = {2023-05-22},
  abstract = {While standing as one of the most widely considered and successful supervised classification algorithms, the k-nearest Neighbor (kNN) classifier generally depicts a poor efficiency due to being an instance-based method. In this sense, Approximated Similarity Search (ASS) stands as a possible alternative to improve those efficiency issues at the expense of typically lowering the performance of the classifier. In this paper we take as initial point an ASS strategy based on clustering. We then improve its performance by solving issues related to instances located close to the cluster boundaries by enlarging their size and considering the use of Deep Neural Networks for learning a suitable representation for the classification task at issue. Results using a collection of eight different datasets show that the combined use of these two strategies entails a significant improvement in the accuracy performance, with a considerable reduction in the number of distances needed to classify a sample in comparison to the basic kNN rule.},
  langid = {english},
  keywords = {Clustering,Deep neural networks,Efficient NN classification},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\7Y2YEE2T\\Gallego et al. - 2018 - Clustering-based k-nearest neighbor classification.pdf;C\:\\Users\\cleme\\Zotero\\storage\\FNXMVSXJ\\S0031320317303898.html}
}

@article{gangnonSeverityAgeRelatedMacular2015,
  title = {Severity of {{Age-Related Macular Degeneration}} in 1 {{Eye}} and the {{Incidence}} and {{Progression}} of {{Age-Related Macular Degeneration}} in the {{Fellow Eye}}},
  author = {Gangnon, Ronald E. and Lee, Kristine E. and Klein, Barbara E. K. and Iyengar, Sudha K. and Sivakumaran, Theru A. and Klein, Ronald},
  year = {2015},
  month = feb,
  journal = {JAMA ophthalmology},
  volume = {133},
  number = {2},
  pages = {125--132},
  issn = {2168-6165},
  doi = {10.1001/jamaophthalmol.2014.4252},
  urldate = {2023-06-13},
  abstract = {Importance Previous studies of the implications of age-related macular degeneration (AMD) severity in one eye on prognosis for the fellow eye have focused on incidence of neovascular AMD in the fellow eye of subjects with neovascular AMD in the other eye. It is unclear to what extent AMD severity in one eye impacts incidence, progression, and regression of AMD in its fellow eye across the entire range of AMD severity. Objective To investigate the impact of severity of AMD in one eye on incidence, progression, and regression of AMD in the fellow eye. Design, Setting and Participants The Beaver Dam Eye Study, a longitudinal population-based study of age-related eye diseases conducted in the city and township of Beaver Dam, Wisconsin. Examinations were performed every 5 years over a 20-year period (1988-1990 through 2008-2010). Study participants (N=4379) were aged 43 to 86 years at the baseline examination. At baseline and up to 4 subsequent examinations, retinal photographs were taken. Exposures Age, sex, and the Y402H polymorphism in the Complement Factor H gene on chromosome 1q; AMD severity in the fellow eye. Main Outcome Measures Incidence, progression, and regression of AMD assessed in retinal photographs according to the Wisconsin Age-Related Maculopathy Grading System; mortality. Results More severe AMD in one eye was associated with increased incidence and progression of AMD in its fellow eye (Level 1 to 2: hazard ratio [HR] 4.90, 95\% confidence interval [CI] 4.26-5.63; Level 2 to 3: HR 2.09, CI 1.42-3.06; Level 3 to 4: HR 2.38, CI 1.74-3.25; Level 4 to Level 5: HR 2.46, CI 1.65-3.66). Less severe AMD in one eye was associated with less progression of AMD in its fellow eye (Level 2 to 3: HR 0.42, CI 0.33-0.55; Level 3 to 4: HR 0.50, CI 0.34-0.83). We estimate that 51\% of subjects who develop any AMD always maintain AMD severity states within 1 step of each other between eyes; 90\% stay within 2 steps. Conclusions and Relevance Using multi-state models, we show that AMD severity in one eye tracks AMD severity in its fellow eye.},
  pmcid = {PMC4326536},
  pmid = {25340497}
}

@article{gangnonSeverityAgeRelatedMacular2015a,
  title = {Severity of {{Age-Related Macular Degeneration}} in 1 {{Eye}} and the {{Incidence}} and {{Progression}} of {{Age-Related Macular Degeneration}} in the {{Fellow Eye}}},
  author = {Gangnon, Ronald E. and Lee, Kristine E. and Klein, Barbara E. K. and Iyengar, Sudha K. and Sivakumaran, Theru A. and Klein, Ronald},
  year = {2015},
  month = feb,
  journal = {JAMA ophthalmology},
  volume = {133},
  number = {2},
  pages = {125--132},
  issn = {2168-6165},
  doi = {10.1001/jamaophthalmol.2014.4252},
  urldate = {2023-06-13},
  abstract = {Importance Previous studies of the implications of age-related macular degeneration (AMD) severity in one eye on prognosis for the fellow eye have focused on incidence of neovascular AMD in the fellow eye of subjects with neovascular AMD in the other eye. It is unclear to what extent AMD severity in one eye impacts incidence, progression, and regression of AMD in its fellow eye across the entire range of AMD severity. Objective To investigate the impact of severity of AMD in one eye on incidence, progression, and regression of AMD in the fellow eye. Design, Setting and Participants The Beaver Dam Eye Study, a longitudinal population-based study of age-related eye diseases conducted in the city and township of Beaver Dam, Wisconsin. Examinations were performed every 5 years over a 20-year period (1988-1990 through 2008-2010). Study participants (N=4379) were aged 43 to 86 years at the baseline examination. At baseline and up to 4 subsequent examinations, retinal photographs were taken. Exposures Age, sex, and the Y402H polymorphism in the Complement Factor H gene on chromosome 1q; AMD severity in the fellow eye. Main Outcome Measures Incidence, progression, and regression of AMD assessed in retinal photographs according to the Wisconsin Age-Related Maculopathy Grading System; mortality. Results More severe AMD in one eye was associated with increased incidence and progression of AMD in its fellow eye (Level 1 to 2: hazard ratio [HR] 4.90, 95\% confidence interval [CI] 4.26-5.63; Level 2 to 3: HR 2.09, CI 1.42-3.06; Level 3 to 4: HR 2.38, CI 1.74-3.25; Level 4 to Level 5: HR 2.46, CI 1.65-3.66). Less severe AMD in one eye was associated with less progression of AMD in its fellow eye (Level 2 to 3: HR 0.42, CI 0.33-0.55; Level 3 to 4: HR 0.50, CI 0.34-0.83). We estimate that 51\% of subjects who develop any AMD always maintain AMD severity states within 1 step of each other between eyes; 90\% stay within 2 steps. Conclusions and Relevance Using multi-state models, we show that AMD severity in one eye tracks AMD severity in its fellow eye.},
  pmcid = {PMC4326536},
  pmid = {25340497},
  file = {C:\Users\cleme\Zotero\storage\DHPVZUWX\Gangnon et al. - 2015 - Severity of Age-Related Macular Degeneration in 1 .pdf}
}

@incollection{ganinDomainAdversarialTrainingNeural2017,
  title = {Domain-{{Adversarial Training}} of {{Neural Networks}}},
  booktitle = {Domain {{Adaptation}} in {{Computer Vision Applications}}},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c c}ois and Marchand, Mario and Lempitsky, Victor},
  editor = {Csurka, Gabriela},
  year = {2017},
  pages = {189--209},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-58347-1_10},
  urldate = {2023-06-23},
  abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.},
  isbn = {978-3-319-58346-4 978-3-319-58347-1},
  langid = {english}
}

@incollection{ganinDomainAdversarialTrainingNeural2017a,
  title = {Domain-{{Adversarial Training}} of {{Neural Networks}}},
  booktitle = {Domain {{Adaptation}} in {{Computer Vision Applications}}},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c c}ois and Marchand, Mario and Lempitsky, Victor},
  editor = {Csurka, Gabriela},
  year = {2017},
  pages = {189--209},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-58347-1_10},
  urldate = {2023-06-23},
  abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.},
  isbn = {978-3-319-58346-4 978-3-319-58347-1},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\86BD43RI\Ganin et al. - 2017 - Domain-Adversarial Training of Neural Networks.pdf}
}

@article{gaoGraphUNets2019,
  title = {Graph {{U-Nets}}},
  author = {Gao, Hongyang and Ji, Shuiwang},
  year = {2019},
  month = may,
  journal = {arXiv:1905.05178 [cs, stat]},
  eprint = {1905.05178},
  primaryclass = {cs, stat},
  urldate = {2019-12-30},
  abstract = {We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{gaoGraphUNets2019a,
  title = {Graph {{U-Nets}}},
  author = {Gao, Hongyang and Ji, Shuiwang},
  year = {2019},
  month = may,
  journal = {arXiv:1905.05178 [cs, stat]},
  eprint = {1905.05178},
  primaryclass = {cs, stat},
  urldate = {2019-12-30},
  abstract = {We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\FZ3H65IV\\Gao et Ji - 2019 - Graph U-Nets.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6A6CZ9DD\\1905.html}
}

@article{gaoGraphUNets2022,
  title = {Graph {{U-Nets}}},
  author = {Gao, Hongyang and Ji, Shuiwang},
  year = {2022},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {9},
  pages = {4948--4960},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3081010},
  urldate = {2023-10-03},
  abstract = {We consider the problem of representation learning for graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied to image pixel-wise prediction tasks, similar methods are lacking for graph data. This is because pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling and unpooling operations. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values. We further propose the gUnpool layer as the inverse operation of the gPool layer. Based on our proposed methods, we develop an encoder-decoder model, known as the graph U-Nets. Experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models. Along this direction, we extend our methods by integrating attention mechanisms. Based on attention operators, we proposed attention-based pooling and unpooling layers, which can better capture graph topology information. The empirical results on graph classification tasks demonstrate the promising capability of our methods.}
}

@article{gaoGraphUNets2022a,
  title = {Graph {{U-Nets}}},
  author = {Gao, Hongyang and Ji, Shuiwang},
  year = {2022},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {9},
  pages = {4948--4960},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3081010},
  urldate = {2023-10-03},
  abstract = {We consider the problem of representation learning for graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied to image pixel-wise prediction tasks, similar methods are lacking for graph data. This is because pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling and unpooling operations. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values. We further propose the gUnpool layer as the inverse operation of the gPool layer. Based on our proposed methods, we develop an encoder-decoder model, known as the graph U-Nets. Experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models. Along this direction, we extend our methods by integrating attention mechanisms. Based on attention operators, we proposed attention-based pooling and unpooling layers, which can better capture graph topology information. The empirical results on graph classification tasks demonstrate the promising capability of our methods.},
  file = {C:\Users\cleme\Zotero\storage\FE8ZMLYQ\Gao et Ji - 2022 - Graph U-Nets.pdf}
}

@article{garcia-florianoMachineLearningApproach2019,
  title = {A Machine Learning Approach to Medical Image Classification: {{Detecting}} Age-Related Macular Degeneration in Fundus Images},
  shorttitle = {A Machine Learning Approach to Medical Image Classification},
  author = {{Garc{\'i}a-Floriano}, Andr{\'e}s and {Ferreira-Santiago}, {\'A}ngel and {Camacho-Nieto}, Oscar and {Y{\'a}{\~n}ez-M{\'a}rquez}, Cornelio},
  year = {2019},
  month = may,
  journal = {Computers \& Electrical Engineering},
  volume = {75},
  pages = {218--229},
  issn = {0045-7906},
  doi = {10.1016/j.compeleceng.2017.11.008},
  urldate = {2020-01-13},
  abstract = {Age-Related Macular Degeneration (AMD) is a dangerous, chronic, and progressive illness that mostly affects people over 60 years old. This disease is related to the appearance of drusen: deposits of extracellular material located in the macular region. One way to effectively and non-invasively pre-diagnose AMD is by detecting the presence of drusen in fundus images. In this work we propose a new method that combines Digital Image Processing, Mathematical Morphology and a robust and powerful Machine Learning model: a Support Vector Machine (SVM). The enclosed macular region is subjected to a contrast enhancement method, followed by the application of basic morphological operations. We use invariant moments as the features of the processed image. The resulting vector is classified by an SVM as positive or negative for drusen. The proposed method is able to discriminate between healthy and afflicted cases with a classification accuracy that outperforms many well-regarded state-of-the-art methods.},
  langid = {english},
  keywords = {Age-related macular degeneration,Feature selection,Hu invariant moments,Mathematical morphology,Support vector machines},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\V9UF6D9V\\García-Floriano et al. - 2019 - A machine learning approach to medical image class.pdf;C\:\\Users\\cleme\\Zotero\\storage\\MP9NZVVA\\S004579061731577X.html}
}

@article{gardnerAutomaticDetectionDiabetic1996,
  title = {Automatic Detection of Diabetic Retinopathy Using an Artificial Neural Network: A Screening Tool.},
  shorttitle = {Automatic Detection of Diabetic Retinopathy Using an Artificial Neural Network},
  author = {Gardner, G. G. and Keating, D. and Williamson, T. H. and Elliott, A. T.},
  year = {1996},
  month = nov,
  journal = {British Journal of Ophthalmology},
  volume = {80},
  number = {11},
  pages = {940--944},
  issn = {0007-1161, 1468-2079},
  doi = {10.1136/bjo.80.11.940},
  urldate = {2019-11-19},
  abstract = {AIMS: To determine if neural networks can detect diabetic features in fundus images and compare the network against an ophthalmologist screening a set of fundus images. METHODS: 147 diabetic and 32 normal images were captured from a fundus camera, stored on computer, and analysed using a back propagation neural network. The network was trained to recognise features in the retinal image. The effects of digital filtering techniques and different network variables were assessed. 200 diabetic and 101 normal images were then randomised and used to evaluate the network's performance for the detection of diabetic retinopathy against an ophthalmologist. RESULTS: Detection rates for the recognition of vessels, exudates, and haemorrhages were 91.7\%, 93.1\%, and 73.8\% respectively. When compared with the results of the ophthalmologist, the network achieved a sensitivity of 88.4\% and a specificity of 83.5\% for the detection of diabetic retinopathy. CONCLUSIONS: Detection of vessels, exudates, and haemorrhages was possible, with success rates dependent upon preprocessing and the number of images used in training. When compared with the ophthalmologist, the network achieved good accuracy for the detection of diabetic retinopathy. The system could be used as an aid to the screening of diabetic patients for retinopathy.},
  langid = {english},
  pmid = {8976718}
}

@article{gardnerAutomaticDetectionDiabetic1996a,
  title = {Automatic Detection of Diabetic Retinopathy Using an Artificial Neural Network: A Screening Tool.},
  shorttitle = {Automatic Detection of Diabetic Retinopathy Using an Artificial Neural Network},
  author = {Gardner, G. G. and Keating, D. and Williamson, T. H. and Elliott, A. T.},
  year = {1996},
  month = nov,
  journal = {British Journal of Ophthalmology},
  volume = {80},
  number = {11},
  pages = {940--944},
  issn = {0007-1161, 1468-2079},
  doi = {10.1136/bjo.80.11.940},
  urldate = {2019-11-19},
  abstract = {AIMS: To determine if neural networks can detect diabetic features in fundus images and compare the network against an ophthalmologist screening a set of fundus images. METHODS: 147 diabetic and 32 normal images were captured from a fundus camera, stored on computer, and analysed using a back propagation neural network. The network was trained to recognise features in the retinal image. The effects of digital filtering techniques and different network variables were assessed. 200 diabetic and 101 normal images were then randomised and used to evaluate the network's performance for the detection of diabetic retinopathy against an ophthalmologist. RESULTS: Detection rates for the recognition of vessels, exudates, and haemorrhages were 91.7\%, 93.1\%, and 73.8\% respectively. When compared with the results of the ophthalmologist, the network achieved a sensitivity of 88.4\% and a specificity of 83.5\% for the detection of diabetic retinopathy. CONCLUSIONS: Detection of vessels, exudates, and haemorrhages was possible, with success rates dependent upon preprocessing and the number of images used in training. When compared with the ophthalmologist, the network achieved good accuracy for the detection of diabetic retinopathy. The system could be used as an aid to the screening of diabetic patients for retinopathy.},
  langid = {english},
  pmid = {8976718},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\KMTIEBJ8\\Gardner et al. - 1996 - Automatic detection of diabetic retinopathy using .pdf;C\:\\Users\\cleme\\Zotero\\storage\\W7HU9FL4\\gardner1996.pdf;C\:\\Users\\cleme\\Zotero\\storage\\CBCUSTTN\\940.html}
}

@article{gargeyaAutomatedIdentificationDiabetic2017,
  title = {Automated {{Identification}} of {{Diabetic Retinopathy Using Deep Learning}}},
  author = {Gargeya, Rishab and Leng, Theodore},
  year = {2017},
  month = jul,
  journal = {Ophthalmology},
  volume = {124},
  number = {7},
  pages = {962--969},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2017.02.008},
  urldate = {2019-11-20},
  abstract = {Purpose Diabetic retinopathy (DR) is one of the leading causes of preventable blindness globally. Performing retinal screening examinations on all diabetic patients is an unmet need, and there are many undiagnosed and untreated cases of DR. The objective of this study was to develop robust diagnostic technology to automate DR screening. Referral of eyes with DR to an ophthalmologist for further evaluation and treatment would aid in reducing the rate of vision loss, enabling timely and accurate diagnoses. Design We developed and evaluated a data-driven deep learning algorithm as a novel diagnostic tool for automated DR detection. The algorithm processed color fundus images and classified them as healthy (no retinopathy) or having DR, identifying relevant cases for medical referral. Methods A total of 75\,137 publicly available fundus images from diabetic patients were used to train and test an artificial intelligence model to differentiate healthy fundi from those with DR. A panel of retinal specialists determined the ground truth for our data set before experimentation. We also tested our model using the public MESSIDOR 2 and E-Ophtha databases for external validation. Information learned in our automated method was visualized readily through an automatically generated abnormality heatmap, highlighting subregions within each input fundus image for further clinical review. Main Outcome Measures We used area under the receiver operating characteristic curve (AUC) as a metric to measure the precision--recall trade-off of our algorithm, reporting associated sensitivity and specificity metrics on the receiver operating characteristic curve. Results Our model achieved a 0.97 AUC with a 94\% and 98\% sensitivity and specificity, respectively, on 5-fold cross-validation using our local data set. Testing against the independent MESSIDOR 2 and E-Ophtha databases achieved a 0.94 and 0.95 AUC score, respectively. Conclusions A fully data-driven artificial intelligence--based grading algorithm can be used to screen fundus photographs obtained from diabetic patients and to identify, with high reliability, which cases should be referred to an ophthalmologist for further evaluation and treatment. The implementation of such an algorithm on a global basis could reduce drastically the rate of vision loss attributed to DR.},
  langid = {english}
}

@article{gargeyaAutomatedIdentificationDiabetic2017a,
  title = {Automated {{Identification}} of {{Diabetic Retinopathy Using Deep Learning}}},
  author = {Gargeya, Rishab and Leng, Theodore},
  year = {2017},
  month = jul,
  journal = {Ophthalmology},
  volume = {124},
  number = {7},
  pages = {962--969},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2017.02.008},
  urldate = {2019-11-20},
  abstract = {Purpose Diabetic retinopathy (DR) is one of the leading causes of preventable blindness globally. Performing retinal screening examinations on all diabetic patients is an unmet need, and there are many undiagnosed and untreated cases of DR. The objective of this study was to develop robust diagnostic technology to automate DR screening. Referral of eyes with DR to an ophthalmologist for further evaluation and treatment would aid in reducing the rate of vision loss, enabling timely and accurate diagnoses. Design We developed and evaluated a data-driven deep learning algorithm as a novel diagnostic tool for automated DR detection. The algorithm processed color fundus images and classified them as healthy (no~retinopathy) or having DR, identifying relevant cases for medical referral. Methods A total of 75\,137 publicly available fundus images from diabetic patients were used to train and test an artificial intelligence model to differentiate healthy fundi from those with DR. A panel of retinal specialists determined the ground truth for our data set before experimentation. We also tested our model using the public MESSIDOR 2 and E-Ophtha databases for external validation. Information learned in our automated method was visualized readily through an automatically generated abnormality heatmap, highlighting subregions within each input fundus image for further clinical review. Main Outcome Measures We used area under the receiver operating characteristic curve (AUC) as a metric to measure the precision--recall trade-off of our algorithm, reporting associated sensitivity and specificity metrics on the receiver operating characteristic curve. Results Our model achieved a 0.97 AUC with a 94\% and 98\% sensitivity and specificity, respectively, on 5-fold cross-validation using our local data set. Testing against the independent MESSIDOR 2 and E-Ophtha databases achieved a 0.94 and 0.95 AUC score, respectively. Conclusions A fully data-driven artificial intelligence--based grading algorithm can be used to screen fundus photographs obtained from diabetic patients and to identify, with high reliability, which cases should be referred to an ophthalmologist for further evaluation and treatment. The implementation of such an algorithm on a global basis could reduce drastically the rate of vision loss attributed to DR.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9D6KFVH4\\Gargeya et Leng - 2017 - Automated Identification of Diabetic Retinopathy U.pdf;C\:\\Users\\cleme\\Zotero\\storage\\9YJ4UZSQ\\S0161642016317742.html}
}

@article{garifullinDeepBayesianBaseline2021,
  title = {Deep {{Bayesian}} Baseline for Segmenting Diabetic Retinopathy Lesions: {{Advances}} and Challenges},
  shorttitle = {Deep {{Bayesian}} Baseline for Segmenting Diabetic Retinopathy Lesions},
  author = {Garifullin, Azat and Lensu, Lasse and Uusitalo, Hannu},
  year = {2021},
  month = sep,
  journal = {Computers in Biology and Medicine},
  volume = {136},
  pages = {104725},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104725},
  urldate = {2022-10-23},
  abstract = {Early diagnosis of retinopathy is essential for preventing retinal complications and visual impairment due to diabetes. For the detection of retinopathy lesions from retinal images, several automatic approaches based on deep neural networks have been developed in the recent years. Most of the proposed methods produce point estimates of pixels belonging to the lesion areas and give no or little information on the uncertainty of method predictions. However, the latter can be essential in the examination of the medical condition of the patient when the goal is early detection of abnormalities. This work extends the recent research with a Bayesian framework by considering the parameters of a convolutional neural network as random variables and utilizing stochastic variational dropout based approximation for uncertainty quantification. The framework includes an extended validation procedure and it allows analyzing lesion segmentation distributions, model calibration and prediction uncertainties. Also the challenges related to the deep probabilistic model and uncertainty quantification are presented. The proposed method achieves area under precision-recall curve of 0.84 for hard exudates, 0.641 for soft exudates, 0.593 for haemorrhages, and 0.484 for microaneurysms on IDRiD dataset.},
  langid = {english},
  keywords = {Bayesian deep learning,Diabetic retinopathy,Haemorrhage,Hard exudate,Lesion segmentation,Microaneurysm,Soft exudate}
}

@article{garifullinDeepBayesianBaseline2021a,
  title = {Deep {{Bayesian}} Baseline for Segmenting Diabetic Retinopathy Lesions: {{Advances}} and Challenges},
  shorttitle = {Deep {{Bayesian}} Baseline for Segmenting Diabetic Retinopathy Lesions},
  author = {Garifullin, Azat and Lensu, Lasse and Uusitalo, Hannu},
  year = {2021},
  month = sep,
  journal = {Computers in Biology and Medicine},
  volume = {136},
  pages = {104725},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104725},
  urldate = {2022-10-23},
  abstract = {Early diagnosis of retinopathy is essential for preventing retinal complications and visual impairment due to diabetes. For the detection of retinopathy lesions from retinal images, several automatic approaches based on deep neural networks have been developed in the recent years. Most of the proposed methods produce point estimates of pixels belonging to the lesion areas and give no or little information on the uncertainty of method predictions. However, the latter can be essential in the examination of the medical condition of the patient when the goal is early detection of abnormalities. This work extends the recent research with a Bayesian framework by considering the parameters of a convolutional neural network as random variables and utilizing stochastic variational dropout based approximation for uncertainty quantification. The framework includes an extended validation procedure and it allows analyzing lesion segmentation distributions, model calibration and prediction uncertainties. Also the challenges related to the deep probabilistic model and uncertainty quantification are presented. The proposed method achieves area under precision-recall curve of 0.84 for hard exudates, 0.641 for soft exudates, 0.593 for haemorrhages, and 0.484 for microaneurysms on IDRiD dataset.},
  langid = {english},
  keywords = {Bayesian deep learning,Diabetic retinopathy,Haemorrhage,Hard exudate,Lesion segmentation,Microaneurysm,Soft exudate},
  file = {C:\Users\cleme\Zotero\storage\3H4IRMT7\Garifullin et al. - 2021 - Deep Bayesian baseline for segmenting diabetic ret.pdf}
}

@article{garvinAutomated3DIntraretinal2009,
  title = {Automated 3-{{D}} Intraretinal Layer Segmentation of Macular Spectral-Domain Optical Coherence Tomography Images},
  author = {Garvin, Mona Kathryn and Abr{\`a}moff, Michael David and Wu, Xiaodong and Russell, Stephen R. and Burns, Trudy L. and Sonka, Milan},
  year = {2009},
  month = sep,
  journal = {IEEE transactions on medical imaging},
  volume = {28},
  number = {9},
  pages = {1436--1447},
  issn = {1558-254X},
  doi = {10.1109/TMI.2009.2016958},
  abstract = {With the introduction of spectral-domain optical coherence tomography (OCT), much larger image datasets are routinely acquired compared to what was possible using the previous generation of time-domain OCT. Thus, the need for 3-D segmentation methods for processing such data is becoming increasingly important. We report a graph-theoretic segmentation method for the simultaneous segmentation of multiple 3-D surfaces that is guaranteed to be optimal with respect to the cost function and that is directly applicable to the segmentation of 3-D spectral OCT image data. We present two extensions to the general layered graph segmentation method: the ability to incorporate varying feasibility constraints and the ability to incorporate true regional information. Appropriate feasibility constraints and cost functions were learned from a training set of 13 spectral-domain OCT images from 13 subjects. After training, our approach was tested on a test set of 28 images from 14 subjects. An overall mean unsigned border positioning error of 5.69+/-2.41 microm was achieved when segmenting seven surfaces (six layers) and using the average of the manual tracings of two ophthalmologists as the reference standard. This result is very comparable to the measured interobserver variability of 5.71+/-1.98 microm.},
  langid = {english},
  pmcid = {PMC2911837},
  pmid = {19278927},
  keywords = {Algorithms,Analysis of Variance,Computer-Assisted,Databases,Factual,Humans,Image Processing,Information Storage and Retrieval,Macula Lutea,Optical Coherence,Reproducibility of Results,Retina,Tomography}
}

@article{garvinAutomated3DIntraretinal2009a,
  title = {Automated 3-{{D}} Intraretinal Layer Segmentation of Macular Spectral-Domain Optical Coherence Tomography Images},
  author = {Garvin, Mona Kathryn and Abr{\`a}moff, Michael David and Wu, Xiaodong and Russell, Stephen R. and Burns, Trudy L. and Sonka, Milan},
  year = {2009},
  month = sep,
  journal = {IEEE transactions on medical imaging},
  volume = {28},
  number = {9},
  pages = {1436--1447},
  issn = {1558-254X},
  doi = {10.1109/TMI.2009.2016958},
  abstract = {With the introduction of spectral-domain optical coherence tomography (OCT), much larger image datasets are routinely acquired compared to what was possible using the previous generation of time-domain OCT. Thus, the need for 3-D segmentation methods for processing such data is becoming increasingly important. We report a graph-theoretic segmentation method for the simultaneous segmentation of multiple 3-D surfaces that is guaranteed to be optimal with respect to the cost function and that is directly applicable to the segmentation of 3-D spectral OCT image data. We present two extensions to the general layered graph segmentation method: the ability to incorporate varying feasibility constraints and the ability to incorporate true regional information. Appropriate feasibility constraints and cost functions were learned from a training set of 13 spectral-domain OCT images from 13 subjects. After training, our approach was tested on a test set of 28 images from 14 subjects. An overall mean unsigned border positioning error of 5.69+/-2.41 microm was achieved when segmenting seven surfaces (six layers) and using the average of the manual tracings of two ophthalmologists as the reference standard. This result is very comparable to the measured interobserver variability of 5.71+/-1.98 microm.},
  langid = {english},
  pmcid = {PMC2911837},
  pmid = {19278927},
  keywords = {Algorithms,Analysis of Variance,Databases Factual,Humans,Image Processing Computer-Assisted,Information Storage and Retrieval,Macula Lutea,Reproducibility of Results,Retina,Tomography Optical Coherence},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\8WV9KKLI\\Garvin et al. - 2009 - Automated 3-D intraretinal layer segmentation of m.pdf;C\:\\Users\\cleme\\Zotero\\storage\\GTYH958M\\garvin2009.pdf}
}

@article{ghassemiFalseHopeCurrent2021,
  title = {The False Hope of Current Approaches to Explainable Artificial Intelligence in Health Care},
  author = {Ghassemi, Marzyeh and {Oakden-Rayner}, Luke and Beam, Andrew L.},
  year = {2021},
  month = nov,
  journal = {The Lancet Digital Health},
  volume = {3},
  number = {11},
  pages = {e745-e750},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(21)00208-9},
  urldate = {2023-05-24},
  langid = {english},
  pmid = {34711379}
}

@article{ghassemiFalseHopeCurrent2021a,
  title = {The False Hope of Current Approaches to Explainable Artificial Intelligence in Health Care},
  author = {Ghassemi, Marzyeh and {Oakden-Rayner}, Luke and Beam, Andrew L.},
  year = {2021},
  month = nov,
  journal = {The Lancet Digital Health},
  volume = {3},
  number = {11},
  pages = {e745-e750},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(21)00208-9},
  urldate = {2023-05-24},
  langid = {english},
  pmid = {34711379},
  file = {C:\Users\cleme\Zotero\storage\ATKCTHP3\Ghassemi et al. - 2021 - The false hope of current approaches to explainabl.pdf}
}

@article{ghoshInterpretableArtificialIntelligence2020,
  title = {Interpretable {{Artificial Intelligence}}: {{Why}} and {{When}}},
  shorttitle = {Interpretable {{Artificial Intelligence}}},
  author = {Ghosh, Adarsh and Kandasamy, Devasenathipathy},
  year = {2020},
  month = may,
  journal = {American Journal of Roentgenology},
  volume = {214},
  number = {5},
  pages = {1137--1138},
  publisher = {American Roentgen Ray Society},
  issn = {0361-803X},
  doi = {10.2214/AJR.19.22145},
  urldate = {2021-11-16},
  abstract = {: OBJECTIVE. The purpose of this article is to discuss the problem of interpretability of artificial intelligence (AI) and highlight the need for continuing scientific discovery using AI algorithms to deal with medical big data. CONCLUSION. A plethora of AI algorithms are currently being used in medical research, but the opacity of these algorithms makes their clinical implementation a dilemma. Clinical decision making cannot be assigned to something that we do not understand. Therefore, AI research should not be limited to reporting accuracy and sensitivity but, rather, should try to explain the underlying reasons for the predictions, in an attempt to enrich biologic understanding and knowledge.},
  keywords = {biomedical research,deep learning,machine learning}
}

@article{ghoshInterpretableArtificialIntelligence2020a,
  title = {Interpretable {{Artificial Intelligence}}: {{Why}} and {{When}}},
  shorttitle = {Interpretable {{Artificial Intelligence}}},
  author = {Ghosh, Adarsh and Kandasamy, Devasenathipathy},
  year = {2020},
  month = may,
  journal = {American Journal of Roentgenology},
  volume = {214},
  number = {5},
  pages = {1137--1138},
  publisher = {American Roentgen Ray Society},
  issn = {0361-803X},
  doi = {10.2214/AJR.19.22145},
  urldate = {2021-11-16},
  abstract = {: OBJECTIVE. The purpose of this article is to discuss the problem of interpretability of artificial intelligence (AI) and highlight the need for continuing scientific discovery using AI algorithms to deal with medical big data. CONCLUSION. A plethora of AI algorithms are currently being used in medical research, but the opacity of these algorithms makes their clinical implementation a dilemma. Clinical decision making cannot be assigned to something that we do not understand. Therefore, AI research should not be limited to reporting accuracy and sensitivity but, rather, should try to explain the underlying reasons for the predictions, in an attempt to enrich biologic understanding and knowledge.},
  keywords = {biomedical research,deep learning,machine learning}
}

@inproceedings{giancardoAutomaticRetinaExudates2011,
  title = {Automatic Retina Exudates Segmentation without a Manually Labelled Training Set},
  booktitle = {2011 {{IEEE International Symposium}} on {{Biomedical Imaging}}: {{From Nano}} to {{Macro}}},
  author = {Giancardo, L. and Meriaudeau, F. and Karnowski, T. P. and Li, Y. and Tobin, K. W. and Chaum, E.},
  year = {2011},
  month = mar,
  pages = {1396--1400},
  doi = {10.1109/ISBI.2011.5872661},
  abstract = {Diabetic macular edema (DME) is a common vision threatening complication of diabetic retinopathy which can be assessed by detecting exudates (a type of bright lesion) in fundus images. In this work, two new methods for the detection of exudates are presented which do not use a supervised learning step; therefore, they do not require labelled lesion training sets which are time consuming to create, difficult to obtain and prone to human error. We introduce a new dataset of fundus images from various ethnic groups and levels of DME which we have made publicly available. We evaluate our algorithm with this dataset and compare our results with two recent exudate segmentation algorithms. In all of our tests, our algorithms perform better or comparable with an order of magnitude reduction in computational time.},
  keywords = {automatic retina exudate segmentation,Biomedical imaging,bright lesion,computer-aided diagnosis,diabetic macular edema,diabetic retinopathy,diseases,ethnic groups,exudate segmentation algorithms,eye,fundus image database,fundus images,Gold,image segmentation,manually labelled training set,medical image processing,Radiography,retina normalisation,segmentation,Variable speed drives,vision defects,vision threatening complication}
}

@inproceedings{giancardoAutomaticRetinaExudates2011a,
  title = {Automatic Retina Exudates Segmentation without a Manually Labelled Training Set},
  booktitle = {2011 {{IEEE International Symposium}} on {{Biomedical Imaging}}: {{From Nano}} to {{Macro}}},
  author = {Giancardo, L. and Meriaudeau, F. and Karnowski, T. P. and Li, Y. and Tobin, K. W. and Chaum, E.},
  year = {2011},
  month = mar,
  pages = {1396--1400},
  doi = {10.1109/ISBI.2011.5872661},
  abstract = {Diabetic macular edema (DME) is a common vision threatening complication of diabetic retinopathy which can be assessed by detecting exudates (a type of bright lesion) in fundus images. In this work, two new methods for the detection of exudates are presented which do not use a supervised learning step; therefore, they do not require labelled lesion training sets which are time consuming to create, difficult to obtain and prone to human error. We introduce a new dataset of fundus images from various ethnic groups and levels of DME which we have made publicly available. We evaluate our algorithm with this dataset and compare our results with two recent exudate segmentation algorithms. In all of our tests, our algorithms perform better or comparable with an order of magnitude reduction in computational time.},
  keywords = {automatic retina exudate segmentation,Biomedical imaging,bright lesion,computer-aided diagnosis,diabetic macular edema,diabetic retinopathy,diseases,ethnic groups,exudate segmentation algorithms,eye,fundus image database,fundus images,Gold,image segmentation,manually labelled training set,medical image processing,Radiography,retina normalisation,segmentation,Variable speed drives,vision defects,vision threatening complication},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\H2JHI5IV\\Giancardo et al. - 2011 - Automatic retina exudates segmentation without a m.pdf;C\:\\Users\\cleme\\Zotero\\storage\\VGRJ72PJ\\5872661.html}
}

@article{gilpinExplainingExplanationsOverview2019,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  shorttitle = {Explaining {{Explanations}}},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  year = {2019},
  month = feb,
  journal = {arXiv:1806.00069 [cs, stat]},
  eprint = {1806.00069},
  primaryclass = {cs, stat},
  urldate = {2019-12-06},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{gilpinExplainingExplanationsOverview2019a,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  shorttitle = {Explaining {{Explanations}}},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  year = {2019},
  month = feb,
  journal = {arXiv:1806.00069 [cs, stat]},
  eprint = {1806.00069},
  primaryclass = {cs, stat},
  urldate = {2019-12-06},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\QJLMCN5H\Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf}
}

@inproceedings{girardArteryVeinClassification2017,
  title = {Artery/Vein Classification in Fundus Images Using {{CNN}} and Likelihood Score Propagation},
  booktitle = {2017 {{IEEE Global Conference}} on {{Signal}} and {{Information Processing}} ({{GlobalSIP}})},
  author = {Girard, Fantin and Cheriet, Farida},
  year = {2017},
  month = nov,
  pages = {720--724},
  doi = {10.1109/GlobalSIP.2017.8309054},
  urldate = {2023-09-30},
  abstract = {Artery/vein classification in fundus images is a prerequisite for the assessment of diseases such as diabetes, hypertension or other cardiovascular pathologies. One clinical measure used to assess the severity of cardiovascular risk is the retinal arterio-venous ratio (AVR), which significantly depends on the accuracy of vessel classification into arteries or veins. This paper proposes a novel method for artery/vein classification combining deep learning and graph propagation strategies. First, a convolutional neural network (CNN) is trained for the task of labeling vessel pixels into arteries or veins. A graph is then constructed from the retinal vascular network. The nodes are defined as the vessel branches and each edge gets associated to a cost evaluating if the two branches should have the same label. The CNN's artery/vein classification is efficiently propagated through the minimum spanning tree of the graph. We validated our method on two publicly available databases. Our method achieves an accuracy of 93.3\% on the DRIVE database compared to the state of the art accuracy of 91.7\%.}
}

@inproceedings{girardArteryVeinClassification2017a,
  title = {Artery/Vein Classification in Fundus Images Using {{CNN}} and Likelihood Score Propagation},
  booktitle = {2017 {{IEEE Global Conference}} on {{Signal}} and {{Information Processing}} ({{GlobalSIP}})},
  author = {Girard, Fantin and Cheriet, Farida},
  year = {2017},
  month = nov,
  pages = {720--724},
  doi = {10.1109/GlobalSIP.2017.8309054},
  urldate = {2023-09-30},
  abstract = {Artery/vein classification in fundus images is a prerequisite for the assessment of diseases such as diabetes, hypertension or other cardiovascular pathologies. One clinical measure used to assess the severity of cardiovascular risk is the retinal arterio-venous ratio (AVR), which significantly depends on the accuracy of vessel classification into arteries or veins. This paper proposes a novel method for artery/vein classification combining deep learning and graph propagation strategies. First, a convolutional neural network (CNN) is trained for the task of labeling vessel pixels into arteries or veins. A graph is then constructed from the retinal vascular network. The nodes are defined as the vessel branches and each edge gets associated to a cost evaluating if the two branches should have the same label. The CNN's artery/vein classification is efficiently propagated through the minimum spanning tree of the graph. We validated our method on two publicly available databases. Our method achieves an accuracy of 93.3\% on the DRIVE database compared to the state of the art accuracy of 91.7\%.},
  file = {C:\Users\cleme\Zotero\storage\H9NPHPKY\8309054.html}
}

@article{girardJointSegmentationClassification2019,
  title = {Joint Segmentation and Classification of Retinal Arteries/Veins from Fundus Images},
  author = {Girard, Fantin and Kavalec, Conrad and Cheriet, Farida},
  year = {2019},
  month = mar,
  journal = {Artificial Intelligence in Medicine},
  volume = {94},
  pages = {96--109},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2019.02.004},
  urldate = {2023-09-30},
  abstract = {Objective Automatic artery/vein (A/V) segmentation from fundus images is required to track blood vessel changes occurring with many pathologies including retinopathy and cardiovascular pathologies. One of the clinical measures that quantifies vessel changes is the arterio-venous ratio (AVR) which represents the ratio between artery and vein diameters. This measure significantly depends on the accuracy of vessel segmentation and classification into arteries and veins. This paper proposes a fast, novel method for semantic A/V segmentation combining deep learning and graph propagation. Methods A convolutional neural network (CNN) is proposed to jointly segment and classify vessels into arteries and veins. The initial CNN labeling is propagated through a graph representation of the retinal vasculature, whose nodes are defined as the vessel branches and edges are weighted by the cost of linking pairs of branches. To efficiently propagate the labels, the graph is simplified into its minimum spanning tree. Results The method achieves an accuracy of 94.8\% for vessels segmentation. The A/V classification achieves a specificity of 92.9\% with a sensitivity of 93.7\% on the CT-DRIVE database compared to the state-of-the-art-specificity and sensitivity, both of 91.7\%. Conclusion The results show that our method outperforms the leading previous works on a public dataset for A/V classification and is by far the fastest. Significance The proposed global AVR calculated on the whole fundus image using our automatic A/V segmentation method can better track vessel changes associated to diabetic retinopathy than the standard local AVR calculated only around the optic disc.},
  keywords = {Artery and vein classification,CNN,Fundus images,Retina,Vessel segmentation}
}

@article{girardJointSegmentationClassification2019a,
  title = {Joint Segmentation and Classification of Retinal Arteries/Veins from Fundus Images},
  author = {Girard, Fantin and Kavalec, Conrad and Cheriet, Farida},
  year = {2019},
  month = mar,
  journal = {Artificial Intelligence in Medicine},
  volume = {94},
  pages = {96--109},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2019.02.004},
  urldate = {2023-09-30},
  abstract = {Objective Automatic artery/vein (A/V) segmentation from fundus images is required to track blood vessel changes occurring with many pathologies including retinopathy and cardiovascular pathologies. One of the clinical measures that quantifies vessel changes is the arterio-venous ratio (AVR) which represents the ratio between artery and vein diameters. This measure significantly depends on the accuracy of vessel segmentation and classification into arteries and veins. This paper proposes a fast, novel method for semantic A/V segmentation combining deep learning and graph propagation. Methods A convolutional neural network (CNN) is proposed to jointly segment and classify vessels into arteries and veins. The initial CNN labeling is propagated through a graph representation of the retinal vasculature, whose nodes are defined as the vessel branches and edges are weighted by the cost of linking pairs of branches. To efficiently propagate the labels, the graph is simplified into its minimum spanning tree. Results The method achieves an accuracy of 94.8\% for vessels segmentation. The A/V classification achieves a specificity of 92.9\% with a sensitivity of 93.7\% on the CT-DRIVE database compared to the state-of-the-art-specificity and sensitivity, both of 91.7\%. Conclusion The results show that our method outperforms the leading previous works on a public dataset for A/V classification and is by far the fastest. Significance The proposed global AVR calculated on the whole fundus image using our automatic A/V segmentation method can better track vessel changes associated to diabetic retinopathy than the standard local AVR calculated only around the optic disc.},
  keywords = {Artery and vein classification,CNN,Fundus images,Retina,Vessel segmentation},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\SG9K8BBU\\Girard et al. - 2019 - Joint segmentation and classification of retinal a.pdf;C\:\\Users\\cleme\\Zotero\\storage\\8CIMDQX8\\S0933365717306978.html}
}

@inproceedings{girardSimultaneousMaculaDetection2016,
  title = {Simultaneous Macula Detection and Optic Disc Boundary Segmentation in Retinal Fundus Images},
  booktitle = {Medical {{Imaging}} 2016: {{Image Processing}}},
  author = {Girard, Fantin and Kavalec, Conrad and Grenier, S{\'e}bastien and Tahar, Houssem Ben and Cheriet, Farida},
  year = {2016},
  month = mar,
  volume = {9784},
  pages = {382--390},
  publisher = {SPIE},
  doi = {10.1117/12.2216397},
  urldate = {2023-09-30},
  abstract = {The optic disc (OD) and the macula are important structures in automatic diagnosis of most retinal diseases inducing vision defects such as glaucoma, diabetic or hypertensive retinopathy and age-related macular degeneration. We propose a new method to detect simultaneously the macula and the OD boundary. First, the color fundus images are processed to compute several maps highlighting the different anatomical structures such as vessels, the macula and the OD. Then, macula candidates and OD candidates are found simultaneously and independently using seed detectors identified on the corresponding maps. After selecting a set of macula/OD pairs, the top candidates are sent to the OD segmentation method. The segmentation method is based on local K-means applied to color coordinates in polar space followed by a polynomial fitting regularization step. Pair scores are updated, resulting in the final best macula/OD pair. The method was evaluated on two public image databases: ONHSD and MESSIDOR. The results show an overlapping area of 0.84 on ONHSD and 0.90 on MESSIDOR, which is better than recent state of the art methods. Our segmentation method is robust to contrast and illumination problems and outputs the exact boundary of the OD, not just a circular or elliptical model. The macula detection has an accuracy of 94\%, which again outperforms other macula detection methods. This shows that combining the OD and macula detections improves the overall accuracy. The computation time for the whole process is 6.4 seconds, which is faster than other methods in the literature.}
}

@inproceedings{girardSimultaneousMaculaDetection2016a,
  title = {Simultaneous Macula Detection and Optic Disc Boundary Segmentation in Retinal Fundus Images},
  booktitle = {Medical {{Imaging}} 2016: {{Image Processing}}},
  author = {Girard, Fantin and Kavalec, Conrad and Grenier, S{\'e}bastien and Tahar, Houssem Ben and Cheriet, Farida},
  year = {2016},
  month = mar,
  volume = {9784},
  pages = {382--390},
  publisher = {SPIE},
  doi = {10.1117/12.2216397},
  urldate = {2023-09-30},
  abstract = {The optic disc (OD) and the macula are important structures in automatic diagnosis of most retinal diseases inducing vision defects such as glaucoma, diabetic or hypertensive retinopathy and age-related macular degeneration. We propose a new method to detect simultaneously the macula and the OD boundary. First, the color fundus images are processed to compute several maps highlighting the different anatomical structures such as vessels, the macula and the OD. Then, macula candidates and OD candidates are found simultaneously and independently using seed detectors identified on the corresponding maps. After selecting a set of macula/OD pairs, the top candidates are sent to the OD segmentation method. The segmentation method is based on local K-means applied to color coordinates in polar space followed by a polynomial fitting regularization step. Pair scores are updated, resulting in the final best macula/OD pair. The method was evaluated on two public image databases: ONHSD and MESSIDOR. The results show an overlapping area of 0.84 on ONHSD and 0.90 on MESSIDOR, which is better than recent state of the art methods. Our segmentation method is robust to contrast and illumination problems and outputs the exact boundary of the OD, not just a circular or elliptical model. The macula detection has an accuracy of 94\%, which again outperforms other macula detection methods. This shows that combining the OD and macula detections improves the overall accuracy. The computation time for the whole process is 6.4 seconds, which is faster than other methods in the literature.}
}

@inproceedings{glorot2010understanding,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}}
}

@inproceedings{glorot2010understanding,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}}
}

@article{glorotUnderstandingDifficultyTraining,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  author = {Glorot, Xavier and Bengio, Yoshua},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  urldate = {2023-02-22},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert \& Weston, 2008; Mnih \& Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a ``better'' basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010a,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  urldate = {2023-02-22},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert \& Weston, 2008; Mnih \& Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a ``better'' basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).}
}

@article{glorotUnderstandingDifficultyTraininga,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  author = {Glorot, Xavier and Bengio, Yoshua},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\CFD25L3C\Glorot et Bengio - Understanding the difﬁculty of training deep feedf.pdf}
}

@article{godichon-baggioniAlgorithmesStochastiques,
  title = {{Algorithmes stochastiques}},
  author = {{Godichon-Baggioni}, Antoine},
  journal = {Cours Statistique M2 - Sorbonne Universit{\'e}},
  langid = {french}
}

@article{godichon-baggioniAlgorithmesStochastiquesa,
  title = {{Algorithmes stochastiques}},
  author = {{Godichon-Baggioni}, Antoine},
  journal = {Cours Statistique M2 - Sorbonne Universit{\'e}},
  langid = {french},
  file = {C:\Users\cleme\Zotero\storage\MADY3ERP\Godichon-Baggioni - Algorithmes stochastiques.pdf}
}

@article{goebelRETINALTHICKNESSDIABETIC2002,
  title = {{{RETINAL THICKNESS IN DIABETIC RETINOPATHY}}: {{A Study Using Optical Coherence Tomography}} ({{OCT}})},
  shorttitle = {{{RETINAL THICKNESS IN DIABETIC RETINOPATHY}}},
  author = {Goebel, Winfried and {Kretzchmar-Gross}, Tatjana},
  year = {2002},
  month = dec,
  journal = {Retina (Philadelphia, Pa.)},
  volume = {22},
  number = {6},
  pages = {759--767},
  issn = {0275-004X},
  urldate = {2022-06-27},
  abstract = {Background and Objective  The authors conducted a controlled study to quantify macular retinal thickness in diabetic retinopathy using optical coherence tomography (OCT) as an objective and noninvasive tool. The relationship between retinal thickness and standard methods of evaluating macular edema was investigated. Patients and Methods  A total of 136 patients in different stages of diabetic retinopathy were examined with OCT. In addition, fluorescein angiograms as well as standard eye examinations were conducted. The control group consisted of 30 individuals with a normal macula. Results  In the controls, retinal thickness was 153 {\textpm} 15 {$\mu$}m in the fovea, 249 {\textpm} 19 {$\mu$}m in the temporal parafoveal region, and 268 {\textpm} 20 {$\mu$}m in the nasal parafoveal region. In diabetic patients, retinal thickness was increased to 307 {\textpm} 136 {$\mu$}m in the fovea, 337 {\textpm} 88 {$\mu$}m in the temporal retina, and 353 {\textpm} 95 {$\mu$}m in the nasal retina, respectively. The differences between diabetics and controls were highly significant (P {$<$} 0.001). Retinal thickening correlated with fluorescein leakage in the angiograms to some extent. There was an intermediate correlation between retinal thickness and visual acuity, particularly in patients without macular ischemia. Sensitivity of detecting clinically significant macular edema by measuring foveal retinal thickness was 89\% and specificity was 96\%. Conclusion  Optical coherence tomography allows us to quantify retinal thickness in diabetic retinopathy with excellent reproducibility. OCT is able to detect sight-threatening macular edema with great reliability.},
  langid = {american}
}

@article{goebelRETINALTHICKNESSDIABETIC2002a,
  title = {{{RETINAL THICKNESS IN DIABETIC RETINOPATHY}}: {{A Study Using Optical Coherence Tomography}} ({{OCT}})},
  shorttitle = {{{RETINAL THICKNESS IN DIABETIC RETINOPATHY}}},
  author = {Goebel, Winfried and {Kretzchmar-Gross}, Tatjana},
  year = {2002},
  month = dec,
  journal = {RETINA},
  volume = {22},
  number = {6},
  pages = {759--767},
  issn = {0275-004X},
  urldate = {2022-06-27},
  abstract = {Background and Objective~         The authors conducted a controlled study to quantify macular retinal thickness in diabetic retinopathy using optical coherence tomography (OCT) as an objective and noninvasive tool. The relationship between retinal thickness and standard methods of evaluating macular edema was investigated.         Patients and Methods~         A total of 136 patients in different stages of diabetic retinopathy were examined with OCT. In addition, fluorescein angiograms as well as standard eye examinations were conducted. The control group consisted of 30 individuals with a normal macula.         Results~         In the controls, retinal thickness was 153 {\textpm} 15 {$\mu$}m in the fovea, 249 {\textpm} 19 {$\mu$}m in the temporal parafoveal region, and 268 {\textpm} 20 {$\mu$}m in the nasal parafoveal region. In diabetic patients, retinal thickness was increased to 307 {\textpm} 136 {$\mu$}m in the fovea, 337 {\textpm} 88 {$\mu$}m in the temporal retina, and 353 {\textpm} 95 {$\mu$}m in the nasal retina, respectively. The differences between diabetics and controls were highly significant (P {$<$} 0.001). Retinal thickening correlated with fluorescein leakage in the angiograms to some extent. There was an intermediate correlation between retinal thickness and visual acuity, particularly in patients without macular ischemia. Sensitivity of detecting clinically significant macular edema by measuring foveal retinal thickness was 89\% and specificity was 96\%.         Conclusion~         Optical coherence tomography allows us to quantify retinal thickness in diabetic retinopathy with excellent reproducibility. OCT is able to detect sight-threatening macular edema with great reliability.},
  langid = {american},
  file = {C:\Users\cleme\Zotero\storage\EZYQDXFZ\RETINAL_THICKNESS_IN_DIABETIC_RETINOPATHY__A_Study.12.html}
}

@misc{GoingDeepMedical,
  title = {Going {{Deep}} in {{Medical Image Analysis}}: {{Concepts}}, {{Methods}}, {{Challenges}}, and {{Future Directions}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2022-07-10}
}

@misc{GoingDeepMedicala,
  title = {Going {{Deep}} in {{Medical Image Analysis}}: {{Concepts}}, {{Methods}}, {{Challenges}}, and {{Future Directions}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2022-07-10},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/8764525/},
  file = {C:\Users\cleme\Zotero\storage\8GHBHKCM\8764525.html}
}

@article{golabbakhshVesselbasedRegistrationFundus2013,
  title = {Vessel-Based Registration of Fundus and Optical Coherence Tomography Projection Images of Retina Using a Quadratic Registration Model},
  author = {Golabbakhsh, M. and Rabbani, H.},
  year = {2013},
  month = nov,
  journal = {IET Image Processing},
  volume = {7},
  number = {8},
  pages = {768--776},
  issn = {1751-9659},
  doi = {10.1049/iet-ipr.2013.0116},
  abstract = {The new techniques of three-dimensional (3D)-optical coherence tomography (OCT) imaging is very useful for detecting retinal pathologic changes in various diseases and determining retinal thickness `abnormalities'. Fundus colour images have been used for several years for detecting retinal abnormalities too. If the two image modalities were combined, the resulted image would be more informative. The first step to combine these two modalities is to register colour fundus images with an en face representation of OCT. In this study, curvelet transform is used to extract vessels for both modalities. Then the extracted vessels from two modalities are registered together in two stages. At first, images are registered using scaling and translation transformations. Then a quadratic transformation model is assumed between two pairs of images; because retina is imaged as a second-order surface. Twenty-two eyes (17 macular and 5 prepapillary), from random patients, were imaged in this study with Topcon 3D OCT1000 instrument. A new registration error is defined which averages the distance between all the corresponding points in two sets of vessels. Results show that registration error after stage one is 6.01 {\textpm} 1.82 pixels and after stage two is 1.02 {\textpm} 0.02 pixels.},
  keywords = {colour fundus image detection,colour fundus image registration,curvelet transform,curvelet transforms,disease,diseases,eye,face representation,feature extraction,geographic atrophy,image colour analysis,image modality,image registration,image representation,image sensors,k-nearest neighbour search,learning (artificial intelligence),least square method,least squares approximations,macular hemorrhage,medical image processing,optical coherence tomography projection imaging,optical projectors,optical tomography,quadratic registration model,quadratic transformation model,retinal pathologic detection,retinal recognition,retinal thickness abnormality determination,scaling transformation,search problems,second-order surface,three-dimensional-optical coherence tomography imaging,Topcon 3D OCT-imaging,translation transformation,vessel-based fundus registration},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\EU7KEPZ2\\Golabbakhsh et Rabbani - 2013 - Vessel-based registration of fundus and optical co.pdf;C\:\\Users\\cleme\\Zotero\\storage\\8HTWU9Q4\\iet-ipr.2013.html;C\:\\Users\\cleme\\Zotero\\storage\\AF99JAQY\\golabbakhsh2013.html}
}

@article{goldhagenDivingDeepDeep2020,
  title = {Diving {{Deep}} into {{Deep Learning}}: {{An Update}} on {{Artificial Intelligence}} in {{Retina}}},
  shorttitle = {Diving {{Deep}} into {{Deep Learning}}},
  author = {Goldhagen, Brian E. and {Al-Khersan}, Hasenin},
  year = {2020},
  month = sep,
  journal = {Current Ophthalmology Reports},
  volume = {8},
  number = {3},
  pages = {121--128},
  issn = {2167-4868},
  doi = {10.1007/s40135-020-00240-2},
  abstract = {Purpose of Review: In the present article, we will provide an understanding and review of artificial intelligence in the subspecialty of retina and its potential applications within the specialty. Recent Findings: Given the significant use of diagnostic imaging within retina, this subspecialty is a fitting area for the incorporation of artificial intelligence. Researchers have aimed at creating models to assist in the diagnosis and management of retinal disease as well as in the prediction of disease course and treatment response. Most of this work thus far has focused on diabetic retinopathy, age-related macular degeneration, and retinopathy of prematurity, although other retinal diseases have started to be explored as well. Summary: Artificial intelligence is well-suited to transform the practice of ophthalmology. A basic understanding of the technology is important for its effective implementation and growth.},
  langid = {english},
  pmcid = {PMC7679067},
  pmid = {33224635},
  keywords = {Age-related macular degeneration,Artificial intelligence,Diabetic retinopathy,Machine learning,Neural networks,Retinopathy of prematurity}
}

@article{goldhagenDivingDeepDeep2020a,
  title = {Diving {{Deep}} into {{Deep Learning}}: {{An Update}} on {{Artificial Intelligence}} in {{Retina}}},
  shorttitle = {Diving {{Deep}} into {{Deep Learning}}},
  author = {Goldhagen, Brian E. and {Al-Khersan}, Hasenin},
  year = {2020},
  month = sep,
  journal = {Current Ophthalmology Reports},
  volume = {8},
  number = {3},
  pages = {121--128},
  issn = {2167-4868},
  doi = {10.1007/s40135-020-00240-2},
  abstract = {Purpose of Review: In the present article, we will provide an understanding and review of artificial intelligence in the subspecialty of retina and its potential applications within the specialty. Recent Findings: Given the significant use of diagnostic imaging within retina, this subspecialty is a fitting area for the incorporation of artificial intelligence. Researchers have aimed at creating models to assist in the diagnosis and management of retinal disease as well as in the prediction of disease course and treatment response. Most of this work thus far has focused on diabetic retinopathy, age-related macular degeneration, and retinopathy of prematurity, although other retinal diseases have started to be explored as well. Summary: Artificial intelligence is well-suited to transform the practice of ophthalmology. A basic understanding of the technology is important for its effective implementation and growth.},
  langid = {english},
  pmcid = {PMC7679067},
  pmid = {33224635},
  keywords = {Age-related macular degeneration,Artificial intelligence,Diabetic retinopathy,Machine learning,Neural networks,Retinopathy of prematurity}
}

@article{goldhagenDivingDeepDeep2020b,
  title = {Diving {{Deep}} into {{Deep Learning}}: {{An Update}} on {{Artificial Intelligence}} in {{Retina}}},
  shorttitle = {Diving {{Deep}} into {{Deep Learning}}},
  author = {Goldhagen, Brian E. and {Al-Khersan}, Hasenin},
  year = {2020},
  month = sep,
  journal = {Current Ophthalmology Reports},
  volume = {8},
  number = {3},
  pages = {121--128},
  issn = {2167-4868},
  doi = {10.1007/s40135-020-00240-2},
  abstract = {Purpose of Review: In the present article, we will provide an understanding and review of artificial intelligence in the subspecialty of retina and its potential applications within the specialty. Recent Findings: Given the significant use of diagnostic imaging within retina, this subspecialty is a fitting area for the incorporation of artificial intelligence. Researchers have aimed at creating models to assist in the diagnosis and management of retinal disease as well as in the prediction of disease course and treatment response. Most of this work thus far has focused on diabetic retinopathy, age-related macular degeneration, and retinopathy of prematurity, although other retinal diseases have started to be explored as well. Summary: Artificial intelligence is well-suited to transform the practice of ophthalmology. A basic understanding of the technology is important for its effective implementation and growth.},
  langid = {english},
  pmcid = {PMC7679067},
  pmid = {33224635},
  keywords = {Age-related macular degeneration Retinopathy of prematurity,Artificial intelligence,Diabetic retinopathy,Machine learning,Neural networks},
  file = {C:\Users\cleme\Zotero\storage\4P3HZUX9\Goldhagen et Al-Khersan - 2020 - Diving Deep into Deep Learning An Update on Artif.pdf}
}

@article{goldhagenDivingDeepDeep2020c,
  title = {Diving {{Deep}} into {{Deep Learning}}: {{An Update}} on {{Artificial Intelligence}} in {{Retina}}},
  shorttitle = {Diving {{Deep}} into {{Deep Learning}}},
  author = {Goldhagen, Brian E. and {Al-Khersan}, Hasenin},
  year = {2020},
  month = sep,
  journal = {Current Ophthalmology Reports},
  volume = {8},
  number = {3},
  pages = {121--128},
  issn = {2167-4868},
  doi = {10.1007/s40135-020-00240-2},
  abstract = {Purpose of Review: In the present article, we will provide an understanding and review of artificial intelligence in the subspecialty of retina and its potential applications within the specialty. Recent Findings: Given the significant use of diagnostic imaging within retina, this subspecialty is a fitting area for the incorporation of artificial intelligence. Researchers have aimed at creating models to assist in the diagnosis and management of retinal disease as well as in the prediction of disease course and treatment response. Most of this work thus far has focused on diabetic retinopathy, age-related macular degeneration, and retinopathy of prematurity, although other retinal diseases have started to be explored as well. Summary: Artificial intelligence is well-suited to transform the practice of ophthalmology. A basic understanding of the technology is important for its effective implementation and growth.},
  langid = {english},
  pmcid = {PMC7679067},
  pmid = {33224635},
  keywords = {Age-related macular degeneration Retinopathy of prematurity,Artificial intelligence,Diabetic retinopathy,Machine learning,Neural networks}
}

@inproceedings{gondalWeaklysupervisedLocalizationDiabetic2017,
  title = {Weakly-Supervised Localization of Diabetic Retinopathy Lesions in Retinal Fundus Images},
  booktitle = {2017 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Gondal, Waleed M. and K{\"o}hler, Jan M. and Grzeszick, Ren{\'e} and Fink, Gernot A. and Hirsch, Michael},
  year = {2017},
  month = sep,
  pages = {2069--2073},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2017.8296646},
  abstract = {Convolutional neural networks (CNNs) show impressive performance for image classification and detection, extending heavily to the medical image domain. Nevertheless, medical experts are skeptical in these predictions as the nonlinear multilayer structure resulting in a classification outcome is not directly graspable. Recently, approaches have been shown which help the user to understand the discriminative regions within an image which are decisive for the CNN to conclude to a certain class. Although these approaches could help to build trust in the CNNs predictions, they are only slightly shown to work with medical image data which often poses a challenge as the decision for a class relies on different lesion areas scattered around the entire image. Using the DiaretDB1 dataset, we show that on retina images different lesion areas fundamental for diabetic retinopathy are detected on an image level with high accuracy, comparable or exceeding supervised methods. On lesion level, we achieve few false positives with high sensitivity, though, the network is solely trained on image-level labels which do not include information about existing lesions. Classifying between diseased and healthy images, we achieve an AUC of 0.954 on the DiaretDB1.},
  keywords = {Biomedical imaging,Cams,CNNs predictions,convolution,convolutional neural networks,deep learning,Diabetes,diabetic retinopathy,diabetic retinopathy lesions,DiaretDB1 dataset,discriminative regions,diseases,eye,feature extraction,feedforward neural nets,image classification,image detection,image level,lesion detection,lesion level,Lesions,medical experts,medical image data,medical image domain,medical image processing,nonlinear multilayer structure,Retina,retinal fundus images,Retinopathy,Training,weakly-supervised localization,weakly-supervised object localization}
}

@inproceedings{gondalWeaklysupervisedLocalizationDiabetic2017a,
  title = {Weakly-Supervised Localization of Diabetic Retinopathy Lesions in Retinal Fundus Images},
  booktitle = {2017 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Gondal, Waleed M. and K{\"o}hler, Jan M. and Grzeszick, Ren{\'e} and Fink, Gernot A. and Hirsch, Michael},
  year = {2017},
  month = sep,
  pages = {2069--2073},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2017.8296646},
  abstract = {Convolutional neural networks (CNNs) show impressive performance for image classification and detection, extending heavily to the medical image domain. Nevertheless, medical experts are skeptical in these predictions as the nonlinear multilayer structure resulting in a classification outcome is not directly graspable. Recently, approaches have been shown which help the user to understand the discriminative regions within an image which are decisive for the CNN to conclude to a certain class. Although these approaches could help to build trust in the CNNs predictions, they are only slightly shown to work with medical image data which often poses a challenge as the decision for a class relies on different lesion areas scattered around the entire image. Using the DiaretDB1 dataset, we show that on retina images different lesion areas fundamental for diabetic retinopathy are detected on an image level with high accuracy, comparable or exceeding supervised methods. On lesion level, we achieve few false positives with high sensitivity, though, the network is solely trained on image-level labels which do not include information about existing lesions. Classifying between diseased and healthy images, we achieve an AUC of 0.954 on the DiaretDB1.},
  keywords = {Biomedical imaging,Cams,CNNs predictions,convolution,convolutional neural networks,deep learning,Diabetes,diabetic retinopathy,diabetic retinopathy lesions,DiaretDB1 dataset,discriminative regions,diseases,eye,feature extraction,feedforward neural nets,image classification,image detection,image level,lesion detection,lesion level,Lesions,medical experts,medical image data,medical image domain,medical image processing,nonlinear multilayer structure,Retina,retinal fundus images,Retinopathy,Training,weakly-supervised localization,weakly-supervised object localization},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\8NFEJ4MM\\Gondal et al. - 2017 - Weakly-supervised localization of diabetic retinop.pdf;C\:\\Users\\cleme\\Zotero\\storage\\A8BK3XEG\\gondal2017.html;C\:\\Users\\cleme\\Zotero\\storage\\T2MFUN6H\\8296646.html}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {MIT Press}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {MIT Press}
}

@book{goodfellowDeepLearning,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron}
}

@book{goodfellowDeepLearninga,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron}
}

@inproceedings{goodfellowExplainingHarnessingAdversarial2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}}, {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2015},
  urldate = {2023-11-30}
}

@inproceedings{goodfellowExplainingHarnessingAdversarial2015a,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}}, {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2015},
  urldate = {2023-11-30}
}

@inproceedings{gopinathDeepLearningFramework2017,
  title = {A {{Deep Learning Framework}} for {{Segmentation}} of {{Retinal Layers}} from {{OCT Images}}},
  booktitle = {2017 4th {{IAPR Asian Conference}} on {{Pattern Recognition}} ({{ACPR}})},
  author = {Gopinath, Karthik and Rangrej, Samrudhdhi B and Sivaswamy, Jayanthi},
  year = {2017},
  month = nov,
  pages = {888--893},
  issn = {2327-0985},
  doi = {10.1109/ACPR.2017.121},
  abstract = {Segmentation of retinal layers from Optical Coherence Tomography (OCT) volumes is a fundamental problem for any computer aided diagnostic algorithm development. This requires preprocessing steps such as denoising, region of interest extraction, flattening and edge detection all of which involve separate parameter tuning. In this paper, we explore deep learning techniques to automate all these steps and handle the presence/absence of pathologies. A model is proposed consisting of a combination of Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM). The CNN is used to extract layers of interest image and extract the edges, while the LSTM is used to trace the layer boundary. This model is trained on a mixture of normal and AMD cases using minimal data. Validation results on three public datasets show that the pixel-wise mean absolute error obtained with our system is 1.30 {\textpm} 0.48 which is lower than the inter-marker error of 1.79 {\textpm} 0.76. Our model's performance is also on par with the existing methods.},
  keywords = {Data mining,Image edge detection,Image segmentation,Manuals,Pathology,Retina,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\GZ7VIVYS\\Gopinath et al. - 2017 - A Deep Learning Framework for Segmentation of Reti.pdf;C\:\\Users\\cleme\\Zotero\\storage\\S4KEKWYA\\Gopinath et al. - 2017 - A Deep Learning Framework for Segmentation of Reti.pdf;C\:\\Users\\cleme\\Zotero\\storage\\EKEXSVKF\\8575940.html}
}

@article{gorczynskaProjectionOCTFundus2009,
  title = {Projection {{OCT}} Fundus Imaging for Visualising Outer Retinal Pathology in Non-Exudative Age-Related Macular Degeneration},
  author = {Gorczynska, I. and Srinivasan, V. J. and Vuong, L. N. and Chen, R. W. S. and Liu, J. J. and Reichel, E. and Wojtkowski, M. and Schuman, J. S. and Duker, J. S. and Fujimoto, J. G.},
  year = {2009},
  month = may,
  journal = {British Journal of Ophthalmology},
  volume = {93},
  number = {5},
  pages = {603--609},
  issn = {0007-1161, 1468-2079},
  doi = {10.1136/bjo.2007.136101},
  urldate = {2019-12-11},
  abstract = {Aims: To demonstrate ultrahigh-resolution, three-dimensional optical coherence tomography (3D-OCT) and projection OCT fundus imaging for enhanced visualisation of outer retinal pathology in non-exudative age-related macular degeneration (AMD). Methods: A high-speed, 3.5 {$\mu$}m resolution OCT prototype instrument was developed for the ophthalmic clinic. Eighty-three patients with non-exudative AMD were imaged. Projection OCT fundus images were generated from 3D-OCT data by selectively summing different retinal depth levels. Results were compared with standard ophthalmic examination, including fundus photography and fluorescein angiography, when indicated. Results: Projection OCT fundus imaging enhanced the visualisation of outer retinal pathology in non-exudative AMD. Different types of drusen exhibited distinct features in projection OCT images. Photoreceptor disruption was indicated by loss of the photoreceptor inner/outer segment (IS/OS) boundary and external limiting membrane (ELM). RPE atrophy can be assessed using choroid-level projection OCT images. Conclusions: Projection OCT fundus imaging facilities rapid interpretation of large 3D-OCT data sets. Projection OCT enhances contrast and visualises outer retinal pathology not visible with standard fundus imaging or OCT fundus imaging. Projection OCT fundus images enable registration with standard ophthalmic diagnostics and cross-sectional OCT images. Outer retinal alterations can be assessed and drusen morphology, photoreceptor impairment and pigmentary abnormalities identified.},
  copyright = {2009 BMJ Publishing Group Ltd},
  langid = {english},
  pmid = {18662918}
}

@article{gorczynskaProjectionOCTFundus2009a,
  title = {Projection {{OCT}} Fundus Imaging for Visualising Outer Retinal Pathology in Non-Exudative Age-Related Macular Degeneration},
  author = {Gorczynska, I. and Srinivasan, V. J. and Vuong, L. N. and Chen, R. W. S. and Liu, J. J. and Reichel, E. and Wojtkowski, M. and Schuman, J. S. and Duker, J. S. and Fujimoto, J. G.},
  year = {2009},
  month = may,
  journal = {British Journal of Ophthalmology},
  volume = {93},
  number = {5},
  pages = {603--609},
  issn = {0007-1161, 1468-2079},
  doi = {10.1136/bjo.2007.136101},
  urldate = {2019-12-11},
  abstract = {Aims: To demonstrate ultrahigh-resolution, three-dimensional optical coherence tomography (3D-OCT) and projection OCT fundus imaging for enhanced visualisation of outer retinal pathology in non-exudative age-related macular degeneration (AMD). Methods: A high-speed, 3.5 {$\mu$}m resolution OCT prototype instrument was developed for the ophthalmic clinic. Eighty-three patients with non-exudative AMD were imaged. Projection OCT fundus images were generated from 3D-OCT data by selectively summing different retinal depth levels. Results were compared with standard ophthalmic examination, including fundus photography and fluorescein angiography, when indicated. Results: Projection OCT fundus imaging enhanced the visualisation of outer retinal pathology in non-exudative AMD. Different types of drusen exhibited distinct features in projection OCT images. Photoreceptor disruption was indicated by loss of the photoreceptor inner/outer segment (IS/OS) boundary and external limiting membrane (ELM). RPE atrophy can be assessed using choroid-level projection OCT images. Conclusions: Projection OCT fundus imaging facilities rapid interpretation of large 3D-OCT data sets. Projection OCT enhances contrast and visualises outer retinal pathology not visible with standard fundus imaging or OCT fundus imaging. Projection OCT fundus images enable registration with standard ophthalmic diagnostics and cross-sectional OCT images. Outer retinal alterations can be assessed and drusen morphology, photoreceptor impairment and pigmentary abnormalities identified.},
  copyright = {2009 BMJ Publishing Group Ltd},
  langid = {english},
  pmid = {18662918},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\T4LD8FHF\\Gorczynska et al. - 2009 - Projection OCT fundus imaging for visualising oute.pdf;C\:\\Users\\cleme\\Zotero\\storage\\A4EQUVY7\\603.html;C\:\\Users\\cleme\\Zotero\\storage\\L2SNT7HV\\gorczynska2008.html}
}

@article{goutamComprehensiveReviewDeep2022,
  title = {A {{Comprehensive Review}} of {{Deep Learning Strategies}} in {{Retinal Disease Diagnosis Using Fundus Images}}},
  author = {Goutam, Balla and Hashmi, Mohammad Farukh and Geem, Zong Woo and Bokde, Neeraj Dhanraj},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {57796--57823},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3178372},
  abstract = {In recent years, there has been an unprecedented growth in computer vision and deep learning implementation owing to the exponential rise of computation infrastructure. The same was also reflected in retinal image analysis and successful artificial intelligence models were developed for various retinal disease diagnoses using a wide variety of visual markers obtained from eye fundus images. This article presents a comprehensive study of different deep learning strategies employed in recent times for the diagnosis of five major eye diseases, i.e., Diabetic retinopathy, Glaucoma, age-related macular degeneration, Cataract, and Retinopathy of prematurity. This article is organized according to the deep learning implementation process pipeline, where commonly used datasets, evaluation metrics, image pre-processing techniques, and deep learning backbone models are first illustrated followed by an extensive review of different strategies for each of the five mentioned retinal diseases is presented. Finally, this article summarizes eight major research directions available in the field of retinal disease diagnosis and outlines key challenges and future scope for the present research community.},
  keywords = {AMD,artificial intelligence,cataract,Computer vision,deep learning,Deep learning,Diabetes,diabetic retinopathy,fundus image,glaucoma,Measurement,Medical diagnosis,Retina,retinal disease diagnosis,Retinopathy,ROP,Task analysis},
  file = {C:\Users\cleme\Zotero\storage\FX9H86U9\Goutam et al. - 2022 - A Comprehensive Review of Deep Learning Strategies.pdf}
}

@misc{GradientbasedLearningApplied,
  title = {Gradient-Based Learning Applied to Document Recognition - {{IEEE Journals}} \& {{Magazine}}},
  urldate = {2019-06-27}
}

@misc{GradientbasedLearningApplieda,
  title = {Gradient-Based Learning Applied to Document Recognition - {{IEEE Journals}} \& {{Magazine}}},
  urldate = {2019-06-27},
  howpublished = {https://ieeexplore.ieee.org/document/726791},
  file = {C:\Users\cleme\Zotero\storage\LIXDUQ5Q\726791.html}
}

@inproceedings{grahamLeViTVisionTransformer2021,
  title = {{{LeViT}}: {{A Vision Transformer}} in {{ConvNet}}'s {{Clothing}} for {{Faster Inference}}},
  shorttitle = {{{LeViT}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Graham, Benjamin and {El-Nouby}, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J{\'e}gou, Herv{\'e} and Douze, Matthijs},
  year = {2021},
  pages = {12259--12269},
  urldate = {2021-11-02},
  langid = {english}
}

@inproceedings{grahamLeViTVisionTransformer2021a,
  title = {{{LeViT}}: {{A Vision Transformer}} in {{ConvNet}}'s {{Clothing}} for {{Faster Inference}}},
  shorttitle = {{{LeViT}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Graham, Benjamin and {El-Nouby}, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and J{\'e}gou, Herv{\'e} and Douze, Matthijs},
  year = {2021},
  pages = {12259--12269},
  urldate = {2021-11-02},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\PQVTZ8JZ\\Graham et al. - 2021 - LeViT A Vision Transformer in ConvNet's Clothing .pdf;C\:\\Users\\cleme\\Zotero\\storage\\C4MWHPUY\\Graham_LeViT_A_Vision_Transformer_in_ConvNets_Clothing_for_Faster_Inference_ICCV_2021_paper.html}
}

@misc{GraphAttentionNetworks,
  title = {Graph {{Attention Networks}}},
  urldate = {2019-12-30}
}

@misc{GraphAttentionNetworksa,
  title = {Graph {{Attention Networks}}},
  urldate = {2019-12-30},
  howpublished = {http://petar-v.com/GAT/},
  file = {C:\Users\cleme\Zotero\storage\XSGZ82FI\GAT.html}
}

@article{grassmannDeepLearningAlgorithm2018c,
  title = {A {{Deep Learning Algorithm}} for {{Prediction}} of {{Age-Related Eye Disease Study Severity Scale}} for {{Age-Related Macular Degeneration}} from {{Color Fundus Photography}}},
  author = {Grassmann, Felix and Mengelkamp, Judith and Brandl, Caroline and Harsch, Sebastian and Zimmermann, Martina E. and Linkohr, Birgit and Peters, Annette and Heid, Iris M. and Palm, Christoph and Weber, Bernhard H. F.},
  year = {2018},
  month = sep,
  journal = {Ophthalmology},
  volume = {125},
  number = {9},
  pages = {1410--1420},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2018.02.037},
  urldate = {2019-07-30},
  abstract = {Purpose Age-related macular degeneration (AMD) is a common threat to vision. While classification of disease stages is critical to understanding disease risk and progression, several systems based on color fundus photographs are known. Most of these require in-depth and time-consuming analysis of fundus images. Herein, we present an automated computer-based classification algorithm. Design Algorithm development for AMD classification based on a large collection of color fundus images. Validation is performed on a cross-sectional, population-based study. Participants We included 120\,656 manually graded color fundus images from 3654 Age-Related Eye Disease Study (AREDS) participants. AREDS participants were {$>$}55 years of age, and non-AMD sight-threatening diseases were excluded at recruitment. In addition, performance of our algorithm was evaluated in 5555 fundus images from the population-based Kooperative Gesundheitsforschung in der Region Augsburg (KORA; Cooperative Health Research in the Region of Augsburg) study. Methods We defined 13 classes (9 AREDS steps, 3 late AMD stages, and 1 for ungradable images) and trained several convolution deep learning architectures. An ensemble of network architectures improved prediction accuracy. An independent dataset was used to evaluate the performance of our algorithm in a population-based study. Main Outcome Measures {$\kappa$} Statistics and accuracy to evaluate the concordance between predicted and expert human grader classification. Results A network ensemble of 6 different neural net architectures predicted the 13 classes in the AREDS test set with a quadratic weighted {$\kappa$} of 92\% (95\% confidence interval, 89\%--92\%) and an overall accuracy of 63.3\%. In the independent KORA dataset, images wrongly classified as AMD were mainly the result of a macular reflex observed in young individuals. By restricting the KORA analysis to individuals {$>$}55 years of age and prior exclusion of other retinopathies, the weighted and unweighted {$\kappa$} increased to 50\% and 63\%, respectively. Importantly, the algorithm detected 84.2\% of all fundus images with definite signs of early or late AMD. Overall, 94.3\% of healthy fundus images were classified correctly. Conclusions Our deep learning algoritm revealed a weighted {$\kappa$} outperforming human graders in the AREDS study and is suitable to classify AMD fundus images in other datasets using individuals {$>$}55 years of age.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\86EHQW62\\Grassmann et al. - 2018 - A Deep Learning Algorithm for Prediction of Age-Re.pdf;C\:\\Users\\cleme\\Zotero\\storage\\Y4BPSGP8\\Grassmann et al. - 2018 - A Deep Learning Algorithm for Prediction of Age-Re.pdf;C\:\\Users\\cleme\\Zotero\\storage\\KPRVLIKQ\\S0161642017330646.html}
}

@article{groenSystematicReviewUse2022,
  title = {A Systematic Review on the Use of Explainability in Deep Learning Systems for Computer Aided Diagnosis in Radiology: {{Limited}} Use of Explainable {{AI}}?},
  shorttitle = {A Systematic Review on the Use of Explainability in Deep Learning Systems for Computer Aided Diagnosis in Radiology},
  author = {Groen, Arjan M. and Kraan, Rik and Amirkhan, Shahira F. and Daams, Joost G. and Maas, Mario},
  year = {2022},
  month = dec,
  journal = {European Journal of Radiology},
  volume = {157},
  pages = {110592},
  issn = {0720-048X},
  doi = {10.1016/j.ejrad.2022.110592},
  urldate = {2023-05-04},
  abstract = {Objectives This study aims to contribute to an understanding of the explainability of computer aided diagnosis studies in radiology that use end-to-end deep learning by providing a quantitative overview of methodological choices and by discussing the implications of these choices for their explainability. Methods A systematic review was executed using the preferred reporting items for systemic reviews and meta-analysis guidelines. Primary diagnostic test accuracy studies using end-to-end deep learning for radiology were identified from the period January 1st, 2016, to January 20th, 2021. Results were synthesized by identifying the explanation goals, measures, and explainable AI techniques. Results This study identified 490 primary diagnostic test accuracy studies using end-to-end deep learning for radiology, of which 179 (37\%) used explainable AI. In 147 out of 179 (82\%) of studies, explainable AI was used for the goal of model visualization and inspection. Class activation mapping is the most common technique, being used in 117 out of 179 studies (65\%). Only 1 study used measures to evaluate the outcome of their explainable AI. Conclusions A considerable portion of computer aided diagnosis studies provide a form of explainability of their deep learning models for the purpose of model visualization and inspection. The techniques commonly chosen by these studies (class activation mapping, feature activation mapping and t-distributed stochastic neighbor embedding) have potential limitations. Because researchers generally do not measure the quality of their explanations, we are agnostic about how effective these explanations are at addressing the black box issues of deep learning in radiology.},
  langid = {english},
  keywords = {Computer aided diagnosis,Deep learning,Explainable artificial intelligence,Radiology},
  file = {C:\Users\cleme\Zotero\storage\Q8STCQ4T\Groen et al. - 2022 - A systematic review on the use of explainability i.pdf}
}

@article{guClassificationDiabeticRetinopathy2023,
  title = {Classification of {{Diabetic Retinopathy Severity}} in {{Fundus Images Using}} the {{Vision Transformer}} and {{Residual Attention}}},
  author = {Gu, Zongyun and Li, Yan and Wang, Zijian and Kan, Junling and Shu, Jianhua and Wang, Qing},
  year = {2023},
  month = jan,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2023},
  pages = {1305583},
  issn = {1687-5265},
  doi = {10.1155/2023/1305583},
  urldate = {2023-02-25},
  abstract = {Diabetic retinopathy (DR) is a common retinal vascular disease, which can cause severe visual impairment. It is of great clinical significance to use fundus images for intelligent diagnosis of DR. In this paper, an intelligent DR classification model of fundus images is proposed. This method can detect all the five stages of DR, including of no DR, mild, moderate, severe, and proliferative. This model is composed of two key modules. FEB, feature extraction block, is mainly used for feature extraction of fundus images, and GPB, grading prediction block, is used to classify the five stages of DR. The transformer in the FEB has more fine-grained attention that can pay more attention to retinal hemorrhage and exudate areas. The residual attention in the GPB can effectively capture different spatial regions occupied by different classes of objects. Comprehensive experiments on DDR datasets well demonstrate the superiority of our method, and compared with the benchmark method, our method has achieved competitive performance.},
  pmcid = {PMC9831706},
  pmid = {36636467}
}

@article{guClassificationDiabeticRetinopathy2023a,
  title = {Classification of {{Diabetic Retinopathy Severity}} in {{Fundus Images Using}} the {{Vision Transformer}} and {{Residual Attention}}},
  author = {Gu, Zongyun and Li, Yan and Wang, Zijian and Kan, Junling and Shu, Jianhua and Wang, Qing},
  year = {2023},
  month = jan,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2023},
  pages = {e1305583},
  publisher = {Hindawi},
  issn = {1687-5265},
  doi = {10.1155/2023/1305583},
  urldate = {2023-06-28},
  abstract = {Diabetic retinopathy (DR) is a common retinal vascular disease, which can cause severe visual impairment. It is of great clinical significance to use fundus images for intelligent diagnosis of DR. In this paper, an intelligent DR classification model of fundus images is proposed. This method can detect all the five stages of DR, including of no DR, mild, moderate, severe, and proliferative. This model is composed of two key modules. FEB, feature extraction block, is mainly used for feature extraction of fundus images, and GPB, grading prediction block, is used to classify the five stages of DR. The transformer in the FEB has more fine-grained attention that can pay more attention to retinal hemorrhage and exudate areas. The residual attention in the GPB can effectively capture different spatial regions occupied by different classes of objects. Comprehensive experiments on DDR datasets well demonstrate the superiority of our method, and compared with the benchmark method, our method has achieved competitive performance.},
  langid = {english}
}

@article{guClassificationDiabeticRetinopathy2023b,
  title = {Classification of {{Diabetic Retinopathy Severity}} in {{Fundus Images Using}} the {{Vision Transformer}} and {{Residual Attention}}},
  author = {Gu, Zongyun and Li, Yan and Wang, Zijian and Kan, Junling and Shu, Jianhua and Wang, Qing},
  year = {2023},
  month = jan,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2023},
  pages = {e1305583},
  publisher = {Hindawi},
  issn = {1687-5265},
  doi = {10.1155/2023/1305583},
  urldate = {2023-06-28},
  abstract = {Diabetic retinopathy (DR) is a common retinal vascular disease, which can cause severe visual impairment. It is of great clinical significance to use fundus images for intelligent diagnosis of DR. In this paper, an intelligent DR classification model of fundus images is proposed. This method can detect all the five stages of DR, including of no DR, mild, moderate, severe, and proliferative. This model is composed of two key modules. FEB, feature extraction block, is mainly used for feature extraction of fundus images, and GPB, grading prediction block, is used to classify the five stages of DR. The transformer in the FEB has more fine-grained attention that can pay more attention to retinal hemorrhage and exudate areas. The residual attention in the GPB can effectively capture different spatial regions occupied by different classes of objects. Comprehensive experiments on DDR datasets well demonstrate the superiority of our method, and compared with the benchmark method, our method has achieved competitive performance.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\YNCVXMZA\Gu et al. - 2023 - Classification of Diabetic Retinopathy Severity in.pdf}
}

@article{guClassificationDiabeticRetinopathy2023c,
  title = {Classification of {{Diabetic Retinopathy Severity}} in {{Fundus Images Using}} the {{Vision Transformer}} and {{Residual Attention}}},
  author = {Gu, Zongyun and Li, Yan and Wang, Zijian and Kan, Junling and Shu, Jianhua and Wang, Qing},
  year = {2023},
  month = jan,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2023},
  pages = {1305583},
  issn = {1687-5265},
  doi = {10.1155/2023/1305583},
  urldate = {2023-02-25},
  abstract = {Diabetic retinopathy (DR) is a common retinal vascular disease, which can cause severe visual impairment. It is of great clinical significance to use fundus images for intelligent diagnosis of DR. In this paper, an intelligent DR classification model of fundus images is proposed. This method can detect all the five stages of DR, including of no DR, mild, moderate, severe, and proliferative. This model is composed of two key modules. FEB, feature extraction block, is mainly used for feature extraction of fundus images, and GPB, grading prediction block, is used to classify the five stages of DR. The transformer in the FEB has more fine-grained attention that can pay more attention to retinal hemorrhage and exudate areas. The residual attention in the GPB can effectively capture different spatial regions occupied by different classes of objects. Comprehensive experiments on DDR datasets well demonstrate the superiority of our method, and compared with the benchmark method, our method has achieved competitive performance.},
  pmcid = {PMC9831706},
  pmid = {36636467},
  file = {C:\Users\cleme\Zotero\storage\RWMM7BML\Gu et al. - 2023 - Classification of Diabetic Retinopathy Severity in.pdf}
}

@inproceedings{guiUnderstandingDeepLearning2021,
  title = {Towards {{Understanding Deep Learning}} from {{Noisy Labels}} with {{Small-Loss Criterion}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Gui, Xian-Jin and Wang, Wei and Tian, Zhang-Hao},
  year = {2021},
  month = aug,
  pages = {2469--2475},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Montreal, Canada},
  doi = {10.24963/ijcai.2021/340},
  urldate = {2023-06-28},
  abstract = {Deep neural networks need large amounts of labeled data to achieve good performance. In realworld applications, labels are usually collected from non-experts such as crowdsourcing to save cost and thus are noisy. In the past few years, deep learning methods for dealing with noisy labels have been developed, many of which are based on the small-loss criterion. However, there are few theoretical analyses to explain why these methods could learn well from noisy labels. In this paper, we theoretically explain why the widely-used small-loss criterion works. Based on the explanation, we reformalize the vanilla small-loss criterion to better tackle noisy labels. The experimental results verify our theoretical explanation and also demonstrate the effectiveness of the reformalization.},
  isbn = {978-0-9992411-9-6},
  langid = {english}
}

@inproceedings{guiUnderstandingDeepLearning2021a,
  title = {Towards {{Understanding Deep Learning}} from {{Noisy Labels}} with {{Small-Loss Criterion}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Gui, Xian-Jin and Wang, Wei and Tian, Zhang-Hao},
  year = {2021},
  month = aug,
  pages = {2469--2475},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Montreal, Canada},
  doi = {10.24963/ijcai.2021/340},
  urldate = {2023-06-28},
  abstract = {Deep neural networks need large amounts of labeled data to achieve good performance. In realworld applications, labels are usually collected from non-experts such as crowdsourcing to save cost and thus are noisy. In the past few years, deep learning methods for dealing with noisy labels have been developed, many of which are based on the small-loss criterion. However, there are few theoretical analyses to explain why these methods could learn well from noisy labels. In this paper, we theoretically explain why the widely-used small-loss criterion works. Based on the explanation, we reformalize the vanilla small-loss criterion to better tackle noisy labels. The experimental results verify our theoretical explanation and also demonstrate the effectiveness of the reformalization.},
  isbn = {978-0-9992411-9-6},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\MVY9RCVK\Gui et al. - 2021 - Towards Understanding Deep Learning from Noisy Lab.pdf}
}

@incollection{gulrajaniImprovedTrainingWasserstein2017,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5767--5777},
  publisher = {Curran Associates, Inc.},
  urldate = {2019-06-13}
}

@incollection{gulrajaniImprovedTrainingWasserstein2017a,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5767--5777},
  publisher = {Curran Associates, Inc.},
  urldate = {2019-06-13},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9X6SCJZ5\\Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZUPFYQ98\\7159-improved-training-of-wasserstein-gans.html}
}

@article{gulshanDevelopmentValidationDeep2016,
  title = {Development and {{Validation}} of a {{Deep Learning Algorithm}} for {{Detection}} of {{Diabetic Retinopathy}} in {{Retinal Fundus Photographs}}},
  author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
  year = {2016},
  month = dec,
  journal = {JAMA : the journal of the American Medical Association},
  volume = {316},
  number = {22},
  pages = {2402--2410},
  issn = {0098-7484},
  doi = {10.1001/jama.2016.17216},
  urldate = {2019-09-23},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.{$<$}/p{$><$}h3{$>$}Design and Setting{$<$}/h3{$><$}p{$>$}A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.{$<$}/p{$><$}h3{$>$}Exposure{$<$}/h3{$><$}p{$>$}Deep learning--trained algorithm.{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2\% women; prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully gradable images [14.6\%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5\% and specificity was 93.4\% and for Messidor-2 the sensitivity was 96.1\% and specificity was 93.9\%.{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.{$<$}/p{$>$}},
  langid = {english}
}

@article{gulshanDevelopmentValidationDeep2016a,
  title = {Development and {{Validation}} of a {{Deep Learning Algorithm}} for {{Detection}} of {{Diabetic Retinopathy}} in {{Retinal Fundus Photographs}}},
  author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
  year = {2016},
  month = dec,
  journal = {JAMA : the journal of the American Medical Association},
  volume = {316},
  number = {22},
  pages = {2402--2410},
  issn = {0098-7484},
  doi = {10.1001/jama.2016.17216},
  urldate = {2019-07-24},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.{$<$}/p{$><$}h3{$>$}Design and Setting{$<$}/h3{$><$}p{$>$}A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.{$<$}/p{$><$}h3{$>$}Exposure{$<$}/h3{$><$}p{$>$}Deep learning--trained algorithm.{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2\% women; prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully gradable images [14.6\%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5\% and specificity was 93.4\% and for Messidor-2 the sensitivity was 96.1\% and specificity was 93.9\%.{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.{$<$}/p{$>$}},
  langid = {english}
}

@article{gulshanDevelopmentValidationDeep2016b,
  title = {Development and {{Validation}} of a {{Deep Learning Algorithm}} for {{Detection}} of {{Diabetic Retinopathy}} in {{Retinal Fundus Photographs}}},
  author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
  year = {2016},
  month = dec,
  journal = {JAMA : the journal of the American Medical Association},
  volume = {316},
  number = {22},
  pages = {2402--2410},
  issn = {0098-7484},
  doi = {10.1001/jama.2016.17216},
  urldate = {2021-11-11},
  abstract = {Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128\,175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.Deep learning--trained algorithm.The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2\% women; prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully gradable images [14.6\%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5\% and specificity was 93.4\% and for Messidor-2 the sensitivity was 96.1\% and specificity was 93.9\%.In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.}
}

@article{gulshanDevelopmentValidationDeep2016c,
  title = {Development and {{Validation}} of a {{Deep Learning Algorithm}} for {{Detection}} of {{Diabetic Retinopathy}} in {{Retinal Fundus Photographs}}},
  author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
  year = {2016},
  month = dec,
  journal = {JAMA},
  volume = {316},
  number = {22},
  pages = {2402--2410},
  issn = {0098-7484},
  doi = {10.1001/jama.2016.17216},
  urldate = {2019-07-24},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.{$<$}/p{$><$}h3{$>$}Design and Setting{$<$}/h3{$><$}p{$>$}A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.{$<$}/p{$><$}h3{$>$}Exposure{$<$}/h3{$><$}p{$>$}Deep learning--trained algorithm.{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2\% women; prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully gradable images [14.6\%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5\% and specificity was 93.4\% and for Messidor-2 the sensitivity was 96.1\% and specificity was 93.9\%.{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.{$<$}/p{$>$}},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\EG5KIE9P\\Gulshan et al. - 2016 - Development and Validation of a Deep Learning Algo.pdf;C\:\\Users\\cleme\\Zotero\\storage\\PE8RHZ5F\\2588763.html}
}

@article{gulshanDevelopmentValidationDeep2016d,
  title = {Development and {{Validation}} of a {{Deep Learning Algorithm}} for {{Detection}} of {{Diabetic Retinopathy}} in {{Retinal Fundus Photographs}}},
  author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
  year = {2016},
  month = dec,
  journal = {JAMA},
  volume = {316},
  number = {22},
  pages = {2402--2410},
  issn = {0098-7484},
  doi = {10.1001/jama.2016.17216},
  urldate = {2019-09-23},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.{$<$}/p{$><$}h3{$>$}Design and Setting{$<$}/h3{$><$}p{$>$}A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.{$<$}/p{$><$}h3{$>$}Exposure{$<$}/h3{$><$}p{$>$}Deep learning--trained algorithm.{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2\% women; prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully gradable images [14.6\%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5\% and specificity was 93.4\% and for Messidor-2 the sensitivity was 96.1\% and specificity was 93.9\%.{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.{$<$}/p{$>$}},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9Q7JVJL5\\Gulshan et al. - 2016 - Development and Validation of a Deep Learning Algo.pdf;C\:\\Users\\cleme\\Zotero\\storage\\3PS9KMKB\\2588763.html}
}

@article{gulshanDevelopmentValidationDeep2016e,
  title = {Development and {{Validation}} of a {{Deep Learning Algorithm}} for {{Detection}} of {{Diabetic Retinopathy}} in {{Retinal Fundus Photographs}}},
  author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
  year = {2016},
  month = dec,
  journal = {JAMA},
  volume = {316},
  number = {22},
  pages = {2402--2410},
  issn = {0098-7484},
  doi = {10.1001/jama.2016.17216},
  urldate = {2021-11-11},
  abstract = {Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128\,175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.Deep learning--trained algorithm.The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2\% women; prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully gradable images [14.6\%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5\% and specificity was 93.4\% and for Messidor-2 the sensitivity was 96.1\% and specificity was 93.9\%.In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.},
  file = {C:\Users\cleme\Zotero\storage\YU24CWHI\2588763.html}
}

@article{gulshanPerformanceDeepLearningAlgorithm2019,
  title = {Performance of a {{Deep-Learning Algorithm}} vs {{Manual Grading}} for {{Detecting Diabetic Retinopathy}} in {{India}}},
  author = {Gulshan, Varun and Rajan, Renu P. and Widner, Kasumi and Wu, Derek and Wubbels, Peter and Rhodes, Tyler and Whitehouse, Kira and Coram, Marc and Corrado, Greg and Ramasamy, Kim and Raman, Rajiv and Peng, Lily and Webster, Dale R.},
  year = {2019},
  month = jun,
  journal = {JAMA ophthalmology},
  issn = {2168-6173},
  doi = {10.1001/jamaophthalmol.2019.2004},
  abstract = {Importance: More than 60 million people in India have diabetes and are at risk for diabetic retinopathy (DR), a vision-threatening disease. Automated interpretation of retinal fundus photographs can help support and scale a robust screening program to detect DR. Objective: To prospectively validate the performance of an automated DR system across 2 sites in India. Design, Setting, and Participants: This prospective observational study was conducted at 2 eye care centers in India (Aravind Eye Hospital and Sankara Nethralaya) and included 3049 patients with diabetes. Data collection and patient enrollment took place between April 2016 and July 2016 at Aravind and May 2016 and April 2017 at Sankara Nethralaya. The model was trained and fixed in March 2016. Interventions: Automated DR grading system compared with manual grading by 1 trained grader and 1 retina specialist from each site. Adjudication by a panel of 3 retinal specialists served as the reference standard in the cases of disagreement. Main Outcomes and Measures: Sensitivity and specificity for moderate or worse DR or referable diabetic macula edema. Results: Of 3049 patients, 1091 (35.8\%) were women and the mean (SD) age for patients at Aravind and Sankara Nethralaya was 56.6 (9.0) years and 56.0 (10.0) years, respectively. For moderate or worse DR, the sensitivity and specificity for manual grading by individual nonadjudicator graders ranged from 73.4\% to 89.8\% and from 83.5\% to 98.7\%, respectively. The automated DR system's performance was equal to or exceeded manual grading, with an 88.9\% sensitivity (95\% CI, 85.8-91.5), 92.2\% specificity (95\% CI, 90.3-93.8), and an area under the curve of 0.963 on the data set from Aravind Eye Hospital and 92.1\% sensitivity (95\% CI, 90.1-93.8), 95.2\% specificity (95\% CI, 94.2-96.1), and an area under the curve of 0.980 on the data set from Sankara Nethralaya. Conclusions and Relevance: This study shows that the automated DR system generalizes to this population of Indian patients in a prospective setting and demonstrates the feasibility of using an automated DR grading system to expand screening programs.},
  langid = {english},
  pmcid = {PMC6567842},
  pmid = {31194246}
}

@article{gulshanPerformanceDeepLearningAlgorithm2019a,
  title = {Performance of a {{Deep-Learning Algorithm}} vs {{Manual Grading}} for {{Detecting Diabetic Retinopathy}} in {{India}}},
  author = {Gulshan, Varun and Rajan, Renu P. and Widner, Kasumi and Wu, Derek and Wubbels, Peter and Rhodes, Tyler and Whitehouse, Kira and Coram, Marc and Corrado, Greg and Ramasamy, Kim and Raman, Rajiv and Peng, Lily and Webster, Dale R.},
  year = {2019},
  month = jun,
  journal = {JAMA ophthalmology},
  issn = {2168-6173},
  doi = {10.1001/jamaophthalmol.2019.2004},
  abstract = {Importance: More than 60 million people in India have diabetes and are at risk for diabetic retinopathy (DR), a vision-threatening disease. Automated interpretation of retinal fundus photographs can help support and scale a robust screening program to detect DR. Objective: To prospectively validate the performance of an automated DR system across 2 sites in India. Design, Setting, and Participants: This prospective observational study was conducted at 2 eye care centers in India (Aravind Eye Hospital and Sankara Nethralaya) and included 3049 patients with diabetes. Data collection and patient enrollment took place between April 2016 and July 2016 at Aravind and May 2016 and April 2017 at Sankara Nethralaya. The model was trained and fixed in March 2016. Interventions: Automated DR grading system compared with manual grading by 1 trained grader and 1 retina specialist from each site. Adjudication by a panel of 3 retinal specialists served as the reference standard in the cases of disagreement. Main Outcomes and Measures: Sensitivity and specificity for moderate or worse DR or referable diabetic macula edema. Results: Of 3049 patients, 1091 (35.8\%) were women and the mean (SD) age for patients at Aravind and Sankara Nethralaya was 56.6 (9.0) years and 56.0 (10.0) years, respectively. For moderate or worse DR, the sensitivity and specificity for manual grading by individual nonadjudicator graders ranged from 73.4\% to 89.8\% and from 83.5\% to 98.7\%, respectively. The automated DR system's performance was equal to or exceeded manual grading, with an 88.9\% sensitivity (95\% CI, 85.8-91.5), 92.2\% specificity (95\% CI, 90.3-93.8), and an area under the curve of 0.963 on the data set from Aravind Eye Hospital and 92.1\% sensitivity (95\% CI, 90.1-93.8), 95.2\% specificity (95\% CI, 94.2-96.1), and an area under the curve of 0.980 on the data set from Sankara Nethralaya. Conclusions and Relevance: This study shows that the automated DR system generalizes to this population of Indian patients in a prospective setting and demonstrates the feasibility of using an automated DR grading system to expand screening programs.},
  langid = {english},
  pmcid = {PMC6567842},
  pmid = {31194246},
  file = {C:\Users\cleme\Zotero\storage\XWAER49F\Gulshan et al. - 2019 - Performance of a Deep-Learning Algorithm vs Manual.pdf}
}

@article{gundersenEndtoendTrainingDeep,
  title = {End-to-End Training of Deep Probabilistic {{CCA}} for Joint Modeling of Paired Biomedical Observations},
  author = {Gundersen, Gregory and Dumitrascu, Bianca},
  pages = {5},
  langid = {english}
}

@article{gundersenEndtoendTrainingDeepa,
  title = {End-to-End Training of Deep Probabilistic {{CCA}} for Joint Modeling of Paired Biomedical Observations},
  author = {Gundersen, Gregory and Dumitrascu, Bianca},
  pages = {5},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\ALIIW7I3\Gundersen et Dumitrascu - End-to-end training of deep probabilistic CCA for .pdf}
}

@article{guoCARNetCascadeAttentive2022,
  title = {{{CARNet}}: {{Cascade}} Attentive {{RefineNet}} for Multi-Lesion Segmentation of Diabetic Retinopathy Images},
  shorttitle = {{{CARNet}}},
  author = {Guo, Yanfei and Peng, Yanjun},
  year = {2022},
  month = apr,
  journal = {Complex \& Intelligent Systems},
  volume = {8},
  number = {2},
  pages = {1681--1701},
  issn = {2198-6053},
  doi = {10.1007/s40747-021-00630-4},
  urldate = {2023-02-16},
  abstract = {Diabetic retinopathy is the leading cause of blindness in working population. Lesion segmentation from fundus images helps ophthalmologists accurately diagnose and grade of diabetic retinopathy. However, the task of lesion segmentation is full of challenges due to the complex structure, the various sizes and the interclass similarity with other fundus tissues. To address the issue, this paper proposes a cascade attentive RefineNet (CARNet) for automatic and accurate multi-lesion segmentation of diabetic retinopathy. It can make full use of the fine local details and coarse global information from the fundus image. CARNet is composed of global image encoder, local image encoder and attention refinement decoder. We take the whole image and the patch image as the dual input, and feed them to ResNet50 and ResNet101, respectively, for downsampling to extract lesion features. The high-level refinement decoder uses dual attention mechanism to integrate the same-level features in the two encoders with the output of the low-level attention refinement module for multiscale information fusion, which focus the model on the lesion area to generate accurate predictions. We evaluated the segmentation performance of the proposed CARNet on the IDRiD, E-ophtha and DDR data sets. Extensive comparison experiments and ablation studies on various data sets demonstrate the proposed framework outperforms the state-of-the-art approaches and has better accuracy and robustness. It not only overcomes the interference of similar tissues and noises to achieve accurate multi-lesion segmentation, but also preserves the contour details and shape features of small lesions without overloading GPU memory usage.},
  langid = {english},
  keywords = {Attention fusion,Diabetic retinopathy,Fundus image,Multi-lesion segmentation,RefineNet}
}

@article{guoCARNetCascadeAttentive2022a,
  title = {{{CARNet}}: {{Cascade}} Attentive {{RefineNet}} for Multi-Lesion Segmentation of Diabetic Retinopathy Images},
  shorttitle = {{{CARNet}}},
  author = {Guo, Yanfei and Peng, Yanjun},
  year = {2022},
  month = apr,
  journal = {Complex \& Intelligent Systems},
  volume = {8},
  number = {2},
  pages = {1681--1701},
  issn = {2198-6053},
  doi = {10.1007/s40747-021-00630-4},
  urldate = {2023-09-03},
  abstract = {Diabetic retinopathy is the leading cause of blindness in working population. Lesion segmentation from fundus images helps ophthalmologists accurately diagnose and grade of diabetic retinopathy. However, the task of lesion segmentation is full of challenges due to the complex structure, the various sizes and the interclass similarity with other fundus tissues. To address the issue, this paper proposes a cascade attentive RefineNet (CARNet) for automatic and accurate multi-lesion segmentation of diabetic retinopathy. It can make full use of the fine local details and coarse global information from the fundus image. CARNet is composed of global image encoder, local image encoder and attention refinement decoder. We take the whole image and the patch image as the dual input, and feed them to ResNet50 and ResNet101, respectively, for downsampling to extract lesion features. The high-level refinement decoder uses dual attention mechanism to integrate the same-level features in the two encoders with the output of the low-level attention refinement module for multiscale information fusion, which focus the model on the lesion area to generate accurate predictions. We evaluated the segmentation performance of the proposed CARNet on the IDRiD, E-ophtha and DDR data sets. Extensive comparison experiments and ablation studies on various data sets demonstrate the proposed framework outperforms the state-of-the-art approaches and has better accuracy and robustness. It not only overcomes the interference of similar tissues and noises to achieve accurate multi-lesion segmentation, but also preserves the contour details and shape features of small lesions without overloading GPU memory usage.},
  langid = {english},
  keywords = {Attention fusion,Diabetic retinopathy,Fundus image,Multi-lesion segmentation,RefineNet}
}

@article{guoCARNetCascadeAttentive2022b,
  title = {{{CARNet}}: {{Cascade}} Attentive {{RefineNet}} for Multi-Lesion Segmentation of Diabetic Retinopathy Images},
  shorttitle = {{{CARNet}}},
  author = {Guo, Yanfei and Peng, Yanjun},
  year = {2022},
  month = apr,
  journal = {Complex \& Intelligent Systems},
  volume = {8},
  number = {2},
  pages = {1681--1701},
  issn = {2198-6053},
  doi = {10.1007/s40747-021-00630-4},
  urldate = {2023-09-03},
  abstract = {Diabetic retinopathy is the leading cause of blindness in working population. Lesion segmentation from fundus images helps ophthalmologists accurately diagnose and grade of diabetic retinopathy. However, the task of lesion segmentation is full of challenges due to the complex structure, the various sizes and the interclass similarity with other fundus tissues. To address the issue, this paper proposes a cascade attentive RefineNet (CARNet) for automatic and accurate multi-lesion segmentation of diabetic retinopathy. It can make full use of the fine local details and coarse global information from the fundus image. CARNet is composed of global image encoder, local image encoder and attention refinement decoder. We take the whole image and the patch image as the dual input, and feed them to ResNet50 and ResNet101, respectively, for downsampling to extract lesion features. The high-level refinement decoder uses dual attention mechanism to integrate the same-level features in the two encoders with the output of the low-level attention refinement module for multiscale information fusion, which focus the model on the lesion area to generate accurate predictions. We evaluated the segmentation performance of the proposed CARNet on the IDRiD, E-ophtha and DDR data sets. Extensive comparison experiments and ablation studies on various data sets demonstrate the proposed framework outperforms the state-of-the-art approaches and has better accuracy and robustness. It not only overcomes the interference of similar tissues and noises to achieve accurate multi-lesion segmentation, but also preserves the contour details and shape features of small lesions without overloading GPU memory usage.},
  langid = {english},
  keywords = {Attention fusion,Diabetic retinopathy,Fundus image,Multi-lesion segmentation,RefineNet},
  file = {C:\Users\cleme\Zotero\storage\9IEYAEQ8\Guo et Peng - 2022 - CARNet Cascade attentive RefineNet for multi-lesi.pdf}
}

@article{guoCARNetCascadeAttentive2022c,
  title = {{{CARNet}}: {{Cascade}} Attentive {{RefineNet}} for Multi-Lesion Segmentation of Diabetic Retinopathy Images},
  shorttitle = {{{CARNet}}},
  author = {Guo, Yanfei and Peng, Yanjun},
  year = {2022},
  month = apr,
  journal = {Complex \& Intelligent Systems},
  volume = {8},
  number = {2},
  pages = {1681--1701},
  issn = {2198-6053},
  doi = {10.1007/s40747-021-00630-4},
  urldate = {2023-02-16},
  abstract = {Diabetic retinopathy is the leading cause of blindness in working population. Lesion segmentation from fundus images helps ophthalmologists accurately diagnose and grade of diabetic retinopathy. However, the task of lesion segmentation is full of challenges due to the complex structure, the various sizes and the interclass similarity with other fundus tissues. To address the issue, this paper proposes a cascade attentive RefineNet (CARNet) for automatic and accurate multi-lesion segmentation of diabetic retinopathy. It can make full use of the fine local details and coarse global information from the fundus image. CARNet is composed of global image encoder, local image encoder and attention refinement decoder. We take the whole image and the patch image as the dual input, and feed them to ResNet50 and ResNet101, respectively, for downsampling to extract lesion features. The high-level refinement decoder uses dual attention mechanism to integrate the same-level features in the two encoders with the output of the low-level attention refinement module for multiscale information fusion, which focus the model on the lesion area to generate accurate predictions. We evaluated the segmentation performance of the proposed CARNet on the IDRiD, E-ophtha and DDR data sets. Extensive comparison experiments and ablation studies on various data sets demonstrate the proposed framework outperforms the state-of-the-art approaches and has better accuracy and robustness. It not only overcomes the interference of similar tissues and noises to achieve accurate multi-lesion segmentation, but also preserves the contour details and shape features of small lesions without overloading GPU memory usage.},
  langid = {english},
  keywords = {Attention fusion,Diabetic retinopathy,Fundus image,Multi-lesion segmentation,RefineNet},
  file = {C:\Users\cleme\Zotero\storage\PQQCU6IM\Guo et Peng - 2022 - CARNet Cascade attentive RefineNet for multi-lesi.pdf}
}

@article{guoComputeraidedHealthcareSystem2015,
  title = {A Computer-Aided Healthcare System for Cataract Classification and Grading Based on Fundus Image Analysis},
  author = {Guo, Liye and Yang, Ji-Jiang and Peng, Lihui and Li, Jianqiang and Liang, Qingfeng},
  year = {2015},
  month = may,
  journal = {Computers in Industry},
  series = {Special {{Issue}}: {{Information Technologies}} for {{Enhanced Healthcare}}},
  volume = {69},
  pages = {72--80},
  issn = {0166-3615},
  doi = {10.1016/j.compind.2014.09.005},
  urldate = {2019-11-19},
  abstract = {This paper presents a fundus image analysis based computer aided system for automatic classification and grading of cataract, which provides great potentials to reduce the burden of well-experienced ophthalmologists (the scarce resources) and help cataract patients in under-developed areas to know timely their cataract conditions and obtain treatment suggestions from doctors. The system is composed of fundus image pre-processing, image feature extraction, and automatic cataract classification and grading. The wavelet transform and the sketch based methods are investigated to extract from fundus image the features suitable for cataract classification and grading. After feature extraction, a multiclass discriminant analysis algorithm is used for cataract classification, including two-class (cataract or non-cataract) classification and cataract grading in mild, moderate, and severe. A real-world dataset, including fundus image samples with mild, moderate, and severe cataract, is used for training and testing. The preliminary results show that, for the wavelet transform based method, the correct classification rates of two-class classification and cataract grading are 90.9\% and 77.1\%, respectively. The correct classification rates of two-class classification and cataract grading are 86.1\% and 74.0\% for the sketch based method, which is comparable to the wavelet transform based method. The pilot study demonstrates that our research on fundus image analysis for cataract classification and grading is very helpful for improving the efficiency of fundus image review and ophthalmic healthcare quality. We believe that this work can serve as an important reference for the development of similar health information system to solve other medical diagnosis problems.},
  langid = {english},
  keywords = {Cataract detection,Fundus image classification,Healthcare improvement,Healthcare system,Ophthalmic disease},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\WEIXNN62\\Guo et al. - 2015 - A computer-aided healthcare system for cataract cl.pdf;C\:\\Users\\cleme\\Zotero\\storage\\INXLN39K\\S0166361514001754.html}
}

@article{guoLSegEndtoendUnified2019,
  title = {L-{{Seg}}: {{An}} End-to-End Unified Framework for Multi-Lesion Segmentation of Fundus Images},
  shorttitle = {L-{{Seg}}},
  author = {Guo, Song and Li, Tao and Kang, Hong and Li, Ning and Zhang, Yujun and Wang, Kai},
  year = {2019},
  month = jul,
  journal = {Neurocomputing},
  volume = {349},
  pages = {52--63},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.04.019},
  urldate = {2022-03-18},
  abstract = {Diabetic retinopathy and diabetic macular edema are the two leading causes for blindness in working-age people, and the quantitative and qualitative diagnosis of these two diseases usually depends on the presence and areas of lesions in fundus images. The main related lesions include soft exudates, hard exudates, microaneurysms, and haemorrhages. However, segmentation of these four kinds of lesions is difficult due to their uncertainty in size, contrast, and high interclass similarity. Therefore, we aim to design a multi-lesion segmentation model. We have designed the first small object segmentation network (L-Seg) that can segment the four kinds of lesions simultaneously. Taking into account that small lesion regions could not response at high level of network, we propose a multi-scale feature fusion method to handle this problem. In addition, when considering the cases of both class-imbalance and loss-imbalance problems, we propose a multi-channel bin loss. We have evaluated L-Seg on three fundus datasets including two publicly available datasets - IDRiD and e-ophtha and one private dataset - DDR. Extensive experiments have demonstrated that L-Seg achieves better performance in small lesion segmentation than other deep learning models and traditional methods. Specially, the mAUC score of L-Seg is over 16.8\%, 1.51\% and 3.11\% higher than that of DeepLab v3+ on IDRiD, e-ophtha and DDR datasets, respectively. Moreover, our framework shows competitive performance compared with top-3 teams in IDRiD challenge. The source code of L-Seg is available at: https://github.com/guomugong/L-Seg.},
  langid = {english},
  keywords = {Class-imbalance,Diabetic retinopathy,Fundus image,Multi-lesion segmentation}
}

@article{guoLSegEndtoendUnified2019a,
  title = {L-{{Seg}}: {{An}} End-to-End Unified Framework for Multi-Lesion Segmentation of Fundus Images},
  shorttitle = {L-{{Seg}}},
  author = {Guo, Song and Li, Tao and Kang, Hong and Li, Ning and Zhang, Yujun and Wang, Kai},
  year = {2019},
  month = jul,
  journal = {Neurocomputing},
  volume = {349},
  pages = {52--63},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.04.019},
  urldate = {2022-10-23},
  abstract = {Diabetic retinopathy and diabetic macular edema are the two leading causes for blindness in working-age people, and the quantitative and qualitative diagnosis of these two diseases usually depends on the presence and areas of lesions in fundus images. The main related lesions include soft exudates, hard exudates, microaneurysms, and haemorrhages. However, segmentation of these four kinds of lesions is difficult due to their uncertainty in size, contrast, and high interclass similarity. Therefore, we aim to design a multi-lesion segmentation model. We have designed the first small object segmentation network (L-Seg) that can segment the four kinds of lesions simultaneously. Taking into account that small lesion regions could not response at high level of network, we propose a multi-scale feature fusion method to handle this problem. In addition, when considering the cases of both class-imbalance and loss-imbalance problems, we propose a multi-channel bin loss. We have evaluated L-Seg on three fundus datasets including two publicly available datasets - IDRiD and e-ophtha and one private dataset - DDR. Extensive experiments have demonstrated that L-Seg achieves better performance in small lesion segmentation than other deep learning models and traditional methods. Specially, the mAUC score of L-Seg is over 16.8\%, 1.51\% and 3.11\% higher than that of DeepLab v3+ on IDRiD, e-ophtha and DDR datasets, respectively. Moreover, our framework shows competitive performance compared with top-3 teams in IDRiD challenge. The source code of L-Seg is available at: https://github.com/guomugong/L-Seg.},
  langid = {english},
  keywords = {Class-imbalance,Diabetic retinopathy,Fundus image,Multi-lesion segmentation}
}

@article{guoLSegEndtoendUnified2019b,
  title = {L-{{Seg}}: {{An}} End-to-End Unified Framework for Multi-Lesion Segmentation of Fundus Images},
  shorttitle = {L-{{Seg}}},
  author = {Guo, Song and Li, Tao and Kang, Hong and Li, Ning and Zhang, Yujun and Wang, Kai},
  year = {2019},
  month = jul,
  journal = {Neurocomputing},
  volume = {349},
  pages = {52--63},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.04.019},
  urldate = {2022-03-18},
  abstract = {Diabetic retinopathy and diabetic macular edema are the two leading causes for blindness in working-age people, and the quantitative and qualitative diagnosis of these two diseases usually depends on the presence and areas of lesions in fundus images. The main related lesions include soft exudates, hard exudates, microaneurysms, and haemorrhages. However, segmentation of these four kinds of lesions is difficult due to their uncertainty in size, contrast, and high interclass similarity. Therefore, we aim to design a multi-lesion segmentation model. We have designed the first small object segmentation network (L-Seg) that can segment the four kinds of lesions simultaneously. Taking into account that small lesion regions could not response at high level of network, we propose a multi-scale feature fusion method to handle this problem. In addition, when considering the cases of both class-imbalance and loss-imbalance problems, we propose a multi-channel bin loss. We have evaluated L-Seg on three fundus datasets including two publicly available datasets - IDRiD and e-ophtha and one private dataset - DDR. Extensive experiments have demonstrated that L-Seg achieves better performance in small lesion segmentation than other deep learning models and traditional methods. Specially, the mAUC score of L-Seg is over 16.8\%, 1.51\% and 3.11\% higher than that of DeepLab v3+ on IDRiD, e-ophtha and DDR datasets, respectively. Moreover, our framework shows competitive performance compared with top-3 teams in IDRiD challenge. The source code of L-Seg is available at: https://github.com/guomugong/L-Seg.},
  langid = {english},
  keywords = {Class-imbalance,Diabetic retinopathy,Fundus image,Multi-lesion segmentation}
}

@article{guoLSegEndtoendUnified2019c,
  title = {L-{{Seg}}: {{An}} End-to-End Unified Framework for Multi-Lesion Segmentation of Fundus Images},
  shorttitle = {L-{{Seg}}},
  author = {Guo, Song and Li, Tao and Kang, Hong and Li, Ning and Zhang, Yujun and Wang, Kai},
  year = {2019},
  month = jul,
  journal = {Neurocomputing},
  volume = {349},
  pages = {52--63},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.04.019},
  urldate = {2022-10-23},
  abstract = {Diabetic retinopathy and diabetic macular edema are the two leading causes for blindness in working-age people, and the quantitative and qualitative diagnosis of these two diseases usually depends on the presence and areas of lesions in fundus images. The main related lesions include soft exudates, hard exudates, microaneurysms, and haemorrhages. However, segmentation of these four kinds of lesions is difficult due to their uncertainty in size, contrast, and high interclass similarity. Therefore, we aim to design a multi-lesion segmentation model. We have designed the first small object segmentation network (L-Seg) that can segment the four kinds of lesions simultaneously. Taking into account that small lesion regions could not response at high level of network, we propose a multi-scale feature fusion method to handle this problem. In addition, when considering the cases of both class-imbalance and loss-imbalance problems, we propose a multi-channel bin loss. We have evaluated L-Seg on three fundus datasets including two publicly available datasets - IDRiD and e-ophtha and one private dataset - DDR. Extensive experiments have demonstrated that L-Seg achieves better performance in small lesion segmentation than other deep learning models and traditional methods. Specially, the mAUC score of L-Seg is over 16.8\%, 1.51\% and 3.11\% higher than that of DeepLab v3+ on IDRiD, e-ophtha and DDR datasets, respectively. Moreover, our framework shows competitive performance compared with top-3 teams in IDRiD challenge. The source code of L-Seg is available at: https://github.com/guomugong/L-Seg.},
  langid = {english},
  keywords = {Class-imbalance,Diabetic retinopathy,Fundus image,Multi-lesion segmentation},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\PY4JG6W3\\Guo et al. - 2019 - L-Seg An end-to-end unified framework for multi-l.pdf;C\:\\Users\\cleme\\Zotero\\storage\\SUYQDRLF\\guo2019.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\9UGIXYMD\\S0925231219305430.html}
}

@data{h25w98-18,
  title = {Indian Diabetic Retinopathy Image Dataset ({{IDRiD}})},
  author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
  year = {2018},
  publisher = {IEEE Dataport},
  doi = {10.21227/H25W98}
}

@data{h25w98-18,
  title = {Indian Diabetic Retinopathy Image Dataset ({{IDRiD}})},
  author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
  year = {2018},
  publisher = {IEEE Dataport},
  doi = {10.21227/H25W98}
}

@data{h25w98-18,
  title = {Indian Diabetic Retinopathy Image Dataset ({{IDRiD}})},
  author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
  year = {2018},
  publisher = {IEEE Dataport},
  doi = {10.21227/H25W98}
}

@data{h25w98-18,
  title = {Indian Diabetic Retinopathy Image Dataset ({{IDRiD}})},
  author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
  year = {2018},
  publisher = {IEEE Dataport},
  doi = {10.21227/H25W98}
}

@article{hanSurveyVisualTransformer2021,
  title = {A {{Survey}} on {{Visual Transformer}}},
  author = {Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and Yang, Zhaohui and Zhang, Yiman and Tao, Dacheng},
  year = {2021},
  month = jan,
  journal = {arXiv:2012.12556 [cs]},
  eprint = {2012.12556},
  primaryclass = {cs},
  urldate = {2021-02-28},
  abstract = {Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent networks. Given its high performance and no need for human-defined inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these visual transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Furthermore, we include efficient transformer methods for pushing transformer into real device-based applications. Toward the end of this paper, we discuss the challenges and provide several further research directions for visual transformers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\NF9LWEUZ\Han et al. - 2021 - A Survey on Visual Transformer.pdf}
}

@article{harasymowyczMedicalManagementGlaucoma2016,
  title = {Medical {{Management}} of {{Glaucoma}} in the 21st {{Century}} from a {{Canadian Perspective}}},
  author = {Harasymowycz, Paul and Birt, Catherine and Gooi, Patrick and Heckler, Lisa and Hutnik, Cindy and Jinapriya, Delan and Shuba, Lesya and Yan, David and Day, Radmila},
  year = {2016},
  journal = {Journal of Ophthalmology},
  volume = {2016},
  pages = {6509809},
  issn = {2090-004X},
  doi = {10.1155/2016/6509809},
  abstract = {Glaucoma is a medical term describing a group of progressive optic neuropathies characterized by degeneration of retinal ganglion cells and retinal nerve fibre layer and resulting in changes in the optic nerve head. Glaucoma is a leading cause of irreversible vision loss worldwide. With the aging population it is expected that the prevalence of glaucoma will continue to increase. Despite recent advances in imaging and visual field testing techniques that allow establishment of earlier diagnosis and treatment initiation, significant numbers of glaucoma patients are undiagnosed and present late in the course of their disease. This can lead to irreversible vision loss, reduced quality of life, and a higher socioeconomic burden. Selection of therapeutic approaches for glaucoma should be based on careful ocular examination, patient medical history, presence of comorbidities, and awareness of concomitant systemic therapies. Therapy should also be individualized to patients' needs and preferences. Recent developments in this therapeutic field require revisiting treatment algorithms and integration of traditional and novel approaches in order to ensure optimal visual outcomes. This article provides an overview of recent developments and practice trends in the medical management of glaucoma in Canada. A discussion of the surgical management is beyond the scope of this paper.},
  langid = {english},
  pmcid = {PMC5118538},
  pmid = {27895937}
}

@article{harasymowyczMedicalManagementGlaucoma2016a,
  title = {Medical {{Management}} of {{Glaucoma}} in the 21st {{Century}} from a {{Canadian Perspective}}},
  author = {Harasymowycz, Paul and Birt, Catherine and Gooi, Patrick and Heckler, Lisa and Hutnik, Cindy and Jinapriya, Delan and Shuba, Lesya and Yan, David and Day, Radmila},
  year = {2016},
  journal = {Journal of Ophthalmology},
  volume = {2016},
  pages = {6509809},
  issn = {2090-004X},
  doi = {10.1155/2016/6509809},
  abstract = {Glaucoma is a medical term describing a group of progressive optic neuropathies characterized by degeneration of retinal ganglion cells and retinal nerve fibre layer and resulting in changes in the optic nerve head. Glaucoma is a leading cause of irreversible vision loss worldwide. With the aging population it is expected that the prevalence of glaucoma will continue to increase. Despite recent advances in imaging and visual field testing techniques that allow establishment of earlier diagnosis and treatment initiation, significant numbers of glaucoma patients are undiagnosed and present late in the course of their disease. This can lead to irreversible vision loss, reduced quality of life, and a higher socioeconomic burden. Selection of therapeutic approaches for glaucoma should be based on careful ocular examination, patient medical history, presence of comorbidities, and awareness of concomitant systemic therapies. Therapy should also be individualized to patients' needs and preferences. Recent developments in this therapeutic field require revisiting treatment algorithms and integration of traditional and novel approaches in order to ensure optimal visual outcomes. This article provides an overview of recent developments and practice trends in the medical management of glaucoma in Canada. A discussion of the surgical management is beyond the scope of this paper.},
  langid = {english},
  pmcid = {PMC5118538},
  pmid = {27895937},
  file = {C:\Users\cleme\Zotero\storage\5WLCTI4W\Harasymowycz et al. - 2016 - Medical Management of Glaucoma in the 21st Century.pdf}
}

@article{haridasSensitivitySpecificityPseudocolor2022,
  title = {Sensitivity and Specificity of Pseudocolor Ultrawide Field Imaging in Comparison to Wide Field Fundus Fluorescein Angiography in Detecting Retinal Neovascularization in Diabetic Retinopathy},
  author = {Haridas, Swathy and Indurkhya, Swati and Kumar, Sailesh and Giridhar, Anantharaman and Sivaprasad, Sobha},
  year = {2022},
  month = oct,
  journal = {Eye},
  volume = {36},
  number = {10},
  pages = {1940--1944},
  publisher = {Nature Publishing Group},
  issn = {1476-5454},
  doi = {10.1038/s41433-021-01772-y},
  urldate = {2024-02-22},
  abstract = {To evaluate the diagnostic accuracy of ultrawide pseudocolor retinal photography (pseudocolor UWF) compared to wide field fundus fluorescein angiography (WFFFA) in the detection of retinal neovascularization (NV) and NV of the disc (NVD) in patients with diabetic retinopathy (DR).},
  copyright = {2021 The Author(s), under exclusive licence to The Royal College of Ophthalmologists},
  langid = {english},
  keywords = {Eye manifestations,Retinal diseases,Risk factors},
  file = {C:\Users\cleme\Zotero\storage\4LK59J38\Haridas et al. - 2022 - Sensitivity and specificity of pseudocolor ultrawi.pdf}
}

@article{hassanDeepLearningBased2021,
  title = {Deep Learning Based Joint Segmentation and Characterization of Multi-Class Retinal Fluid Lesions on {{OCT}} Scans for Clinical Use in Anti-{{VEGF}} Therapy},
  author = {Hassan, Bilal and Qin, Shiyin and Ahmed, Ramsha and Hassan, Taimur and Taguri, Abdel Hakeem and Hashmi, Shahrukh and Werghi, Naoufel},
  year = {2021},
  month = sep,
  journal = {Computers in Biology and Medicine},
  volume = {136},
  pages = {104727},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104727},
  urldate = {2022-07-08},
  abstract = {Background In anti-vascular endothelial growth factor (anti-VEGF) therapy, an accurate estimation of multi-class retinal fluid (MRF) is required for the activity prescription and intravitreal dose. This study proposes an end-to-end deep learning-based retinal fluids segmentation network (RFS-Net) to segment and recognize three MRF lesion manifestations, namely, intraretinal fluid (IRF), subretinal fluid (SRF), and pigment epithelial detachment (PED), from multi-vendor optical coherence tomography (OCT) imagery. The proposed image analysis tool will optimize anti-VEGF therapy and contribute to reducing the inter- and intra-observer variability. Method The proposed RFS-Net architecture integrates the atrous spatial pyramid pooling (ASPP), residual, and inception modules in the encoder path to learn better features and conserve more global information for precise segmentation and characterization of MRF lesions. The RFS-Net model is trained and validated using OCT scans from multiple vendors (Topcon, Cirrus, Spectralis), collected from three publicly available datasets. The first dataset consisted of OCT volumes obtained from 112 subjects (a total of 11,334 B-scans) is used for both training and evaluation purposes. Moreover, the remaining two datasets are only used for evaluation purposes to check the trained RFS-Net's generalizability on unseen OCT scans. The two evaluation datasets contain a total of 1572 OCT B-scans from 1255 subjects. The performance of the proposed RFS-Net model is assessed through various evaluation metrics. Results The proposed RFS-Net model achieved the mean F1 scores of 0.762, 0.796, and 0.805 for segmenting IRF, SRF, and PED. Moreover, with the automated segmentation of the three retinal manifestations, the RFS-Net brings a considerable gain in efficiency compared to the tedious and demanding manual segmentation procedure of the MRF. Conclusions Our proposed RFS-Net is a potential diagnostic tool for the automatic segmentation of MRF (IRF, SRF, and PED) lesions. It is expected to strengthen the inter-observer agreement, and standardization of dosimetry is envisaged as a result.},
  langid = {english},
  keywords = {Deep learning,Lesions detection,Medical image analysis,Optical coherence tomography (OCT),Radiomics,Retinal fluids segmentation}
}

@article{hassanDeepLearningBased2021a,
  title = {Deep Learning Based Joint Segmentation and Characterization of Multi-Class Retinal Fluid Lesions on {{OCT}} Scans for Clinical Use in Anti-{{VEGF}} Therapy},
  author = {Hassan, Bilal and Qin, Shiyin and Ahmed, Ramsha and Hassan, Taimur and Taguri, Abdel Hakeem and Hashmi, Shahrukh and Werghi, Naoufel},
  year = {2021},
  month = sep,
  journal = {Computers in Biology and Medicine},
  volume = {136},
  pages = {104727},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104727},
  urldate = {2022-07-08},
  abstract = {Background In anti-vascular endothelial growth factor (anti-VEGF) therapy, an accurate estimation of multi-class retinal fluid (MRF) is required for the activity prescription and intravitreal dose. This study proposes an end-to-end deep learning-based retinal fluids segmentation network (RFS-Net) to segment and recognize three MRF lesion manifestations, namely, intraretinal fluid (IRF), subretinal fluid (SRF), and pigment epithelial detachment (PED), from multi-vendor optical coherence tomography (OCT) imagery. The proposed image analysis tool will optimize anti-VEGF therapy and contribute to reducing the inter- and intra-observer variability. Method The proposed RFS-Net architecture integrates the atrous spatial pyramid pooling (ASPP), residual, and inception modules in the encoder path to learn better features and conserve more global information for precise segmentation and characterization of MRF lesions. The RFS-Net model is trained and validated using OCT scans from multiple vendors (Topcon, Cirrus, Spectralis), collected from three publicly available datasets. The first dataset consisted of OCT volumes obtained from 112 subjects (a total of 11,334 B-scans) is used for both training and evaluation purposes. Moreover, the remaining two datasets are only used for evaluation purposes to check the trained RFS-Net's generalizability on unseen OCT scans. The two evaluation datasets contain a total of 1572 OCT B-scans from 1255 subjects. The performance of the proposed RFS-Net model is assessed through various evaluation metrics. Results The proposed RFS-Net model achieved the mean F1 scores of 0.762, 0.796, and 0.805 for segmenting IRF, SRF, and PED. Moreover, with the automated segmentation of the three retinal manifestations, the RFS-Net brings a considerable gain in efficiency compared to the tedious and demanding manual segmentation procedure of the MRF. Conclusions Our proposed RFS-Net is a potential diagnostic tool for the automatic segmentation of MRF (IRF, SRF, and PED) lesions. It is expected to strengthen the inter-observer agreement, and standardization of dosimetry is envisaged as a result.},
  langid = {english},
  keywords = {Deep learning,Lesions detection,Medical image analysis,Optical coherence tomography (OCT),Radiomics,Retinal fluids segmentation}
}

@article{hassanDeepLearningBased2021b,
  title = {Deep Learning Based Joint Segmentation and Characterization of Multi-Class Retinal Fluid Lesions on {{OCT}} Scans for Clinical Use in Anti-{{VEGF}} Therapy},
  author = {Hassan, Bilal and Qin, Shiyin and Ahmed, Ramsha and Hassan, Taimur and Taguri, Abdel Hakeem and Hashmi, Shahrukh and Werghi, Naoufel},
  year = {2021},
  month = sep,
  journal = {Computers in Biology and Medicine},
  volume = {136},
  pages = {104727},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104727},
  urldate = {2022-07-08},
  abstract = {Background In anti-vascular endothelial growth factor (anti-VEGF) therapy, an accurate estimation of multi-class retinal fluid (MRF) is required for the activity prescription and intravitreal dose. This study proposes an end-to-end deep learning-based retinal fluids segmentation network (RFS-Net) to segment and recognize three MRF lesion manifestations, namely, intraretinal fluid (IRF), subretinal fluid (SRF), and pigment epithelial detachment (PED), from multi-vendor optical coherence tomography (OCT) imagery. The proposed image analysis tool will optimize anti-VEGF therapy and contribute to reducing the inter- and intra-observer variability. Method The proposed RFS-Net architecture integrates the atrous spatial pyramid pooling (ASPP), residual, and inception modules in the encoder path to learn better features and conserve more global information for precise segmentation and characterization of MRF lesions. The RFS-Net model is trained and validated using OCT scans from multiple vendors (Topcon, Cirrus, Spectralis), collected from three publicly available datasets. The first dataset consisted of OCT volumes obtained from 112 subjects (a total of 11,334~B-scans) is used for both training and evaluation purposes. Moreover, the remaining two datasets are only used for evaluation purposes to check the trained RFS-Net's generalizability on unseen OCT scans. The two evaluation datasets contain a total of 1572 OCT B-scans from 1255 subjects. The performance of the proposed RFS-Net model is assessed through various evaluation metrics. Results The proposed RFS-Net model achieved the mean F1 scores of 0.762, 0.796, and 0.805 for segmenting IRF, SRF, and PED. Moreover, with the automated segmentation of the three retinal manifestations, the RFS-Net brings a considerable gain in efficiency compared to the tedious and demanding manual segmentation procedure of the MRF. Conclusions Our proposed RFS-Net is a potential diagnostic tool for the automatic segmentation of MRF (IRF, SRF, and PED) lesions. It is expected to strengthen the inter-observer agreement, and standardization of dosimetry is envisaged as a result.},
  langid = {english},
  keywords = {Deep learning,Lesions detection,Medical image analysis,Optical coherence tomography (OCT),Radiomics,Retinal fluids segmentation}
}

@article{hassanDeepLearningBased2021c,
  title = {Deep Learning Based Joint Segmentation and Characterization of Multi-Class Retinal Fluid Lesions on {{OCT}} Scans for Clinical Use in Anti-{{VEGF}} Therapy},
  author = {Hassan, Bilal and Qin, Shiyin and Ahmed, Ramsha and Hassan, Taimur and Taguri, Abdel Hakeem and Hashmi, Shahrukh and Werghi, Naoufel},
  year = {2021},
  month = sep,
  journal = {Computers in Biology and Medicine},
  volume = {136},
  pages = {104727},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104727},
  urldate = {2022-07-08},
  abstract = {Background In anti-vascular endothelial growth factor (anti-VEGF) therapy, an accurate estimation of multi-class retinal fluid (MRF) is required for the activity prescription and intravitreal dose. This study proposes an end-to-end deep learning-based retinal fluids segmentation network (RFS-Net) to segment and recognize three MRF lesion manifestations, namely, intraretinal fluid (IRF), subretinal fluid (SRF), and pigment epithelial detachment (PED), from multi-vendor optical coherence tomography (OCT) imagery. The proposed image analysis tool will optimize anti-VEGF therapy and contribute to reducing the inter- and intra-observer variability. Method The proposed RFS-Net architecture integrates the atrous spatial pyramid pooling (ASPP), residual, and inception modules in the encoder path to learn better features and conserve more global information for precise segmentation and characterization of MRF lesions. The RFS-Net model is trained and validated using OCT scans from multiple vendors (Topcon, Cirrus, Spectralis), collected from three publicly available datasets. The first dataset consisted of OCT volumes obtained from 112 subjects (a total of 11,334~B-scans) is used for both training and evaluation purposes. Moreover, the remaining two datasets are only used for evaluation purposes to check the trained RFS-Net's generalizability on unseen OCT scans. The two evaluation datasets contain a total of 1572 OCT B-scans from 1255 subjects. The performance of the proposed RFS-Net model is assessed through various evaluation metrics. Results The proposed RFS-Net model achieved the mean F1 scores of 0.762, 0.796, and 0.805 for segmenting IRF, SRF, and PED. Moreover, with the automated segmentation of the three retinal manifestations, the RFS-Net brings a considerable gain in efficiency compared to the tedious and demanding manual segmentation procedure of the MRF. Conclusions Our proposed RFS-Net is a potential diagnostic tool for the automatic segmentation of MRF (IRF, SRF, and PED) lesions. It is expected to strengthen the inter-observer agreement, and standardization of dosimetry is envisaged as a result.},
  langid = {english},
  keywords = {Deep learning,Lesions detection,Medical image analysis,Optical coherence tomography (OCT),Radiomics,Retinal fluids segmentation},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HQSFW5UJ\\S0010482521005217.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\GUCUSY2Z\\S0010482521005217.html}
}

@article{hassanIncrementalCrossDomainAdaptation2021,
  title = {Incremental {{Cross-Domain Adaptation}} for {{Robust Retinopathy Screening}} via {{Bayesian Deep Learning}}},
  author = {Hassan, Taimur and Hassan, Bilal and Akram, Muhammad Usman and Hashmi, Shahrukh and Taguri, Abdel Hakim and Werghi, Naoufel},
  year = {2021},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {70},
  pages = {1--14},
  issn = {1557-9662},
  doi = {10.1109/TIM.2021.3122172},
  abstract = {Retinopathy represents a group of retinal diseases that, if not treated timely, can cause severe visual impairments or even blindness. Many researchers have developed autonomous systems to recognize retinopathy via fundus and optical coherence tomography (OCT) imagery. However, most of these frameworks employ conventional transfer learning and fine-tuning approaches, requiring a decent amount of well-annotated training data to produce accurate diagnostic performance. This article presents a novel incremental cross-domain adaptation instrument that allows any deep classification model to progressively learn abnormal retinal pathologies in OCT and fundus imagery via few-shot training. Furthermore, unlike its competitors, the proposed instrument is driven via a Bayesian multiobjective function that not only enforces the candidate classification network to retain its prior learned knowledge during incremental training, but also ensures that the network understands the structural and semantic relationships between previously learned pathologies and newly added disease categories to effectively recognize them at the inference stage. The proposed framework, evaluated on six public datasets acquired with three different scanners to screen 13 retinal pathologies, outperforms the state-of-the-art competitors by achieving an overall accuracy and F1 score of 0.9826 and 0.9846, respectively.},
  keywords = {Adaptation models,Bayesian deep learning,Deep learning,fundus photography,Image recognition,incremental domain adaptation (DA),optical coherence tomography,Pathology,Retina,retinopathy,Retinopathy,Training}
}

@article{hassanIncrementalCrossDomainAdaptation2021a,
  title = {Incremental {{Cross-Domain Adaptation}} for {{Robust Retinopathy Screening}} via {{Bayesian Deep Learning}}},
  author = {Hassan, Taimur and Hassan, Bilal and Akram, Muhammad Usman and Hashmi, Shahrukh and Taguri, Abdel Hakim and Werghi, Naoufel},
  year = {2021},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {70},
  pages = {1--14},
  issn = {1557-9662},
  doi = {10.1109/TIM.2021.3122172},
  abstract = {Retinopathy represents a group of retinal diseases that, if not treated timely, can cause severe visual impairments or even blindness. Many researchers have developed autonomous systems to recognize retinopathy via fundus and optical coherence tomography (OCT) imagery. However, most of these frameworks employ conventional transfer learning and fine-tuning approaches, requiring a decent amount of well-annotated training data to produce accurate diagnostic performance. This article presents a novel incremental cross-domain adaptation instrument that allows any deep classification model to progressively learn abnormal retinal pathologies in OCT and fundus imagery via few-shot training. Furthermore, unlike its competitors, the proposed instrument is driven via a Bayesian multiobjective function that not only enforces the candidate classification network to retain its prior learned knowledge during incremental training, but also ensures that the network understands the structural and semantic relationships between previously learned pathologies and newly added disease categories to effectively recognize them at the inference stage. The proposed framework, evaluated on six public datasets acquired with three different scanners to screen 13 retinal pathologies, outperforms the state-of-the-art competitors by achieving an overall accuracy and F1 score of 0.9826 and 0.9846, respectively.},
  keywords = {Adaptation models,Bayesian deep learning,Deep learning,fundus photography,Image recognition,incremental domain adaptation (DA),optical coherence tomography,Pathology,Retina,retinopathy,Retinopathy,Training}
}

@article{hassanRAGFWHybridConvolutional2021,
  title = {{{RAG-FW}}: {{A Hybrid Convolutional Framework}} for the {{Automated Extraction}} of {{Retinal Lesions}} and {{Lesion-Influenced Grading}} of {{Human Retinal Pathology}}},
  shorttitle = {{{RAG-FW}}},
  author = {Hassan, Taimur and Akram, Muhammad Usman and Werghi, Naoufel and Nazir, Muhammad Noman},
  year = {2021},
  month = jan,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {25},
  number = {1},
  pages = {108--120},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2020.2982914},
  abstract = {The identification of retinal lesions plays a vital role in accurately classifying and grading retinopathy. Many researchers have presented studies on optical coherence tomography (OCT) based retinal image analysis over the past. However, to the best of our knowledge, there is no framework yet available that can extract retinal lesions from multi-vendor OCT scans and utilize them for the intuitive severity grading of the human retina. To cater this lack, we propose a deep retinal analysis and grading framework (RAG-FW). RAG-FW is a hybrid convolutional framework that extracts multiple retinal lesions from OCT scans and utilizes them for lesion-influenced grading of retinopathy as per the clinical standards. RAG-FW has been rigorously tested on 43,613 scans from five highly complex publicly available datasets, containing multi-vendor scans, where it achieved the mean intersection-over-union score of 0.8055 for extracting the retinal lesions and the accuracy of 98.70\% for the correct severity grading of retinopathy.},
  keywords = {deep learning,Feature extraction,Informatics,Lesions,Ophthalmo-logy,optical coherence tomography (OCT),Pathology,Retina,Retinopathy,Tensors}
}

@article{hassanRAGFWHybridConvolutional2021a,
  title = {{{RAG-FW}}: {{A Hybrid Convolutional Framework}} for the {{Automated Extraction}} of {{Retinal Lesions}} and {{Lesion-Influenced Grading}} of {{Human Retinal Pathology}}},
  shorttitle = {{{RAG-FW}}},
  author = {Hassan, Taimur and Akram, Muhammad Usman and Werghi, Naoufel and Nazir, Muhammad Noman},
  year = {2021},
  month = jan,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {25},
  number = {1},
  pages = {108--120},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2020.2982914},
  abstract = {The identification of retinal lesions plays a vital role in accurately classifying and grading retinopathy. Many researchers have presented studies on optical coherence tomography (OCT) based retinal image analysis over the past. However, to the best of our knowledge, there is no framework yet available that can extract retinal lesions from multi-vendor OCT scans and utilize them for the intuitive severity grading of the human retina. To cater this lack, we propose a deep retinal analysis and grading framework (RAG-FW). RAG-FW is a hybrid convolutional framework that extracts multiple retinal lesions from OCT scans and utilizes them for lesion-influenced grading of retinopathy as per the clinical standards. RAG-FW has been rigorously tested on 43,613 scans from five highly complex publicly available datasets, containing multi-vendor scans, where it achieved the mean intersection-over-union score of 0.8055 for extracting the retinal lesions and the accuracy of 98.70\% for the correct severity grading of retinopathy.},
  keywords = {deep learning,Feature extraction,Informatics,Lesions,Ophthalmo-logy,optical coherence tomography (OCT),Pathology,Retina,Retinopathy,Tensors}
}

@article{haugerInterferometerOpticalCoherence2003,
  title = {Interferometer for Optical Coherence Tomography},
  author = {Hauger, Christoph and W{\"o}rz, Marco and Hellmuth, Thomas},
  year = {2003},
  month = jul,
  journal = {Applied Optics},
  volume = {42},
  number = {19},
  pages = {3896--3902},
  issn = {2155-3165},
  doi = {10.1364/AO.42.003896},
  urldate = {2019-11-14},
  abstract = {We describe a new interferometer setup for optical coherence tomography (OCT). The interferometer is based on a fiber arrangement similar to Young's two-pinhole interference experiment with spatial coherent and temporal incoherent light. Depth gating is achieved detection of the interference signal on a linear CCD array. Therefore no reference optical delay scanning is needed. The interference signal, the modulation of the signal, the axial resolution, and the depth range are derived theoretically and compared with experiments. The dynamic range of the setup is compared with OCT sensors in the time domain. To our knowledge, the first images of porcine brain and heart tissue and human skin are presented.},
  copyright = {\&\#169; 2003 Optical Society of America},
  langid = {english},
  keywords = {Charge coupled devices,Detector arrays,High speed imaging,Optical coherence tomography,Sensors,Tunable lasers}
}

@article{haugerInterferometerOpticalCoherence2003a,
  title = {Interferometer for Optical Coherence Tomography},
  author = {Hauger, Christoph and W{\"o}rz, Marco and Hellmuth, Thomas},
  year = {2003},
  month = jul,
  journal = {Applied Optics},
  volume = {42},
  number = {19},
  pages = {3896--3902},
  issn = {2155-3165},
  doi = {10.1364/AO.42.003896},
  urldate = {2019-11-14},
  abstract = {We describe a new interferometer setup for optical coherence tomography (OCT). The interferometer is based on a fiber arrangement similar to Young's two-pinhole interference experiment with spatial coherent and temporal incoherent light. Depth gating is achieved detection of the interference signal on a linear CCD array. Therefore no reference optical delay scanning is needed. The interference signal, the modulation of the signal, the axial resolution, and the depth range are derived theoretically and compared with experiments. The dynamic range of the setup is compared with OCT sensors in the time domain. To our knowledge, the first images of porcine brain and heart tissue and human skin are presented.},
  copyright = {\&\#169; 2003 Optical Society of America},
  langid = {english},
  keywords = {Charge coupled devices,Detector arrays,High speed imaging,Optical coherence tomography,Sensors,Tunable lasers},
  file = {C:\Users\cleme\Zotero\storage\FVN3ENL9\abstract.html}
}

@article{heConvolutionalNeuralNetworks,
  title = {Convolutional {{Neural Networks}} at {{Constrained Time Cost}}},
  author = {He, Kaiming and Sun, Jian},
  abstract = {Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and timeconsuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8\% top-5 error, 10-view test), yet is 20\% faster than ``AlexNet'' [14] (16.0\% top-5 error, 10-view test).},
  langid = {english}
}

@inproceedings{heConvolutionalNeuralNetworks2015,
  title = {Convolutional Neural Networks at Constrained Time Cost},
  author = {He, Kaiming and Sun, Jian},
  year = {2015},
  month = jun,
  pages = {5353--5360},
  doi = {10.1109/CVPR.2015.7299173}
}

@inproceedings{heConvolutionalNeuralNetworks2015a,
  title = {Convolutional Neural Networks at Constrained Time Cost},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Sun, Jian},
  year = {2015},
  month = jun,
  pages = {5353--5360},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2015.7299173},
  abstract = {Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8\% top-5 error, 10-view test), yet is 20\% faster than ``AlexNet'' [14] (16.0\% top-5 error, 10-view test).},
  keywords = {Accuracy,Computer architecture,Testing,Time complexity,Time factors,Training}
}

@inproceedings{heConvolutionalNeuralNetworks2015b,
  title = {Convolutional Neural Networks at Constrained Time Cost},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Sun, Jian},
  year = {2015},
  month = jun,
  pages = {5353--5360},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2015.7299173},
  abstract = {Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8\% top-5 error, 10-view test), yet is 20\% faster than ``AlexNet'' [14] (16.0\% top-5 error, 10-view test).},
  keywords = {Accuracy,Computer architecture,Testing,Time complexity,Time factors,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\DW8HUC3U\\He et Sun - 2015 - Convolutional neural networks at constrained time .pdf;C\:\\Users\\cleme\\Zotero\\storage\\GSF8R932\\7299173.html}
}

@inproceedings{heConvolutionalNeuralNetworks2015c,
  title = {Convolutional Neural Networks at Constrained Time Cost},
  author = {He, Kaiming and Sun, Jian},
  year = {2015},
  month = jun,
  pages = {5353--5360},
  doi = {10.1109/CVPR.2015.7299173},
  file = {C:\Users\cleme\Zotero\storage\UGUBKGWA\He et Sun - 2015 - Convolutional neural networks at constrained time .pdf}
}

@article{heConvolutionalNeuralNetworksa,
  title = {Convolutional {{Neural Networks}} at {{Constrained Time Cost}}},
  author = {He, Kaiming and Sun, Jian},
  abstract = {Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and timeconsuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8\% top-5 error, 10-view test), yet is 20\% faster than ``AlexNet'' [14] (16.0\% top-5 error, 10-view test).},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\EK63BSMA\He et Sun - Convolutional Neural Networks at Constrained Time .pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {CIFAR-10,COCO object detection dataset,COCO segmentation,Complexity theory,deep residual learning,deep residual nets,deeper neural network training,Degradation,ILSVRC \& COCO 2015 competitions,ILSVRC 2015 classification task,image classification,image recognition,Image recognition,Image segmentation,ImageNet dataset,ImageNet localization,ImageNet test set,learning (artificial intelligence),neural nets,Neural networks,object detection,residual function learning,residual nets,Training,VGG nets,visual recognition tasks,Visualization}
}

@inproceedings{heDeepResidualLearning2016a,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization}
}

@inproceedings{heDeepResidualLearning2016b,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2023-02-21},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  isbn = {978-1-4673-8851-1},
  langid = {english}
}

@inproceedings{heDeepResidualLearning2016c,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {CIFAR-10,COCO object detection dataset,COCO segmentation,Complexity theory,deep residual learning,deep residual nets,deeper neural network training,Degradation,ILSVRC & COCO 2015 competitions,ILSVRC 2015 classification task,image classification,image recognition,Image recognition,Image segmentation,ImageNet dataset,ImageNet localization,ImageNet test set,learning (artificial intelligence),neural nets,Neural networks,object detection,residual function learning,residual nets,Training,VGG nets,visual recognition tasks,Visualization},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\Q6TGTKRD\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\cleme\\Zotero\\storage\\X99IJDA7\\7780459.html}
}

@inproceedings{heDeepResidualLearning2016d,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2023-02-21},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\DJKNU5SS\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{heDeepResidualLearning2016e,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\LN7LV5UV\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RXVA325R\\he2016.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\46J9XWY9\\7780459.html}
}

@inproceedings{heDelvingDeepRectifiers2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  pages = {1026--1034},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2015.123},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
  keywords = {Adaptation models,Biological neural networks,Computational modeling,Gaussian distribution,Testing,Training}
}

@inproceedings{heDelvingDeepRectifiers2015a,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  pages = {1026--1034},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2015.123},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
  keywords = {Adaptation models,Biological neural networks,Computational modeling,Gaussian distribution,Testing,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\8IYXL3YU\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;C\:\\Users\\cleme\\Zotero\\storage\\TGW7E88P\\7410480.html}
}

@article{hendrycks2016bridging,
  title = {Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2016}
}

@article{hendrycks2016bridging,
  title = {Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2016}
}

@article{hendrycksGaussianErrorLinear2020,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2020},
  month = jul,
  journal = {arXiv:1606.08415 [cs]},
  eprint = {1606.08415},
  primaryclass = {cs},
  urldate = {2021-11-29},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is x{$\Phi$}(x), where {$\Phi$}(x) the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (x1x{$>$}0). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@article{hendrycksGaussianErrorLinear2020a,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2020},
  month = jul,
  journal = {arXiv:1606.08415 [cs]},
  eprint = {1606.08415},
  primaryclass = {cs},
  urldate = {2021-11-29},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{$>$}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{hendrycksGaussianErrorLinear2020b,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2020},
  month = jul,
  journal = {arXiv:1606.08415 [cs]},
  eprint = {1606.08415},
  primaryclass = {cs},
  urldate = {2021-11-29},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{$>$}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\2SB2FU6V\\Hendrycks et Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf;C\:\\Users\\cleme\\Zotero\\storage\\4G23NTIT\\1606.html}
}

@article{hendrycksGaussianErrorLinear2020c,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2020},
  month = jul,
  journal = {arXiv:1606.08415 [cs]},
  eprint = {1606.08415},
  primaryclass = {cs},
  urldate = {2021-11-29},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is x{$\Phi$}(x), where {$\Phi$}(x) the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (x1x{$>$}0). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\LTG2CADN\Hendrycks et Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf}
}

@article{heProgressiveMultiscaleConsistent2022,
  title = {Progressive {{Multiscale Consistent Network}} for {{Multiclass Fundus Lesion Segmentation}}},
  author = {He, Along and Wang, Kai and Li, Tao and Bo, Wang and Kang, Hong and Fu, Huazhu},
  year = {2022},
  month = nov,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {41},
  number = {11},
  pages = {3146--3157},
  issn = {1558-254X},
  doi = {10.1109/TMI.2022.3177803},
  abstract = {Effectively integrating multi-scale information is of considerable significance for the challenging multi-class segmentation of fundus lesions because different lesions vary significantly in scales and shapes. Several methods have been proposed to successfully handle the multi-scale object segmentation. However, two issues are not considered in previous studies. The first is the lack of interaction between adjacent feature levels, and this will lead to the deviation of high-level features from low-level features and the loss of detailed cues. The second is the conflict between the low-level and high-level features, this occurs because they learn different scales of features, thereby confusing the model and decreasing the accuracy of the final prediction. In this paper, we propose a progressive multi-scale consistent network (PMCNet) that integrates the proposed progressive feature fusion (PFF) block and dynamic attention block (DAB) to address the aforementioned issues. Specifically, PFF block progressively integrates multi-scale features from adjacent encoding layers, facilitating feature learning of each layer by aggregating fine-grained details and high-level semantics. As features at different scales should be consistent, DAB is designed to dynamically learn the attentive cues from the fused features at different scales, thus aiming to smooth the essential conflicts existing in multi-scale features. The two proposed PFF and DAB blocks can be integrated with the off-the-shelf backbone networks to address the two issues of multi-scale and feature inconsistency in the multi-class segmentation of fundus lesions, which will produce better feature representation in the feature space. Experimental results on three public datasets indicate that the proposed method is more effective than recent state-of-the-art methods.},
  keywords = {Biomedical optical imaging,consistent multi-scale,dynamic attention block,Feature extraction,Image segmentation,Lesions,Multi-class segmentation,multi-scale fundus lesions,Optical imaging,progressive feature fusion,Semantics,Task analysis}
}

@article{heProgressiveMultiscaleConsistent2022a,
  title = {Progressive {{Multiscale Consistent Network}} for {{Multiclass Fundus Lesion Segmentation}}},
  author = {He, Along and Wang, Kai and Li, Tao and Bo, Wang and Kang, Hong and Fu, Huazhu},
  year = {2022},
  month = nov,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {41},
  number = {11},
  pages = {3146--3157},
  issn = {1558-254X},
  doi = {10.1109/TMI.2022.3177803},
  abstract = {Effectively integrating multi-scale information is of considerable significance for the challenging multi-class segmentation of fundus lesions because different lesions vary significantly in scales and shapes. Several methods have been proposed to successfully handle the multi-scale object segmentation. However, two issues are not considered in previous studies. The first is the lack of interaction between adjacent feature levels, and this will lead to the deviation of high-level features from low-level features and the loss of detailed cues. The second is the conflict between the low-level and high-level features, this occurs because they learn different scales of features, thereby confusing the model and decreasing the accuracy of the final prediction. In this paper, we propose a progressive multi-scale consistent network (PMCNet) that integrates the proposed progressive feature fusion (PFF) block and dynamic attention block (DAB) to address the aforementioned issues. Specifically, PFF block progressively integrates multi-scale features from adjacent encoding layers, facilitating feature learning of each layer by aggregating fine-grained details and high-level semantics. As features at different scales should be consistent, DAB is designed to dynamically learn the attentive cues from the fused features at different scales, thus aiming to smooth the essential conflicts existing in multi-scale features. The two proposed PFF and DAB blocks can be integrated with the off-the-shelf backbone networks to address the two issues of multi-scale and feature inconsistency in the multi-class segmentation of fundus lesions, which will produce better feature representation in the feature space. Experimental results on three public datasets indicate that the proposed method is more effective than recent state-of-the-art methods.},
  keywords = {Biomedical optical imaging,consistent multi-scale,dynamic attention block,Feature extraction,Image segmentation,Lesions,Multi-class segmentation,multi-scale fundus lesions,Optical imaging,progressive feature fusion,Semantics,Task analysis},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5VMCUXFA\\He et al. - 2022 - Progressive Multiscale Consistent Network for Mult.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZGHLDQ2E\\9781413.html}
}

@article{heRetinalOpticalCoherence2020,
  title = {Retinal Optical Coherence Tomography Image Classification with Label Smoothing Generative Adversarial Network},
  author = {He, Xingxin and Fang, Leyuan and Rabbani, Hossein and Chen, Xiangdong and Liu, Zhimin},
  year = {2020},
  month = sep,
  journal = {Neurocomputing},
  volume = {405},
  pages = {37--47},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.04.044},
  urldate = {2022-07-10},
  abstract = {In this paper, we propose a label smoothing generative adversarial network (LSGAN) for optical coherence tomography (OCT) image classification to identify drusen, i.e., the early stage of age-related macular degeneration (AMD), choroidal neovascularization (CNV), diabetic macular edema (DME) and normal OCT images. The LSGAN can expand the dataset to address the issue of overfitting when only limited OCT training samples are available. Specifically, our LSGAN consists of three components: generator, discriminator, and classifier. The generator generates synthetic unlabeled images that are similar to the real OCT images, while the discriminator distinguishes whether the synthetic images are real or generated. To train the classifier with both real labeled images and synthetic unlabeled images, we design artificial pseudo labels as label smoothing for the synthetic unlabeled images. Thus, the mixing of the synthetic images and real images can be used as training data to improve the classification performance. Experimental results on two real OCT datasets demonstrate the superiority of our LSGAN method over several well-known classifiers, especially under the condition of limited training data.},
  langid = {english},
  keywords = {Classification,Convolutional neural network (CNN),Generative Adversarial Network (GAN),Optical coherence tomography (OCT),Retinal}
}

@article{heRetinalOpticalCoherence2020a,
  title = {Retinal Optical Coherence Tomography Image Classification with Label Smoothing Generative Adversarial Network},
  author = {He, Xingxin and Fang, Leyuan and Rabbani, Hossein and Chen, Xiangdong and Liu, Zhimin},
  year = {2020},
  month = sep,
  journal = {Neurocomputing},
  volume = {405},
  pages = {37--47},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.04.044},
  urldate = {2022-07-10},
  abstract = {In this paper, we propose a label smoothing generative adversarial network (LSGAN) for optical coherence tomography (OCT) image classification to identify drusen, i.e., the early stage of age-related macular degeneration (AMD), choroidal neovascularization (CNV), diabetic macular edema (DME) and normal OCT images. The LSGAN can expand the dataset to address the issue of overfitting when only limited OCT training samples are available. Specifically, our LSGAN consists of three components: generator, discriminator, and classifier. The generator generates synthetic unlabeled images that are similar to the real OCT images, while the discriminator distinguishes whether the synthetic images are real or generated. To train the classifier with both real labeled images and synthetic unlabeled images, we design artificial pseudo labels as label smoothing for the synthetic unlabeled images. Thus, the mixing of the synthetic images and real images can be used as training data to improve the classification performance. Experimental results on two real OCT datasets demonstrate the superiority of our LSGAN method over several well-known classifiers, especially under the condition of limited training data.},
  langid = {english},
  keywords = {Classification,Convolutional neural network (CNN),Generative Adversarial Network (GAN),Optical coherence tomography (OCT),Retinal}
}

@article{heuilletExplainabilityDeepReinforcement2021,
  title = {Explainability in Deep Reinforcement Learning},
  author = {Heuillet, Alexandre and Couthouis, Fabien and {D{\'i}az-Rodr{\'i}guez}, Natalia},
  year = {2021},
  month = feb,
  journal = {Knowledge-Based Systems},
  volume = {214},
  pages = {106685},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2020.106685},
  urldate = {2023-05-04},
  abstract = {A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.},
  langid = {english},
  keywords = {Deep Learning,Explainable artificial intelligence,Machine Learning,Reinforcement Learning,Representation learning,Responsible artificial intelligence}
}

@article{heuilletExplainabilityDeepReinforcement2021a,
  title = {Explainability in Deep Reinforcement Learning},
  author = {Heuillet, Alexandre and Couthouis, Fabien and {D{\'i}az-Rodr{\'i}guez}, Natalia},
  year = {2021},
  month = feb,
  journal = {Knowledge-Based Systems},
  volume = {214},
  pages = {106685},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2020.106685},
  urldate = {2023-05-04},
  abstract = {A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.},
  langid = {english},
  keywords = {Deep Learning,Explainable artificial intelligence,Machine Learning,Reinforcement Learning,Representation learning,Responsible artificial intelligence},
  file = {C:\Users\cleme\Zotero\storage\LABTJZTL\Heuillet et al. - 2021 - Explainability in deep reinforcement learning.pdf}
}

@misc{HighwayNetworksFirst,
  title = {Highway {{Networks}}, the First Working Really Deep Feedforward Neural Networks with over 100 Layers},
  urldate = {2023-02-22}
}

@misc{HighwayNetworksFirsta,
  title = {Highway {{Networks}}, the First Working Really Deep Feedforward Neural Networks with over 100 Layers},
  urldate = {2023-02-22},
  howpublished = {https://people.idsia.ch/{\textasciitilde}juergen/highway-networks.html},
  file = {C:\Users\cleme\Zotero\storage\64K2TDRC\highway-networks.html}
}

@article{hintonImprovingNeuralNetworks2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2012},
  journal = {CoRR},
  volume = {abs/1207.0580}
}

@article{hintonImprovingNeuralNetworks2012a,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2012},
  journal = {CoRR},
  volume = {abs/1207.0580}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2019-06-26},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@article{hochreiterLongShortTermMemory1997a,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2019-06-26},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {C:\Users\cleme\Zotero\storage\2IDQMJZV\neco.1997.9.8.html}
}

@inproceedings{hoffmanCyCADACycleConsistentAdversarial2018,
  title = {{{CyCADA}}: {{Cycle-Consistent Adversarial Domain Adaptation}}},
  shorttitle = {{{CyCADA}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
  year = {2018},
  month = jul,
  pages = {1989--1998},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-06-23},
  abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.},
  langid = {english}
}

@inproceedings{hoffmanCyCADACycleConsistentAdversarial2018a,
  title = {{{CyCADA}}: {{Cycle-Consistent Adversarial Domain Adaptation}}},
  shorttitle = {{{CyCADA}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
  year = {2018},
  month = jul,
  pages = {1989--1998},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-06-23},
  abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5MFXVXY6\\Hoffman et al. - 2018 - CyCADA Cycle-Consistent Adversarial Domain Adapta.pdf;C\:\\Users\\cleme\\Zotero\\storage\\AFGHH3FV\\Hoffman et al. - 2018 - CyCADA Cycle-Consistent Adversarial Domain Adapta.pdf}
}

@article{hogartyCurrentStateFuture2019,
  title = {Current State and Future Prospects of Artificial Intelligence in Ophthalmology: A Review},
  shorttitle = {Current State and Future Prospects of Artificial Intelligence in Ophthalmology},
  author = {Hogarty, Daniel T and Mackey, David A and Hewitt, Alex W},
  year = {2019},
  journal = {Clinical \& Experimental Ophthalmology},
  volume = {47},
  number = {1},
  pages = {128--139},
  issn = {1442-9071},
  doi = {10.1111/ceo.13381},
  urldate = {2022-07-10},
  abstract = {Artificial intelligence (AI) has emerged as a major frontier in computer science research. Although AI has broad application across many medical fields, it will have particular utility in ophthalmology and will dramatically change the diagnostic and treatment pathways for many eye conditions such as corneal ectasias, glaucoma, age-related macular degeneration and diabetic retinopathy. However, given that AI has primarily been driven as a computer science, its concepts and terminology are unfamiliar to many medical professionals. Important key terms such as machine learning and deep learning are often misunderstood and incorrectly used interchangeably. This article presents an overview of AI and new developments relevant to ophthalmology.},
  langid = {english},
  keywords = {artificial intelligence,deep learning,diabetic retinopathy,machine learning,ophthalmology}
}

@article{hogartyCurrentStateFuture2019a,
  title = {Current State and Future Prospects of Artificial Intelligence in Ophthalmology: A Review},
  shorttitle = {Current State and Future Prospects of Artificial Intelligence in Ophthalmology},
  author = {Hogarty, Daniel T and Mackey, David A and Hewitt, Alex W},
  year = {2019},
  journal = {Clinical \& Experimental Ophthalmology},
  volume = {47},
  number = {1},
  pages = {128--139},
  issn = {1442-9071},
  doi = {10.1111/ceo.13381},
  urldate = {2022-07-10},
  abstract = {Artificial intelligence (AI) has emerged as a major frontier in computer science research. Although AI has broad application across many medical fields, it will have particular utility in ophthalmology and will dramatically change the diagnostic and treatment pathways for many eye conditions such as corneal ectasias, glaucoma, age-related macular degeneration and diabetic retinopathy. However, given that AI has primarily been driven as a computer science, its concepts and terminology are unfamiliar to many medical professionals. Important key terms such as machine learning and deep learning are often misunderstood and incorrectly used interchangeably. This article presents an overview of AI and new developments relevant to ophthalmology.},
  langid = {english},
  keywords = {artificial intelligence,deep learning,diabetic retinopathy,machine learning,ophthalmology},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\I36FMXP8\\hogarty2018.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\LB55CP6P\\Hogarty et al. - 2019 - Current state and future prospects of artificial i.pdf;C\:\\Users\\cleme\\Zotero\\storage\\Y3WAF2GG\\ceo.html}
}

@article{holzingerCausabilityExplainabilityArtificial2019,
  title = {Causability and Explainability of Artificial Intelligence in Medicine},
  author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and M{\"u}ller, Heimo},
  year = {2019},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {e1312},
  issn = {1942-4795},
  doi = {10.1002/widm.1312},
  urldate = {2023-05-04},
  abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Human Centricity and User Interaction},
  langid = {english},
  keywords = {artificial intelligence,causability,explainability,explainable AI,histopathology,medicine}
}

@article{holzingerCausabilityExplainabilityArtificial2019a,
  title = {Causability and Explainability of Artificial Intelligence in Medicine},
  author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and M{\"u}ller, Heimo},
  year = {2019},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {e1312},
  issn = {1942-4795},
  doi = {10.1002/widm.1312},
  urldate = {2023-05-04},
  abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Human Centricity and User Interaction},
  langid = {english},
  keywords = {artificial intelligence,causability,explainability,explainable AI,histopathology,medicine},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\U5ZW5WBD\\Holzinger et al. - 2019 - Causability and explainability of artificial intel.pdf;C\:\\Users\\cleme\\Zotero\\storage\\HX55RWBX\\widm.html}
}

@misc{Home,
  title = {{Home}},
  urldate = {2021-09-14},
  abstract = {Harnessing deep learning to prevent blindness Learn how Eyenuk can help On-Demand Webinar: How AI Empowers Community Based Eye Screening in Low Resource Areas June 11, 2021 Diabetes is a national public health threat in Armenia, and a lack of resources, public awareness, and specialists has been a barrier to effective diabetic eye screening programs. {\dots}},
  langid = {us-en}
}

@misc{Homea,
  title = {{Home}},
  journal = {Eyenuk, Inc. {\textasciitilde} Artificial Intelligence Eye Screening},
  urldate = {2021-09-14},
  abstract = {Harnessing deep learning to prevent blindness Learn how Eyenuk can help On-Demand Webinar: How AI Empowers Community Based Eye Screening in Low Resource Areas June 11, 2021 Diabetes is a national public health threat in Armenia, and a lack of resources, public awareness, and specialists has been a barrier to effective diabetic eye screening programs. {\dots}},
  howpublished = {https://www.eyenuk.com/us-en/},
  langid = {us-en},
  file = {C:\Users\cleme\Zotero\storage\59W5HMW7\us-en.html}
}

@article{hooverLocatingBloodVessels2000,
  title = {Locating Blood Vessels in Retinal Images by Piecewise Threshold Probing of a Matched Filter Response},
  author = {Hoover, A.D. and Kouznetsova, V. and Goldbaum, M.},
  year = {2000},
  month = mar,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {19},
  number = {3},
  pages = {203--210},
  issn = {1558-254X},
  doi = {10.1109/42.845178},
  abstract = {Describes an automated method to locate and outline blood vessels in images of the ocular fundus. Such a tool should prove useful to eye care specialists for purposes of patient screening, treatment evaluation, and clinical study. The authors' method differs from previously known methods in that it uses local and global vessel features cooperatively to segment the vessel network. The authors evaluate their method using hand-labeled ground truth segmentations of 20 images. A plot of the operating characteristic shows that the authors' method reduces false positives by as much as 15 times over basic thresholding of a matched filter response (MFR), at up to a 75\% true positive rate. For a baseline, they also compared the ground truth against a second hand-labeling, yielding a 90\% true positive and a 4\% false positive detection rate, on average. These numbers suggest there is still room for a 15\% true positive rate improvement, with the same false positive rate, over the authors' method. They are making all their images and hand labelings publicly available for interested researchers to use in evaluating related methods.},
  keywords = {Algorithms,Arteriosclerosis,Biomedical imaging,blood vessels,Blood vessels,blood vessels location,clinical study,Computer-Assisted,Diabetes,eye,eye care specialists,false positive detection rate,global vessel features,Humans,Hypertension,Image Processing,image segmentation,Image segmentation,Labeling,local vessel features,Magnetic Resonance Imaging,matched filter response,matched filters,Matched filters,medical image processing,Medical treatment,operating characteristic,optical images,patient screening,piecewise threshold probing,Reproducibility of Results,Retina,Retinal Diseases,retinal images,Retinal Vessels,treatment evaluation,true positive rate,vessel network segmentation}
}

@article{hooverLocatingBloodVessels2000a,
  title = {Locating Blood Vessels in Retinal Images by Piecewise Threshold Probing of a Matched Filter Response},
  author = {Hoover, A.D. and Kouznetsova, V. and Goldbaum, M.},
  year = {2000},
  month = mar,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {19},
  number = {3},
  pages = {203--210},
  issn = {1558-254X},
  doi = {10.1109/42.845178},
  abstract = {Describes an automated method to locate and outline blood vessels in images of the ocular fundus. Such a tool should prove useful to eye care specialists for purposes of patient screening, treatment evaluation, and clinical study. The authors' method differs from previously known methods in that it uses local and global vessel features cooperatively to segment the vessel network. The authors evaluate their method using hand-labeled ground truth segmentations of 20 images. A plot of the operating characteristic shows that the authors' method reduces false positives by as much as 15 times over basic thresholding of a matched filter response (MFR), at up to a 75\% true positive rate. For a baseline, they also compared the ground truth against a second hand-labeling, yielding a 90\% true positive and a 4\% false positive detection rate, on average. These numbers suggest there is still room for a 15\% true positive rate improvement, with the same false positive rate, over the authors' method. They are making all their images and hand labelings publicly available for interested researchers to use in evaluating related methods.},
  keywords = {Algorithms,Arteriosclerosis,Biomedical imaging,blood vessels,Blood vessels,blood vessels location,clinical study,Diabetes,eye,eye care specialists,false positive detection rate,global vessel features,Humans,Hypertension,Image Processing Computer-Assisted,image segmentation,Image segmentation,Labeling,local vessel features,Magnetic Resonance Imaging,matched filter response,matched filters,Matched filters,medical image processing,Medical treatment,operating characteristic,optical images,patient screening,piecewise threshold probing,Reproducibility of Results,Retina,Retinal Diseases,retinal images,Retinal Vessels,treatment evaluation,true positive rate,vessel network segmentation},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\PY2NHIFM\\Hoover et al. - 2000 - Locating blood vessels in retinal images by piecew.pdf;C\:\\Users\\cleme\\Zotero\\storage\\4DI5KJ6F\\hoover2000.html;C\:\\Users\\cleme\\Zotero\\storage\\65IBV8KZ\\845178.html}
}

@article{hooverLocatingOpticNerve2003,
  title = {Locating the Optic Nerve in a Retinal Image Using the Fuzzy Convergence of the Blood Vessels},
  author = {Hoover, A. and Goldbaum, M.},
  year = {2003},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {22},
  number = {8},
  pages = {951--958},
  issn = {1558-254X},
  doi = {10.1109/TMI.2003.815900},
  abstract = {We describe an automated method to locate the optic nerve in images of the ocular fundus. Our method uses a novel algorithm we call fuzzy convergence to determine the origination of the blood vessel network. We evaluate our method using 31 images of healthy retinas and 50 images of diseased retinas, containing such diverse symptoms as tortuous vessels, choroidal neovascularization, and hemorrhages that completely obscure the actual nerve. On this difficult data set, our method achieved 89\% correct detection. We also compare our method against three simpler methods, demonstrating the performance improvement. All our images and data are freely available for other researchers to use in evaluating related methods.},
  keywords = {Biomedical imaging,Biomedical optical imaging,Blood vessels,Brightness,Convergence,Lesions,Optical distortion,Optical imaging,Retina,Shape}
}

@article{hooverLocatingOpticNerve2003a,
  title = {Locating the Optic Nerve in a Retinal Image Using the Fuzzy Convergence of the Blood Vessels},
  author = {Hoover, A. and Goldbaum, M.},
  year = {2003},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {22},
  number = {8},
  pages = {951--958},
  issn = {1558-254X},
  doi = {10.1109/TMI.2003.815900},
  abstract = {We describe an automated method to locate the optic nerve in images of the ocular fundus. Our method uses a novel algorithm we call fuzzy convergence to determine the origination of the blood vessel network. We evaluate our method using 31 images of healthy retinas and 50 images of diseased retinas, containing such diverse symptoms as tortuous vessels, choroidal neovascularization, and hemorrhages that completely obscure the actual nerve. On this difficult data set, our method achieved 89\% correct detection. We also compare our method against three simpler methods, demonstrating the performance improvement. All our images and data are freely available for other researchers to use in evaluating related methods.},
  keywords = {Biomedical imaging,Biomedical optical imaging,Blood vessels,Brightness,Convergence,Lesions,Optical distortion,Optical imaging,Retina,Shape},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\A8AKQPIS\\Hoover et Goldbaum - 2003 - Locating the optic nerve in a retinal image using .pdf;C\:\\Users\\cleme\\Zotero\\storage\\PRGJP69Y\\1216219.html}
}

@article{huangAutomaticClassificationRetinal2019,
  title = {Automatic {{Classification}} of {{Retinal Optical Coherence Tomography Images With Layer Guided Convolutional Neural Network}}},
  author = {Huang, L. and He, X. and Fang, L. and Rabbani, H. and Chen, X.},
  year = {2019},
  month = jul,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {7},
  pages = {1026--1030},
  issn = {1070-9908},
  doi = {10.1109/LSP.2019.2917779},
  abstract = {Optical coherence tomography (OCT) enables instant and direct imaging of morphological retinal tissue and has become an essential imaging modality for ophthalmology diagnosis. As one of the important morphological retinal characteristics, the structural information of retinal layers provides meaningful diagnostic information and is closely related to several retinal diseases. In this letter, we propose a novel layer guided convolutional neural network (LGCNN) to identify normal retina and three common types of macular pathologies, namely, diabetic macular edema, drusen, and choroidal neovascularization. Specifically, an efficient segmentation network is first employed to generate the retinal layer segmentation maps, which can delineate two lesion-related retinal layers associated with the meaningful retinal lesions. Then, two well-designed subnetworks in LGCNN are utilized to integrate the information of two lesion-related layers. Consequently, LGCNN can efficiently focus on the meaningful lesion-related layer regions to improve OCT classification. The experimental results conducted on two clinically acquired datasets demonstrate the effectiveness of the proposed method.},
  keywords = {automatic classification,biological tissues,biomedical optical imaging,convolutional neural network (CNN),diabetic macular edema,direct imaging,diseases,Diseases,Distortion,efficient segmentation network,essential imaging modality,eye,Feature extraction,image segmentation,Image segmentation,important morphological retinal characteristics,layer guided convolutional neural network,lesion-related layers,lesion-related retinal layers,Lesions,LGCNN,macular pathologies,meaningful diagnostic information,meaningful lesion-related layer regions,meaningful retinal lesions,medical image processing,morphological retinal tissue,neural nets,OCT classification,ophthalmology diagnosis,Optical coherence tomography (OCT),optical tomography,Retina,retinal diseases,retinal layer segmentation maps,retinal optical coherence tomography images,structural information,Training}
}

@article{huangAutomaticClassificationRetinal2019a,
  title = {Automatic {{Classification}} of {{Retinal Optical Coherence Tomography Images With Layer Guided Convolutional Neural Network}}},
  author = {Huang, L. and He, X. and Fang, L. and Rabbani, H. and Chen, X.},
  year = {2019},
  month = jul,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {7},
  pages = {1026--1030},
  issn = {1070-9908},
  doi = {10.1109/LSP.2019.2917779},
  abstract = {Optical coherence tomography (OCT) enables instant and direct imaging of morphological retinal tissue and has become an essential imaging modality for ophthalmology diagnosis. As one of the important morphological retinal characteristics, the structural information of retinal layers provides meaningful diagnostic information and is closely related to several retinal diseases. In this letter, we propose a novel layer guided convolutional neural network (LGCNN) to identify normal retina and three common types of macular pathologies, namely, diabetic macular edema, drusen, and choroidal neovascularization. Specifically, an efficient segmentation network is first employed to generate the retinal layer segmentation maps, which can delineate two lesion-related retinal layers associated with the meaningful retinal lesions. Then, two well-designed subnetworks in LGCNN are utilized to integrate the information of two lesion-related layers. Consequently, LGCNN can efficiently focus on the meaningful lesion-related layer regions to improve OCT classification. The experimental results conducted on two clinically acquired datasets demonstrate the effectiveness of the proposed method.},
  keywords = {automatic classification,biological tissues,biomedical optical imaging,convolutional neural network (CNN),diabetic macular edema,direct imaging,diseases,Diseases,Distortion,efficient segmentation network,essential imaging modality,eye,Feature extraction,image segmentation,Image segmentation,important morphological retinal characteristics,layer guided convolutional neural network,lesion-related layers,lesion-related retinal layers,Lesions,LGCNN,macular pathologies,meaningful diagnostic information,meaningful lesion-related layer regions,meaningful retinal lesions,medical image processing,morphological retinal tissue,neural nets,OCT classification,ophthalmology diagnosis,Optical coherence tomography (OCT),optical tomography,Retina,retinal diseases,retinal layer segmentation maps,retinal optical coherence tomography images,structural information,Training}
}

@article{huangAutomaticClassificationRetinal2019b,
  title = {Automatic {{Classification}} of {{Retinal Optical Coherence Tomography Images With Layer Guided Convolutional Neural Network}}},
  author = {Huang, L. and He, X. and Fang, L. and Rabbani, H. and Chen, X.},
  year = {2019},
  month = jul,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {7},
  pages = {1026--1030},
  issn = {1070-9908},
  doi = {10.1109/LSP.2019.2917779},
  abstract = {Optical coherence tomography (OCT) enables instant and direct imaging of morphological retinal tissue and has become an essential imaging modality for ophthalmology diagnosis. As one of the important morphological retinal characteristics, the structural information of retinal layers provides meaningful diagnostic information and is closely related to several retinal diseases. In this letter, we propose a novel layer guided convolutional neural network (LGCNN) to identify normal retina and three common types of macular pathologies, namely, diabetic macular edema, drusen, and choroidal neovascularization. Specifically, an efficient segmentation network is first employed to generate the retinal layer segmentation maps, which can delineate two lesion-related retinal layers associated with the meaningful retinal lesions. Then, two well-designed subnetworks in LGCNN are utilized to integrate the information of two lesion-related layers. Consequently, LGCNN can efficiently focus on the meaningful lesion-related layer regions to improve OCT classification. The experimental results conducted on two clinically acquired datasets demonstrate the effectiveness of the proposed method.},
  keywords = {automatic classification,biological tissues,biomedical optical imaging,convolutional neural network (CNN),diabetic macular edema,direct imaging,diseases,Diseases,Distortion,efficient segmentation network,essential imaging modality,eye,Feature extraction,image segmentation,Image segmentation,important morphological retinal characteristics,layer guided convolutional neural network,lesion-related layers,lesion-related retinal layers,Lesions,LGCNN,macular pathologies,meaningful diagnostic information,meaningful lesion-related layer regions,meaningful retinal lesions,medical image processing,morphological retinal tissue,neural nets,OCT classification,ophthalmology diagnosis,Optical coherence tomography (OCT),optical tomography,Retina,retinal diseases,retinal layer segmentation maps,retinal optical coherence tomography images,structural information,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\XUJHWJYD\\Huang et al. - 2019 - Automatic Classification of Retinal Optical Cohere.pdf;C\:\\Users\\cleme\\Zotero\\storage\\P4U8TRWG\\8718310.html}
}

@article{huangAutomaticClassificationRetinal2019c,
  title = {Automatic {{Classification}} of {{Retinal Optical Coherence Tomography Images With Layer Guided Convolutional Neural Network}}},
  author = {Huang, L. and He, X. and Fang, L. and Rabbani, H. and Chen, X.},
  year = {2019},
  month = jul,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {7},
  pages = {1026--1030},
  issn = {1070-9908},
  doi = {10.1109/LSP.2019.2917779},
  abstract = {Optical coherence tomography (OCT) enables instant and direct imaging of morphological retinal tissue and has become an essential imaging modality for ophthalmology diagnosis. As one of the important morphological retinal characteristics, the structural information of retinal layers provides meaningful diagnostic information and is closely related to several retinal diseases. In this letter, we propose a novel layer guided convolutional neural network (LGCNN) to identify normal retina and three common types of macular pathologies, namely, diabetic macular edema, drusen, and choroidal neovascularization. Specifically, an efficient segmentation network is first employed to generate the retinal layer segmentation maps, which can delineate two lesion-related retinal layers associated with the meaningful retinal lesions. Then, two well-designed subnetworks in LGCNN are utilized to integrate the information of two lesion-related layers. Consequently, LGCNN can efficiently focus on the meaningful lesion-related layer regions to improve OCT classification. The experimental results conducted on two clinically acquired datasets demonstrate the effectiveness of the proposed method.},
  keywords = {automatic classification,biological tissues,biomedical optical imaging,convolutional neural network (CNN),diabetic macular edema,direct imaging,diseases,Diseases,Distortion,efficient segmentation network,essential imaging modality,eye,Feature extraction,image segmentation,Image segmentation,important morphological retinal characteristics,layer guided convolutional neural network,lesion-related layers,lesion-related retinal layers,Lesions,LGCNN,macular pathologies,meaningful diagnostic information,meaningful lesion-related layer regions,meaningful retinal lesions,medical image processing,morphological retinal tissue,neural nets,OCT classification,ophthalmology diagnosis,Optical coherence tomography (OCT),optical tomography,Retina,retinal diseases,retinal layer segmentation maps,retinal optical coherence tomography images,structural information,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\2B4R223A\\Huang et al. - 2019 - Automatic Classification of Retinal Optical Cohere.pdf;C\:\\Users\\cleme\\Zotero\\storage\\2THJXHUE\\8718310.html}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  pages = {4700--4708},
  urldate = {2023-03-15}
}

@inproceedings{huangDenselyConnectedConvolutional2017a,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  pages = {4700--4708},
  urldate = {2023-03-15},
  file = {C:\Users\cleme\Zotero\storage\PVR7UA2L\Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf}
}

@article{huangDiseasePredictionEdgevariational2022,
  title = {Disease Prediction with Edge-Variational Graph Convolutional Networks},
  author = {Huang, Yongxiang and Chung, Albert C. S.},
  year = {2022},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {77},
  pages = {102375},
  issn = {1361-8415},
  doi = {10.1016/j.media.2022.102375},
  urldate = {2023-10-03},
  abstract = {The need for computational models that can incorporate imaging data with non-imaging data while investigating inter-subject associations arises in the task of population-based disease analysis. Although off-the-shelf deep convolutional neural networks have empowered representation learning from imaging data, incorporating data of different modalities complementarily in a unified model to improve the disease diagnostic quality is still challenging. In this work, we propose a generalizable graph-convolutional framework for population-based disease prediction on multi-modal medical data. Unlike previous methods constructing a static affinity population graph in a hand-crafting manner, the proposed framework can automatically learn to build a population graph with variational edges, which we show can be optimized jointly with spectral graph convolutional networks. In addition, to estimate the predictive uncertainty related to the constructed graph, we propose Monte--Carlo edge dropout uncertainty estimation. Experimental results on four multi-modal datasets demonstrate that the proposed method can substantially improve the predictive accuracy for Autism Spectrum Disorder, Alzheimer's disease, and ocular diseases. A sufficient ablation study with in-depth discussion is conducted to evaluate the effectiveness of each component and the choice of algorithmic details of the proposed method. The results indicate the potential and extendability of the proposed framework in leveraging multi-modal data for population-based disease prediction.},
  keywords = {Computer-aided diagnosis,Deep learning,Graph neural network}
}

@article{huangDiseasePredictionEdgevariational2022a,
  title = {Disease Prediction with Edge-Variational Graph Convolutional Networks},
  author = {Huang, Yongxiang and Chung, Albert C. S.},
  year = {2022},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {77},
  pages = {102375},
  issn = {1361-8415},
  doi = {10.1016/j.media.2022.102375},
  urldate = {2023-10-03},
  abstract = {The need for computational models that can incorporate imaging data with non-imaging data while investigating inter-subject associations arises in the task of population-based disease analysis. Although off-the-shelf deep convolutional neural networks have empowered representation learning from imaging data, incorporating data of different modalities complementarily in a unified model to improve the disease diagnostic quality is still challenging. In this work, we propose a generalizable graph-convolutional framework for population-based disease prediction on multi-modal medical data. Unlike previous methods constructing a static affinity population graph in a hand-crafting manner, the proposed framework can automatically learn to build a population graph with variational edges, which we show can be optimized jointly with spectral graph convolutional networks. In addition, to estimate the predictive uncertainty related to the constructed graph, we propose Monte--Carlo edge dropout uncertainty estimation. Experimental results on four multi-modal datasets demonstrate that the proposed method can substantially improve the predictive accuracy for Autism Spectrum Disorder, Alzheimer's disease, and ocular diseases. A sufficient ablation study with in-depth discussion is conducted to evaluate the effectiveness of each component and the choice of algorithmic details of the proposed method. The results indicate the potential and extendability of the proposed framework in leveraging multi-modal data for population-based disease prediction.},
  keywords = {Computer-aided diagnosis,Deep learning,Graph neural network}
}

@article{huangDiseasePredictionEdgevariational2022b,
  title = {Disease Prediction with Edge-Variational Graph Convolutional Networks},
  author = {Huang, Yongxiang and Chung, Albert C. S.},
  year = {2022},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {77},
  pages = {102375},
  issn = {1361-8415},
  doi = {10.1016/j.media.2022.102375},
  urldate = {2023-10-03},
  abstract = {The need for computational models that can incorporate imaging data with non-imaging data while investigating inter-subject associations arises in the task of population-based disease analysis. Although off-the-shelf deep convolutional neural networks have empowered representation learning from imaging data, incorporating data of different modalities complementarily in a unified model to improve the disease diagnostic quality is still challenging. In this work, we propose a generalizable graph-convolutional framework for population-based disease prediction on multi-modal medical data. Unlike previous methods constructing a static affinity population graph in a hand-crafting manner, the proposed framework can automatically learn to build a population graph with variational edges, which we show can be optimized jointly with spectral graph convolutional networks. In addition, to estimate the predictive uncertainty related to the constructed graph, we propose Monte--Carlo edge dropout uncertainty estimation. Experimental results on four multi-modal datasets demonstrate that the proposed method can substantially improve the predictive accuracy for Autism Spectrum Disorder, Alzheimer's disease, and ocular diseases. A sufficient ablation study with in-depth discussion is conducted to evaluate the effectiveness of each component and the choice of algorithmic details of the proposed method. The results indicate the potential and extendability of the proposed framework in leveraging multi-modal data for population-based disease prediction.},
  keywords = {Computer-aided diagnosis,Deep learning,Graph neural network},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\U95D8CY6\\Huang et Chung - 2022 - Disease prediction with edge-variational graph con.pdf;C\:\\Users\\cleme\\Zotero\\storage\\XQRF64GM\\S1361841522000287.html}
}

@article{huangDiseasePredictionEdgevariational2022c,
  title = {Disease Prediction with Edge-Variational Graph Convolutional Networks},
  author = {Huang, Yongxiang and Chung, Albert C. S.},
  year = {2022},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {77},
  pages = {102375},
  issn = {1361-8415},
  doi = {10.1016/j.media.2022.102375},
  urldate = {2023-10-03},
  abstract = {The need for computational models that can incorporate imaging data with non-imaging data while investigating inter-subject associations arises in the task of population-based disease analysis. Although off-the-shelf deep convolutional neural networks have empowered representation learning from imaging data, incorporating data of different modalities complementarily in a unified model to improve the disease diagnostic quality is still challenging. In this work, we propose a generalizable graph-convolutional framework for population-based disease prediction on multi-modal medical data. Unlike previous methods constructing a static affinity population graph in a hand-crafting manner, the proposed framework can automatically learn to build a population graph with variational edges, which we show can be optimized jointly with spectral graph convolutional networks. In addition, to estimate the predictive uncertainty related to the constructed graph, we propose Monte--Carlo edge dropout uncertainty estimation. Experimental results on four multi-modal datasets demonstrate that the proposed method can substantially improve the predictive accuracy for Autism Spectrum Disorder, Alzheimer's disease, and ocular diseases. A sufficient ablation study with in-depth discussion is conducted to evaluate the effectiveness of each component and the choice of algorithmic details of the proposed method. The results indicate the potential and extendability of the proposed framework in leveraging multi-modal data for population-based disease prediction.},
  keywords = {Computer-aided diagnosis,Deep learning,Graph neural network},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\8H5FNH8H\\Huang et Chung - 2022 - Disease prediction with edge-variational graph con.pdf;C\:\\Users\\cleme\\Zotero\\storage\\NMBJ6DSD\\S1361841522000287.html}
}

@incollection{huangHandTransformerNonAutoregressiveStructured2020,
  title = {Hand-{{Transformer}}: {{Non-Autoregressive Structured Modeling}} for {{3D Hand Pose Estimation}}},
  shorttitle = {Hand-{{Transformer}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Huang, Lin and Tan, Jianchao and Liu, Ji and Yuan, Junsong},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12370},
  pages = {17--33},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58595-2_2},
  urldate = {2021-11-05},
  isbn = {978-3-030-58594-5 978-3-030-58595-2},
  langid = {english}
}

@incollection{huangHandTransformerNonAutoregressiveStructured2020a,
  title = {Hand-{{Transformer}}: {{Non-Autoregressive Structured Modeling}} for {{3D Hand Pose Estimation}}},
  shorttitle = {Hand-{{Transformer}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Huang, Lin and Tan, Jianchao and Liu, Ji and Yuan, Junsong},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12370},
  pages = {17--33},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58595-2_2},
  urldate = {2021-11-05},
  isbn = {978-3-030-58594-5 978-3-030-58595-2},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\MV8T4E5S\Huang et al. - 2020 - Hand-Transformer Non-Autoregressive Structured Mo.pdf}
}

@article{huangOpticalCoherenceTomography1991,
  title = {Optical Coherence Tomography},
  author = {Huang, D. and Swanson, E. A. and Lin, C. P. and Schuman, J. S. and Stinson, W. G. and Chang, W. and Hee, M. R. and Flotte, T. and Gregory, K. and Puliafito, C. A. and Et, Al},
  year = {1991},
  month = nov,
  journal = {Science},
  volume = {254},
  number = {5035},
  pages = {1178--1181},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1957169},
  urldate = {2019-11-14},
  abstract = {A technique called optical coherence tomography (OCT) has been developed for noninvasive cross-sectional imaging in biological systems. OCT uses low-coherence interferometry to produce a two-dimensional image of optical scattering from internal tissue microstructures in a way that is analogous to ultrasonic pulse-echo imaging. OCT has longitudinal and lateral spatial resolutions of a few micrometers and can detect reflected signals as small as approximately 10(-10) of the incident optical power. Tomographic imaging is demonstrated in vitro in the peripapillary area of the retina and in the coronary artery, two clinically relevant examples that are representative of transparent and turbid media, respectively.},
  copyright = {{\copyright} 1991},
  langid = {english},
  pmid = {1957169}
}

@article{huangOpticalCoherenceTomography1991a,
  title = {Optical Coherence Tomography},
  author = {Huang, D. and Swanson, E. A. and Lin, C. P. and Schuman, J. S. and Stinson, W. G. and Chang, W. and Hee, M. R. and Flotte, T. and Gregory, K. and Puliafito, C. A. and Et, Al},
  year = {1991},
  month = nov,
  journal = {Science},
  volume = {254},
  number = {5035},
  pages = {1178--1181},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1957169},
  urldate = {2019-11-14},
  abstract = {A technique called optical coherence tomography (OCT) has been developed for noninvasive cross-sectional imaging in biological systems. OCT uses low-coherence interferometry to produce a two-dimensional image of optical scattering from internal tissue microstructures in a way that is analogous to ultrasonic pulse-echo imaging. OCT has longitudinal and lateral spatial resolutions of a few micrometers and can detect reflected signals as small as approximately 10(-10) of the incident optical power. Tomographic imaging is demonstrated in vitro in the peripapillary area of the retina and in the coronary artery, two clinically relevant examples that are representative of transparent and turbid media, respectively.},
  copyright = {{\copyright} 1991},
  langid = {english},
  pmid = {1957169},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\3NCNDELM\\Huang et al. - 1991 - Optical coherence tomography.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RXUXBIC4\\huang1991.pdf;C\:\\Users\\cleme\\Zotero\\storage\\7T8BEHQJ\\1178.html}
}

@misc{huangVisionTransformerSuper2024,
  title = {Vision {{Transformer}} with {{Super Token Sampling}}},
  author = {Huang, Huaibo and Zhou, Xiaoqiang and Cao, Jie and He, Ran and Tan, Tieniu},
  year = {2024},
  month = jan,
  number = {arXiv:2211.11167},
  eprint = {2211.11167},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.11167},
  urldate = {2024-02-13},
  abstract = {Vision transformer has achieved impressive performance for many vision tasks. However, it may suffer from high redundancy in capturing local features for shallow layers. Local self-attention or early-stage convolutions are thus utilized, which sacrifice the capacity to capture long-range dependency. A challenge then arises: can we access efficient and effective global context modeling at the early stages of a neural network? To address this issue, we draw inspiration from the design of superpixels, which reduces the number of image primitives in subsequent processing, and introduce super tokens into vision transformer. Super tokens attempt to provide a semantically meaningful tessellation of visual content, thus reducing the token number in self-attention as well as preserving global modeling. Specifically, we propose a simple yet strong super token attention (STA) mechanism with three steps: the first samples super tokens from visual tokens via sparse association learning, the second performs self-attention on super tokens, and the last maps them back to the original token space. STA decomposes vanilla global attention into multiplications of a sparse association map and a low-dimensional attention, leading to high efficiency in capturing global dependencies. Based on STA, we develop a hierarchical vision transformer. Extensive experiments demonstrate its strong performance on various vision tasks. In particular, without any extra training data or label, it achieves 86.4\% top-1 accuracy on ImageNet-1K with less than 100M parameters. It also achieves 53.9 box AP and 46.8 mask AP on the COCO detection task, and 51.9 mIOU on the ADE20K semantic segmentation task. Code is released at https://github.com/hhb072/STViT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9M534LDG\\Huang et al. - 2024 - Vision Transformer with Super Token Sampling.pdf;C\:\\Users\\cleme\\Zotero\\storage\\DDLYNRL3\\2211.html}
}

@inproceedings{huClinicalDecisionSupport2016,
  title = {Clinical Decision Support for {{Alzheimer}}'s Disease Based on Deep Learning and Brain Network},
  booktitle = {2016 {{IEEE International Conference}} on {{Communications}} ({{ICC}})},
  author = {Hu, Chenhui and Ju, Ronghui and Shen, Yusong and Zhou, Pan and Li, Quanzheng},
  year = {2016},
  month = may,
  pages = {1--6},
  issn = {1938-1883},
  doi = {10.1109/ICC.2016.7510831},
  abstract = {Modern e-health systems have undergone rapid development thanks to the advances in communications, computing and machine learning technology. Especially, deep learning has great superiority in image analysis and disease prediction. In this paper, we use Alzheimer's Disease (AD) as an example to show advantages of deep learning in diagnosing brain diseases and providing clinical decision support. Firstly, we convert raw functional magnetic resonance imaging (fMRI) to a matrix to represent activity of 90 brain regions. Secondly, to represent the functional connectivity between different brain regions, a correlation matrix is obtained by calculating the correlation between each pair of brain regions. In the next, a targeted autoencoder network is built to classify the correlation matrix, which is sensitive to AD. Finally, the experiment results show that our proposed method for AD prediction achieves much better effects than traditional means. It finds the correlations between different brain regions efficiently, provides strong reference for AD prediction. Compared to Support Vector Machine (SVM), about 25\% improvement is gained in prediction accuracy. The e-health field becomes more complete and effective owing to that. Our work helps predict AD at an early stage and take measures to slow down or even prevent the onset of it.},
  keywords = {AD prediction,Alzheimer's Disease,biomedical MRI,brain,brain diseases diagnosing,brain network,clinical decision support,Correlation,correlation matrix,Cost function,decision support systems,deep learning,disease prediction,diseases,Diseases,e-health field,fMRI,functional connectivity,image analysis,learning (artificial intelligence),Machine learning,matrix algebra,medical image processing,modern e-health systems,Positron emission tomography,raw functional magnetic resonance imaging,Support vector machines,targeted autoencoder network,Time series analysis}
}

@inproceedings{huClinicalDecisionSupport2016a,
  title = {Clinical Decision Support for {{Alzheimer}}'s Disease Based on Deep Learning and Brain Network},
  booktitle = {2016 {{IEEE International Conference}} on {{Communications}} ({{ICC}})},
  author = {Hu, Chenhui and Ju, Ronghui and Shen, Yusong and Zhou, Pan and Li, Quanzheng},
  year = {2016},
  month = may,
  pages = {1--6},
  issn = {1938-1883},
  doi = {10.1109/ICC.2016.7510831},
  abstract = {Modern e-health systems have undergone rapid development thanks to the advances in communications, computing and machine learning technology. Especially, deep learning has great superiority in image analysis and disease prediction. In this paper, we use Alzheimer's Disease (AD) as an example to show advantages of deep learning in diagnosing brain diseases and providing clinical decision support. Firstly, we convert raw functional magnetic resonance imaging (fMRI) to a matrix to represent activity of 90 brain regions. Secondly, to represent the functional connectivity between different brain regions, a correlation matrix is obtained by calculating the correlation between each pair of brain regions. In the next, a targeted autoencoder network is built to classify the correlation matrix, which is sensitive to AD. Finally, the experiment results show that our proposed method for AD prediction achieves much better effects than traditional means. It finds the correlations between different brain regions efficiently, provides strong reference for AD prediction. Compared to Support Vector Machine (SVM), about 25\% improvement is gained in prediction accuracy. The e-health field becomes more complete and effective owing to that. Our work helps predict AD at an early stage and take measures to slow down or even prevent the onset of it.},
  keywords = {AD prediction,Alzheimer's Disease,biomedical MRI,brain,brain diseases diagnosing,brain network,clinical decision support,Correlation,correlation matrix,Cost function,decision support systems,deep learning,disease prediction,diseases,Diseases,e-health field,fMRI,functional connectivity,image analysis,learning (artificial intelligence),Machine learning,matrix algebra,medical image processing,modern e-health systems,Positron emission tomography,raw functional magnetic resonance imaging,Support vector machines,targeted autoencoder network,Time series analysis},
  file = {C:\Users\cleme\Zotero\storage\8F7HVI3Z\7510831.html}
}

@article{huMultimodalRetinalVessel2012,
  title = {Multimodal {{Retinal Vessel Segmentation From Spectral-Domain Optical Coherence Tomography}} and {{Fundus Photography}}},
  author = {Hu, Z. and Niemeijer, M. and Abramoff, M. D. and Garvin, M. K.},
  year = {2012},
  month = oct,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {31},
  number = {10},
  pages = {1900--1911},
  issn = {0278-0062},
  doi = {10.1109/TMI.2012.2206822},
  abstract = {Segmenting retinal vessels in optic nerve head (ONH) centered spectral-domain optical coherence tomography (SD-OCT) volumes is particularly challenging due to the projected neural canal opening (NCO) and relatively low visibility in the ONH center. Color fundus photographs provide a relatively high vessel contrast in the region inside the NCO, but have not been previously used to aid the SD-OCT vessel segmentation process. Thus, in this paper, we present two approaches for the segmentation of retinal vessels in SD-OCT volumes that each take advantage of complimentary information from fundus photographs. In the first approach (referred to as the registered-fundus vessel segmentation approach), vessels are first segmented on the fundus photograph directly (using a k-NN pixel classifier) and this vessel segmentation result is mapped to the SD-OCT volume through the registration of the fundus photograph to the SD-OCT volume. In the second approach (referred to as the multimodal vessel segmentation approach), after fundus-to-SD-OCT registration, vessels are simultaneously segmented with a k-NN classifier using features from both modalities. Three-dimensional structural information from the intraretinal layers and neural canal opening obtained through graph-theoretic segmentation approaches of the SD-OCT volume are used in combination with Gaussian filter banks and Gabor wavelets to generate the features. The approach is trained on 15 and tested on 19 randomly chosen independent image pairs of SD-OCT volumes and fundus images from 34 subjects with glaucoma. Based on a receiver operating characteristic (ROC) curve analysis, the present registered-fundus and multimodal vessel segmentation approaches [area under the curve (AUC) of 0.85 and 0.89, respectively] both perform significantly better than the two previous OCT-based approaches (AUC of 0.78 and 0.83, p {$<$}; 0.05). The multimodal approach overall performs significantly better than the other three approaches (p {$<$}; 0.05).},
  keywords = {Algorithms,Area Under Curve,biomedical optical imaging,blood vessels,Cities and towns,color fundus photography,colour photography,Diagnostic Techniques,diseases,Educational institutions,eye,feature extraction,Feature extraction,Fundus Oculi,Fundus photography,Gabor filters,Gabor wavelets,Gaussian filter banks,glaucoma,Glaucoma,graph theory,graph-theoretic segmentation,high-vessel contrast,Humans,image classification,image registration,image segmentation,Image segmentation,Imaging,intraretinal layers,Irrigation,k-NN pixel classifier,medical image processing,multimodal retinal vessel segmentation,multimodal vessel segmentation,neurophysiology,OCT,Ophthalmological,optic nerve head,Optical Coherence,Optical imaging,optical tomography,projected neural canal opening,receiver operating characteristic curve analysis,registered-fundus vessel segmentation,Retinal vessels,Retinal Vessels,spectral-domain optical coherence tomography,Three-Dimensional,three-dimensional structural information,Tomography,wavelet transforms}
}

@article{huMultimodalRetinalVessel2012a,
  title = {Multimodal {{Retinal Vessel Segmentation From Spectral-Domain Optical Coherence Tomography}} and {{Fundus Photography}}},
  author = {Hu, Zhihong and Niemeijer, Meindert and Abramoff, Michael D. and Garvin, Mona K.},
  year = {2012},
  month = oct,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {31},
  number = {10},
  pages = {1900--1911},
  issn = {1558-254X},
  doi = {10.1109/TMI.2012.2206822},
  abstract = {Segmenting retinal vessels in optic nerve head (ONH) centered spectral-domain optical coherence tomography (SD-OCT) volumes is particularly challenging due to the projected neural canal opening (NCO) and relatively low visibility in the ONH center. Color fundus photographs provide a relatively high vessel contrast in the region inside the NCO, but have not been previously used to aid the SD-OCT vessel segmentation process. Thus, in this paper, we present two approaches for the segmentation of retinal vessels in SD-OCT volumes that each take advantage of complimentary information from fundus photographs. In the first approach (referred to as the registered-fundus vessel segmentation approach), vessels are first segmented on the fundus photograph directly (using a k-NN pixel classifier) and this vessel segmentation result is mapped to the SD-OCT volume through the registration of the fundus photograph to the SD-OCT volume. In the second approach (referred to as the multimodal vessel segmentation approach), after fundus-to-SD-OCT registration, vessels are simultaneously segmented with a k-NN classifier using features from both modalities. Three-dimensional structural information from the intraretinal layers and neural canal opening obtained through graph-theoretic segmentation approaches of the SD-OCT volume are used in combination with Gaussian filter banks and Gabor wavelets to generate the features. The approach is trained on 15 and tested on 19 randomly chosen independent image pairs of SD-OCT volumes and fundus images from 34 subjects with glaucoma. Based on a receiver operating characteristic (ROC) curve analysis, the present registered-fundus and multimodal vessel segmentation approaches [area under the curve (AUC) of 0.85 and 0.89, respectively] both perform significantly better than the two previous OCT-based approaches (AUC of 0.78 and 0.83, p {$<$}; 0.05). The multimodal approach overall performs significantly better than the other three approaches (p {$<$}; 0.05).},
  keywords = {Algorithms,Area Under Curve,biomedical optical imaging,blood vessels,Cities and towns,color fundus photography,colour photography,Diagnostic Techniques,diseases,Educational institutions,eye,feature extraction,Feature extraction,Fundus Oculi,Fundus photography,Gabor filters,Gabor wavelets,Gaussian filter banks,glaucoma,Glaucoma,graph theory,graph-theoretic segmentation,high-vessel contrast,Humans,image classification,image registration,image segmentation,Image segmentation,Imaging,intraretinal layers,Irrigation,k-NN pixel classifier,medical image processing,multimodal retinal vessel segmentation,multimodal vessel segmentation,neurophysiology,OCT,Ophthalmological,optic nerve head,Optical Coherence,Optical imaging,optical tomography,projected neural canal opening,receiver operating characteristic curve analysis,registered-fundus vessel segmentation,Retinal vessels,Retinal Vessels,spectral-domain optical coherence tomography,Three-Dimensional,three-dimensional structural information,Tomography,wavelet transforms}
}

@article{huMultimodalRetinalVessel2012b,
  title = {Multimodal {{Retinal Vessel Segmentation From Spectral-Domain Optical Coherence Tomography}} and {{Fundus Photography}}},
  author = {Hu, Zhihong and Niemeijer, Meindert and Abramoff, Michael D. and Garvin, Mona K.},
  year = {2012},
  month = oct,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {31},
  number = {10},
  pages = {1900--1911},
  issn = {1558-254X},
  doi = {10.1109/TMI.2012.2206822},
  abstract = {Segmenting retinal vessels in optic nerve head (ONH) centered spectral-domain optical coherence tomography (SD-OCT) volumes is particularly challenging due to the projected neural canal opening (NCO) and relatively low visibility in the ONH center. Color fundus photographs provide a relatively high vessel contrast in the region inside the NCO, but have not been previously used to aid the SD-OCT vessel segmentation process. Thus, in this paper, we present two approaches for the segmentation of retinal vessels in SD-OCT volumes that each take advantage of complimentary information from fundus photographs. In the first approach (referred to as the registered-fundus vessel segmentation approach), vessels are first segmented on the fundus photograph directly (using a k-NN pixel classifier) and this vessel segmentation result is mapped to the SD-OCT volume through the registration of the fundus photograph to the SD-OCT volume. In the second approach (referred to as the multimodal vessel segmentation approach), after fundus-to-SD-OCT registration, vessels are simultaneously segmented with a k-NN classifier using features from both modalities. Three-dimensional structural information from the intraretinal layers and neural canal opening obtained through graph-theoretic segmentation approaches of the SD-OCT volume are used in combination with Gaussian filter banks and Gabor wavelets to generate the features. The approach is trained on 15 and tested on 19 randomly chosen independent image pairs of SD-OCT volumes and fundus images from 34 subjects with glaucoma. Based on a receiver operating characteristic (ROC) curve analysis, the present registered-fundus and multimodal vessel segmentation approaches [area under the curve (AUC) of 0.85 and 0.89, respectively] both perform significantly better than the two previous OCT-based approaches (AUC of 0.78 and 0.83, p {$<$}; 0.05). The multimodal approach overall performs significantly better than the other three approaches (p {$<$}; 0.05).},
  keywords = {Algorithms,Area Under Curve,biomedical optical imaging,blood vessels,Cities and towns,color fundus photography,colour photography,Diagnostic Techniques Ophthalmological,diseases,Educational institutions,eye,feature extraction,Feature extraction,Fundus Oculi,Fundus photography,Gabor filters,Gabor wavelets,Gaussian filter banks,glaucoma,Glaucoma,graph theory,graph-theoretic segmentation,high-vessel contrast,Humans,image classification,image registration,image segmentation,Image segmentation,Imaging Three-Dimensional,intraretinal layers,Irrigation,k-NN pixel classifier,medical image processing,multimodal retinal vessel segmentation,multimodal vessel segmentation,neurophysiology,OCT,optic nerve head,Optical imaging,optical tomography,projected neural canal opening,receiver operating characteristic curve analysis,registered-fundus vessel segmentation,Retinal vessels,Retinal Vessels,spectral-domain optical coherence tomography,three-dimensional structural information,Tomography Optical Coherence,wavelet transforms},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HFKIN8DE\\zhihonghu2012.pdf;C\:\\Users\\cleme\\Zotero\\storage\\Z8YN7C58\\Hu et al. - 2012 - Multimodal Retinal Vessel Segmentation From Spectr.pdf;C\:\\Users\\cleme\\Zotero\\storage\\METBKM7R\\6228540.html}
}

@article{huMultimodalRetinalVessel2012c,
  title = {Multimodal {{Retinal Vessel Segmentation From Spectral-Domain Optical Coherence Tomography}} and {{Fundus Photography}}},
  author = {Hu, Z. and Niemeijer, M. and Abramoff, M. D. and Garvin, M. K.},
  year = {2012},
  month = oct,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {31},
  number = {10},
  pages = {1900--1911},
  issn = {0278-0062},
  doi = {10.1109/TMI.2012.2206822},
  abstract = {Segmenting retinal vessels in optic nerve head (ONH) centered spectral-domain optical coherence tomography (SD-OCT) volumes is particularly challenging due to the projected neural canal opening (NCO) and relatively low visibility in the ONH center. Color fundus photographs provide a relatively high vessel contrast in the region inside the NCO, but have not been previously used to aid the SD-OCT vessel segmentation process. Thus, in this paper, we present two approaches for the segmentation of retinal vessels in SD-OCT volumes that each take advantage of complimentary information from fundus photographs. In the first approach (referred to as the registered-fundus vessel segmentation approach), vessels are first segmented on the fundus photograph directly (using a k-NN pixel classifier) and this vessel segmentation result is mapped to the SD-OCT volume through the registration of the fundus photograph to the SD-OCT volume. In the second approach (referred to as the multimodal vessel segmentation approach), after fundus-to-SD-OCT registration, vessels are simultaneously segmented with a k-NN classifier using features from both modalities. Three-dimensional structural information from the intraretinal layers and neural canal opening obtained through graph-theoretic segmentation approaches of the SD-OCT volume are used in combination with Gaussian filter banks and Gabor wavelets to generate the features. The approach is trained on 15 and tested on 19 randomly chosen independent image pairs of SD-OCT volumes and fundus images from 34 subjects with glaucoma. Based on a receiver operating characteristic (ROC) curve analysis, the present registered-fundus and multimodal vessel segmentation approaches [area under the curve (AUC) of 0.85 and 0.89, respectively] both perform significantly better than the two previous OCT-based approaches (AUC of 0.78 and 0.83, p {$<$}; 0.05). The multimodal approach overall performs significantly better than the other three approaches (p {$<$}; 0.05).},
  keywords = {Algorithms,Area Under Curve,biomedical optical imaging,blood vessels,Cities and towns,color fundus photography,colour photography,Diagnostic Techniques Ophthalmological,diseases,Educational institutions,eye,feature extraction,Feature extraction,Fundus Oculi,Fundus photography,Gabor filters,Gabor wavelets,Gaussian filter banks,glaucoma,Glaucoma,graph theory,graph-theoretic segmentation,high-vessel contrast,Humans,image classification,image registration,image segmentation,Image segmentation,Imaging Three-Dimensional,intraretinal layers,Irrigation,k-NN pixel classifier,medical image processing,multimodal retinal vessel segmentation,multimodal vessel segmentation,neurophysiology,OCT,optic nerve head,Optical imaging,optical tomography,projected neural canal opening,receiver operating characteristic curve analysis,registered-fundus vessel segmentation,Retinal vessels,Retinal Vessels,spectral-domain optical coherence tomography,three-dimensional structural information,Tomography Optical Coherence,wavelet transforms}
}

@inproceedings{huSqueezeandExcitationNetworks2018,
  title = {Squeeze-and-{{Excitation Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hu, Jie and Shen, Li and Sun, Gang},
  year = {2018},
  month = jun,
  pages = {7132--7141},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2018.00745},
  abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251\%, achieving a 25\% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.},
  keywords = {Adaptation models,Computational modeling,Computer architecture,Convolution,Convolutional codes,Stacking,Task analysis}
}

@inproceedings{huSqueezeandExcitationNetworks2018a,
  title = {Squeeze-and-{{Excitation Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hu, Jie and Shen, Li and Sun, Gang},
  year = {2018},
  month = jun,
  pages = {7132--7141},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2018.00745},
  abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251\%, achieving a 25\% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.},
  keywords = {Adaptation models,Computational modeling,Computer architecture,Convolution,Convolutional codes,Stacking,Task analysis},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5LKT49K3\\hu2018.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UP54JGV9\\Hu et al. - 2018 - Squeeze-and-Excitation Networks.pdf;C\:\\Users\\cleme\\Zotero\\storage\\7E3CWUCG\\8578843.html}
}

@misc{HyperbandNovelBanditbased,
  title = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization: {{The Journal}} of {{Machine Learning Research}}: {{Vol}} 18, {{No}} 1},
  urldate = {2023-03-27}
}

@misc{HyperbandNovelBanditbaseda,
  title = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization: {{The Journal}} of {{Machine Learning Research}}: {{Vol}} 18, {{No}} 1},
  urldate = {2023-03-27},
  howpublished = {https://dl.acm.org/doi/10.5555/3122009.3242042},
  file = {C:\Users\cleme\Zotero\storage\4PNUCQL8\3122009.html}
}

@misc{HypergraphConvolutionHypergraph,
  title = {Hypergraph {{Convolution}} and {{Hypergraph Attention}}},
  urldate = {2019-07-30},
  abstract = {Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. {\dots}},
  langid = {english}
}

@misc{HypergraphConvolutionHypergrapha,
  title = {Hypergraph {{Convolution}} and {{Hypergraph Attention}}},
  journal = {GroundAI},
  urldate = {2019-07-30},
  abstract = {Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. {\dots}},
  howpublished = {https://www.groundai.com/project/hypergraph-convolution-and-hypergraph-attention/1},
  langid = {english}
}

@misc{Iakubovskii:2019,
  title = {Segmentation Models Pytorch},
  author = {Iakubovskii, Pavel},
  year = {2019},
  publisher = {GitHub}
}

@misc{ImageProjectionNetwork,
  title = {Image {{Projection Network}}: {{3D}} to {{2D Image Segmentation}} in {{OCTA Images}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2022-07-10}
}

@misc{ImageProjectionNetworka,
  title = {Image {{Projection Network}}: {{3D}} to {{2D Image Segmentation}} in {{OCTA Images}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2022-07-10},
  howpublished = {https://ieeexplore.ieee.org/document/9085991}
}

@misc{ImageQualityAssessment,
  title = {Image {{Quality Assessment}}: {{From Error Visibility}} to {{Structural Similarity}}},
  urldate = {2019-06-13}
}

@misc{ImageQualityAssessmenta,
  title = {Image {{Quality Assessment}}: {{From Error Visibility}} to {{Structural Similarity}}},
  urldate = {2019-06-13},
  howpublished = {https://ece.uwaterloo.ca/{\textasciitilde}z70wang/publications/ssim.html},
  file = {C:\Users\cleme\Zotero\storage\7IJQHXTD\ssim.html}
}

@article{info11020125,
  title = {Albumentations: {{Fast}} and Flexible Image Augmentations},
  author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
  year = {2020},
  journal = {Information-an International Interdisciplinary Journal},
  volume = {11},
  number = {2},
  issn = {2078-2489},
  doi = {10.3390/info11020125},
  article-number = {125}
}

@article{info11020125,
  title = {Albumentations: {{Fast}} and Flexible Image Augmentations},
  author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
  year = {2020},
  journal = {Information-an International Interdisciplinary Journal},
  volume = {11},
  number = {125},
  issn = {2078-2489},
  doi = {10.3390/info11020125}
}

@article{innesDifferentiableProgrammingSystem2019,
  title = {A {{Differentiable Programming System}} to {{Bridge Machine Learning}} and {{Scientific Computing}}},
  author = {Innes, Mike and Edelman, Alan and Fischer, Keno and Rackauckas, Chris and Saba, Elliot and Shah, Viral B. and Tebbutt, Will},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.07587 [cs]},
  eprint = {1907.07587},
  primaryclass = {cs},
  urldate = {2019-12-27},
  abstract = {Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {C:\Users\cleme\Zotero\storage\X2XDA57S\Innes et al. - 2019 - A Differentiable Programming System to Bridge Mach.pdf}
}

@article{inodaDeeplearningbasedAIEvaluating2022,
  title = {Deep-Learning-Based {{AI}} for Evaluating Estimated Nonperfusion Areas Requiring Further Examination in Ultra-Widefield Fundus Images},
  author = {Inoda, Satoru and Takahashi, Hidenori and Yamagata, Hitoshi and Hisadome, Yoichiro and Kondo, Yusuke and Tampo, Hironobu and Sakamoto, Shinichi and Katada, Yusaku and Kurihara, Toshihide and Kawashima, Hidetoshi and Yanagi, Yasuo},
  year = {2022},
  month = dec,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {21826},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-25894-9},
  urldate = {2024-02-22},
  abstract = {We herein propose a PraNet-based deep-learning model for estimating the size of non-perfusion area (NPA) in pseudo-color fundus photos from an ultra-wide-field (UWF) image. We trained the model with focal loss and weighted binary cross-entropy loss to deal with the class-imbalanced dataset, and optimized hyperparameters in order to minimize validation loss. As expected, the resultant PraNet-based deep-learning model outperformed previously published methods. For verification, we used UWF fundus images with NPA and used Bland--Altman plots to compare estimated NPA with the ground truth in FA, which demonstrated that bias between the eNPA and ground truth was smaller than 10\% of the confidence limits zone and that the number of outliers was less than 10\% of observed paired images. The accuracy of the model was also tested on an external dataset from another institution, which confirmed the generalization of the model. For validation, we employed a contingency table for ROC analysis to judge the sensitivity and specificity of the estimated-NPA (eNPA). The results demonstrated that the sensitivity and specificity ranged from 83.3--87.0\% and 79.3--85.7\%, respectively. In conclusion, we developed an AI model capable of estimating NPA size from only an UWF image without angiography using PraNet-based deep learning. This is a potentially useful tool in monitoring eyes with ischemic retinal diseases.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Machine learning,Retinal diseases},
  file = {C:\Users\cleme\Zotero\storage\5DVVBAZH\Inoda et al. - 2022 - Deep-learning-based AI for evaluating estimated no.pdf}
}

@misc{InterpretableClassificationAlzheimer,
  title = {Interpretable Classification of {{Alzheimer}}'s Disease Pathologies with a Convolutional Neural Network Pipeline {\textbar} {{Nature Communications}}},
  urldate = {2021-11-16}
}

@misc{InterpretableClassificationAlzheimera,
  title = {Interpretable Classification of {{Alzheimer}}'s Disease Pathologies with a Convolutional Neural Network Pipeline {\textbar} {{Nature Communications}}},
  urldate = {2021-11-16},
  howpublished = {https://www.nature.com/articles/s41467-019-10212-1},
  file = {C:\Users\cleme\Zotero\storage\FDKXM3B5\s41467-019-10212-1.html}
}

@inproceedings{ioffeBatchNormalizationAccelerating2015,
  title = {Batch Normalization: {{Accelerating}} Deep Network Training by Reducing Internal Covariate Shift},
  shorttitle = {Batch Normalization},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = jul,
  series = {{{ICML}}'15},
  pages = {448--456},
  publisher = {JMLR.org},
  address = {Lille, France},
  urldate = {2023-02-22},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.}
}

@inproceedings{ioffeBatchNormalizationAccelerating2015a,
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  shorttitle = {Batch Normalization},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = jul,
  series = {{{ICML}}'15},
  pages = {448--456},
  publisher = {JMLR.org},
  address = {Lille, France},
  urldate = {2023-02-22},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.}
}

@article{irvinCheXpertLargeChest2019,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.07031 [cs, eess]},
  eprint = {1901.07031},
  primaryclass = {cs, eess},
  urldate = {2021-11-16},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@article{irvinCheXpertLargeChest2019a,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.07031 [cs, eess]},
  eprint = {1901.07031},
  primaryclass = {cs, eess},
  urldate = {2021-11-16},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\NQB7NJUE\\Irvin et al. - 2019 - CheXpert A Large Chest Radiograph Dataset with Un.pdf;C\:\\Users\\cleme\\Zotero\\storage\\7PQ2YXYT\\1901.html}
}

@article{islamDeepLearningAlgorithms2020,
  title = {Deep Learning Algorithms for Detection of Diabetic Retinopathy in Retinal Fundus Photographs: {{A}} Systematic Review and Meta-Analysis},
  shorttitle = {Deep Learning Algorithms for Detection of Diabetic Retinopathy in Retinal Fundus Photographs},
  author = {Islam, Md Mohaimenul and Yang, Hsuan-Chia and Poly, Tahmina Nasrin and Jian, Wen-Shan and (Jack) Li, Yu-Chuan},
  year = {2020},
  month = jul,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {191},
  pages = {105320},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2020.105320},
  urldate = {2021-10-04},
  abstract = {Background Diabetic retinopathy (DR) is one of the leading causes of blindness globally. Earlier detection and timely treatment of DR are desirable to reduce the incidence and progression of vision loss. Currently, deep learning (DL) approaches have offered better performance in detecting DR from retinal fundus images. We, therefore, performed a systematic review with a meta-analysis of relevant studies to quantify the performance of DL algorithms for detecting DR. Methods A systematic literature search on EMBASE, PubMed, Google Scholar, Scopus was performed between January 1, 2000, and March 31, 2019. The search strategy was based on the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) reporting guidelines, and DL-based study design was mandatory for articles inclusion. Two independent authors screened abstracts and titles against inclusion and exclusion criteria. Data were extracted by two authors independently using a standard form and the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) tool was used for the risk of bias and applicability assessment. Results Twenty-three studies were included in the systematic review; 20 studies met inclusion criteria for the meta-analysis. The pooled area under the receiving operating curve (AUROC) of DR was 0.97 (95\%CI: 0.95--0.98), sensitivity was 0.83 (95\%CI: 0.83--0.83), and specificity was 0.92 (95\%CI: 0.92--0.92). The positive- and negative-likelihood ratio were 14.11 (95\%CI: 9.91--20.07), and 0.10 (95\%CI: 0.07--0.16), respectively. Moreover, the diagnostic odds ratio for DL models was 136.83 (95\%CI: 79.03--236.93). All the studies provided a DR-grading scale, a human grader (e.g. trained caregivers, ophthalmologists) as a reference standard. Conclusion The findings of our study showed that DL algorithms had high sensitivity and specificity for detecting referable DR from retinal fundus photographs. Applying a DL-based automated tool of assessing DR from color fundus images could provide an alternative solution to reduce misdiagnosis and improve workflow. A DL-based automated tool offers substantial benefits to reduce screening costs, accessibility to healthcare and ameliorate earlier treatments.},
  langid = {english},
  keywords = {Deep learning,Diabetic,Diabetic retinopathy,Fundus photograph,Retinopathy}
}

@article{islamDeepLearningAlgorithms2020a,
  title = {Deep Learning Algorithms for Detection of Diabetic Retinopathy in Retinal Fundus Photographs: {{A}} Systematic Review and Meta-Analysis},
  shorttitle = {Deep Learning Algorithms for Detection of Diabetic Retinopathy in Retinal Fundus Photographs},
  author = {Islam, Md Mohaimenul and Yang, Hsuan-Chia and Poly, Tahmina Nasrin and Jian, Wen-Shan and (Jack) Li, Yu-Chuan},
  year = {2020},
  month = jul,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {191},
  pages = {105320},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2020.105320},
  urldate = {2021-10-04},
  abstract = {Background Diabetic retinopathy (DR) is one of the leading causes of blindness globally. Earlier detection and timely treatment of DR are desirable to reduce the incidence and progression of vision loss. Currently, deep learning (DL) approaches have offered better performance in detecting DR from retinal fundus images. We, therefore, performed a systematic review with a meta-analysis of relevant studies to quantify the performance of DL algorithms for detecting DR. Methods A systematic literature search on EMBASE, PubMed, Google Scholar, Scopus was performed between January 1, 2000, and March 31, 2019. The search strategy was based on the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) reporting guidelines, and DL-based study design was mandatory for articles inclusion. Two independent authors screened abstracts and titles against inclusion and exclusion criteria. Data were extracted by two authors independently using a standard form and the Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) tool was used for the risk of bias and applicability assessment. Results Twenty-three studies were included in the systematic review; 20 studies met inclusion criteria for the meta-analysis. The pooled area under the receiving operating curve (AUROC) of DR was 0.97 (95\%CI: 0.95--0.98), sensitivity was 0.83 (95\%CI: 0.83--0.83), and specificity was 0.92 (95\%CI: 0.92--0.92). The positive- and negative-likelihood ratio were 14.11 (95\%CI: 9.91--20.07), and 0.10 (95\%CI: 0.07--0.16), respectively. Moreover, the diagnostic odds ratio for DL models was 136.83 (95\%CI: 79.03--236.93). All the studies provided a DR-grading scale, a human grader (e.g. trained caregivers, ophthalmologists) as a reference standard. Conclusion The findings of our study showed that DL algorithms had high sensitivity and specificity for detecting referable DR from retinal fundus photographs. Applying a DL-based automated tool of assessing DR from color fundus images could provide an alternative solution to reduce misdiagnosis and improve workflow. A DL-based automated tool offers substantial benefits to reduce screening costs, accessibility to healthcare and ameliorate earlier treatments.},
  langid = {english},
  keywords = {Deep learning,Diabetic,Diabetic retinopathy,Fundus photograph,Retinopathy},
  file = {C:\Users\cleme\Zotero\storage\G2R9TZ34\S0169260719311010.html}
}

@article{islamQuantificationExplainabilityExplainable2019,
  title = {Towards {{Quantification}} of {{Explainability}} in {{Explainable Artificial Intelligence Methods}}},
  author = {Islam, Sheikh Rabiul and Eberle, William and Ghafoor, Sheikh K.},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.10104 [cs, q-fin]},
  eprint = {1911.10104},
  primaryclass = {cs, q-fin},
  urldate = {2021-03-01},
  abstract = {Artificial Intelligence (AI) has become an integral part of domains such as security, finance, healthcare, medicine, and criminal justice. Explaining the decisions of AI systems in human terms is a key challenge---due to the high complexity of the model, as well as the potential implications on human interests, rights, and lives . While Explainable AI is an emerging field of research, there is no consensus on the definition, quantification, and formalization of explainability. In fact, the quantification of explainability is an open challenge. In our previous work, we incorporated domain knowledge for better explainability, however, we were unable to quantify the extent of explainability. In this work, we (1) briefly analyze the definitions of explainability from the perspective of different disciplines (e.g., psychology, social science), properties of explanation, explanation methods, and human-friendly explanations; and (2) propose and formulate an approach to quantify the extent of explainability. Our experimental result suggests a reasonable and model-agnostic way to quantify explainability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Finance - Risk Management}
}

@article{islamQuantificationExplainabilityExplainable2019a,
  title = {Towards {{Quantification}} of {{Explainability}} in {{Explainable Artificial Intelligence Methods}}},
  author = {Islam, Sheikh Rabiul and Eberle, William and Ghafoor, Sheikh K.},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.10104 [cs, q-fin]},
  eprint = {1911.10104},
  primaryclass = {cs, q-fin},
  urldate = {2021-03-01},
  abstract = {Artificial Intelligence (AI) has become an integral part of domains such as security, finance, healthcare, medicine, and criminal justice. Explaining the decisions of AI systems in human terms is a key challenge---due to the high complexity of the model, as well as the potential implications on human interests, rights, and lives . While Explainable AI is an emerging field of research, there is no consensus on the definition, quantification, and formalization of explainability. In fact, the quantification of explainability is an open challenge. In our previous work, we incorporated domain knowledge for better explainability, however, we were unable to quantify the extent of explainability. In this work, we (1) briefly analyze the definitions of explainability from the perspective of different disciplines (e.g., psychology, social science), properties of explanation, explanation methods, and human-friendly explanations; and (2) propose and formulate an approach to quantify the extent of explainability. Our experimental result suggests a reasonable and model-agnostic way to quantify explainability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Finance - Risk Management},
  file = {C:\Users\cleme\Zotero\storage\NG9M7RWS\Islam et al. - 2019 - Towards Quantification of Explainability in Explai.pdf}
}

@misc{ItTimeReplace2021,
  title = {Is It {{Time}} to {{Replace CNNs}} with {{Transformers}} for {{Medical Images}}?},
  year = {2021},
  month = aug,
  urldate = {2021-11-02},
  abstract = {08/20/21 - Convolutional Neural Networks (CNNs) have reigned for a decade as the de facto approach to automated medical image diagnosis. Rece...}
}

@misc{ItTimeReplace2021a,
  title = {Is It {{Time}} to {{Replace CNNs}} with {{Transformers}} for {{Medical Images}}?},
  year = {2021},
  month = aug,
  journal = {DeepAI},
  urldate = {2021-11-02},
  abstract = {08/20/21 - Convolutional Neural Networks (CNNs) have reigned for a decade as the de facto approach to automated medical image diagnosis. Rece...},
  howpublished = {https://deepai.org/publication/is-it-time-to-replace-cnns-with-transformers-for-medical-images},
  file = {C:\Users\cleme\Zotero\storage\35B382WA\is-it-time-to-replace-cnns-with-transformers-for-medical-images.html}
}

@article{izmailovAveragingWeightsLeads,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  pages = {10},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and ShakeShake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  langid = {english}
}

@article{izmailovAveragingWeightsLeads2018,
  title = {Averaging Weights Leads to Wider Optima and Better Generalization: 34th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} 2018, {{UAI}} 2018},
  shorttitle = {Averaging Weights Leads to Wider Optima and Better Generalization},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  editor = {Silva, Ricardo and Globerson, Amir and Globerson, Amir},
  year = {2018},
  journal = {34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018},
  series = {34th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} 2018, {{UAI}} 2018},
  pages = {876--885},
  publisher = {Association For Uncertainty in Artificial Intelligence (AUAI)},
  urldate = {2022-11-09},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensem-bling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and ShakeShake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.}
}

@article{izmailovAveragingWeightsLeads2018a,
  title = {Averaging Weights Leads to Wider Optima and Better Generalization: 34th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} 2018, {{UAI}} 2018},
  shorttitle = {Averaging Weights Leads to Wider Optima and Better Generalization},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  editor = {Silva, Ricardo and Globerson, Amir and Globerson, Amir},
  year = {2018},
  journal = {34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018},
  series = {34th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} 2018, {{UAI}} 2018},
  pages = {876--885},
  publisher = {Association For Uncertainty in Artificial Intelligence (AUAI)},
  urldate = {2022-11-09},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensem-bling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and ShakeShake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.}
}

@article{izmailovAveragingWeightsLeadsa,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  pages = {10},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and ShakeShake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\PSZL46AJ\Izmailov et al. - Averaging Weights Leads to Wider Optima and Better.pdf}
}

@article{jaffeOpticalCoherenceTomography2004,
  title = {Optical Coherence Tomography to Detect and Manage Retinal Disease and Glaucoma},
  author = {Jaffe, Glenn J. and Caprioli, Joseph},
  year = {2004},
  month = jan,
  journal = {American Journal of Ophthalmology},
  volume = {137},
  number = {1},
  pages = {156--169},
  issn = {0002-9394},
  doi = {10.1016/S0002-9394(03)00792-X},
  urldate = {2019-10-30},
  abstract = {Purpose To review basic principles of optical coherence tomography, and to describe its use in the diagnosis and management of retinal diseases and glaucoma. Design Perspective. Methods Literature review. Results Optical coherence tomography is a noninvasive imaging technique that has been used increasingly to diagnose and manage a variety of retinal diseases and glaucoma. Optical coherence tomography (OCT) is based on the principal of Michelson interferometry. Interference patterns produced by low coherence light reflected from retinal tissues and a reference mirror are processed into an ``A-scan'' signal. Multiple A-scan signals are aligned to produce a two-dimensional image that can be thought of as a form of ``in vivo histology.'' Optical coherence tomography has been used to identify macular holes, to differentiate macular holes from simulating lesions, to identify lamellar macular holes, macular cysts, vitreomacular traction, subretinal fluid, pigment epithelial detachment, and choroidal neovascularization. It can be used to identify and quantify macular edema, and to measure retinal thickness changes in response to therapy. Macular thickness measurements determined by OCT correlate well with visual acuity and with leakage observed by fluorescein angiography. Optical coherence tomography is an accurate and reproducible method to measure retinal nerve fiber layer thickness. Particularly, when used in combination with other optic nerve imaging techniques, it can be used to differentiate glaucomatous eyes from normal eyes. Despite its usefulness, OCT has its limitations. Optical coherence tomography equipment is expensive, and not all insurance companies reimburse this procedure. Image quality is dependent on operator technique and can be degraded in the presence of media opacity. Change analysis software for glaucoma applications is not fully developed, and there is a scarcity of age, gender, and race-specific normative data upon which to compare eyes with retinal disease and glaucoma. In the next few years, it is likely that the role of OCT as a method to diagnose and manage retinal disease and glaucoma will be further defined, and many of the current limitations will be overcome. Conclusions Optical coherence tomography is a useful imaging technique to diagnose and manage a variety of retinal diseases and glaucoma. Care is needed to avoid artifacts and image misinterpretation.},
  langid = {english}
}

@article{jaffeOpticalCoherenceTomography2004a,
  title = {Optical Coherence Tomography to Detect and Manage Retinal Disease and Glaucoma},
  author = {Jaffe, Glenn J. and Caprioli, Joseph},
  year = {2004},
  month = jan,
  journal = {American Journal of Ophthalmology},
  volume = {137},
  number = {1},
  pages = {156--169},
  issn = {0002-9394},
  doi = {10.1016/S0002-9394(03)00792-X},
  urldate = {2019-10-30},
  abstract = {Purpose To review basic principles of optical coherence tomography, and to describe its use in the diagnosis and management of retinal diseases and glaucoma. Design Perspective. Methods Literature review. Results Optical coherence tomography is a noninvasive imaging technique that has been used increasingly to diagnose and manage a variety of retinal diseases and glaucoma. Optical coherence tomography (OCT) is based on the principal of Michelson interferometry. Interference patterns produced by low coherence light reflected from retinal tissues and a reference mirror are processed into an ``A-scan'' signal. Multiple A-scan signals are aligned to produce a two-dimensional image that can be thought of as a form of ``in vivo histology.'' Optical coherence tomography has been used to identify macular holes, to differentiate macular holes from simulating lesions, to identify lamellar macular holes, macular cysts, vitreomacular traction, subretinal fluid, pigment epithelial detachment, and choroidal neovascularization. It can be used to identify and quantify macular edema, and to measure retinal thickness changes in response to therapy. Macular thickness measurements determined by OCT correlate well with visual acuity and with leakage observed by fluorescein angiography. Optical coherence tomography is an accurate and reproducible method to measure retinal nerve fiber layer thickness. Particularly, when used in combination with other optic nerve imaging techniques, it can be used to differentiate glaucomatous eyes from normal eyes. Despite its usefulness, OCT has its limitations. Optical coherence tomography equipment is expensive, and not all insurance companies reimburse this procedure. Image quality is dependent on operator technique and can be degraded in the presence of media opacity. Change analysis software for glaucoma applications is not fully developed, and there is a scarcity of age, gender, and race-specific normative data upon which to compare eyes with retinal disease and glaucoma. In the next few years, it is likely that the role of OCT as a method to diagnose and manage retinal disease and glaucoma will be further defined, and many of the current limitations will be overcome. Conclusions Optical coherence tomography is a useful imaging technique to diagnose and manage a variety of retinal diseases and glaucoma. Care is needed to avoid artifacts and image misinterpretation.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HRU3W9YN\\Jaffe et Caprioli - 2004 - Optical coherence tomography to detect and manage .pdf;C\:\\Users\\cleme\\Zotero\\storage\\TN3G4SCI\\S000293940300792X.html}
}

@article{jainQuantitativeComparisonDrusen2010,
  title = {Quantitative {{Comparison}} of {{Drusen Segmented}} on {{SD-OCT}} versus {{Drusen Delineated}} on {{Color Fundus Photographs}}},
  author = {Jain, Nieraj and Farsiu, Sina and Khanifar, Aziz A. and Bearelly, Srilaxmi and Smith, R. Theodore and Izatt, Joseph A. and Toth, Cynthia A.},
  year = {2010},
  month = oct,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {51},
  number = {10},
  pages = {4875--4883},
  issn = {1552-5783},
  doi = {10.1167/iovs.09-4962},
  urldate = {2019-12-06},
  langid = {english}
}

@article{jainQuantitativeComparisonDrusen2010a,
  title = {Quantitative {{Comparison}} of {{Drusen Segmented}} on {{SD-OCT}} versus {{Drusen Delineated}} on {{Color Fundus Photographs}}},
  author = {Jain, Nieraj and Farsiu, Sina and Khanifar, Aziz A. and Bearelly, Srilaxmi and Smith, R. Theodore and Izatt, Joseph A. and Toth, Cynthia A.},
  year = {2010},
  month = oct,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {51},
  number = {10},
  pages = {4875--4883},
  issn = {1552-5783},
  doi = {10.1167/iovs.09-4962},
  urldate = {2019-12-06},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\V4SWNHKC\\Jain et al. - 2010 - Quantitative Comparison of Drusen Segmented on SD-.pdf;C\:\\Users\\cleme\\Zotero\\storage\\9W6XN3QW\\article.html;C\:\\Users\\cleme\\Zotero\\storage\\AGGJNK47\\jain2010.html}
}

@article{jangLateralityClassificationFundus2018,
  title = {Laterality {{Classification}} of {{Fundus Images Using Interpretable Deep Neural Network}}},
  author = {Jang, Yeonwoo and Son, Jaemin and Park, Kyu Hyung and Park, Sang Jun and Jung, Kyu-Hwan},
  year = {2018},
  month = dec,
  journal = {Journal of Digital Imaging},
  volume = {31},
  number = {6},
  pages = {923--928},
  issn = {1618-727X},
  doi = {10.1007/s10278-018-0099-2},
  urldate = {2019-12-13},
  abstract = {In this paper, we aimed to understand and analyze the outputs of a convolutional neural network model that classifies the laterality of fundus images. Our model not only automatizes the classification process, which results in reducing the labors of clinicians, but also highlights the key regions in the image and evaluates the uncertainty for the decision with proper analytic tools. Our model was trained and tested with 25,911 fundus images (43.4\% of macula-centered images and 28.3\% each of superior and nasal retinal fundus images). Also, activation maps were generated to mark important regions in the image for the classification. Then, uncertainties were quantified to support explanations as to why certain images were incorrectly classified under the proposed model. Our model achieved a mean training accuracy of 99\%, which is comparable to the performance of clinicians. Strong activations were detected at the location of optic disc and retinal blood vessels around the disc, which matches to the regions that clinicians attend when deciding the laterality. Uncertainty analysis discovered that misclassified images tend to accompany with high prediction uncertainties and are likely ungradable. We believe that visualization of informative regions and the estimation of uncertainty, along with presentation of the prediction result, would enhance the interpretability of neural network models in a way that clinicians can be benefitted from using the automatic classification system.},
  langid = {english},
  keywords = {Deep learning,Deep neural network,Fundus images,Interpretability,Laterality classification}
}

@article{jangLateralityClassificationFundus2018a,
  title = {Laterality {{Classification}} of {{Fundus Images Using Interpretable Deep Neural Network}}},
  author = {Jang, Yeonwoo and Son, Jaemin and Park, Kyu Hyung and Park, Sang Jun and Jung, Kyu-Hwan},
  year = {2018},
  month = dec,
  journal = {Journal of Digital Imaging},
  volume = {31},
  number = {6},
  pages = {923--928},
  issn = {1618-727X},
  doi = {10.1007/s10278-018-0099-2},
  urldate = {2019-12-13},
  abstract = {In this paper, we aimed to understand and analyze the outputs of a convolutional neural network model that classifies the laterality of fundus images. Our model not only automatizes the classification process, which results in reducing the labors of clinicians, but also highlights the key regions in the image and evaluates the uncertainty for the decision with proper analytic tools. Our model was trained and tested with 25,911 fundus images (43.4\% of macula-centered images and 28.3\% each of superior and nasal retinal fundus images). Also, activation maps were generated to mark important regions in the image for the classification. Then, uncertainties were quantified to support explanations as to why certain images were incorrectly classified under the proposed model. Our model achieved a mean training accuracy of 99\%, which is comparable to the performance of clinicians. Strong activations were detected at the location of optic disc and retinal blood vessels around the disc, which matches to the regions that clinicians attend when deciding the laterality. Uncertainty analysis discovered that misclassified images tend to accompany with high prediction uncertainties and are likely ungradable. We believe that visualization of informative regions and the estimation of uncertainty, along with presentation of the prediction result, would enhance the interpretability of neural network models in a way that clinicians can be benefitted from using the automatic classification system.},
  langid = {english},
  keywords = {Deep learning,Deep neural network,Fundus images,Interpretability,Laterality classification},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9M5GDAQW\\Jang et al. - 2018 - Laterality Classification of Fundus Images Using I.pdf;C\:\\Users\\cleme\\Zotero\\storage\\M2SPJR9Q\\jang2018.html}
}

@article{jeongReviewMachineLearning2022,
  title = {Review of {{Machine Learning Applications Using Retinal Fundus Images}}},
  author = {Jeong, Yeonwoo and Hong, Yu-Jin and Han, Jae-Ho},
  year = {2022},
  month = jan,
  journal = {Diagnostics (Basel, Switzerland)},
  volume = {12},
  number = {1},
  pages = {134},
  issn = {2075-4418},
  doi = {10.3390/diagnostics12010134},
  abstract = {Automating screening and diagnosis in the medical field saves time and reduces the chances of misdiagnosis while saving on labor and cost for physicians. With the feasibility and development of deep learning methods, machines are now able to interpret complex features in medical data, which leads to rapid advancements in automation. Such efforts have been made in ophthalmology to analyze retinal images and build frameworks based on analysis for the identification of retinopathy and the assessment of its severity. This paper reviews recent state-of-the-art works utilizing the color fundus image taken from one of the imaging modalities used in ophthalmology. Specifically, the deep learning methods of automated screening and diagnosis for diabetic retinopathy (DR), age-related macular degeneration (AMD), and glaucoma are investigated. In addition, the machine learning techniques applied to the retinal vasculature extraction from the fundus image are covered. The challenges in developing these systems are also discussed.},
  langid = {english},
  pmcid = {PMC8774893},
  pmid = {35054301},
  keywords = {deep learning,fundus image,machine learning,retinal image}
}

@article{jeongReviewMachineLearning2022a,
  title = {Review of {{Machine Learning Applications Using Retinal Fundus Images}}},
  author = {Jeong, Yeonwoo and Hong, Yu-Jin and Han, Jae-Ho},
  year = {2022},
  month = jan,
  journal = {Diagnostics},
  volume = {12},
  number = {1},
  pages = {134},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-4418},
  doi = {10.3390/diagnostics12010134},
  urldate = {2023-04-23},
  abstract = {Automating screening and diagnosis in the medical field saves time and reduces the chances of misdiagnosis while saving on labor and cost for physicians. With the feasibility and development of deep learning methods, machines are now able to interpret complex features in medical data, which leads to rapid advancements in automation. Such efforts have been made in ophthalmology to analyze retinal images and build frameworks based on analysis for the identification of retinopathy and the assessment of its severity. This paper reviews recent state-of-the-art works utilizing the color fundus image taken from one of the imaging modalities used in ophthalmology. Specifically, the deep learning methods of automated screening and diagnosis for diabetic retinopathy (DR), age-related macular degeneration (AMD), and glaucoma are investigated. In addition, the machine learning techniques applied to the retinal vasculature extraction from the fundus image are covered. The challenges in developing these systems are also discussed.},
  langid = {english},
  keywords = {deep learning,fundus image,machine learning,retinal image}
}

@article{jeongReviewMachineLearning2022b,
  title = {Review of {{Machine Learning Applications Using Retinal Fundus Images}}},
  author = {Jeong, Yeonwoo and Hong, Yu-Jin and Han, Jae-Ho},
  year = {2022},
  month = jan,
  journal = {Diagnostics},
  volume = {12},
  number = {1},
  pages = {134},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-4418},
  doi = {10.3390/diagnostics12010134},
  urldate = {2023-04-23},
  abstract = {Automating screening and diagnosis in the medical field saves time and reduces the chances of misdiagnosis while saving on labor and cost for physicians. With the feasibility and development of deep learning methods, machines are now able to interpret complex features in medical data, which leads to rapid advancements in automation. Such efforts have been made in ophthalmology to analyze retinal images and build frameworks based on analysis for the identification of retinopathy and the assessment of its severity. This paper reviews recent state-of-the-art works utilizing the color fundus image taken from one of the imaging modalities used in ophthalmology. Specifically, the deep learning methods of automated screening and diagnosis for diabetic retinopathy (DR), age-related macular degeneration (AMD), and glaucoma are investigated. In addition, the machine learning techniques applied to the retinal vasculature extraction from the fundus image are covered. The challenges in developing these systems are also discussed.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,fundus image,machine learning,retinal image},
  file = {C:\Users\cleme\Zotero\storage\IVQHY56I\Jeong et al. - 2022 - Review of Machine Learning Applications Using Reti.pdf}
}

@article{jeongReviewMachineLearning2022c,
  title = {Review of {{Machine Learning Applications Using Retinal Fundus Images}}},
  author = {Jeong, Yeonwoo and Hong, Yu-Jin and Han, Jae-Ho},
  year = {2022},
  month = jan,
  journal = {Diagnostics (Basel, Switzerland)},
  volume = {12},
  number = {1},
  pages = {134},
  issn = {2075-4418},
  doi = {10.3390/diagnostics12010134},
  abstract = {Automating screening and diagnosis in the medical field saves time and reduces the chances of misdiagnosis while saving on labor and cost for physicians. With the feasibility and development of deep learning methods, machines are now able to interpret complex features in medical data, which leads to rapid advancements in automation. Such efforts have been made in ophthalmology to analyze retinal images and build frameworks based on analysis for the identification of retinopathy and the assessment of its severity. This paper reviews recent state-of-the-art works utilizing the color fundus image taken from one of the imaging modalities used in ophthalmology. Specifically, the deep learning methods of automated screening and diagnosis for diabetic retinopathy (DR), age-related macular degeneration (AMD), and glaucoma are investigated. In addition, the machine learning techniques applied to the retinal vasculature extraction from the fundus image are covered. The challenges in developing these systems are also discussed.},
  langid = {english},
  pmcid = {PMC8774893},
  pmid = {35054301},
  keywords = {deep learning,fundus image,machine learning,retinal image},
  file = {C:\Users\cleme\Zotero\storage\3N5WYQQ6\Jeong et al. - 2022 - Review of Machine Learning Applications Using Reti.pdf}
}

@article{jiangFantasticGeneralizationMeasures2019,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.02178 [cs, stat]},
  eprint = {1912.02178},
  primaryclass = {cs, stat},
  urldate = {2019-12-06},
  abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{jiangFantasticGeneralizationMeasures2019a,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.02178 [cs, stat]},
  eprint = {1912.02178},
  primaryclass = {cs, stat},
  urldate = {2019-12-06},
  abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\TVTIJRR6\Jiang et al. - 2019 - Fantastic Generalization Measures and Where to Fin.pdf}
}

@article{jinExplainableDeepLearning2022,
  title = {Explainable Deep Learning in Healthcare: {{A}} Methodological Survey from an Attribution View},
  shorttitle = {Explainable Deep Learning in Healthcare},
  author = {Jin, Di and Sergeeva, Elena and Weng, Wei-Hung and Chauhan, Geeticka and Szolovits, Peter},
  year = {2022},
  journal = {WIREs Mechanisms of Disease},
  volume = {14},
  number = {3},
  pages = {e1548},
  issn = {2692-9368},
  doi = {10.1002/wsbm.1548},
  urldate = {2023-05-05},
  abstract = {The increasing availability of large collections of electronic health record (EHR) data and unprecedented technical advances in deep learning (DL) have sparked a surge of research interest in developing DL based clinical decision support systems for diagnosis, prognosis, and treatment. Despite the recognition of the value of deep learning in healthcare, impediments to further adoption in real healthcare settings remain due to the black-box nature of DL. Therefore, there is an emerging need for interpretable DL, which allows end users to evaluate the model decision making to know whether to accept or reject predictions and recommendations before an action is taken. In this review, we focus on the interpretability of the DL models in healthcare. We start by introducing the methods for interpretability in depth and comprehensively as a methodological reference for future researchers or clinical practitioners in this field. Besides the methods' details, we also include a discussion of advantages and disadvantages of these methods and which scenarios each of them is suitable for, so that interested readers can know how to compare and choose among them for use. Moreover, we discuss how these methods, originally developed for solving general-domain problems, have been adapted and applied to healthcare problems and how they can help physicians better understand these data-driven technologies. Overall, we hope this survey can help researchers and practitioners in both artificial intelligence and clinical fields understand what methods we have for enhancing the interpretability of their DL models and choose the optimal one accordingly. This article is categorized under: Cancer {$>$} Computational Models},
  langid = {english},
  keywords = {deep learning in medicine,interpretable deep learning}
}

@article{jinExplainableDeepLearning2022a,
  title = {Explainable Deep Learning in Healthcare: {{A}} Methodological Survey from an Attribution View},
  shorttitle = {Explainable Deep Learning in Healthcare},
  author = {Jin, Di and Sergeeva, Elena and Weng, Wei-Hung and Chauhan, Geeticka and Szolovits, Peter},
  year = {2022},
  journal = {WIREs Mechanisms of Disease},
  volume = {14},
  number = {3},
  pages = {e1548},
  issn = {2692-9368},
  doi = {10.1002/wsbm.1548},
  urldate = {2023-05-05},
  abstract = {The increasing availability of large collections of electronic health record (EHR) data and unprecedented technical advances in deep learning (DL) have sparked a surge of research interest in developing DL based clinical decision support systems for diagnosis, prognosis, and treatment. Despite the recognition of the value of deep learning in healthcare, impediments to further adoption in real healthcare settings remain due to the black-box nature of DL. Therefore, there is an emerging need for interpretable DL, which allows end users to evaluate the model decision making to know whether to accept or reject predictions and recommendations before an action is taken. In this review, we focus on the interpretability of the DL models in healthcare. We start by introducing the methods for interpretability in depth and comprehensively as a methodological reference for future researchers or clinical practitioners in this field. Besides the methods' details, we also include a discussion of advantages and disadvantages of these methods and which scenarios each of them is suitable for, so that interested readers can know how to compare and choose among them for use. Moreover, we discuss how these methods, originally developed for solving general-domain problems, have been adapted and applied to healthcare problems and how they can help physicians better understand these data-driven technologies. Overall, we hope this survey can help researchers and practitioners in both artificial intelligence and clinical fields understand what methods we have for enhancing the interpretability of their DL models and choose the optimal one accordingly. This article is categorized under: Cancer {$>$} Computational Models},
  langid = {english},
  keywords = {deep learning in medicine,interpretable deep learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\8QT7XV75\\Jin et al. - 2022 - Explainable deep learning in healthcare A methodo.pdf;C\:\\Users\\cleme\\Zotero\\storage\\XPAT4PEG\\wsbm.html}
}

@article{johnsonAutomatedMedicalAlgorithms2001,
  title = {Automated {{Medical Algorithms}}: {{Issues}} for {{Medical Errors}}},
  shorttitle = {Automated {{Medical Algorithms}}},
  author = {Johnson, Kathy A. and Svirbely, John R and Sriram, {\relax MG} and Smith, Jack W. and Kantor, Gareth and Rodriguez, Jorge Raul},
  year = {2001},
  journal = {Proceedings of the AMIA Symposium},
  pages = {939},
  issn = {1531-605X},
  urldate = {2019-11-01},
  pmcid = {PMC2243630},
  pmid = {null}
}

@article{johnsonAutomatedMedicalAlgorithms2001a,
  title = {Automated {{Medical Algorithms}}: {{Issues}} for {{Medical Errors}}},
  shorttitle = {Automated {{Medical Algorithms}}},
  author = {Johnson, Kathy A. and Svirbely, John R and Sriram, {\relax MG} and Smith, Jack W. and Kantor, Gareth and Rodriguez, Jorge Raul},
  year = {2001},
  journal = {Proceedings of the AMIA Symposium},
  pages = {939},
  issn = {1531-605X},
  urldate = {2019-11-01},
  pmcid = {PMC2243630},
  pmid = {null},
  file = {C:\Users\cleme\Zotero\storage\TFRMQ4XY\Johnson et al. - 2001 - Automated Medical Algorithms Issues for Medical E.pdf}
}

@article{johnsonMIMICCXRDeidentifiedPublicly2019,
  title = {{{MIMIC-CXR}}, a de-{{Identified}} Publicly Available Database of Chest Radiographs with Free-Text Reports},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Berkowitz, Seth J. and Greenbaum, Nathaniel R. and Lungren, Matthew P. and Deng, Chih-ying and Mark, Roger G. and Horng, Steven},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {1--8},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0322-0},
  urldate = {2020-02-20},
  abstract = {Measurement(s) Radiograph {$\bullet$} investigation results report Technology Type(s) Chest Radiography {$\bullet$} digital curation {$\bullet$} Radiologist Sample Characteristic - Organism Homo sapiens Machine-accessible metadata file describing the reported data: https://doi.org/10.6084/m9.figshare.10303823},
  copyright = {2019 The Author(s)},
  langid = {english}
}

@article{johnsonMIMICCXRDeidentifiedPublicly2019a,
  title = {{{MIMIC-CXR}}, a de-Identified Publicly Available Database of Chest Radiographs with Free-Text Reports},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Berkowitz, Seth J. and Greenbaum, Nathaniel R. and Lungren, Matthew P. and Deng, Chih-ying and Mark, Roger G. and Horng, Steven},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {1--8},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0322-0},
  urldate = {2020-02-20},
  abstract = {Measurement(s)   Radiograph {$\bullet$} investigation results report    Technology Type(s)   Chest Radiography {$\bullet$} digital curation {$\bullet$} Radiologist    Sample Characteristic - Organism   Homo sapiens          Machine-accessible metadata file describing the reported data: https://doi.org/10.6084/m9.figshare.10303823},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\B7EIPJ7P\\Johnson et al. - 2019 - MIMIC-CXR, a de-identified publicly available data.pdf;C\:\\Users\\cleme\\Zotero\\storage\\WSBJCKJE\\10.1038@s41597-019-0322-0.pdf;C\:\\Users\\cleme\\Zotero\\storage\\EQJAGTLJ\\s41597-019-0322-0.html}
}

@article{jonasFactsMythsCerebrospinal2015,
  title = {Facts and Myths of Cerebrospinal Fluid Pressure for the Physiology of the Eye},
  author = {Jonas, Jost B. and Wang, Ningli and Yang, Diya and Ritch, Robert and {Panda-Jonas}, Songhomitra},
  year = {2015},
  month = may,
  journal = {Progress in Retinal and Eye Research},
  volume = {46},
  pages = {67--83},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2015.01.002},
  urldate = {2019-11-15},
  abstract = {The orbital cerebrospinal fluid pressure (CSFP) represents the true counter-pressure against the intraocular pressure (IOP) across the lamina cribrosa and is, therefore, one of the two determinants of the trans-lamina cribrosa pressure difference (TLPD). From this anatomic point of view, an elevated TLPD could be due to elevated IOP or abnormally low orbital CSFP. Both experimental and clinical studies have suggested that a low CSFP could be associated with glaucomatous optic neuropathy in normal-pressure glaucoma. These included monkey studies with an experimental long-term reduction in CSFP, and clinical retrospective and prospective studies on patients with normal-pressure glaucoma. Since the choroidal blood drains via the vortex veins through the superior ophthalmic vein into the intracranial cavernous sinus, anatomy suggests that the CSFP could influence choroidal thickness. A population-based study revealed that thicker subfoveal choroidal thickness was associated with higher CSFP. Since the central retinal vein passes through the orbital CSF space, anatomy suggests that the retinal venous pressure should be at least as high as the orbital CSFP. Other experimental, clinical or population-based studies suggested an association between higher CSFP and higher retinal venous pressure and wider retinal veins. Consequently, a higher estimated CSFP was associated with arterial hypertensive retinopathy (with respect to the dilated retinal vein diameter and higher arterial-to-venous diameter) and with the prevalence, severity and incidence of diabetic retinopathy. Physiologically, CSFP was related with higher IOP. The influence of the CSFP on the episcleral venous pressure and/or a regulation of both CSFP and IOP by a center in the dorsomedial/perifornical hypothalamus may be responsible for this. In summary, the CSFP may be an overlooked parameter in ocular physiology and pathology. Abnormal changes in the CSFP, in particular in relationship to the IOP, may have pathophysiologic importance.},
  langid = {english},
  keywords = {Brain pressure,Cerebrospinal fluid pressure,Diabetic retinopathy,Glaucoma,Normal-pressure glaucoma,Open-angle glaucoma,Retinal vein occlusion}
}

@article{jonasFactsMythsCerebrospinal2015a,
  title = {Facts and Myths of Cerebrospinal Fluid Pressure for the Physiology of~the Eye},
  author = {Jonas, Jost B. and Wang, Ningli and Yang, Diya and Ritch, Robert and {Panda-Jonas}, Songhomitra},
  year = {2015},
  month = may,
  journal = {Progress in Retinal and Eye Research},
  volume = {46},
  pages = {67--83},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2015.01.002},
  urldate = {2019-11-15},
  abstract = {The orbital cerebrospinal fluid pressure (CSFP) represents the true counter-pressure against the intraocular pressure (IOP) across the lamina cribrosa and is, therefore, one of the two determinants of the trans-lamina cribrosa pressure difference (TLPD). From this anatomic point of view, an elevated TLPD could be due to elevated IOP or abnormally low orbital CSFP. Both experimental and clinical studies have suggested that a low CSFP could be associated with glaucomatous optic neuropathy in normal-pressure glaucoma. These included monkey studies with an experimental long-term reduction in CSFP, and clinical retrospective and prospective studies on patients with normal-pressure glaucoma. Since the choroidal blood drains via the vortex veins through the superior ophthalmic vein into the intracranial cavernous sinus, anatomy suggests that the CSFP could influence choroidal thickness. A population-based study revealed that thicker subfoveal choroidal thickness was associated with higher CSFP. Since the central retinal vein passes through the orbital CSF space, anatomy suggests that the retinal venous pressure should be at least as high as the orbital CSFP. Other experimental, clinical or population-based studies suggested an association between higher CSFP and higher retinal venous pressure and wider retinal veins. Consequently, a higher estimated CSFP was associated with arterial hypertensive retinopathy (with respect to the dilated retinal vein diameter and higher arterial-to-venous diameter) and with the prevalence, severity and incidence of diabetic retinopathy. Physiologically, CSFP was related with higher IOP. The influence of the CSFP on the episcleral venous pressure and/or a regulation of both CSFP and IOP by a center in the dorsomedial/perifornical hypothalamus may be responsible for this. In~summary, the CSFP may be an overlooked parameter in ocular physiology and pathology. Abnormal changes in the CSFP, in particular in relationship to the IOP, may have pathophysiologic importance.},
  langid = {english},
  keywords = {Brain pressure,Cerebrospinal fluid pressure,Diabetic retinopathy,Glaucoma,Normal-pressure glaucoma,Open-angle glaucoma,Retinal vein occlusion},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\XY6LHY8E\\Jonas et al. - 2015 - Facts and myths of cerebrospinal fluid pressure fo.pdf;C\:\\Users\\cleme\\Zotero\\storage\\AAPEIW5H\\S1350946215000038.html}
}

@article{jonesExtratropicalTransitionTropical2003,
  title = {The {{Extratropical Transition}} of {{Tropical Cyclones}}: {{Forecast Challenges}}, {{Current Understanding}}, and {{Future Directions}}},
  shorttitle = {The {{Extratropical Transition}} of {{Tropical Cyclones}}},
  author = {Jones, Sarah C. and Harr, Patrick A. and Abraham, Jim and Bosart, Lance F. and Bowyer, Peter J. and Evans, Jenni L. and Hanley, Deborah E. and Hanstrum, Barry N. and Hart, Robert E. and Lalaurette, Fran{\c c}ois and Sinclair, Mark R. and Smith, Roger K. and Thorncroft, Chris},
  year = {2003},
  month = dec,
  journal = {Weather and Forecasting},
  volume = {18},
  number = {6},
  pages = {1052--1092},
  issn = {0882-8156},
  doi = {10.1175/1520-0434(2003)018<1052:TETOTC>2.0.CO;2},
  urldate = {2019-06-14},
  abstract = {A significant number of tropical cyclones move into the midlatitudes and transform into extratropical cyclones. This process is generally referred to as extratropical transition (ET). During ET a cyclone frequently produces intense rainfall and strong winds and has increased forward motion, so that such systems pose a serious threat to land and maritime activities. Changes in the structure of a system as it evolves from a tropical to an extratropical cyclone during ET necessitate changes in forecast strategies. In this paper a brief climatology of ET is given and the challenges associated with forecasting extratropical transition are described in terms of the forecast variables (track, intensity, surface winds, precipitation) and their impacts (flooding, bush fires, ocean response). The problems associated with the numerical prediction of ET are discussed. A comprehensive review of the current understanding of the processes involved in ET is presented. Classifications of extratropical transition are described and potential vorticity thinking is presented as an aid to understanding ET. Further sections discuss the interaction between a tropical cyclone and the midlatitude environment, the role of latent heat release, convection and the underlying surface in ET, the structural changes due to frontogenesis, the mechanisms responsible for precipitation, and the energy budget during ET. Finally, a summary of the future directions for research into ET is given.}
}

@article{jonesExtratropicalTransitionTropical2003a,
  title = {The {{Extratropical Transition}} of {{Tropical Cyclones}}: {{Forecast Challenges}}, {{Current Understanding}}, and {{Future Directions}}},
  shorttitle = {The {{Extratropical Transition}} of {{Tropical Cyclones}}},
  author = {Jones, Sarah C. and Harr, Patrick A. and Abraham, Jim and Bosart, Lance F. and Bowyer, Peter J. and Evans, Jenni L. and Hanley, Deborah E. and Hanstrum, Barry N. and Hart, Robert E. and Lalaurette, Fran{\c c}ois and Sinclair, Mark R. and Smith, Roger K. and Thorncroft, Chris},
  year = {2003},
  month = dec,
  journal = {Weather and Forecasting},
  volume = {18},
  number = {6},
  pages = {1052--1092},
  issn = {0882-8156},
  doi = {10.1175/1520-0434(2003)018<1052:TETOTC>2.0.CO;2},
  urldate = {2019-06-14},
  abstract = {A significant number of tropical cyclones move into the midlatitudes and transform into extratropical cyclones. This process is generally referred to as extratropical transition (ET). During ET a cyclone frequently produces intense rainfall and strong winds and has increased forward motion, so that such systems pose a serious threat to land and maritime activities. Changes in the structure of a system as it evolves from a tropical to an extratropical cyclone during ET necessitate changes in forecast strategies. In this paper a brief climatology of ET is given and the challenges associated with forecasting extratropical transition are described in terms of the forecast variables (track, intensity, surface winds, precipitation) and their impacts (flooding, bush fires, ocean response). The problems associated with the numerical prediction of ET are discussed. A comprehensive review of the current understanding of the processes involved in ET is presented. Classifications of extratropical transition are described and potential vorticity thinking is presented as an aid to understanding ET. Further sections discuss the interaction between a tropical cyclone and the midlatitude environment, the role of latent heat release, convection and the underlying surface in ET, the structural changes due to frontogenesis, the mechanisms responsible for precipitation, and the energy budget during ET. Finally, a summary of the future directions for research into ET is given.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HLP7F8SF\\Jones et al. - 2003 - The Extratropical Transition of Tropical Cyclones.pdf;C\:\\Users\\cleme\\Zotero\\storage\\TUQCXWV8\\1520-0434(2003)0181052TETOTC2.0.html}
}

@article{joshiOpticDiskCup2011,
  title = {Optic {{Disk}} and {{Cup Segmentation From Monocular Color Retinal Images}} for {{Glaucoma Assessment}}},
  author = {Joshi, G. D. and Sivaswamy, J. and Krishnadas, S. R.},
  year = {2011},
  month = jun,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {30},
  number = {6},
  pages = {1192--1205},
  doi = {10.1109/TMI.2011.2106509},
  abstract = {Automatic retinal image analysis is emerging as an important screening tool for early detection of eye diseases. Glaucoma is one of the most common causes of blindness. The manual examination of optic disk (OD) is a standard procedure used for detecting glaucoma. In this paper, we present an automatic OD parameterization technique based on segmented OD and cup regions obtained from monocular retinal images. A novel OD segmentation method is proposed which integrates the local image information around each point of interest in multidimensional feature space to provide robustness against variations found in and around the OD region. We also propose a novel cup segmentation method which is based on anatomical evidence such as vessel bends at the cup boundary, considered relevant by glaucoma experts. Bends in a vessel are robustly detected using a region of support concept, which automatically selects the right scale for analysis. A multi-stage strategy is employed to derive a reliable subset of vessel bends called r-bends followed by a local spline fitting to derive the desired cup boundary. The method has been evaluated on 138 images comprising 33 normal and 105 glaucomatous images against three glaucoma experts. The obtained segmentation results show consistency in handling various geometric and photometric variations found across the dataset. The estimation error of the method for vertical cup-to-disk diameter ratio is 0.09/0.08 (mean/standard deviation) while for cup-to-disk area ratio it is 0.12/0.10. Overall, the obtained qualitative and quantitative results show effectiveness in both segmentation and subsequent OD parameterization for glaucoma assessment.},
  keywords = {Active contour,Active contours,Algorithms,Automated,automatic retinal image analysis,Biomedical optical imaging,blood vessels,Capacitance-voltage characteristics,Color,Colorimetry,Computer-Assisted,cup,cup segmentation,cup-to-disk ratio (CDR),data analysis,dataset,eye,feature extraction,Fluorescein Angiography,glaucoma,Glaucoma,glaucoma assessment,glaucomatous images,Humans,Image color analysis,image colour analysis,Image Enhancement,Image Interpretation,image segmentation,Image segmentation,local image information,local spline fitting,medical disorders,medical image processing,monocular color retinal images,multidimensional feature space,neurophysiology,neuroretinal rim,optic disk,Optic Disk,optic disk (OD),optic nerve degeneration,Optical imaging,parameterization technique,Pattern Recognition,Reproducibility of Results,Retina,retinal images,Retinoscopy,segmentation,Sensitivity and Specificity,splines (mathematics),vessel bend,vessel bends}
}

@article{joshiOpticDiskCup2011a,
  title = {Optic {{Disk}} and {{Cup Segmentation From Monocular Color Retinal Images}} for {{Glaucoma Assessment}}},
  author = {Joshi, G. D. and Sivaswamy, J. and Krishnadas, S. R.},
  year = {2011},
  month = jun,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {30},
  number = {6},
  pages = {1192--1205},
  doi = {10.1109/TMI.2011.2106509},
  abstract = {Automatic retinal image analysis is emerging as an important screening tool for early detection of eye diseases. Glaucoma is one of the most common causes of blindness. The manual examination of optic disk (OD) is a standard procedure used for detecting glaucoma. In this paper, we present an automatic OD parameterization technique based on segmented OD and cup regions obtained from monocular retinal images. A novel OD segmentation method is proposed which integrates the local image information around each point of interest in multidimensional feature space to provide robustness against variations found in and around the OD region. We also propose a novel cup segmentation method which is based on anatomical evidence such as vessel bends at the cup boundary, considered relevant by glaucoma experts. Bends in a vessel are robustly detected using a region of support concept, which automatically selects the right scale for analysis. A multi-stage strategy is employed to derive a reliable subset of vessel bends called r-bends followed by a local spline fitting to derive the desired cup boundary. The method has been evaluated on 138 images comprising 33 normal and 105 glaucomatous images against three glaucoma experts. The obtained segmentation results show consistency in handling various geometric and photometric variations found across the dataset. The estimation error of the method for vertical cup-to-disk diameter ratio is 0.09/0.08 (mean/standard deviation) while for cup-to-disk area ratio it is 0.12/0.10. Overall, the obtained qualitative and quantitative results show effectiveness in both segmentation and subsequent OD parameterization for glaucoma assessment.},
  keywords = {Active contour,Active contours,Algorithms,automatic retinal image analysis,Biomedical optical imaging,blood vessels,Capacitance-voltage characteristics,Color,Colorimetry,cup,cup segmentation,cup-to-disk ratio (CDR),data analysis,dataset,eye,feature extraction,Fluorescein Angiography,glaucoma,Glaucoma,glaucoma assessment,glaucomatous images,Humans,Image color analysis,image colour analysis,Image Enhancement,Image Interpretation Computer-Assisted,image segmentation,Image segmentation,local image information,local spline fitting,medical disorders,medical image processing,monocular color retinal images,multidimensional feature space,neurophysiology,neuroretinal rim,optic disk,Optic Disk,optic disk (OD),optic nerve degeneration,Optical imaging,parameterization technique,Pattern Recognition Automated,Reproducibility of Results,Retina,retinal images,Retinoscopy,segmentation,Sensitivity and Specificity,splines (mathematics),vessel bend,vessel bends},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\6YACRKJU\\Joshi et al. - 2011 - Optic Disk and Cup Segmentation From Monocular Col.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RAMJ7SHK\\5762351.html}
}

@inproceedings{joshiReviewDeepLearning2022,
  title = {Review of {{Deep Learning}} and {{Interpretability}}},
  booktitle = {Soft {{Computing}} for {{Security Applications}}},
  author = {Joshi, Hrushikesh and Rajeswari, Kannan and Joshi, Sneha},
  editor = {Ranganathan, G. and Fernando, Xavier and Shi, Fuqian and El Allioui, Youssouf},
  year = {2022},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {501--516},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-16-5301-8_37},
  abstract = {Deep learning has revolutionized machine learning. Deep convolution networks play a pivotal role in image classification. A conventional image classification technique requires extensive feature engineering; this approach is exhaustive as rigorous manual efforts are needed. Deep learning automatically detects the feature set required for classification. It uses backpropagation to achieve this. Deep learning is being used in a variety of applications including healthcare. The use of deep learning specifically in cancer biology is significant. For Pathologists, classifying tumors as benign or malignant is a complicated task. Classification of malignant lymphoma is challenging as features are not uniform across whole slides, and there are various patterns for classification of cancer. Deep learning is ideal for such a scenario where automated feature learning is required further deep learning detects features that are invariant to the location in an image. Tumor region-wise classification of a slide can also be achieved which can be further subjected to recurrent neural networks. Textual explanations of cancer regions can be generated, providing further insights into diagnosis. Here, three different cancer datasets are reviewed and subjected to deep learning methods. Explainable AI approaches can be applied to decision trees as well as on gradient boosted trees to obtain explanations.},
  isbn = {9789811653018},
  langid = {english},
  keywords = {Cancer classification,Deep CNN,Deep learning,Explainable AI,Pathology}
}

@inproceedings{joshiReviewDeepLearning2022a,
  title = {Review of {{Deep Learning}} and {{Interpretability}}},
  booktitle = {Soft {{Computing}} for {{Security Applications}}},
  author = {Joshi, Hrushikesh and Rajeswari, Kannan and Joshi, Sneha},
  editor = {Ranganathan, G. and Fernando, Xavier and Shi, Fuqian and El Allioui, Youssouf},
  year = {2022},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {501--516},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-16-5301-8_37},
  abstract = {Deep learning has revolutionized machine learning. Deep convolution networks play a pivotal role in image classification. A conventional image classification technique requires extensive feature engineering; this approach is exhaustive as rigorous manual efforts are needed. Deep learning automatically detects the feature set required for classification. It uses backpropagation to achieve this. Deep learning is being used in a variety of applications including healthcare. The use of deep learning specifically in cancer biology is significant. For Pathologists, classifying tumors as benign or malignant is a complicated task. Classification of malignant lymphoma is challenging as features are not uniform across whole slides, and there are various patterns for classification of cancer. Deep learning is ideal for such a scenario where automated feature learning is required further deep learning detects features that are invariant to the location in an image. Tumor region-wise classification of a slide can also be achieved which can be further subjected to recurrent neural networks. Textual explanations of cancer regions can be generated, providing further insights into diagnosis. Here, three different cancer datasets are reviewed and subjected to deep learning methods. Explainable AI approaches can be applied to decision trees as well as on gradient boosted trees to obtain explanations.},
  isbn = {9789811653018},
  langid = {english},
  keywords = {Cancer classification,Deep CNN,Deep learning,Explainable AI,Pathology}
}

@inproceedings{joshiReviewDeepLearning2022b,
  title = {Review of {{Deep Learning}} and {{Interpretability}}},
  booktitle = {Soft {{Computing}} for {{Security Applications}}},
  author = {Joshi, Hrushikesh and Rajeswari, Kannan and Joshi, Sneha},
  editor = {Ranganathan, G. and Fernando, Xavier and Shi, Fuqian and El Allioui, Youssouf},
  year = {2022},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {501--516},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-16-5301-8_37},
  abstract = {Deep learning has revolutionized machine learning. Deep convolution networks play a pivotal role in image classification. A conventional image classification technique requires extensive feature engineering; this approach is exhaustive as rigorous manual efforts are needed. Deep learning automatically detects the feature set required for classification. It uses backpropagation to achieve this. Deep learning is being used in a variety of applications including healthcare. The use of deep learning specifically in cancer biology is significant. For Pathologists, classifying tumors as benign or malignant is a complicated task. Classification of malignant lymphoma is challenging as features are not uniform across whole slides, and there are various patterns for classification of cancer. Deep learning is ideal for such a scenario where automated feature learning is required further deep learning detects features that are invariant to the location in an image. Tumor region-wise classification of a slide can also be achieved which can be further subjected to recurrent neural networks. Textual explanations of cancer regions can be generated, providing further insights into diagnosis. Here, three different cancer datasets are reviewed and subjected to deep learning methods. Explainable AI approaches can be applied to decision trees as well as on gradient boosted trees to obtain explanations.},
  isbn = {9789811653018},
  langid = {english},
  keywords = {Cancer classification,Deep CNN,Deep learning,Explainable AI,Pathology}
}

@inproceedings{joshiReviewDeepLearning2022c,
  title = {Review of {{Deep Learning}} and {{Interpretability}}},
  booktitle = {Soft {{Computing}} for {{Security Applications}}},
  author = {Joshi, Hrushikesh and Rajeswari, Kannan and Joshi, Sneha},
  editor = {Ranganathan, G. and Fernando, Xavier and Shi, Fuqian and El Allioui, Youssouf},
  year = {2022},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {501--516},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-16-5301-8_37},
  abstract = {Deep learning has revolutionized machine learning. Deep convolution networks play a pivotal role in image classification. A conventional image classification technique requires extensive feature engineering; this approach is exhaustive as rigorous manual efforts are needed. Deep learning automatically detects the feature set required for classification. It uses backpropagation to achieve this. Deep learning is being used in a variety of applications including healthcare. The use of deep learning specifically in cancer biology is significant. For Pathologists, classifying tumors as benign or malignant is a complicated task. Classification of malignant lymphoma is challenging as features are not uniform across whole slides, and there are various patterns for classification of cancer. Deep learning is ideal for such a scenario where automated feature learning is required further deep learning detects features that are invariant to the location in an image. Tumor region-wise classification of a slide can also be achieved which can be further subjected to recurrent neural networks. Textual explanations of cancer regions can be generated, providing further insights into diagnosis. Here, three different cancer datasets are reviewed and subjected to deep learning methods. Explainable AI approaches can be applied to decision trees as well as on gradient boosted trees to obtain explanations.},
  isbn = {9789811653018},
  langid = {english},
  keywords = {Cancer classification,Deep CNN,Deep learning,Explainable AI,Pathology}
}

@article{joshiReviewExplainabilityMultimodal2021,
  title = {A {{Review}} on {{Explainability}} in {{Multimodal Deep Neural Nets}}},
  author = {Joshi, Gargi and Walambe, Rahee and Kotecha, Ketan},
  year = {2021},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {9},
  pages = {59800--59821},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3070212},
  abstract = {Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain.},
  keywords = {Artificial intelligence,Biomedical imaging,Data models,Deep learning,Deep multimodal learning,explainable AI,interpretability,Neural networks,survey,Task analysis,trends,vision and language research,Visualization,XAI},
  file = {C:\Users\cleme\Zotero\storage\Y6XFUEZZ\Joshi et al. - 2021 - A Review on Explainability in Multimodal Deep Neur.pdf}
}

@article{joshiReviewPreprocessingTechniques2017,
  title = {Review of {{Preprocessing Techniques}} for {{Fundus Image Analysis}}},
  author = {Joshi, Shilpa and Karule, P. T.},
  year = {2017},
  month = sep,
  journal = {Advances in Modelling and Analysis B},
  volume = {60},
  number = {3},
  pages = {593--612},
  issn = {12404543},
  doi = {10.18280/ama_b.600306},
  urldate = {2023-03-16},
  abstract = {The principal target of preprocessing is to get more appropriate resultant image than its original for further additional analysis. Enhancement of retinal images creates several challenges. The main obstacle is to develop a technique to accommodate the wide variation in contrast inside the image. Necessity of preprocessing methods are for image normalization and to increase the contrast for achieving accurate analysis. This work examined literature in the prior process of digital imaging, in the field of the analysis of fundus image to extract normal and pathologic retinal traits within the context of diabetic retinopathy (DR).},
  langid = {english}
}

@article{joshiReviewPreprocessingTechniques2017a,
  title = {Review of {{Preprocessing Techniques}} for {{Fundus Image Analysis}}},
  author = {Joshi, Shilpa and Karule, P. T.},
  year = {2017},
  month = sep,
  journal = {Advances in Modelling and Analysis B},
  volume = {60},
  number = {3},
  pages = {593--612},
  issn = {12404543},
  doi = {10.18280/ama_b.600306},
  urldate = {2023-03-16},
  abstract = {The principal target of preprocessing is to get more appropriate resultant image than its original for further additional analysis. Enhancement of retinal images creates several challenges. The main obstacle is to develop a technique to accommodate the wide variation in contrast inside the image. Necessity of preprocessing methods are for image normalization and to increase the contrast for achieving accurate analysis. This work examined literature in the prior process of digital imaging, in the field of the analysis of fundus image to extract normal and pathologic retinal traits within the context of diabetic retinopathy (DR).},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\F225IJNR\Joshi et Karule - 2017 - Review of Preprocessing Techniques for Fundus Imag.pdf}
}

@article{joyceExplainableArtificialIntelligence2023,
  title = {Explainable Artificial Intelligence for Mental Health through Transparency and Interpretability for Understandability},
  author = {Joyce, Dan W. and Kormilitzin, Andrey and Smith, Katharine A. and Cipriani, Andrea},
  year = {2023},
  month = jan,
  journal = {npj Digital Medicine},
  volume = {6},
  number = {1},
  pages = {1--7},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-023-00751-9},
  urldate = {2023-05-05},
  abstract = {The literature on artificial intelligence (AI) or machine learning (ML) in mental health and psychiatry lacks consensus on what ``explainability'' means. In the more general XAI (eXplainable AI) literature, there has been some convergence on explainability meaning model-agnostic techniques that augment a complex model (with internal mechanics intractable for human understanding) with a simpler model argued to deliver results that humans can comprehend. Given the differing usage and intended meaning of the term ``explainability'' in AI and ML, we propose instead to approximate model/algorithm explainability by understandability defined as a function of transparency and interpretability. These concepts are easier to articulate, to ``ground'' in our understanding of how algorithms and models operate and are used more consistently in the literature. We describe the TIFU (Transparency and Interpretability For Understandability) framework and examine how this applies to the landscape of AI/ML in mental health research. We argue that the need for understandablity is heightened in psychiatry because data describing the syndromes, outcomes, disorders and signs/symptoms possess probabilistic relationships to each other---as do the tentative aetiologies and multifactorial social- and psychological-determinants of disorders. If we develop and deploy AI/ML models, ensuring human understandability of the inputs, processes and outputs of these models is essential to develop trustworthy systems fit for deployment.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Health care}
}

@article{joyceExplainableArtificialIntelligence2023a,
  title = {Explainable Artificial Intelligence for Mental Health through Transparency and Interpretability for Understandability},
  author = {Joyce, Dan W. and Kormilitzin, Andrey and Smith, Katharine A. and Cipriani, Andrea},
  year = {2023},
  month = jan,
  journal = {npj Digital Medicine},
  volume = {6},
  number = {1},
  pages = {1--7},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-023-00751-9},
  urldate = {2023-05-05},
  abstract = {The literature on artificial intelligence (AI) or machine learning (ML) in mental health and psychiatry lacks consensus on what ``explainability'' means. In the more general XAI (eXplainable AI) literature, there has been some convergence on explainability meaning model-agnostic techniques that augment a complex model (with internal mechanics intractable for human understanding) with a simpler model argued to deliver results that humans can comprehend. Given the differing usage and intended meaning of the term ``explainability'' in AI and ML, we propose instead to approximate model/algorithm explainability by understandability defined as a function of transparency and interpretability. These concepts are easier to articulate, to ``ground'' in our understanding of how algorithms and models operate and are used more consistently in the literature. We describe the TIFU (Transparency and Interpretability For Understandability) framework and examine how this applies to the landscape of AI/ML in mental health research. We argue that the need for understandablity is heightened in psychiatry because data describing the syndromes, outcomes, disorders and signs/symptoms possess probabilistic relationships to each other---as do the tentative aetiologies and multifactorial social- and psychological-determinants of disorders. If we develop and deploy AI/ML models, ensuring human understandability of the inputs, processes and outputs of these models is essential to develop trustworthy systems fit for deployment.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Health care},
  file = {C:\Users\cleme\Zotero\storage\FMHN362X\Joyce et al. - 2023 - Explainable artificial intelligence for mental hea.pdf}
}

@article{kalawUltrawideFieldNew2023,
  title = {Ultra-Wide Field and New Wide Field Composite Retinal Image Registration with {{AI-enabled}} Pipeline and {{3D}} Distortion Correction Algorithm},
  author = {Kalaw, Fritz Gerald P. and Cavichini, Melina and Zhang, Junkang and Wen, Bo and Lin, Andrew C. and Heinke, Anna and Nguyen, Truong and An, Cheolhong and Bartsch, Dirk-Uwe G. and Cheng, Lingyun and Freeman, William R.},
  year = {2023},
  month = dec,
  journal = {Eye},
  pages = {1--7},
  publisher = {Nature Publishing Group},
  issn = {1476-5454},
  doi = {10.1038/s41433-023-02868-3},
  urldate = {2024-02-22},
  abstract = {This study aimed to compare a new Artificial Intelligence (AI) method to conventional mathematical warping in accurately overlaying peripheral retinal vessels from two different imaging devices: confocal scanning laser ophthalmoscope (cSLO) wide-field images and SLO ultra-wide field images.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Anatomy,Education,Medical imaging},
  file = {C:\Users\cleme\Zotero\storage\3WC9Z5EM\Kalaw et al. - 2023 - Ultra-wide field and new wide field composite reti.pdf}
}

@article{kalogeropoulosRoleTeleophthalmologyDiabetic2020,
  title = {The Role of Tele-Ophthalmology in Diabetic Retinopathy Screening},
  author = {Kalogeropoulos, Dimitrios and Kalogeropoulos, Chris and Stefaniotou, Maria and Neofytou, Marios},
  year = {2020},
  month = oct,
  journal = {Journal of Optometry},
  volume = {13},
  number = {4},
  pages = {262--268},
  issn = {1888-4296},
  doi = {10.1016/j.optom.2019.12.004},
  urldate = {2022-10-20},
  abstract = {Diabetic retinopathy (DR) is the leading cause of legal blindness in the United States. Considering the increasing incidence of DR, it is extremely important to detect the most cost-effective tools for DR screening, so as to manage this surge in demand and the socioeconomic burden it places on the health care system. Despite the advances in retinal imaging, analysis techniques are still superseded by expert ophthalmologist interpretation. Teleophthalmology presents an immense opportunity, with high rates of sensitivity and specificity, to manage the steadily increasing demand for eye care of patients with diabetes, but challenges remain in the delivery of practical, viable, and clinically proven solutions. RESUMEN La retinopat{\'i}a diab{\'e}tica (RD) es la causa principal de ceguera legal en los Estados Unidos. Teniendo en cuenta la creciente incidencia de RD, es extremadamente importante detectar las herramientas m{\'a}s econ{\'o}micas para su cribado, para poder gestionar esta demanda creciente, as{\'i} como la carga socioecon{\'o}mica que supone para el sistema sanitario. A pesar de los avances en t{\'e}rminos de imagen retiniana, las t{\'e}cnicas de an{\'a}lisis siguen siendo reemplazadas por la interpretaci{\'o}n de los oftalm{\'o}logos expertos. La tele-oftalmolog{\'i}a se presenta como una gran oportunidad, con altas tasas de sensibilidad y especificidad, para gestionar el aumento constante de la atenci{\'o}n ocular en los pacientes diab{\'e}ticos, aunque la aportaci{\'o}n de soluciones cl{\'i}nicamente probadas sigue suponiendo un reto.},
  langid = {english},
  keywords = {Cribado,Diabetic retinopathy,Retinopat\'ia diab\'etica,Screening,Tele-oftalmolog\'ia,Teleophthalmology}
}

@article{kalogeropoulosRoleTeleophthalmologyDiabetic2020a,
  title = {The Role of Tele-Ophthalmology in Diabetic Retinopathy Screening},
  author = {Kalogeropoulos, Dimitrios and Kalogeropoulos, Chris and Stefaniotou, Maria and Neofytou, Marios},
  year = {2020},
  month = oct,
  journal = {Journal of Optometry},
  volume = {13},
  number = {4},
  pages = {262--268},
  issn = {1888-4296},
  doi = {10.1016/j.optom.2019.12.004},
  urldate = {2022-10-20},
  abstract = {Diabetic retinopathy (DR) is the leading cause of legal blindness in the United States. Considering the increasing incidence of DR, it is extremely important to detect the most cost-effective tools for DR screening, so as to manage this surge in demand and the socioeconomic burden it places on the health care system. Despite the advances in retinal imaging, analysis techniques are still superseded by expert ophthalmologist interpretation. Teleophthalmology presents an immense opportunity, with high rates of sensitivity and specificity, to manage the steadily increasing demand for eye care of patients with diabetes, but challenges remain in the delivery of practical, viable, and clinically proven solutions. RESUMEN La retinopat{\'i}a diab{\'e}tica (RD) es la causa principal de ceguera legal en los Estados Unidos. Teniendo en cuenta la creciente incidencia de RD, es extremadamente importante detectar las herramientas m{\'a}s econ{\'o}micas para su cribado, para poder gestionar esta demanda creciente, as{\'i} como la carga socioecon{\'o}mica que supone para el sistema sanitario. A pesar de los avances en t{\'e}rminos de imagen retiniana, las t{\'e}cnicas de an{\'a}lisis siguen siendo reemplazadas por la interpretaci{\'o}n de los oftalm{\'o}logos expertos. La tele-oftalmolog{\'i}a se presenta como una gran oportunidad, con altas tasas de sensibilidad y especificidad, para gestionar el aumento constante de la atenci{\'o}n ocular en los pacientes diab{\'e}ticos, aunque la aportaci{\'o}n de soluciones cl{\'i}nicamente probadas sigue suponiendo un reto.},
  langid = {english},
  keywords = {Cribado,Diabetic retinopathy,Retinopatia diabetica,Screening,Tele-oftalmologia,Teleophthalmology},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\4HPHYXDC\\kalogeropoulos2020.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\BXLLT4N6\\Kalogeropoulos et al. - 2020 - The role of tele-ophthalmology in diabetic retinop.pdf}
}

@inproceedings{kamranOpticNetNovelConvolutional2019,
  title = {Optic-{{Net}}: {{A Novel Convolutional Neural Network}} for {{Diagnosis}} of {{Retinal Diseases}} from {{Optical Tomography Images}}},
  shorttitle = {Optic-{{Net}}},
  booktitle = {2019 18th {{IEEE International Conference On Machine Learning And Applications}} ({{ICMLA}})},
  author = {Kamran, S. A. and Saha, S. and Sabbir, A. S. and Tavakkoli, A.},
  year = {2019},
  month = dec,
  pages = {964--971},
  doi = {10.1109/ICMLA.2019.00165},
  abstract = {Diagnosing different retinal diseases from Spectral Domain Optical Coherence Tomography (SD-OCT) images is a challenging task. Different automated approaches such as image processing, machine learning and deep learning algorithms have been used for early detection and diagnosis of retinal diseases. Unfortunately, these are prone to error and computational inefficiency, which requires further intervention from human experts. In this paper, we propose a novel convolution neural network architecture to successfully distinguish between different degeneration of retinal layers and their underlying causes. The proposed novel architecture outperforms other classification models while addressing the issue of gradient explosion. Our approach reaches near perfect accuracy of 99.8\% and 100\% for two separately available Retinal SD-OCT data-set respectively. Additionally, our architecture predicts retinal diseases in real time while outperforming human diagnosticians.},
  keywords = {biomedical optical imaging,Computer architecture,Computer Vision,Convolution,convolution neural network architecture,convolutional neural nets,Convolutional Neural Networks,Deep Learning,deep learning algorithms,Diabetes,diseases,Diseases,eye,feature extraction,human diagnostician,image processing,image segmentation,learning (artificial intelligence),machine learning,Machine learning,medical image processing,Optic-net,Optical imaging,optical tomography,Residual Neural Network,Retina,Retinal Degeneration,retinal diseases,retinal layers,retinal SD-OCT data,SD-OCT,spectral domain optical coherence tomography}
}

@inproceedings{kamranOpticNetNovelConvolutional2019a,
  title = {Optic-{{Net}}: {{A Novel Convolutional Neural Network}} for {{Diagnosis}} of {{Retinal Diseases}} from {{Optical Tomography Images}}},
  shorttitle = {Optic-{{Net}}},
  booktitle = {2019 18th {{IEEE International Conference On Machine Learning And Applications}} ({{ICMLA}})},
  author = {Kamran, S. A. and Saha, S. and Sabbir, A. S. and Tavakkoli, A.},
  year = {2019},
  month = dec,
  pages = {964--971},
  doi = {10.1109/ICMLA.2019.00165},
  abstract = {Diagnosing different retinal diseases from Spectral Domain Optical Coherence Tomography (SD-OCT) images is a challenging task. Different automated approaches such as image processing, machine learning and deep learning algorithms have been used for early detection and diagnosis of retinal diseases. Unfortunately, these are prone to error and computational inefficiency, which requires further intervention from human experts. In this paper, we propose a novel convolution neural network architecture to successfully distinguish between different degeneration of retinal layers and their underlying causes. The proposed novel architecture outperforms other classification models while addressing the issue of gradient explosion. Our approach reaches near perfect accuracy of 99.8\% and 100\% for two separately available Retinal SD-OCT data-set respectively. Additionally, our architecture predicts retinal diseases in real time while outperforming human diagnosticians.},
  keywords = {biomedical optical imaging,Computer architecture,Computer Vision,Convolution,convolution neural network architecture,convolutional neural nets,Convolutional Neural Networks,Deep Learning,deep learning algorithms,Diabetes,diseases,Diseases,eye,feature extraction,human diagnostician,image processing,image segmentation,learning (artificial intelligence),machine learning,Machine learning,medical image processing,Optic-net,Optical imaging,optical tomography,Residual Neural Network,Retina,Retinal Degeneration,retinal diseases,retinal layers,retinal SD-OCT data,SD-OCT,spectral domain optical coherence tomography},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\L2ENQJQK\\Kamran et al. - 2019 - Optic-Net A Novel Convolutional Neural Network fo.pdf;C\:\\Users\\cleme\\Zotero\\storage\\HFVWZABI\\8999264.html}
}

@article{kankanahalliAutomatedClassificationSeverity2013,
  title = {Automated {{Classification}} of {{Severity}} of {{Age-Related Macular Degeneration}} from {{Fundus Photographs}}},
  author = {Kankanahalli, Srihari and Burlina, Philippe M. and Wolfson, Yulia and Freund, David E. and Bressler, Neil M.},
  year = {2013},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {54},
  number = {3},
  pages = {1789--1796},
  issn = {1552-5783},
  doi = {10.1167/iovs.12-10928},
  urldate = {2020-01-13},
  langid = {english}
}

@article{kankanahalliAutomatedClassificationSeverity2013a,
  title = {Automated {{Classification}} of {{Severity}} of {{Age-Related Macular Degeneration}} from {{Fundus Photographs}}},
  author = {Kankanahalli, Srihari and Burlina, Philippe M. and Wolfson, Yulia and Freund, David E. and Bressler, Neil M.},
  year = {2013},
  month = mar,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {54},
  number = {3},
  pages = {1789--1796},
  issn = {1552-5783},
  doi = {10.1167/iovs.12-10928},
  urldate = {2020-01-13},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\7674QJTD\\Kankanahalli et al. - 2013 - Automated Classification of Severity of Age-Relate.pdf;C\:\\Users\\cleme\\Zotero\\storage\\DTS2XG9E\\article.html}
}

@inproceedings{karimiConvolutionFreeMedicalImage2021,
  title = {Convolution-{{Free Medical Image Segmentation Using Transformers}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Karimi, Davood and Vasylechko, Serge Didenko and Gholipour, Ali},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {78--88},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_8},
  abstract = {Like other applications in computer vision, medical image segmentation and his email address have been most successfully addressed using deep learning models that rely on the convolution operation as their main building block. Convolutions enjoy important properties such as sparse interactions, weight sharing, and translation equivariance. These properties give convolutional neural networks (CNNs) a strong and useful inductive bias for vision tasks. However, the convolution operation also has important shortcomings: it performs a fixed operation on every test image regardless of the content and it cannot efficiently model long-range interactions. In this work we show that a network based on self-attention between neighboring patches and without any convolution operations can achieve better results. Given a 3D image block, our network divides it into n3n3n{\^3} 3D patches, where n=3 or 5n=3 or 5n=3 {\textbackslash}text \{ or \} 5 and computes a 1D embedding for each patch. The network predicts the segmentation map for the center patch of the block based on the self-attention between these patch embeddings. We show that the proposed model can achieve higher segmentation accuracies than a state of the art CNN. For scenarios with very few labeled images, we propose methods for pre-training the network on large corpora of unlabeled images. Our experiments show that with pre-training the advantage of our proposed network over CNNs can be significant when labeled training data is small.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Attention,Deep learning,Segmentation,Transformers}
}

@inproceedings{karimiConvolutionFreeMedicalImage2021a,
  title = {Convolution-{{Free Medical Image Segmentation Using Transformers}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Karimi, Davood and Vasylechko, Serge Didenko and Gholipour, Ali},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {78--88},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_8},
  abstract = {Like other applications in computer vision, medical image segmentation and his email address have been most successfully addressed using deep learning models that rely on the convolution operation as their main building block. Convolutions enjoy important properties such as sparse interactions, weight sharing, and translation equivariance. These properties give convolutional neural networks (CNNs) a strong and useful inductive bias for vision tasks. However, the convolution operation also has important shortcomings: it performs a fixed operation on every test image regardless of the content and it cannot efficiently model long-range interactions. In this work we show that a network based on self-attention between neighboring patches and without any convolution operations can achieve better results. Given a 3D image block, our network divides it into n3n3n{\^3} 3D patches, where n=3 or 5n=3 or 5n=3 {\textbackslash}text \{ or \} 5 and computes a 1D embedding for each patch. The network predicts the segmentation map for the center patch of the block based on the self-attention between these patch embeddings. We show that the proposed model can achieve higher segmentation accuracies than a state of the art CNN. For scenarios with very few labeled images, we propose methods for pre-training the network on large corpora of unlabeled images. Our experiments show that with pre-training the advantage of our proposed network over CNNs can be significant when labeled training data is small.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Attention,Deep learning,Segmentation,Transformers}
}

@inproceedings{karimiConvolutionFreeMedicalImage2021b,
  title = {Convolution-{{Free Medical Image Segmentation Using Transformers}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Karimi, Davood and Vasylechko, Serge Didenko and Gholipour, Ali},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {78--88},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_8},
  abstract = {Like other applications in computer vision, medical image segmentation and his email address have been most successfully addressed using deep learning models that rely on the convolution operation as their main building block. Convolutions enjoy important properties such as sparse interactions, weight sharing, and translation equivariance. These properties give convolutional neural networks (CNNs) a strong and useful inductive bias for vision tasks. However, the convolution operation also has important shortcomings: it performs a fixed operation on every test image regardless of the content and it cannot efficiently model long-range interactions. In this work we show that a network based on self-attention between neighboring patches and without any convolution operations can achieve better results. Given a 3D image block, our network divides it into n3n3n{\textasciicircum}3 3D patches, where n=3 or 5n=3 or 5n=3 {\textbackslash}text \{ or \} 5 and computes a 1D embedding for each patch. The network predicts the segmentation map for the center patch of the block based on the self-attention between these patch embeddings. We show that the proposed model can achieve higher segmentation accuracies than a state of the art CNN. For scenarios with very few labeled images, we propose methods for pre-training the network on large corpora of unlabeled images. Our experiments show that with pre-training the advantage of our proposed network over CNNs can be significant when labeled training data is small.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Attention,Deep learning,Segmentation,Transformers},
  file = {C:\Users\cleme\Zotero\storage\GHZWW3TD\Karimi et al. - 2021 - Convolution-Free Medical Image Segmentation Using .pdf}
}

@inproceedings{karimiConvolutionFreeMedicalImage2021c,
  title = {Convolution-{{Free Medical Image Segmentation Using Transformers}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Karimi, Davood and Vasylechko, Serge Didenko and Gholipour, Ali},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {78--88},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_8},
  abstract = {Like other applications in computer vision, medical image segmentation and his email address have been most successfully addressed using deep learning models that rely on the convolution operation as their main building block. Convolutions enjoy important properties such as sparse interactions, weight sharing, and translation equivariance. These properties give convolutional neural networks (CNNs) a strong and useful inductive bias for vision tasks. However, the convolution operation also has important shortcomings: it performs a fixed operation on every test image regardless of the content and it cannot efficiently model long-range interactions. In this work we show that a network based on self-attention between neighboring patches and without any convolution operations can achieve better results. Given a 3D image block, our network divides it into n3n3n{\textasciicircum}3 3D patches, where n=3 or 5n=3 or 5n=3 {\textbackslash}text \{ or \} 5 and computes a 1D embedding for each patch. The network predicts the segmentation map for the center patch of the block based on the self-attention between these patch embeddings. We show that the proposed model can achieve higher segmentation accuracies than a state of the art CNN. For scenarios with very few labeled images, we propose methods for pre-training the network on large corpora of unlabeled images. Our experiments show that with pre-training the advantage of our proposed network over CNNs can be significant when labeled training data is small.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Attention,Deep learning,Segmentation,Transformers},
  file = {C:\Users\cleme\Zotero\storage\Y8VWUHHC\Karimi et al. - 2021 - Convolution-Free Medical Image Segmentation Using .pdf}
}

@article{karriTransferLearningBased2017,
  title = {Transfer Learning Based Classification of Optical Coherence Tomography Images with Diabetic Macular Edema and Dry Age-Related Macular Degeneration},
  author = {Karri, S. P. K. and Chakraborty, Debjani and Chatterjee, Jyotirmoy},
  year = {2017},
  month = feb,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {2},
  pages = {579--592},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.000579},
  abstract = {We present an algorithm for identifying retinal pathologies given retinal optical coherence tomography (OCT) images. Our approach fine-tunes a pre-trained convolutional neural network (CNN), GoogLeNet, to improve its prediction capability (compared to random initialization training) and identifies salient responses during prediction to understand learned filter characteristics. We considered a data set containing subjects with diabetic macular edema, or dry age-related macular degeneration, or no pathology. The fine-tuned CNN could effectively identify pathologies in comparison to classical learning. Our algorithm aims to demonstrate that models trained on non-medical images can be fine-tuned for classifying OCT images with limited training data.},
  langid = {english},
  pmcid = {PMC5330546},
  pmid = {28270969},
  keywords = {(070.5010) Pattern recognition,(100.2960) Image analysis,(110.4500) Optical coherence tomography,(170.1610) Clinical applications,(170.4470) Ophthalmology}
}

@article{karriTransferLearningBased2017a,
  title = {Transfer Learning Based Classification of Optical Coherence Tomography Images with Diabetic Macular Edema and Dry Age-Related Macular Degeneration},
  author = {Karri, S. P. K. and Chakraborty, Debjani and Chatterjee, Jyotirmoy},
  year = {2017},
  month = feb,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {2},
  pages = {579--592},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.000579},
  abstract = {We present an algorithm for identifying retinal pathologies given retinal optical coherence tomography (OCT) images. Our approach fine-tunes a pre-trained convolutional neural network (CNN), GoogLeNet, to improve its prediction capability (compared to random initialization training) and identifies salient responses during prediction to understand learned filter characteristics. We considered a data set containing subjects with diabetic macular edema, or dry age-related macular degeneration, or no pathology. The fine-tuned CNN could effectively identify pathologies in comparison to classical learning. Our algorithm aims to demonstrate that models trained on non-medical images can be fine-tuned for classifying OCT images with limited training data.},
  langid = {english},
  pmcid = {PMC5330546},
  pmid = {28270969},
  keywords = {(070.5010) Pattern recognition,(100.2960) Image analysis,(110.4500) Optical coherence tomography,(170.1610) Clinical applications,(170.4470) Ophthalmology},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\6UUBQQ3Y\\karri2017.pdf;C\:\\Users\\cleme\\Zotero\\storage\\72YNAH38\\Karri et al. - 2017 - Transfer learning based classification of optical .pdf}
}

@misc{karthikAPTOS2019Blindness2019a,
  title = {{{APTOS}} 2019 {{Blindness Detection}}},
  author = {Karthik, Maggie and Sohier, Dane},
  year = {2019},
  urldate = {2021-11-26},
  abstract = {Detect diabetic retinopathy to stop blindness before it's too late},
  howpublished = {https://kaggle.com/c/aptos2019-blindness-detection},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\SZ5VRWIG\aptos-2019.html}
}

@article{katharopoulosTransformersAreRNNs2020,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = aug,
  journal = {arXiv:2006.16236 [cs, stat]},
  eprint = {2006.16236},
  primaryclass = {cs, stat},
  urldate = {2021-12-13},
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O N 2 to O (N ), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{katharopoulosTransformersAreRNNs2020a,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = nov,
  pages = {5156--5165},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2021-12-13},
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from {\textbackslash}bigON2{\textbackslash}bigON2{\textbackslash}bigO\{N{\^2}\} to {\textbackslash}bigON{\textbackslash}bigON{\textbackslash}bigO\{N\}, where NNN is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our {\textbackslash}emph\{Linear Transformers\} achieve similar performance to vanilla Transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  langid = {english}
}

@inproceedings{katharopoulosTransformersAreRNNs2020b,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = nov,
  pages = {5156--5165},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2021-12-13},
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from {\textbackslash}bigON2{\textbackslash}bigON2{\textbackslash}bigO\{N{\textasciicircum}2\} to {\textbackslash}bigON{\textbackslash}bigON{\textbackslash}bigO\{N\}, where NNN is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our {\textbackslash}emph\{Linear Transformers\} achieve similar performance to vanilla Transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\YRKBSH8M\\Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZXNVWKP5\\Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf}
}

@article{katharopoulosTransformersAreRNNs2020c,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = aug,
  journal = {arXiv:2006.16236 [cs, stat]},
  eprint = {2006.16236},
  primaryclass = {cs, stat},
  urldate = {2021-12-13},
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O N 2 to O (N ), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\34I6Y6LE\Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf}
}

@article{kauppiDIARETDB1DiabeticRetinopathy,
  title = {{{DIARETDB1}} Diabetic Retinopathy Database and Evaluation Protocol},
  author = {Kauppi, Tomi and Kalesnykiene, Valentina and Kamarainen, Joni-Kristian and Lensu, Lasse and Sorri, Iiris and Raninen, Asta and Voutilainen, Raija and Uusitalo, Hannu and Kalviainen, Heikki and Pietila, Juhani},
  pages = {18},
  abstract = {For a particularly long time, automatic diagnosis of diabetic retinopathy from digital fundus images has been an active research topic in the medical image processing community. The research interest is justified by the excellent potential for new products in the medical industry and significant reductions in health care costs. However, the maturity of proposed algorithms cannot be judged due to the lack of commonly accepted and representative image database with a verified ground truth and strict evaluation protocol. In this study, an evaluation methodology is proposed and an image database with ground truth is described. The database is publicly available for benchmarking diagnosis algorithms. With the proposed database and protocol, it is possible to compare different algorithms, and correspondingly, analyse their maturity for technology transfer from the research laboratories to the medical practice.},
  langid = {english}
}

@article{kauppiDIARETDB1DiabeticRetinopathy2007,
  title = {{{DIARETDB1}} Diabetic Retinopathy Database and Evaluation Protocol},
  author = {Kauppi, T and Kalesnykienne, V and Kamareinen, J.-K and Lensu, L. and Sorri, I and Raninen, A. and Voutilainen, R. and Pietil{\"a}, J. and K{\"a}lvi{\"a}inen, H. and Uusitalo, H.},
  year = {2007},
  journal = {Medical Image Understanding and Analysis},
  volume = {2007},
  pages = {61}
}

@article{kauppiDIARETDB1DiabeticRetinopathy2007a,
  title = {{{DIARETDB1}} Diabetic Retinopathy Database and Evaluation Protocol},
  author = {Kauppi, T and Kalesnykienne, V and Kamareinen, J.-K and Lensu, L. and Sorri, I and Raninen, A. and Voutilainen, R. and Pietil{\"a}, J. and K{\"a}lvi{\"a}inen, H. and Uusitalo, H.},
  year = {2007},
  journal = {Medical Image Understanding and Analysis},
  volume = {2007},
  pages = {61}
}

@article{kauppiDIARETDB1DiabeticRetinopathya,
  title = {{{DIARETDB1}} Diabetic Retinopathy Database and Evaluation Protocol},
  author = {Kauppi, Tomi and Kalesnykiene, Valentina and Kamarainen, Joni-Kristian and Lensu, Lasse and Sorri, Iiris and Raninen, Asta and Voutilainen, Raija and Uusitalo, Hannu and Kalviainen, Heikki and Pietila, Juhani},
  pages = {18},
  abstract = {For a particularly long time, automatic diagnosis of diabetic retinopathy from digital fundus images has been an active research topic in the medical image processing community. The research interest is justified by the excellent potential for new products in the medical industry and significant reductions in health care costs. However, the maturity of proposed algorithms cannot be judged due to the lack of commonly accepted and representative image database with a verified ground truth and strict evaluation protocol. In this study, an evaluation methodology is proposed and an image database with ground truth is described. The database is publicly available for benchmarking diagnosis algorithms. With the proposed database and protocol, it is possible to compare different algorithms, and correspondingly, analyse their maturity for technology transfer from the research laboratories to the medical practice.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\4FF5P7NQ\Kauppi et al. - DIARETDB1 diabetic retinopathy database and evalua.pdf}
}

@article{kaurGeneralizedMethodSegmentation2018,
  title = {A Generalized Method for the Segmentation of Exudates from Pathological Retinal Fundus Images},
  author = {Kaur, Jaskirat and Mittal, Deepti},
  year = {2018},
  month = jan,
  journal = {Biocybernetics and Biomedical Engineering},
  volume = {38},
  number = {1},
  pages = {27--53},
  issn = {0208-5216},
  doi = {10.1016/j.bbe.2017.10.003},
  urldate = {2019-07-24},
  abstract = {Diabetic retinopathy, an asymptomatic complication of diabetes, is one of the leading causes of blindness in the world. The exudates, abnormal leaked fatty deposits on retina, are one of the most prevalent and earliest clinical signs of diabetic retinopathy. In this paper, a generalized exudates segmentation method to assist ophthalmologists for timely treatment and effective planning in the diagnosis of diabetic retinopathy is developed. The main contribution of the proposed method is the reliable segmentation of exudates using dynamic decision thresholding irrespective of associated heterogeneity, bright and faint edges. The method is robust in the sense that it selects the threshold value dynamically irrespective of the large variations in retinal fundus images from varying databases. Since no performance comparison of state of the art methods is available on common database, therefore, to make a fair comparison of the proposed method, this work has been performed on a diversified database having 1307 retinal fundus images of varying characteristics namely: location, shapes, color and sizes. The database comprises of 649 clinically acquired retinal fundus images from eye hospital and 658 retinal images from publicly available databases such as STARE, MESSIDOR, DIARETDB1 and e-Optha EX. The segmentation results are validated by performing two sets of experiments namely: lesion based evaluation criteria and image based evaluation criteria. Experimental results at lesion level show that the proposed method outperforms other existing methods with a mean sensitivity/specificity/accuracy of 88.85/96.15/93.46 on a composite database of retinal fundus images. The segmentation results for image-based evaluation with a mean sensitivity/specificity/accuracy of 94.62/98.64/96.74 respectively prove the clinical effectiveness of the method. Furthermore, the significant collective performance of these experiments on clinically as well as publicly available standard databases proves the generalization ability and the strong candidature of the proposed method in the real-time diagnosis of diabetic retinopathy.},
  keywords = {Diabetic retinopathy,Exudates,Lesion and image based evaluation,Retinal fundus Image,Segmentation},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\UUKVL7VW\\Kaur et Mittal - 2018 - A generalized method for the segmentation of exuda.pdf;C\:\\Users\\cleme\\Zotero\\storage\\IP44TW3W\\S0208521617302711.html}
}

@misc{kawaguchiGeneralizationDeepLearning2017,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Pack Kaelbling, Leslie and Bengio, Yoshua},
  year = {2017},
  month = oct,
  doi = {10.48550/arXiv.1710.05468},
  urldate = {2023-04-13},
  abstract = {This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@misc{kawaguchiGeneralizationDeepLearning2017a,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Pack Kaelbling, Leslie and Bengio, Yoshua},
  year = {2017},
  month = oct,
  journal = {arXiv e-prints},
  doi = {10.48550/arXiv.1710.05468},
  urldate = {2023-04-13},
  abstract = {This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {ADS Bibcode: 2017arXiv171005468K},
  file = {C:\Users\cleme\Zotero\storage\KKWAWPNT\Kawaguchi et al. - 2017 - Generalization in Deep Learning.pdf}
}

@article{kawaguchiGeneralizationDeepLearning2018,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  year = {2018},
  month = may,
  urldate = {2023-04-13},
  abstract = {With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, answering to an open question in the literature. We also discuss limitations of our results and propose additional open problems.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english}
}

@article{kawaguchiGeneralizationDeepLearning2018a,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  year = {2018},
  month = may,
  urldate = {2023-04-13},
  abstract = {With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, answering to an open question in the literature. We also discuss limitations of our results and propose additional open problems.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english},
  annotation = {Accepted: 2018-05-09T19:55:51Z},
  file = {C:\Users\cleme\Zotero\storage\WAUHPJTR\Kawaguchi et al. - 2018 - Generalization in Deep Learning.pdf}
}

@inproceedings{kaymakAutomatedAgeRelatedMacular2018,
  title = {Automated {{Age-Related Macular Degeneration}} and {{Diabetic Macular Edema Detection}} on {{OCT Images}} Using {{Deep Learning}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on {{Intelligent Computer Communication}} and {{Processing}} ({{ICCP}})},
  author = {Kaymak, Sertan and Serener, Ali},
  year = {2018},
  month = sep,
  pages = {265--269},
  doi = {10.1109/ICCP.2018.8516635},
  abstract = {Age-related macular degeneration (AMD) is an eye disease that damages the retina, causing vision loss. Diabetic macular edema (DME) is also a form of vision loss for diabetic people. It is therefore crucial to detect AMD and DME in the early stages for the timely treatment of the eye and the prevention of any vision impairment. Automatic detection of DME and AMD on optical coherence tomography (OCT) images are presented in this paper. The method used is based on training a deep learning algorithm to classify them into healthy, dry AMD, wet AMD and DME categories. This method outperforms a transfer learning based method proposed recently in the literature for classification of OCT images into AMD and DME categories.},
  keywords = {Diabetes,Feature extraction,Neural networks,Sensitivity,Task analysis,Training}
}

@inproceedings{kaymakAutomatedAgeRelatedMacular2018a,
  title = {Automated {{Age-Related Macular Degeneration}} and {{Diabetic Macular Edema Detection}} on {{OCT Images}} Using {{Deep Learning}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on {{Intelligent Computer Communication}} and {{Processing}} ({{ICCP}})},
  author = {Kaymak, Sertan and Serener, Ali},
  year = {2018},
  month = sep,
  pages = {265--269},
  doi = {10.1109/ICCP.2018.8516635},
  abstract = {Age-related macular degeneration (AMD) is an eye disease that damages the retina, causing vision loss. Diabetic macular edema (DME) is also a form of vision loss for diabetic people. It is therefore crucial to detect AMD and DME in the early stages for the timely treatment of the eye and the prevention of any vision impairment. Automatic detection of DME and AMD on optical coherence tomography (OCT) images are presented in this paper. The method used is based on training a deep learning algorithm to classify them into healthy, dry AMD, wet AMD and DME categories. This method outperforms a transfer learning based method proposed recently in the literature for classification of OCT images into AMD and DME categories.},
  keywords = {Diabetes,Feature extraction,Neural networks,Sensitivity,Task analysis,Training}
}

@inproceedings{kaymakAutomatedAgeRelatedMacular2018b,
  title = {Automated {{Age-Related Macular Degeneration}} and {{Diabetic Macular Edema Detection}} on {{OCT Images}} Using {{Deep Learning}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on {{Intelligent Computer Communication}} and {{Processing}} ({{ICCP}})},
  author = {Kaymak, Sertan and Serener, Ali},
  year = {2018},
  month = sep,
  pages = {265--269},
  doi = {10.1109/ICCP.2018.8516635},
  abstract = {Age-related macular degeneration (AMD) is an eye disease that damages the retina, causing vision loss. Diabetic macular edema (DME) is also a form of vision loss for diabetic people. It is therefore crucial to detect AMD and DME in the early stages for the timely treatment of the eye and the prevention of any vision impairment. Automatic detection of DME and AMD on optical coherence tomography (OCT) images are presented in this paper. The method used is based on training a deep learning algorithm to classify them into healthy, dry AMD, wet AMD and DME categories. This method outperforms a transfer learning based method proposed recently in the literature for classification of OCT images into AMD and DME categories.},
  keywords = {Diabetes,Feature extraction,Neural networks,Sensitivity,Task analysis,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\XHLXQ7GJ\\Kaymak et Serener - 2018 - Automated Age-Related Macular Degeneration and Dia.pdf;C\:\\Users\\cleme\\Zotero\\storage\\2VCAL8C4\\8516635.html}
}

@inproceedings{kaymakAutomatedAgeRelatedMacular2018c,
  title = {Automated {{Age-Related Macular Degeneration}} and {{Diabetic Macular Edema Detection}} on {{OCT Images}} Using {{Deep Learning}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on {{Intelligent Computer Communication}} and {{Processing}} ({{ICCP}})},
  author = {Kaymak, Sertan and Serener, Ali},
  year = {2018},
  month = sep,
  pages = {265--269},
  doi = {10.1109/ICCP.2018.8516635},
  abstract = {Age-related macular degeneration (AMD) is an eye disease that damages the retina, causing vision loss. Diabetic macular edema (DME) is also a form of vision loss for diabetic people. It is therefore crucial to detect AMD and DME in the early stages for the timely treatment of the eye and the prevention of any vision impairment. Automatic detection of DME and AMD on optical coherence tomography (OCT) images are presented in this paper. The method used is based on training a deep learning algorithm to classify them into healthy, dry AMD, wet AMD and DME categories. This method outperforms a transfer learning based method proposed recently in the literature for classification of OCT images into AMD and DME categories.},
  keywords = {Diabetes,Feature extraction,Neural networks,Sensitivity,Task analysis,Training},
  file = {C:\Users\cleme\Zotero\storage\CSQBGDI8\8516635.html}
}

@article{keaneEyeAIAutonomous2018,
  title = {With an Eye to {{AI}} and Autonomous Diagnosis},
  author = {Keane, Pearse A. and Topol, Eric J.},
  year = {2018},
  month = aug,
  journal = {npj Digital Medicine},
  volume = {1},
  number = {1},
  pages = {1--3},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-018-0048-y},
  urldate = {2022-07-10},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Preventive medicine,Randomized controlled trials},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\327X4Y8X\\keane2018.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\SI9XWNIS\\Keane et Topol - 2018 - With an eye to AI and autonomous diagnosis.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZDDHMSYN\\s41746-018-0048-y.html}
}

@article{kermanyIdentifyingMedicalDiagnoses2018,
  title = {Identifying {{Medical Diagnoses}} and {{Treatable Diseases}} by {{Image-Based Deep Learning}}},
  author = {Kermany, Daniel S. and Goldbaum, Michael and Cai, Wenjia and Valentim, Carolina C. S. and Liang, Huiying and Baxter, Sally L. and McKeown, Alex and Yang, Ge and Wu, Xiaokang and Yan, Fangbing and Dong, Justin and Prasadha, Made K. and Pei, Jacqueline and Ting, Magdalene Y. L. and Zhu, Jie and Li, Christina and Hewett, Sierra and Dong, Jason and Ziyar, Ian and Shi, Alexander and Zhang, Runze and Zheng, Lianghong and Hou, Rui and Shi, William and Fu, Xin and Duan, Yaou and Huu, Viet A. N. and Wen, Cindy and Zhang, Edward D. and Zhang, Charlotte L. and Li, Oulan and Wang, Xiaobo and Singer, Michael A. and Sun, Xiaodong and Xu, Jie and Tafreshi, Ali and Lewis, M. Anthony and Xia, Huimin and Zhang, Kang},
  year = {2018},
  month = feb,
  journal = {Cell},
  volume = {172},
  number = {5},
  pages = {1122-1131.e9},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2018.02.010},
  urldate = {2021-03-02},
  abstract = {The implementation of clinical-decision support algorithms for medical imaging faces challenges with reliability and interpretability. Here, we establish a diagnostic tool based on a deep-learning framework for the screening of patients with common treatable blinding retinal diseases. Our framework utilizes transfer learning, which trains a neural network with a fraction of the data of conventional approaches. Applying this approach to a dataset of optical coherence tomography images, we demonstrate performance comparable to that of human experts in classifying age-related macular degeneration and diabetic macular edema. We also provide a more transparent and interpretable diagnosis by highlighting the regions recognized by the neural network. We further demonstrate the general applicability of our AI system for diagnosis of pediatric pneumonia using chest X-ray images. This tool may ultimately aid in expediting the diagnosis and referral of these treatable conditions, thereby facilitating earlier treatment, resulting in improved clinical outcomes. Video Abstract},
  langid = {english},
  keywords = {age-related macular degeneration,artificial intelligence,choroidal neovascularization,deep learning,diabetic macular edema,diabetic retinopathy,optical coherence tomography,pneumonia,screening,transfer learning}
}

@article{kermanyIdentifyingMedicalDiagnoses2018a,
  title = {Identifying {{Medical Diagnoses}} and {{Treatable Diseases}} by {{Image-Based Deep Learning}}},
  author = {Kermany, Daniel S. and Goldbaum, Michael and Cai, Wenjia and Valentim, Carolina C. S. and Liang, Huiying and Baxter, Sally L. and McKeown, Alex and Yang, Ge and Wu, Xiaokang and Yan, Fangbing and Dong, Justin and Prasadha, Made K. and Pei, Jacqueline and Ting, Magdalene Y. L. and Zhu, Jie and Li, Christina and Hewett, Sierra and Dong, Jason and Ziyar, Ian and Shi, Alexander and Zhang, Runze and Zheng, Lianghong and Hou, Rui and Shi, William and Fu, Xin and Duan, Yaou and Huu, Viet A. N. and Wen, Cindy and Zhang, Edward D. and Zhang, Charlotte L. and Li, Oulan and Wang, Xiaobo and Singer, Michael A. and Sun, Xiaodong and Xu, Jie and Tafreshi, Ali and Lewis, M. Anthony and Xia, Huimin and Zhang, Kang},
  year = {2018},
  month = feb,
  journal = {Cell},
  volume = {172},
  number = {5},
  pages = {1122-1131.e9},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2018.02.010},
  urldate = {2021-03-02},
  abstract = {The implementation of clinical-decision support algorithms for medical imaging faces challenges with reliability and interpretability. Here, we establish a diagnostic tool based on a deep-learning framework for the screening of patients with common treatable blinding retinal diseases. Our framework utilizes transfer learning, which trains a neural network with a fraction of the data of conventional approaches. Applying this approach to a dataset of optical coherence tomography images, we demonstrate performance comparable to that of human experts in classifying age-related~macular degeneration and diabetic macular~edema. We also provide a more transparent and~interpretable diagnosis by highlighting the regions recognized by the neural network. We further demonstrate the general applicability of our AI system for diagnosis of pediatric pneumonia~using chest X-ray images. This tool may ultimately aid in expediting the diagnosis and referral~of these treatable conditions, thereby facilitating earlier treatment, resulting in improved clinical outcomes. Video Abstract},
  langid = {english},
  keywords = {age-related macular degeneration,artificial intelligence,choroidal neovascularization,deep learning,diabetic macular edema,diabetic retinopathy,optical coherence tomography,pneumonia,screening,transfer learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5FUZIWZK\\Kermany et al. - 2018 - Identifying Medical Diagnoses and Treatable Diseas.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ARLD32UA\\S0092867418301545.html}
}

@article{khanGlobalReviewPublicly2021a,
  title = {A Global Review of Publicly Available Datasets for Ophthalmological Imaging: Barriers to Access, Usability, and Generalisability},
  shorttitle = {A Global Review of Publicly Available Datasets for Ophthalmological Imaging},
  author = {Khan, Saad M. and Liu, Xiaoxuan and Nath, Siddharth and Korot, Edward and Faes, Livia and Wagner, Siegfried K. and Keane, Pearse A. and Sebire, Neil J. and Burton, Matthew J. and Denniston, Alastair K.},
  year = {2021},
  month = jan,
  journal = {The Lancet Digital Health},
  volume = {3},
  number = {1},
  pages = {e51-e66},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(20)30240-5},
  urldate = {2022-07-09},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5D8XF46D\\Khan et al. - 2021 - A global review of publicly available datasets for.pdf;C\:\\Users\\cleme\\Zotero\\storage\\VQ8XK3QU\\10.1016@S2589-75002030240-5.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UD6NHKBY\\fulltext.html}
}

@article{khanSurveyRecentArchitectures2020,
  title = {A Survey of the Recent Architectures of Deep Convolutional Neural Networks},
  author = {Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
  year = {2020},
  month = dec,
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {8},
  pages = {5455--5516},
  issn = {1573-7462},
  doi = {10.1007/s10462-020-09825-6},
  urldate = {2023-02-11},
  abstract = {Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN architectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided.},
  langid = {english},
  keywords = {Channel boosted CNN,Convolutional neural networks,Deep learning,Representational capacity,Residual learning,Taxonomy},
  file = {C:\Users\cleme\Zotero\storage\6CPZE55J\Khan et al. - 2020 - A survey of the recent architectures of deep convo.pdf}
}

@article{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}.},
  shorttitle = {Adam},
  author = {Kingma, Diederik P.},
  year = {2015},
  file = {C:\Users\cleme\Zotero\storage\KR7MGHDF\KingmaB14.html}
}

@article{kipfSemiSupervisedClassificationGraph2016,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.02907 [cs, stat]},
  eprint = {1609.02907},
  primaryclass = {cs, stat},
  urldate = {2019-08-01},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{kipfSemiSupervisedClassificationGraph2016a,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.02907 [cs, stat]},
  eprint = {1609.02907},
  primaryclass = {cs, stat},
  urldate = {2019-08-01},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\WHJYYWVS\\Kipf et Welling - 2016 - Semi-Supervised Classification with Graph Convolut.pdf;C\:\\Users\\cleme\\Zotero\\storage\\BZNCRJFG\\1609.html}
}

@article{kipfSEMISUPERVISEDCLASSIFICATIONGRAPH2017,
  title = {{{SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS}}},
  author = {Kipf, Thomas N and Welling, Max},
  year = {2017},
  pages = {14},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  langid = {english}
}

@article{kipfSEMISUPERVISEDCLASSIFICATIONGRAPH2017a,
  title = {{{SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS}}},
  author = {Kipf, Thomas N and Welling, Max},
  year = {2017},
  pages = {14},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\KKRCPIKL\Kipf et Welling - 2017 - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUT.pdf}
}

@inproceedings{kitamotoDevelopmentTyphoonImage2000,
  title = {The {{Development}} of {{Typhoon Image Database}} with {{Content-Based Search}}},
  booktitle = {Proceedings of the 1st International Symposium on Advanced Informatics},
  author = {Kitamoto, Asanobu},
  year = {2000},
  pages = {163--170},
  abstract = {The purpose of the paper is to develop typhoon image database system that provides content-based search on both text-based and image-based information. In particular the author focuses on the shape and motion information that can be derived from satellite images, which is a challenging subject of research for complex shape and non-rigid motion analysis. Many kinds of algorithms are developed for this purpose, and the author shows the result of similarity-based retrieval of typhoon images.},
  langid = {english}
}

@inproceedings{kitamotoDevelopmentTyphoonImage2000a,
  title = {The {{Development}} of {{Typhoon Image Database}} with {{Content-Based Search}}},
  booktitle = {Proceedings of the 1st International Symposium on Advanced Informatics},
  author = {Kitamoto, Asanobu},
  year = {2000},
  pages = {163--170},
  abstract = {The purpose of the paper is to develop typhoon image database system that provides content-based search on both text-based and image-based information. In particular the author focuses on the shape and motion information that can be derived from satellite images, which is a challenging subject of research for complex shape and non-rigid motion analysis. Many kinds of algorithms are developed for this purpose, and the author shows the result of similarity-based retrieval of typhoon images.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\3P5XGNI3\Kitamoto - The Development of Typhoon Image Database with Con.pdf}
}

@article{knutsonTropicalCyclonesClimate2010,
  title = {Tropical {{Cyclones}} and {{Climate Change}}},
  author = {Knutson, Thomas R. and McBride, John L. and Chan, Johnny and Emanuel, Kerry and Holland, Greg and Landsea, Chris and Held, Isaac and Kossin, James P. and Srivastava, A. K. and Sugi, Masato},
  year = {2010},
  month = feb,
  journal = {Prof. Emanuel via Chris Sherratt},
  issn = {1752-0908},
  urldate = {2019-06-04},
  abstract = {Whether the characteristics of tropical cyclones have changed or will change in a warming climate --- and if so, how --- has been the subject of considerable investigation, often with conflicting results. Large amplitude fluctuations in the frequency and intensity of tropical cyclones greatly complicate both the detection of long-term trends and their attribution to rising levels of atmospheric greenhouse gases. Trend detection is further impeded by substantial limitations in the availability and quality of global historical records of tropical cyclones. Therefore, it remains uncertain whether past changes in tropical cyclone activity have exceeded the variability expected from natural causes. However, future projections based on theory and high-resolution dynamical models consistently indicate that greenhouse warming will cause the globally averaged intensity of tropical cyclones to shift towards stronger storms, with intensity increases of 2--11\% by 2100. Existing modelling studies also consistently project decreases in the globally averaged frequency of tropical cyclones, by 6--34\%. Balanced against this, higher resolution modelling studies typically project substantial increases in the frequency of the most intense cyclones, and increases of the order of 20\% in the precipitation rate within 100 km of the storm centre. For all cyclone parameters, projected changes for individual basins show large variations between different modelling studies.},
  copyright = {Article is made available in accordance with the publisher's policy and may be subject to US copyright law. Please refer to the publisher's site for terms of use.},
  langid = {american}
}

@article{knutsonTropicalCyclonesClimate2010a,
  title = {Tropical {{Cyclones}} and {{Climate Change}}},
  author = {Knutson, Thomas R. and McBride, John L. and Chan, Johnny and Emanuel, Kerry and Holland, Greg and Landsea, Chris and Held, Isaac and Kossin, James P. and Srivastava, A. K. and Sugi, Masato},
  year = {2010},
  month = feb,
  journal = {Prof. Emanuel via Chris Sherratt},
  issn = {1752-0908},
  urldate = {2019-06-04},
  abstract = {Whether the characteristics of tropical cyclones have changed or will change in a warming climate --- and if so, how --- has been the subject of considerable investigation, often with conflicting results. Large amplitude fluctuations in the frequency and intensity of tropical cyclones greatly complicate both the detection of long-term trends and their attribution to rising levels of atmospheric greenhouse gases. Trend detection is further impeded by substantial limitations in the availability and quality of global historical records of tropical cyclones. Therefore, it remains uncertain whether past changes in tropical cyclone activity have exceeded the variability expected from natural causes. However, future projections based on theory and high-resolution dynamical models consistently indicate that greenhouse warming will cause the globally averaged intensity of tropical cyclones to shift towards stronger storms, with intensity increases of 2--11\% by 2100. Existing modelling studies also consistently project decreases in the globally averaged frequency of tropical cyclones, by 6--34\%. Balanced against this, higher resolution modelling studies typically project substantial increases in the frequency of the most intense cyclones, and increases of the order of 20\% in the precipitation rate within 100 km of the storm centre. For all cyclone parameters, projected changes for individual basins show large variations between different modelling studies.},
  copyright = {Article is made available in accordance with the publisher's policy and may be subject to US copyright law. Please refer to the publisher's site for terms of use.},
  langid = {american},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\C9TG8BEL\\Knutson et al. - 2010 - Tropical Cyclones and Climate Change.pdf;C\:\\Users\\cleme\\Zotero\\storage\\CYRYWQB4\\62558.html}
}

@article{knyazevUnderstandingAttentionGeneralization2019,
  title = {Understanding {{Attention}} and {{Generalization}} in {{Graph Neural Networks}}},
  author = {Knyazev, Boris and Taylor, Graham W. and Amer, Mohamed R.},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.02850 [cs, stat]},
  eprint = {1905.02850},
  primaryclass = {cs, stat},
  urldate = {2019-12-30},
  abstract = {We aim to better understand attention over nodes in graph neural networks (GNNs) and identify factors influencing its effectiveness. We particularly focus on the ability of attention GNNs to generalize to larger, more complex or noisy graphs. Motivated by insights from the work on Graph Isomorphism Networks, we design simple graph reasoning tasks that allow us to study attention in a controlled environment. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 60\% in some of our classification tasks. Satisfying these conditions in practice is challenging and often requires optimal initialization or supervised training of attention. We propose an alternative recipe and train attention in a weakly-supervised fashion that approaches the performance of supervised models, and, compared to unsupervised models, improves results on several synthetic as well as real datasets. Source code and datasets are available at https://github.com/bknyaz/graph\_attention\_pool.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{knyazevUnderstandingAttentionGeneralization2019a,
  title = {Understanding {{Attention}} and {{Generalization}} in {{Graph Neural Networks}}},
  author = {Knyazev, Boris and Taylor, Graham W. and Amer, Mohamed R.},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.02850 [cs, stat]},
  eprint = {1905.02850},
  primaryclass = {cs, stat},
  urldate = {2019-12-30},
  abstract = {We aim to better understand attention over nodes in graph neural networks (GNNs) and identify factors influencing its effectiveness. We particularly focus on the ability of attention GNNs to generalize to larger, more complex or noisy graphs. Motivated by insights from the work on Graph Isomorphism Networks, we design simple graph reasoning tasks that allow us to study attention in a controlled environment. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 60\% in some of our classification tasks. Satisfying these conditions in practice is challenging and often requires optimal initialization or supervised training of attention. We propose an alternative recipe and train attention in a weakly-supervised fashion that approaches the performance of supervised models, and, compared to unsupervised models, improves results on several synthetic as well as real datasets. Source code and datasets are available at https://github.com/bknyaz/graph\_attention\_pool.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\6KD7963I\\Knyazev et al. - 2019 - Understanding Attention and Generalization in Grap.pdf;C\:\\Users\\cleme\\Zotero\\storage\\VHPYGP64\\1905.html}
}

@article{Kohl2019AHP,
  title = {A Hierarchical Probabilistic {{U-net}} for Modeling Multi-Scale Ambiguities},
  author = {Kohl, Simon A. A. and {Romera-Paredes}, Bernardino and {Maier-Hein}, Klaus and Rezende, Danilo Jimenez and Eslami, S. M. Ali and Kohli, Pushmeet and Zisserman, Andrew and Ronneberger, Olaf},
  year = {2019},
  journal = {ArXiv},
  volume = {abs/1905.13077}
}

@inproceedings{kohlerAutomaticNoreferenceQuality2013,
  title = {Automatic No-Reference Quality Assessment for Retinal Fundus Images Using Vessel Segmentation},
  booktitle = {Proceedings of the 26th {{IEEE International Symposium}} on {{Computer-Based Medical Systems}}},
  author = {Kohler, Thomas and Budai, Attila and Kraus, Martin F. and Odstrcilik, Jan and Michelson, Georg and Hornegger, Joachim},
  year = {2013},
  month = jun,
  pages = {95--100},
  publisher = {IEEE},
  address = {Porto, Portugal},
  doi = {10.1109/CBMS.2013.6627771},
  urldate = {2019-12-26},
  abstract = {Fundus imaging is the most commonly used modality to collect information about the human eye background. Objective and quantitative assessment of quality for the acquired images is essential for manual, computer-aided and fully automatic diagnosis. In this paper, we present a noreference quality metric to quantify image noise and blur and its application to fundus image quality assessment. The proposed metric takes the vessel tree visible on the retina as guidance to determine an image quality score. In our experiments, the performance of this approach is demonstrated by correlation analysis with the established full-reference metrics peak-signal-to-noise ratio (PSNR) and structural similarity (SSIM). We found a Spearman rank correlation for PSNR and SSIM of 0.89 and 0.91. For real data, our metric correlates reasonable to a human observer, indicating high agreement to human visual perception.},
  isbn = {978-1-4799-1053-3},
  langid = {english}
}

@inproceedings{kohlerAutomaticNoreferenceQuality2013a,
  title = {Automatic No-Reference Quality Assessment for Retinal Fundus Images Using Vessel Segmentation},
  booktitle = {Proceedings of the 26th {{IEEE International Symposium}} on {{Computer-Based Medical Systems}}},
  author = {Kohler, Thomas and Budai, Attila and Kraus, Martin F. and Odstrcilik, Jan and Michelson, Georg and Hornegger, Joachim},
  year = {2013},
  month = jun,
  pages = {95--100},
  publisher = {IEEE},
  address = {Porto, Portugal},
  doi = {10.1109/CBMS.2013.6627771},
  urldate = {2019-12-26},
  abstract = {Fundus imaging is the most commonly used modality to collect information about the human eye background. Objective and quantitative assessment of quality for the acquired images is essential for manual, computer-aided and fully automatic diagnosis. In this paper, we present a noreference quality metric to quantify image noise and blur and its application to fundus image quality assessment. The proposed metric takes the vessel tree visible on the retina as guidance to determine an image quality score. In our experiments, the performance of this approach is demonstrated by correlation analysis with the established full-reference metrics peak-signal-to-noise ratio (PSNR) and structural similarity (SSIM). We found a Spearman rank correlation for PSNR and SSIM of 0.89 and 0.91. For real data, our metric correlates reasonable to a human observer, indicating high agreement to human visual perception.},
  isbn = {978-1-4799-1053-3},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\DB3NZ8NI\\Kohler et al. - 2013 - Automatic no-reference quality assessment for reti.pdf;C\:\\Users\\cleme\\Zotero\\storage\\N8NMPCYU\\kohler2013.html}
}

@inproceedings{kohlProbabilisticUnetSegmentation2018,
  title = {A Probabilistic {{U-net}} for Segmentation of Ambiguous Images},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Kohl, Simon A. A. and {Romera-Paredes}, Bernardino and Meyer, Clemens and Fauw, Jeffrey De and Ledsam, Joseph R. and {Maier-Hein}, Klaus H. and Eslami, S. M. Ali and Rezende, Danilo Jimenez and Ronneberger, Olaf},
  year = {2018},
  month = dec,
  series = {{{NIPS}}'18},
  pages = {6965--6975},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-09-06},
  abstract = {Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional vari-ational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.},
  file = {C:\Users\cleme\Zotero\storage\T5LH2PRT\Kohl et al. - 2018 - A probabilistic U-net for segmentation of ambiguou.pdf}
}

@misc{kokhlikyanCaptumUnifiedGeneric2020,
  title = {Captum: {{A}} Unified and Generic Model Interpretability Library for {{PyTorch}}},
  author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and {Reblitz-Richardson}, Orion},
  year = {2020},
  eprint = {2009.07896},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@article{kolarClassificationClinicalFeatures2013,
  title = {Classification and {{Clinical Features}} of {{AMD}}},
  author = {Kolar, Petr},
  year = {2013},
  month = jun,
  journal = {Age-Related Macular Degeneration - Etiology, Diagnosis and Management - A Glance at the Future},
  doi = {10.5772/53762},
  urldate = {2019-11-27},
  abstract = {Open access peer-reviewed chapter},
  langid = {english}
}

@article{kolarClassificationClinicalFeatures2013a,
  title = {Classification and {{Clinical Features}} of {{AMD}}},
  author = {Kolar, Petr},
  year = {2013},
  month = jun,
  journal = {Age-Related Macular Degeneration - Etiology, Diagnosis and Management - A Glance at the Future},
  doi = {10.5772/53762},
  urldate = {2019-11-27},
  abstract = {Open access peer-reviewed chapter},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\YHGAP3SR\\Kolar - 2013 - Classification and Clinical Features of AMD.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RQ9Q72VM\\classification-and-clinical-features-of-amd.html}
}

@inproceedings{kolarRegistration3DRetinal2010,
  title = {Registration of {{3D Retinal Optical Coherence Tomography Data}} and {{2D Fundus Images}}},
  booktitle = {Biomedical {{Image Registration}}},
  author = {Kolar, Radim and Tasevsky, Pavel},
  editor = {Fischer, Bernd and Dawant, Beno{\^i}t M. and Lorenz, Cristian},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {72--82},
  publisher = {Springer Berlin Heidelberg},
  abstract = {This paper is focused on multimodal and multidimensional image data registration: the three-dimensional retinal optical coherence tomographic (OCT) data and two-dimensional color images of fundus. The registration of these two modalities is not common in retinal image processing, but it might help to remove the moving artifacts in OCT and correct the true positions of acquired OCT scans on retina. The proposed framework consists of three steps: global dataset pre-registration, preprocessing and OCT to fundus image registration. Two alternating registration criteria are used in the main step due to changing spatial image properties. Three-parametric spatial transformation (shift and scale) for each OCT scan and exhaustive search is used in this preliminarily work. The achieved results are presented on several examples.},
  isbn = {978-3-642-14366-3},
  langid = {english},
  keywords = {Fundus Image,Optic Disc,Optical Coherence Tomography,Retinal Pigment Epithelium,Spatial Transformation}
}

@inproceedings{kolarRegistration3DRetinal2010a,
  title = {Registration of {{3D Retinal Optical Coherence Tomography Data}} and {{2D Fundus Images}}},
  booktitle = {Biomedical {{Image Registration}}},
  author = {Kolar, Radim and Tasevsky, Pavel},
  editor = {Fischer, Bernd and Dawant, Beno{\^i}t M. and Lorenz, Cristian},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {72--82},
  publisher = {Springer Berlin Heidelberg},
  abstract = {This paper is focused on multimodal and multidimensional image data registration: the three-dimensional retinal optical coherence tomographic (OCT) data and two-dimensional color images of fundus. The registration of these two modalities is not common in retinal image processing, but it might help to remove the moving artifacts in OCT and correct the true positions of acquired OCT scans on retina. The proposed framework consists of three steps: global dataset pre-registration, preprocessing and OCT to fundus image registration. Two alternating registration criteria are used in the main step due to changing spatial image properties. Three-parametric spatial transformation (shift and scale) for each OCT scan and exhaustive search is used in this preliminarily work. The achieved results are presented on several examples.},
  isbn = {978-3-642-14366-3},
  langid = {english},
  keywords = {Fundus Image,Optic Disc,Optical Coherence Tomography,Retinal Pigment Epithelium,Spatial Transformation}
}

@article{krauseGraderVariabilityImportance2018,
  title = {Grader {{Variability}} and the {{Importance}} of {{Reference Standards}} for {{Evaluating Machine Learning Models}} for {{Diabetic Retinopathy}}},
  author = {Krause, Jonathan and Gulshan, Varun and Rahimy, Ehsan and Karth, Peter and Widner, Kasumi and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
  year = {2018},
  month = aug,
  journal = {Ophthalmology},
  volume = {125},
  number = {8},
  pages = {1264--1272},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2018.01.034},
  urldate = {2019-12-01},
  abstract = {Purpose Use adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading. Design Retrospective analysis. Participants Retinal fundus images from DR screening programs. Methods Images were each graded by the algorithm, U.S. board-certified ophthalmologists, and retinal specialists. The adjudicated consensus of the retinal specialists served as the reference standard. Main Outcome Measures For agreement between different graders as well as between the graders and the algorithm, we measured the (quadratic-weighted) kappa score. To compare the performance of different forms of manual grading and the algorithm for various DR severity cutoffs (e.g., mild or worse DR, moderate or worse DR), we measured area under the curve (AUC), sensitivity, and specificity. Results Of the 193 discrepancies between adjudication by retinal specialists and majority decision of ophthalmologists, the most common were missing microaneurysm (MAs) (36\%), artifacts (20\%), and misclassified hemorrhages (16\%). Relative to the reference standard, the kappa for individual retinal specialists, ophthalmologists, and algorithm ranged from 0.82 to 0.91, 0.80 to 0.84, and 0.84, respectively. For moderate or worse DR, the majority decision of ophthalmologists had a sensitivity of 0.838 and specificity of 0.981. The algorithm had a sensitivity of 0.971, specificity of 0.923, and AUC of 0.986. For mild or worse DR, the algorithm had a sensitivity of 0.970, specificity of 0.917, and AUC of 0.986. By using a small number of adjudicated consensus grades as a tuning dataset and higher-resolution images as input, the algorithm improved in AUC from 0.934 to 0.986 for moderate or worse DR. Conclusions Adjudication reduces the errors in DR grading. A small set of adjudicated DR grades allows substantial improvements in algorithm performance. The resulting algorithm's performance was on par with that of individual U.S. Board-Certified ophthalmologists and retinal specialists.},
  langid = {english}
}

@article{krauseGraderVariabilityImportance2018a,
  title = {Grader {{Variability}} and the {{Importance}} of {{Reference Standards}} for {{Evaluating Machine Learning Models}} for {{Diabetic Retinopathy}}},
  author = {Krause, Jonathan and Gulshan, Varun and Rahimy, Ehsan and Karth, Peter and Widner, Kasumi and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
  year = {2018},
  month = aug,
  journal = {Ophthalmology},
  volume = {125},
  number = {8},
  pages = {1264--1272},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2018.01.034},
  abstract = {PURPOSE: Use adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading. DESIGN: Retrospective analysis. PARTICIPANTS: Retinal fundus images from DR screening programs. METHODS: Images were each graded by the algorithm, U.S. board-certified ophthalmologists, and retinal specialists. The adjudicated consensus of the retinal specialists served as the reference standard. MAIN OUTCOME MEASURES: For agreement between different graders as well as between the graders and the algorithm, we measured the (quadratic-weighted) kappa score. To compare the performance of different forms of manual grading and the algorithm for various DR severity cutoffs (e.g., mild or worse DR, moderate or worse DR), we measured area under the curve (AUC), sensitivity, and specificity. RESULTS: Of the 193 discrepancies between adjudication by retinal specialists and majority decision of ophthalmologists, the most common were missing microaneurysm (MAs) (36\%), artifacts (20\%), and misclassified hemorrhages (16\%). Relative to the reference standard, the kappa for individual retinal specialists, ophthalmologists, and algorithm ranged from 0.82 to 0.91, 0.80 to 0.84, and 0.84, respectively. For moderate or worse DR, the majority decision of ophthalmologists had a sensitivity of 0.838 and specificity of 0.981. The algorithm had a sensitivity of 0.971, specificity of 0.923, and AUC of 0.986. For mild or worse DR, the algorithm had a sensitivity of 0.970, specificity of 0.917, and AUC of 0.986. By using a small number of adjudicated consensus grades as a tuning dataset and higher-resolution images as input, the algorithm improved in AUC from 0.934 to 0.986 for moderate or worse DR. CONCLUSIONS: Adjudication reduces the errors in DR grading. A small set of adjudicated DR grades allows substantial improvements in algorithm performance. The resulting algorithm's performance was on par with that of individual U.S. Board-Certified ophthalmologists and retinal specialists.},
  langid = {english},
  pmid = {29548646},
  keywords = {Algorithms,Clinical Competence,Diabetic Retinopathy,Female,Humans,Machine Learning,Male,Mass Screening,Middle Aged,Ophthalmologists,Reference Standards,Retrospective Studies,ROC Curve}
}

@article{krauseGraderVariabilityImportance2018b,
  title = {Grader {{Variability}} and the {{Importance}} of {{Reference Standards}} for {{Evaluating Machine Learning Models}} for {{Diabetic Retinopathy}}},
  author = {Krause, Jonathan and Gulshan, Varun and Rahimy, Ehsan and Karth, Peter and Widner, Kasumi and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
  year = {2018},
  month = aug,
  journal = {Ophthalmology},
  volume = {125},
  number = {8},
  pages = {1264--1272},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2018.01.034},
  urldate = {2019-12-01},
  abstract = {Purpose Use adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading. Design Retrospective analysis. Participants Retinal fundus images from DR screening programs. Methods Images were each graded by the algorithm, U.S. board-certified ophthalmologists, and retinal specialists. The adjudicated consensus of the retinal specialists served as the reference standard. Main Outcome Measures For agreement between different graders as well as between the graders and the algorithm, we measured the (quadratic-weighted) kappa score. To compare the performance of different forms of manual grading and the algorithm for various DR severity cutoffs (e.g., mild or worse DR, moderate or worse DR), we measured area under the curve (AUC), sensitivity, and specificity. Results Of the 193 discrepancies between adjudication by retinal specialists and majority decision of ophthalmologists, the most common were missing microaneurysm (MAs) (36\%), artifacts (20\%), and misclassified hemorrhages (16\%). Relative to the reference standard, the kappa for individual retinal specialists, ophthalmologists, and algorithm ranged from 0.82 to 0.91, 0.80 to 0.84, and 0.84, respectively. For moderate or worse DR, the majority decision of ophthalmologists had a sensitivity of 0.838 and specificity of 0.981. The algorithm had a sensitivity of 0.971, specificity of 0.923, and AUC of 0.986. For mild or worse DR, the algorithm had a sensitivity of 0.970, specificity of 0.917, and AUC of 0.986. By using a small number of adjudicated consensus grades as a tuning dataset and higher-resolution images as input, the algorithm improved in AUC from 0.934 to 0.986 for moderate or worse DR. Conclusions Adjudication reduces the errors in DR grading. A small set of adjudicated DR grades allows substantial improvements in algorithm performance. The resulting algorithm's performance was on par with that of individual U.S. Board-Certified ophthalmologists and retinal specialists.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\4MCKINSR\\Krause et al. - 2018 - Grader Variability and the Importance of Reference.pdf;C\:\\Users\\cleme\\Zotero\\storage\\U236DCJK\\S0161642017326982.html}
}

@article{krauseGraderVariabilityImportance2018c,
  title = {Grader {{Variability}} and the {{Importance}} of {{Reference Standards}} for {{Evaluating Machine Learning Models}} for {{Diabetic Retinopathy}}},
  author = {Krause, Jonathan and Gulshan, Varun and Rahimy, Ehsan and Karth, Peter and Widner, Kasumi and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
  year = {2018},
  month = aug,
  journal = {Ophthalmology},
  volume = {125},
  number = {8},
  pages = {1264--1272},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2018.01.034},
  abstract = {PURPOSE: Use adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading. DESIGN: Retrospective analysis. PARTICIPANTS: Retinal fundus images from DR screening programs. METHODS: Images were each graded by the algorithm, U.S. board-certified ophthalmologists, and retinal specialists. The adjudicated consensus of the retinal specialists served as the reference standard. MAIN OUTCOME MEASURES: For agreement between different graders as well as between the graders and the algorithm, we measured the (quadratic-weighted) kappa score. To compare the performance of different forms of manual grading and the algorithm for various DR severity cutoffs (e.g., mild or worse DR, moderate or worse DR), we measured area under the curve (AUC), sensitivity, and specificity. RESULTS: Of the 193 discrepancies between adjudication by retinal specialists and majority decision of ophthalmologists, the most common were missing microaneurysm (MAs) (36\%), artifacts (20\%), and misclassified hemorrhages (16\%). Relative to the reference standard, the kappa for individual retinal specialists, ophthalmologists, and algorithm ranged from 0.82 to 0.91, 0.80 to 0.84, and 0.84, respectively. For moderate or worse DR, the majority decision of ophthalmologists had a sensitivity of 0.838 and specificity of 0.981. The algorithm had a sensitivity of 0.971, specificity of 0.923, and AUC of 0.986. For mild or worse DR, the algorithm had a sensitivity of 0.970, specificity of 0.917, and AUC of 0.986. By using a small number of adjudicated consensus grades as a tuning dataset and higher-resolution images as input, the algorithm improved in AUC from 0.934 to 0.986 for moderate or worse DR. CONCLUSIONS: Adjudication reduces the errors in DR grading. A small set of adjudicated DR grades allows substantial improvements in algorithm performance. The resulting algorithm's performance was on par with that of individual U.S. Board-Certified ophthalmologists and retinal specialists.},
  langid = {english},
  pmid = {29548646},
  keywords = {Algorithms,Clinical Competence,Diabetic Retinopathy,Female,Humans,Machine Learning,Male,Mass Screening,Middle Aged,Ophthalmologists,Reference Standards,Retrospective Studies,ROC Curve}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-02-15},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012a,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-02-15},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {C:\Users\cleme\Zotero\storage\35ZRLM9N\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@article{kumarUltrawideFieldRetinal2021,
  title = {Ultra-Wide Field Retinal Imaging: {{A}} Wider Clinical Perspective},
  shorttitle = {Ultra-Wide Field Retinal Imaging},
  author = {Kumar, Vinod and Surve, Abhidnya and Kumawat, Devesh and Takkar, Brijesh and Azad, Shorya and Chawla, Rohan and Shroff, Daraius and Arora, Atul and Singh, Ramandeep and Venkatesh, Pradeep},
  year = {2021},
  month = apr,
  journal = {Indian Journal of Ophthalmology},
  volume = {69},
  number = {4},
  pages = {824},
  publisher = {Wolters Kluwer -- Medknow Publications},
  doi = {10.4103/ijo.IJO_1403_20},
  urldate = {2024-02-22},
  abstract = {The peripheral retina is affected in a variety of retinal disorders. Traditional fundus cameras capture only a part of the fundus even when montaging techniques are used. Ultra-wide field imaging enables us to delve into the retinal periphery in greater ...},
  langid = {english},
  pmid = {33727441},
  file = {C:\Users\cleme\Zotero\storage\BMA8ULMF\Kumar et al. - 2021 - Ultra-wide field retinal imaging A wider clinical.pdf}
}

@article{kunduNestedUNetSegmentation2022,
  title = {Nested {{U-Net}} for {{Segmentation}} of {{Red Lesions}} in {{Retinal Fundus Images}} and {{Sub-image Classification}} for {{Removal}} of {{False Positives}}},
  author = {Kundu, Swagata and Karale, Vikrant and Ghorai, Goutam and Sarkar, Gautam and Ghosh, Sambuddha and Dhara, Ashis Kumar},
  year = {2022},
  month = oct,
  journal = {Journal of Digital Imaging},
  volume = {35},
  number = {5},
  pages = {1111--1119},
  issn = {1618-727X},
  doi = {10.1007/s10278-022-00629-4},
  urldate = {2023-02-03},
  abstract = {Diabetic retinopathy is a pathological change of the retina that occurs for long-term diabetes. The patients become symptomatic in advanced stages of diabetic retinopathy resulting in severe non-proliferative diabetic retinopathy or proliferative diabetic retinopathy stages. There is a need of an automated screening tool for the early detection and treatment of patients with diabetic retinopathy. This paper focuses on the segmentation of red lesions using nested U-Net Zhou et al. (Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Springer, 2018) followed by removal of false positives based on the sub-image classification method. Different sizes of sub-images were studied for the reduction in false positives in the sub-image classification method. The network could capture semantic features and fine details due to dense convolutional blocks connected via skip connections in between down sampling and up sampling paths. False-negative candidates were very few and the sub-image classification network effectively reduced the falsely detected candidates. The proposed framework achieves a sensitivity of \$\$88.79{\textbackslash}\%\$\$, precision of \$\$71.50{\textbackslash}\%\$\$, and F1-Score of \$\$79.21{\textbackslash}\%\$\$for the DIARETDB1 data set Kalviainen and Uusutalo (Medical Image Understanding and Analysis, Citeseer, 2007). It outperforms the state-of-the-art networks such as U-Net Ronneberger et al. (International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, 2015) and attention U-Net Oktay et al. (Attention u-net: Learning where to look for the pancreas, 2018).},
  langid = {english},
  keywords = {Diabetic retinopathy,Fundus images,Nested U-Net,Segmentation of red lesions,Sub-image classification}
}

@article{kunduNestedUNetSegmentation2022a,
  title = {Nested {{U-Net}} for {{Segmentation}} of {{Red Lesions}} in {{Retinal Fundus Images}} and {{Sub-image Classification}} for {{Removal}} of {{False Positives}}},
  author = {Kundu, Swagata and Karale, Vikrant and Ghorai, Goutam and Sarkar, Gautam and Ghosh, Sambuddha and Dhara, Ashis Kumar},
  year = {2022},
  month = oct,
  journal = {Journal of Digital Imaging},
  volume = {35},
  number = {5},
  pages = {1111--1119},
  issn = {1618-727X},
  doi = {10.1007/s10278-022-00629-4},
  urldate = {2023-02-03},
  abstract = {Diabetic retinopathy is a pathological change of the retina that occurs for long-term diabetes. The patients become symptomatic in advanced stages of diabetic retinopathy resulting in severe non-proliferative diabetic retinopathy or proliferative diabetic retinopathy stages. There is a need of an automated screening tool for the early detection and treatment of patients with diabetic retinopathy. This paper focuses on the segmentation of red lesions using nested U-Net Zhou et al. (Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Springer, 2018)~followed by removal of false positives based on the sub-image classification method. Different sizes of sub-images were studied for the reduction in false positives in the sub-image classification method. The network could capture semantic features and fine details due to dense convolutional blocks connected via skip connections in between down sampling and up sampling paths. False-negative candidates were very few and the sub-image classification network effectively reduced the falsely detected candidates. The proposed framework achieves a sensitivity of \$\$88.79{\textbackslash}\%\$\$, precision of \$\$71.50{\textbackslash}\%\$\$, and F1-Score of \$\$79.21{\textbackslash}\%\$\$for the DIARETDB1 data set Kalviainen and Uusutalo (Medical Image Understanding and Analysis, Citeseer, 2007). It outperforms the state-of-the-art networks such as U-Net Ronneberger et al. (International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, 2015)~and attention U-Net~Oktay et al. (Attention u-net: Learning where to look for the pancreas, 2018).},
  langid = {english},
  keywords = {Diabetic retinopathy,Fundus images,Nested U-Net,Segmentation of red lesions,Sub-image classification}
}

@misc{LabJupyterLab,
  title = {Lab? (5) - {{JupyterLab}}},
  urldate = {2023-04-22}
}

@misc{LabJupyterLaba,
  title = {Lab? (5) - {{JupyterLab}}},
  urldate = {2023-04-22},
  howpublished = {http://localhost:6068/lab?}
}

@article{lajevardiRetinaVerificationSystem2013,
  title = {Retina {{Verification System Based}} on {{Biometric Graph Matching}}},
  author = {Lajevardi, Seyed Mehdi and Arakala, Arathi and Davis, Stephen A. and Horadam, Kathy J.},
  year = {2013},
  month = sep,
  journal = {IEEE Transactions on Image Processing},
  volume = {22},
  number = {9},
  pages = {3625--3635},
  issn = {1941-0042},
  doi = {10.1109/TIP.2013.2266257},
  abstract = {This paper presents an automatic retina verification framework based on the biometric graph matching (BGM) algorithm. The retinal vasculature is extracted using a family of matched filters in the frequency domain and morphological operators. Then, retinal templates are defined as formal spatial graphs derived from the retinal vasculature. The BGM algorithm, a noisy graph matching algorithm, robust to translation, non-linear distortion, and small rotations, is used to compare retinal templates. The BGM algorithm uses graph topology to define three distance measures between a pair of graphs, two of which are new. A support vector machine (SVM) classifier is used to distinguish between genuine and imposter comparisons. Using single as well as multiple graph measures, the classifier achieves complete separation on a training set of images from the VARIA database (60\% of the data), equaling the state-of-the-art for retina verification. Because the available data set is small, kernel density estimation (KDE) of the genuine and imposter score distributions of the training set are used to measure performance of the BGM algorithm. In the one dimensional case, the KDE model is validated with the testing set. A 0 EER on testing shows that the KDE model is a good fit for the empirical distribution. For the multiple graph measures, a novel combination of the SVM boundary and the KDE model is used to obtain a fair comparison with the KDE model for the single measure. A clear benefit in using multiple graph measures over a single measure to distinguish genuine and imposter comparisons is demonstrated by a drop in theoretical error of between 60\% and more than two orders of magnitude.},
  keywords = {automatic retina verification framework,BGM algorithm,biometric graph matching,Biometric Identification,biometrics (access control),Computer-Assisted,Databases,Factual,formal spatial graphs,frequency domain,graph theory,graph topology,Humans,image classification,image matching,Image Processing,KDE,kernel density estimation,matched filters,mathematical morphology,morphological operators,multiple graph attributes,multiple graph measures,noisy graph matching,nonlinear distortion,person verification,Retina,retina verification system,retina vessels detection,Retinal feature extraction,retinal recognition,retinal templates,retinal vasculature,Retinal Vessels,support vector machine,support vector machines,Support Vector Machines,SVM classifier,VARIA database}
}

@article{lajevardiRetinaVerificationSystem2013a,
  title = {Retina {{Verification System Based}} on {{Biometric Graph Matching}}},
  author = {Lajevardi, Seyed Mehdi and Arakala, Arathi and Davis, Stephen A. and Horadam, Kathy J.},
  year = {2013},
  month = sep,
  journal = {IEEE Transactions on Image Processing},
  volume = {22},
  number = {9},
  pages = {3625--3635},
  issn = {1941-0042},
  doi = {10.1109/TIP.2013.2266257},
  abstract = {This paper presents an automatic retina verification framework based on the biometric graph matching (BGM) algorithm. The retinal vasculature is extracted using a family of matched filters in the frequency domain and morphological operators. Then, retinal templates are defined as formal spatial graphs derived from the retinal vasculature. The BGM algorithm, a noisy graph matching algorithm, robust to translation, non-linear distortion, and small rotations, is used to compare retinal templates. The BGM algorithm uses graph topology to define three distance measures between a pair of graphs, two of which are new. A support vector machine (SVM) classifier is used to distinguish between genuine and imposter comparisons. Using single as well as multiple graph measures, the classifier achieves complete separation on a training set of images from the VARIA database (60\% of the data), equaling the state-of-the-art for retina verification. Because the available data set is small, kernel density estimation (KDE) of the genuine and imposter score distributions of the training set are used to measure performance of the BGM algorithm. In the one dimensional case, the KDE model is validated with the testing set. A 0 EER on testing shows that the KDE model is a good fit for the empirical distribution. For the multiple graph measures, a novel combination of the SVM boundary and the KDE model is used to obtain a fair comparison with the KDE model for the single measure. A clear benefit in using multiple graph measures over a single measure to distinguish genuine and imposter comparisons is demonstrated by a drop in theoretical error of between 60\% and more than two orders of magnitude.},
  keywords = {automatic retina verification framework,BGM algorithm,biometric graph matching,Biometric Identification,biometrics (access control),Databases Factual,formal spatial graphs,frequency domain,graph theory,graph topology,Humans,image classification,image matching,Image Processing Computer-Assisted,KDE,kernel density estimation,matched filters,mathematical morphology,morphological operators,multiple graph attributes,multiple graph measures,noisy graph matching,nonlinear distortion,person verification,Retina,retina verification system,retina vessels detection,Retinal feature extraction,retinal recognition,retinal templates,retinal vasculature,Retinal Vessels,support vector machine,support vector machines,Support Vector Machines,SVM classifier,VARIA database},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\RLNDEQSE\\Lajevardi et al. - 2013 - Retina Verification System Based on Biometric Grap.pdf;C\:\\Users\\cleme\\Zotero\\storage\\U8S8Y2UA\\lajevardi2013.pdf;C\:\\Users\\cleme\\Zotero\\storage\\NM4TTNKQ\\6523153.html}
}

@article{lamirelQualityNonmydriaticDigital2012,
  title = {Quality of Nonmydriatic Digital Fundus Photography Obtained by Nurse Practitioners in the Emergency Department: {{The FOTO-ED}} Study},
  shorttitle = {Quality of Nonmydriatic Digital Fundus Photography Obtained by Nurse Practitioners in the Emergency Department},
  author = {Lamirel, C{\'e}dric and Bruce, Beau B. and Wright, David W. and Delaney, Kevin P. and Newman, Nancy J. and Biousse, Val{\'e}rie},
  year = {2012},
  month = mar,
  journal = {Ophthalmology},
  volume = {119},
  number = {3},
  pages = {617--624},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2011.09.013},
  abstract = {OBJECTIVE: Nonmydriatic fundus photography by non-ophthalmic-trained personnel has recently been shown to be a potential alternative to direct ophthalmoscopy in the emergency department (ED). We evaluated the reliability of a novel quality rating scale and applied this scale to nonmydriatic fundus photographs taken during routine ED patient encounters to determine factors associated with diminished photograph quality. DESIGN: Prospective, cross-sectional study. PARTICIPANTS: We included 350 patients enrolled in the Fundus photography versus Ophthalmoscopy Trials Outcomes in the Emergency Department study who were photographed by nurse practitioners after {$<$}30 minutes of training followed by supervision. METHODS: Photographs of both eyes were graded for quality on 2 occasions by 2 neuro-ophthalmologists. Four regions were independently evaluated for quality: Optic disc, macula, and superior and inferior vascular arcades. Quality as a function of the number of photographs taken was evaluated by Kaplan-Meier analysis. Mixed effects ordinal logistic regression was used to evaluate for predictors of image quality while accounting for the repeated measures design. MAIN OUTCOME MEASURES: Overall photographic quality (1-5 scale; 5 best). RESULTS: We evaluated 1734 photographs. Inter- and intraobserver agreements between neuro-ophthalmologists were very good (weighted kappa, 0.84-0.87). Quality of the optic disc area was better than those of other retinal areas (P{$<$}0.002). Kaplan-Meier analysis showed that if a high-quality photograph of an eye was not obtained by the third attempt, it was unlikely that one would be obtained at all. A 10-second increase in the interphotograph interval before a total of 40 seconds increased the odds of a 1-unit higher quality rating by 1.81 times (95\% confidence interval [CI], 1.68-1.98), and a 10-year increase in age decreased the odds by 0.76 times (95\% CI, 0.69-0.85). Black patients had 0.42 times (95\% CI, 0.28-0.63) the odds of a 1-unit higher quality rating compared with whites. CONCLUSIONS: Our 5-point scale is a reliable measure of nonmydriatic photograph quality. The region of interest, interphotograph interval, and patient age and race are significant predictors of image quality for nonmydriatic photographs taken by nurse practitioners in the ED. Addressing these factors may have a direct impact on the successful implementation of nonmydriatic fundus photography into the ED.},
  langid = {english},
  pmcid = {PMC3294008},
  pmid = {22218140},
  keywords = {Adult,African Continental Ancestry Group,Cross-Sectional Studies,Emergency Medical Services,European Continental Ancestry Group,Fundus Oculi,Health Care,Humans,Middle Aged,Mydriatics,Nurse Practitioners,Observer Variation,Ophthalmology,Ophthalmoscopy,Photography,Prospective Studies,Pupil,Quality Indicators,Reproducibility of Results,Retinal Diseases,Vision Screening}
}

@article{lamirelQualityNonmydriaticDigital2012a,
  title = {Quality of Nonmydriatic Digital Fundus Photography Obtained by Nurse Practitioners in the Emergency Department: The {{FOTO-ED}} Study},
  shorttitle = {Quality of Nonmydriatic Digital Fundus Photography Obtained by Nurse Practitioners in the Emergency Department},
  author = {Lamirel, C{\'e}dric and Bruce, Beau B. and Wright, David W. and Delaney, Kevin P. and Newman, Nancy J. and Biousse, Val{\'e}rie},
  year = {2012},
  month = mar,
  journal = {Ophthalmology},
  volume = {119},
  number = {3},
  pages = {617--624},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2011.09.013},
  abstract = {OBJECTIVE: Nonmydriatic fundus photography by non-ophthalmic-trained personnel has recently been shown to be a potential alternative to direct ophthalmoscopy in the emergency department (ED). We evaluated the reliability of a novel quality rating scale and applied this scale to nonmydriatic fundus photographs taken during routine ED patient encounters to determine factors associated with diminished photograph quality. DESIGN: Prospective, cross-sectional study. PARTICIPANTS: We included 350 patients enrolled in the Fundus photography versus Ophthalmoscopy Trials Outcomes in the Emergency Department study who were photographed by nurse practitioners after {$<$}30 minutes of training followed by supervision. METHODS: Photographs of both eyes were graded for quality on 2 occasions by 2 neuro-ophthalmologists. Four regions were independently evaluated for quality: Optic disc, macula, and superior and inferior vascular arcades. Quality as a function of the number of photographs taken was evaluated by Kaplan-Meier analysis. Mixed effects ordinal logistic regression was used to evaluate for predictors of image quality while accounting for the repeated measures design. MAIN OUTCOME MEASURES: Overall photographic quality (1-5 scale; 5 best). RESULTS: We evaluated 1734 photographs. Inter- and intraobserver agreements between neuro-ophthalmologists were very good (weighted kappa, 0.84-0.87). Quality of the optic disc area was better than those of other retinal areas (P{$<$}0.002). Kaplan-Meier analysis showed that if a high-quality photograph of an eye was not obtained by the third attempt, it was unlikely that one would be obtained at all. A 10-second increase in the interphotograph interval before a total of 40 seconds increased the odds of a 1-unit higher quality rating by 1.81 times (95\% confidence interval [CI], 1.68-1.98), and a 10-year increase in age decreased the odds by 0.76 times (95\% CI, 0.69-0.85). Black patients had 0.42 times (95\% CI, 0.28-0.63) the odds of a 1-unit higher quality rating compared with whites. CONCLUSIONS: Our 5-point scale is a reliable measure of nonmydriatic photograph quality. The region of interest, interphotograph interval, and patient age and race are significant predictors of image quality for nonmydriatic photographs taken by nurse practitioners in the ED. Addressing these factors may have a direct impact on the successful implementation of nonmydriatic fundus photography into the ED.},
  langid = {english},
  pmcid = {PMC3294008},
  pmid = {22218140},
  keywords = {Adult,African Continental Ancestry Group,Cross-Sectional Studies,Emergency Medical Services,European Continental Ancestry Group,Fundus Oculi,Humans,Middle Aged,Mydriatics,Nurse Practitioners,Observer Variation,Ophthalmology,Ophthalmoscopy,Photography,Prospective Studies,Pupil,Quality Indicators Health Care,Reproducibility of Results,Retinal Diseases,Vision Screening},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\653V7262\\Lamirel et al. - 2012 - Quality of nonmydriatic digital fundus photography.pdf;C\:\\Users\\cleme\\Zotero\\storage\\EHPNVNWR\\lamirel2012.pdf}
}

@article{lamWorldwideDiabetesEpidemic2012,
  title = {The Worldwide Diabetes Epidemic},
  author = {Lam, David W. and LeRoith, Derek},
  year = {2012},
  month = apr,
  journal = {Current Opinion in Endocrinology, Diabetes and Obesity},
  volume = {19},
  number = {2},
  pages = {93--96},
  issn = {1752-296X},
  doi = {10.1097/MED.0b013e328350583a},
  urldate = {2022-10-20},
  abstract = {Purpose of review  An overview of the global epidemic of diabetes and its impact on the understanding of the disease. Recent findings  Once thought of as a disease of the West, the prevalence of diabetes mellitus is increasing at alarming rates in many other areas of the world. In recent years, significant attention has been placed on the growing Asian diabetes epidemic. As a result, the medical community has come to understand that previously defined risk factors, particularly BMI, may not be applicable to the global community. The heterogeneity of the disease has been demonstrated both through anthropometric and genetic studies. Despite this heterogeneity, some treatments, particularly lifestyle interventions, have been found to have an ethnic nonspecific positive impact on the disease. Summary  Diabetes promises to become an even larger public health issue with significant social and economic burden with clinical practice and public health policy implications. Further population studies and identification of ethnic specific risk factors will guide research to develop a better understanding of the disease.},
  langid = {american}
}

@article{lamWorldwideDiabetesEpidemic2012a,
  title = {The Worldwide Diabetes Epidemic},
  author = {Lam, David W. and LeRoith, Derek},
  year = {2012},
  month = apr,
  journal = {Current Opinion in Endocrinology, Diabetes and Obesity},
  volume = {19},
  number = {2},
  pages = {93--96},
  issn = {1752-296X},
  doi = {10.1097/MED.0b013e328350583a},
  urldate = {2022-10-20},
  abstract = {Purpose of review~         An overview of the global epidemic of diabetes and its impact on the understanding of the disease.         Recent findings~         Once thought of as a disease of the West, the prevalence of diabetes mellitus is increasing at alarming rates in many other areas of the world. In recent years, significant attention has been placed on the growing Asian diabetes epidemic. As a result, the medical community has come to understand that previously defined risk factors, particularly BMI, may not be applicable to the global community. The heterogeneity of the disease has been demonstrated both through anthropometric and genetic studies. Despite this heterogeneity, some treatments, particularly lifestyle interventions, have been found to have an ethnic nonspecific positive impact on the disease.         Summary~         Diabetes promises to become an even larger public health issue with significant social and economic burden with clinical practice and public health policy implications. Further population studies and identification of ethnic specific risk factors will guide research to develop a better understanding of the disease.},
  langid = {american},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\BRSJILCE\\lam2012.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RKEZ3PGG\\The_worldwide_diabetes_epidemic.5.html}
}

@article{langOpticalCoherenceTomography2007,
  title = {Optical Coherence Tomography Findings in Diabetic Retinopathy},
  author = {Lang, Gabriele E.},
  year = {2007},
  journal = {Developments in Ophthalmology},
  volume = {39},
  pages = {31--47},
  issn = {0250-3751},
  doi = {10.1159/000098498},
  abstract = {Ophthalmoscopy, fundus photography and fluorescein angiography are the common tools to diagnose diabetic retinopathy and diabetic macular edema. However, there is an increasing demand for high-resolution imaging of ocular tissues to improve the diagnosis and management of diabetic retinopathy. Optical coherence tomography (OCT) provides important additional information about the retina. It produces reliable, reproducible and objective retinal images especially in diabetic macular edema and provides information about vitreoretinal relationships that can clearly only be detected with OCT. It enhances the ability to exactly diagnose diabetic macular edema, epiretinal membranes, vitreomacular or vitroretinal traction. OCT also brings new insights into morphological changes of the retina in diabetic retinopathy. It demonstrates that macular edema is a complex clinical entity with various morphology. With the OCT, structural changes and quantitative assessment of macular edema have become feasible as determined with retinal thickness and volume. OCT is more sensitive to small changes in retinal thickness than slit-lamp biomicroscopy.},
  langid = {english},
  pmid = {17245077},
  keywords = {Diabetic Retinopathy,Diagnosis,Differential,Humans,Optical Coherence,Retina,Severity of Illness Index,Tomography}
}

@article{langOpticalCoherenceTomography2007a,
  title = {Optical Coherence Tomography Findings in Diabetic Retinopathy},
  author = {Lang, Gabriele E.},
  year = {2007},
  journal = {Developments in Ophthalmology},
  volume = {39},
  pages = {31--47},
  issn = {0250-3751},
  doi = {10.1159/000098498},
  abstract = {Ophthalmoscopy, fundus photography and fluorescein angiography are the common tools to diagnose diabetic retinopathy and diabetic macular edema. However, there is an increasing demand for high-resolution imaging of ocular tissues to improve the diagnosis and management of diabetic retinopathy. Optical coherence tomography (OCT) provides important additional information about the retina. It produces reliable, reproducible and objective retinal images especially in diabetic macular edema and provides information about vitreoretinal relationships that can clearly only be detected with OCT. It enhances the ability to exactly diagnose diabetic macular edema, epiretinal membranes, vitreomacular or vitroretinal traction. OCT also brings new insights into morphological changes of the retina in diabetic retinopathy. It demonstrates that macular edema is a complex clinical entity with various morphology. With the OCT, structural changes and quantitative assessment of macular edema have become feasible as determined with retinal thickness and volume. OCT is more sensitive to small changes in retinal thickness than slit-lamp biomicroscopy.},
  langid = {english},
  pmid = {17245077},
  keywords = {Diabetic Retinopathy,Diagnosis Differential,Humans,Retina,Severity of Illness Index,Tomography Optical Coherence}
}

@article{lazaridisOCTSignalEnhancement2021,
  title = {{{OCT Signal Enhancement}} with {{Deep Learning}}},
  author = {Lazaridis, Georgios and Lorenzi, Marco and {Mohamed-Noriega}, Jibran and {Aguilar-Munoa}, Soledad and Suzuki, Katsuyoshi and Nomoto, Hiroki and Ourselin, Sebastien and {Garway-Heath}, David F. and Crabb, David P. and Bunce, Catey and Amalfitano, Francesca and Anand, Nitin and {Azuara-Blanco}, Augusto and Bourne, Rupert R. and Broadway, David C. and Cunliffe, Ian A. and Diamond, Jeremy P. and Fraser, Scott G. and Ho, Tuan A. and Martin, Keith R. and McNaught, Andrew I. and Negi, Anil and Shah, Ameet and Spry, Paul G. and White, Edward T. and Wormald, Richard P. and Xing, Wen and Zeyen, Thierry G.},
  year = {2021},
  month = may,
  journal = {Ophthalmology Glaucoma},
  volume = {4},
  number = {3},
  pages = {295--304},
  issn = {2589-4196},
  doi = {10.1016/j.ogla.2020.10.008},
  urldate = {2022-07-08},
  abstract = {Purpose To establish whether deep learning methods are able to improve the signal-to-noise ratio of time-domain (TD) OCT images to approach that of spectral-domain (SD) OCT images. Design Method agreement study and progression detection in a randomized, double-masked, placebo-controlled, multicenter trial for open-angle glaucoma (OAG), the United Kingdom Glaucoma Treatment Study (UKGTS). Participants The training and validation cohort comprised 77 stable OAG participants with TD OCT and SD OCT imaging at up to 11 visits within 3 months. The testing cohort comprised 284 newly diagnosed OAG patients with TD OCT images from a cohort of 516 recruited at 10 United Kingdom centers between 2007 and 2010. Methods An ensemble of generative adversarial networks (GANs) was trained on TD OCT and SD OCT image pairs from the training dataset and applied to TD OCT images from the testing dataset. Time-domain OCT images were converted to synthesized SD OCT images and segmented via Bayesian fusion on the output of the GANs. Main Outcome Measures Bland-Altman analysis assessed agreement between TD OCT and synthesized SD OCT average retinal nerve fiber layer thickness (RNFLT) measurements and the SD OCT RNFLT. Analysis of the distribution of the rates of RNFLT change in TD OCT and synthesized SD OCT in the two treatment arms of the UKGTS was compared. A Cox model for predictors of time-to-incident visual field (VF) progression was computed with the TD OCT and the synthesized SD OCT images. Results The 95\% limits of agreement were between TD OCT and SD OCT were 26.64 to --22.95; between synthesized SD OCT and SD OCT were 8.11 to --6.73; and between SD OCT and SD OCT were 4.16 to --4.04. The mean difference in the rate of RNFLT change between UKGTS treatment and placebo arms with TD OCT was 0.24 (P = 0.11) and with synthesized SD OCT was 0.43 (P = 0.0017). The hazard ratio for RNFLT slope in Cox regression modeling for time to incident VF progression was 1.09 (95\% confidence interval [CI], 1.02--1.21; P = 0.035) for TD OCT and 1.24 (95\% CI, 1.08--1.39; P = 0.011) for synthesized SD OCT. Conclusions Image enhancement significantly improved the agreement of TD OCT RNFLT measurements with SD OCT RNFLT measurements. The difference, and its significance, in rates of RNFLT change in the UKGTS treatment arms was enhanced and RNFLT change became a stronger predictor of VF progression.},
  langid = {english},
  keywords = {Deep learning,Glaucoma,Image analysis,OCT,Visual fields}
}

@article{lazaridisOCTSignalEnhancement2021a,
  title = {{{OCT Signal Enhancement}} with {{Deep Learning}}},
  author = {Lazaridis, Georgios and Lorenzi, Marco and {Mohamed-Noriega}, Jibran and {Aguilar-Munoa}, Soledad and Suzuki, Katsuyoshi and Nomoto, Hiroki and Ourselin, Sebastien and {Garway-Heath}, David F. and Crabb, David P. and Bunce, Catey and Amalfitano, Francesca and Anand, Nitin and {Azuara-Blanco}, Augusto and Bourne, Rupert R. and Broadway, David C. and Cunliffe, Ian A. and Diamond, Jeremy P. and Fraser, Scott G. and Ho, Tuan A. and Martin, Keith R. and McNaught, Andrew I. and Negi, Anil and Shah, Ameet and Spry, Paul G. and White, Edward T. and Wormald, Richard P. and Xing, Wen and Zeyen, Thierry G.},
  year = {2021},
  month = may,
  journal = {Ophthalmology Glaucoma},
  volume = {4},
  number = {3},
  pages = {295--304},
  issn = {2589-4196},
  doi = {10.1016/j.ogla.2020.10.008},
  urldate = {2022-07-08},
  abstract = {Purpose To establish whether deep learning methods are able to improve the signal-to-noise ratio of time-domain (TD) OCT images to approach that of spectral-domain (SD) OCT images. Design Method agreement study and progression detection in a randomized, double-masked, placebo-controlled, multicenter trial for open-angle glaucoma (OAG), the United Kingdom Glaucoma Treatment Study (UKGTS). Participants The training and validation cohort comprised 77 stable OAG participants with TD OCT and SD OCT imaging at up to 11 visits within 3 months. The testing cohort comprised 284 newly diagnosed OAG patients with TD OCT images from a cohort of 516 recruited at 10 United Kingdom centers between 2007 and~2010. Methods An ensemble of generative adversarial networks (GANs) was trained on TD OCT and SD OCT image pairs from the training dataset and applied to TD OCT images from the testing dataset. Time-domain OCT images were converted to synthesized SD OCT images and segmented via Bayesian fusion on the output of the GANs. Main Outcome Measures Bland-Altman analysis assessed agreement between TD OCT and synthesized SD OCT average retinal nerve fiber layer thickness (RNFLT) measurements and the SD OCT RNFLT. Analysis of the distribution of the rates of RNFLT change in TD OCT and synthesized SD OCT in the two treatment arms of the UKGTS was compared. A Cox model for predictors of time-to-incident visual field (VF) progression was computed with the TD OCT and the synthesized SD OCT images. Results The 95\% limits of agreement were between TD OCT and SD OCT were 26.64 to --22.95; between synthesized SD OCT and SD OCT were 8.11 to --6.73; and between SD OCT and SD OCT were 4.16 to --4.04. The mean difference in the rate of RNFLT change between UKGTS treatment and placebo arms with TD OCT was 0.24 (P~= 0.11) and with synthesized SD OCT was 0.43 (P~= 0.0017). The hazard ratio for RNFLT slope in Cox regression modeling for time to incident VF progression was 1.09 (95\% confidence interval [CI], 1.02--1.21; P~=~0.035) for TD OCT and 1.24 (95\% CI, 1.08--1.39; P~= 0.011) for synthesized SD OCT. Conclusions Image enhancement significantly improved the agreement of TD OCT RNFLT measurements with SD OCT RNFLT measurements. The difference, and its significance, in rates of RNFLT change in the UKGTS treatment arms was enhanced and RNFLT change became a stronger predictor of VF progression.},
  langid = {english},
  keywords = {Deep learning,Glaucoma,Image analysis,OCT,Visual fields},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\KLBUTYGJ\\Lazaridis et al. - 2021 - OCT Signal Enhancement with Deep Learning.pdf;C\:\\Users\\cleme\\Zotero\\storage\\J8FT3SF6\\S2589419620302726.html}
}

@misc{LearnableStructuralSemantic,
  title = {Learnable {{Structural Semantic Readout}} for {{Graph Classification}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-02-13},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/9679111},
  file = {C:\Users\cleme\Zotero\storage\6PZHAZRS\9679111.html}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis}
}

@article{lecunGradientbasedLearningApplied1998a,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {C:\Users\cleme\Zotero\storage\VPK4PHSF\726791.html}
}

@article{leeDeeplearningBasedAutomated2017,
  title = {Deep-Learning Based, Automated Segmentation of Macular Edema in Optical Coherence Tomography},
  author = {Lee, Cecilia S. and Tyring, Ariel J. and Deruyter, Nicolaas P. and Wu, Yue and Rokem, Ariel and Lee, Aaron Y.},
  year = {2017},
  month = jun,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {7},
  pages = {3440--3448},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.003440},
  urldate = {2019-11-22},
  abstract = {Evaluation of clinical images is essential for diagnosis in many specialties. Therefore the development of computer vision algorithms to help analyze biomedical images will be important. In ophthalmology, optical coherence tomography (OCT) is critical for managing retinal conditions. We developed a convolutional neural network (CNN) that detects intraretinal fluid (IRF) on OCT in a manner indistinguishable from clinicians. Using 1,289 OCT images, the CNN segmented images with a 0.911 cross-validated Dice coefficient, compared with segmentations by experts. Additionally, the agreement between experts and between experts and CNN were similar. Our results reveal that CNN can be trained to perform automated segmentations of clinically relevant image features.},
  pmcid = {PMC5508840},
  pmid = {28717579}
}

@article{leeDeeplearningBasedAutomated2017a,
  title = {Deep-Learning Based, Automated Segmentation of Macular Edema in Optical Coherence Tomography},
  author = {Lee, Cecilia S. and Tyring, Ariel J. and Deruyter, Nicolaas P. and Wu, Yue and Rokem, Ariel and Lee, Aaron Y.},
  year = {2017},
  month = jun,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {7},
  pages = {3440--3448},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.003440},
  urldate = {2019-11-22},
  abstract = {Evaluation of clinical images is essential for diagnosis in many specialties. Therefore the development of computer vision algorithms to help analyze biomedical images will be important. In ophthalmology, optical coherence tomography (OCT) is critical for managing retinal conditions. We developed a convolutional neural network (CNN) that detects intraretinal fluid (IRF) on OCT in a manner indistinguishable from clinicians. Using 1,289 OCT images, the CNN segmented images with a 0.911 cross-validated Dice coefficient, compared with segmentations by experts. Additionally, the agreement between experts and between experts and CNN were similar. Our results reveal that CNN can be trained to perform automated segmentations of clinically relevant image features.},
  pmcid = {PMC5508840},
  pmid = {28717579}
}

@article{leeDeeplearningBasedAutomated2017b,
  title = {Deep-Learning Based, Automated Segmentation of Macular Edema in Optical Coherence Tomography},
  author = {Lee, Cecilia S. and Tyring, Ariel J. and Deruyter, Nicolaas P. and Wu, Yue and Rokem, Ariel and Lee, Aaron Y.},
  year = {2017},
  month = jun,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {7},
  pages = {3440--3448},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.003440},
  urldate = {2019-11-22},
  abstract = {Evaluation of clinical images is essential for diagnosis in many specialties. Therefore the development of computer vision algorithms to help analyze biomedical images will be important. In ophthalmology, optical coherence tomography (OCT) is critical for managing retinal conditions. We developed a convolutional neural network (CNN) that detects intraretinal fluid (IRF) on OCT in a manner indistinguishable from clinicians. Using 1,289 OCT images, the CNN segmented images with a 0.911 cross-validated Dice coefficient, compared with segmentations by experts. Additionally, the agreement between experts and between experts and CNN were similar. Our results reveal that CNN can be trained to perform automated segmentations of clinically relevant image features.},
  pmcid = {PMC5508840},
  pmid = {28717579},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\22UBDYRW\\lee2017.pdf;C\:\\Users\\cleme\\Zotero\\storage\\EU3PB8DA\\Lee et al. - 2017 - Deep-learning based, automated segmentation of mac.pdf}
}

@article{leeDeeplearningBasedAutomated2017c,
  title = {Deep-Learning Based, Automated Segmentation of Macular Edema in Optical Coherence Tomography},
  author = {Lee, Cecilia S. and Tyring, Ariel J. and Deruyter, Nicolaas P. and Wu, Yue and Rokem, Ariel and Lee, Aaron Y.},
  year = {2017},
  month = jun,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {7},
  pages = {3440--3448},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.003440},
  urldate = {2019-11-22},
  abstract = {Evaluation of clinical images is essential for diagnosis in many specialties. Therefore the development of computer vision algorithms to help analyze biomedical images will be important. In ophthalmology, optical coherence tomography (OCT) is critical for managing retinal conditions. We developed a convolutional neural network (CNN) that detects intraretinal fluid (IRF) on OCT in a manner indistinguishable from clinicians. Using 1,289 OCT images, the CNN segmented images with a 0.911 cross-validated Dice coefficient, compared with segmentations by experts. Additionally, the agreement between experts and between experts and CNN were similar. Our results reveal that CNN can be trained to perform automated segmentations of clinically relevant image features.},
  pmcid = {PMC5508840},
  pmid = {28717579},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5VQHKJLT\\Lee et al. - 2017 - Deep-learning based, automated segmentation of mac.pdf;C\:\\Users\\cleme\\Zotero\\storage\\PBQLY8GK\\lee2017.pdf}
}

@article{leeDeepLearningEffective2017,
  title = {Deep {{Learning Is Effective}} for {{Classifying Normal}} versus {{Age-Related Macular Degeneration OCT Images}}},
  author = {Lee, Cecilia S. and Baughman, Doug M. and Lee, Aaron Y.},
  year = {2017},
  month = jul,
  journal = {Ophthalmology Retina},
  volume = {1},
  number = {4},
  pages = {322--327},
  issn = {2468-7219, 2468-6530},
  doi = {10.1016/j.oret.2016.12.009},
  urldate = {2019-07-24},
  abstract = {{$<$}h3{$>$}Purpose{$<$}/h3{$><$}p{$>$}The advent of electronic medical records (EMRs) with large electronic imaging databases along with advances in deep neural networks with machine learning has provided a unique opportunity to achieve milestones in automated image analysis. Optical coherence tomography is the most common imaging modality in ophthalmology and represents a dense and rich data set when combined with labels derived from the EMR. We sought to determine whether deep learning could be utilized to distinguish normal OCT images from images from patients with age-related macular degeneration (AMD).{$<$}/p{$><$}h3{$>$}Design{$<$}/h3{$><$}p{$>$}EMR and OCT database study.{$<$}/p{$><$}h3{$>$}Subjects{$<$}/h3{$><$}p{$>$}Normal and AMD patients who underwent macular OCT.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}Automated extraction of an OCT database was performed and linked to clinical end points from the EMR. Optical coherence tomography scans of the macula were obtained by Heidelberg Spectralis, and each OCT scan was linked to EMR clinical end points extracted from EPIC. The central 11 images were selected from each OCT scan of 2 cohorts of patients: normal and AMD. Cross-validation was performed using a random subset of patients. Receiver operating characteristic (ROC) curves were constructed at an independent image level, macular OCT level, and patient level.{$<$}/p{$><$}h3{$>$}Main Outcome Measure{$<$}/h3{$><$}p{$>$}Area under the ROC curve.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}Of a recent extraction of 2.6 million OCT images linked to clinical data points from the EMR, 52 690 normal macular OCT images and 48 312 AMD macular OCT images were selected. A deep neural network was trained to categorize images as either normal or AMD. At the image level, we achieved an area under the ROC curve of 92.78\% with an accuracy of 87.63\%. At the macula level, we achieved an area under the ROC curve of 93.83\% with an accuracy of 88.98\%. At a patient level, we achieved an area under the ROC curve of 97.45\% with an accuracy of 93.45\%. Peak sensitivity and specificity with optimal cutoffs were 92.64\% and 93.69\%, respectively.{$<$}/p{$><$}h3{$>$}Conclusions{$<$}/h3{$><$}p{$>$}The deep learning technique achieves high accuracy and is effective as a new image classification technique. These findings have important implications in utilizing OCT in automated screening and the development of computer-aided diagnosis tools in the future.{$<$}/p{$>$}},
  langid = {english},
  pmid = {30693348}
}

@article{leeDeepLearningEffective2017a,
  title = {Deep {{Learning Is Effective}} for {{Classifying Normal}} versus {{Age-Related Macular Degeneration OCT Images}}},
  author = {Lee, Cecilia S. and Baughman, Doug M. and Lee, Aaron Y.},
  year = {2017},
  month = jul,
  journal = {Ophthalmology Retina},
  volume = {1},
  number = {4},
  pages = {322--327},
  issn = {2468-6530},
  doi = {10.1016/j.oret.2016.12.009},
  urldate = {2019-11-22},
  abstract = {Purpose The advent of electronic medical records (EMRs) with large electronic imaging databases along with advances in deep neural networks with machine learning has provided a unique opportunity to achieve milestones in automated image analysis. Optical coherence tomography is the most common imaging modality in ophthalmology and represents a dense and rich data set when combined with labels derived from the EMR. We sought to determine whether deep learning could be utilized to distinguish normal OCT images from images from patients with age-related macular degeneration (AMD). Design EMR and OCT database study. Subjects Normal and AMD patients who underwent macular OCT. Methods Automated extraction of an OCT database was performed and linked to clinical end points from the EMR. Optical coherence tomography scans of the macula were obtained by Heidelberg Spectralis, and each OCT scan was linked to EMR clinical end points extracted from EPIC. The central 11 images were selected from each OCT scan of 2 cohorts of patients: normal and AMD. Cross-validation was performed using a random subset of patients. Receiver operating characteristic (ROC) curves were constructed at an independent image level, macular OCT level, and patient level. Main Outcome Measure Area under the ROC curve. Results Of a recent extraction of 2.6 million OCT images linked to clinical data points from the EMR, 52 690 normal macular OCT images and 48 312 AMD macular OCT images were selected. A deep neural network was trained to categorize images as either normal or AMD. At the image level, we achieved an area under the ROC curve of 92.78\% with an accuracy of 87.63\%. At the macula level, we achieved an area under the ROC curve of 93.83\% with an accuracy of 88.98\%. At a patient level, we achieved an area under the ROC curve of 97.45\% with an accuracy of 93.45\%. Peak sensitivity and specificity with optimal cutoffs were 92.64\% and 93.69\%, respectively. Conclusions The deep learning technique achieves high accuracy and is effective as a new image classification technique. These findings have important implications in utilizing OCT in automated screening and the development of computer-aided diagnosis tools in the future.},
  langid = {english}
}

@article{leeDeepLearningEffective2017b,
  title = {Deep {{Learning Is Effective}} for {{Classifying Normal}} versus {{Age-Related Macular Degeneration OCT Images}}},
  author = {Lee, Cecilia S. and Baughman, Doug M. and Lee, Aaron Y.},
  year = {2017},
  month = jul,
  journal = {Ophthalmology Retina},
  volume = {1},
  number = {4},
  pages = {322--327},
  issn = {2468-7219, 2468-6530},
  doi = {10.1016/j.oret.2016.12.009},
  urldate = {2019-07-24},
  abstract = {{$<$}h3{$>$}Purpose{$<$}/h3{$><$}p{$>$}The advent of electronic medical records (EMRs) with large electronic imaging databases along with advances in deep neural networks with machine learning has provided a unique opportunity to achieve milestones in automated image analysis. Optical coherence tomography is the most common imaging modality in ophthalmology and represents a dense and rich data set when combined with labels derived from the EMR. We sought to determine whether deep learning could be utilized to distinguish normal OCT images from images from patients with age-related macular degeneration (AMD).{$<$}/p{$><$}h3{$>$}Design{$<$}/h3{$><$}p{$>$}EMR and OCT database study.{$<$}/p{$><$}h3{$>$}Subjects{$<$}/h3{$><$}p{$>$}Normal and AMD patients who underwent macular OCT.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}Automated extraction of an OCT database was performed and linked to clinical end points from the EMR. Optical coherence tomography scans of the macula were obtained by Heidelberg Spectralis, and each OCT scan was linked to EMR clinical end points extracted from EPIC. The central 11 images were selected from each OCT scan of 2 cohorts of patients: normal and AMD. Cross-validation was performed using a random subset of patients. Receiver operating characteristic (ROC) curves were constructed at an independent image level, macular OCT level, and patient level.{$<$}/p{$><$}h3{$>$}Main Outcome Measure{$<$}/h3{$><$}p{$>$}Area under the ROC curve.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}Of a recent extraction of 2.6 million OCT images linked to clinical data points from the EMR, 52 690 normal macular OCT images and 48 312 AMD macular OCT images were selected. A deep neural network was trained to categorize images as either normal or AMD. At the image level, we achieved an area under the ROC curve of 92.78\% with an accuracy of 87.63\%. At the macula level, we achieved an area under the ROC curve of 93.83\% with an accuracy of 88.98\%. At a patient level, we achieved an area under the ROC curve of 97.45\% with an accuracy of 93.45\%. Peak sensitivity and specificity with optimal cutoffs were 92.64\% and 93.69\%, respectively.{$<$}/p{$><$}h3{$>$}Conclusions{$<$}/h3{$><$}p{$>$}The deep learning technique achieves high accuracy and is effective as a new image classification technique. These findings have important implications in utilizing OCT in automated screening and the development of computer-aided diagnosis tools in the future.{$<$}/p{$>$}},
  langid = {english},
  pmid = {30693348},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\LSI5YY8M\\Lee et al. - 2017 - Deep Learning Is Effective for Classifying Normal .pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZTPD4GPP\\abstract.html}
}

@article{leeDeepLearningEffective2017c,
  title = {Deep {{Learning Is Effective}} for {{Classifying Normal}} versus {{Age-Related Macular Degeneration OCT Images}}},
  author = {Lee, Cecilia S. and Baughman, Doug M. and Lee, Aaron Y.},
  year = {2017},
  month = jul,
  journal = {Ophthalmology Retina},
  volume = {1},
  number = {4},
  pages = {322--327},
  issn = {2468-6530},
  doi = {10.1016/j.oret.2016.12.009},
  urldate = {2019-11-22},
  abstract = {Purpose The advent of electronic medical records (EMRs) with large electronic imaging databases along with advances in deep neural networks with machine learning has provided a unique opportunity to achieve milestones in automated image analysis. Optical coherence tomography is the most common imaging modality in ophthalmology and represents a dense and rich data set when combined with labels derived from the EMR. We sought to determine whether deep learning could be utilized to distinguish normal OCT images from images from patients with age-related macular degeneration (AMD). Design EMR and OCT database study. Subjects Normal and AMD patients who underwent macular OCT. Methods Automated extraction of an OCT database was performed and linked to clinical end points from the EMR. Optical coherence tomography scans of the macula were obtained by Heidelberg Spectralis, and each OCT scan was linked to EMR clinical end points extracted from EPIC. The central 11 images were selected from each OCT scan of 2 cohorts of patients: normal and AMD. Cross-validation was performed using a random subset of patients. Receiver operating characteristic (ROC) curves were constructed at an independent image level, macular OCT level, and patient level. Main Outcome Measure Area under the ROC curve. Results Of a recent extraction of 2.6 million OCT images linked to clinical data points from the EMR, 52 690 normal macular OCT images and 48 312 AMD macular OCT images were selected. A deep neural network was trained to categorize images as either normal or AMD. At the image level, we achieved an area under the ROC curve of 92.78\% with an accuracy of 87.63\%. At the macula level, we achieved an area under the ROC curve of 93.83\% with an accuracy of 88.98\%. At a patient level, we achieved an area under the ROC curve of 97.45\% with an accuracy of 93.45\%. Peak sensitivity and specificity with optimal cutoffs were 92.64\% and 93.69\%, respectively. Conclusions The deep learning technique achieves high accuracy and is effective as a new image classification technique. These findings have important implications in utilizing OCT in automated screening and the development of computer-aided diagnosis tools in the future.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\FAYJ57GU\\lee2017.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UPFSBYXA\\Lee et al. - 2017 - Deep Learning Is Effective for Classifying Normal .pdf;C\:\\Users\\cleme\\Zotero\\storage\\W3AVDPN9\\S2468653016301749.html}
}

@inproceedings{leeLearnableStructuralSemantic2021,
  title = {Learnable {{Structural Semantic Readout}} for {{Graph Classification}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Lee, Dongha and Kim, Su and Lee, Seonghyeon and Park, Chanyoung and Yu, Hwanjo},
  year = {2021},
  month = dec,
  pages = {1180--1185},
  issn = {2374-8486},
  doi = {10.1109/ICDM51629.2021.00142},
  urldate = {2024-02-15},
  abstract = {With the great success of deep learning in various domains, graph neural networks (GNNs) also become a dominant approach to graph classification. By the help of a global readout operation that simply aggregates all node (or node-cluster) representations, existing GNN classifiers obtain a graph-level representation of an input graph and predict its class label using the representation. However, such global aggregation does not consider the structural information of each node, which results in information loss on the global structure. In this work, we propose structural semantic readout (SSRead) to summarize the node representations at the position-level, which allows to model the position-specific weight parameters for classification as well as to effectively capture the graph semantic relevant to the global structure. Given an input graph, SSRead aims to identify structurally-meaningful positions by using the semantic alignment between its nodes and structural prototypes, which encode the prototypical features of each position. The structural prototypes are optimized to minimize the alignment cost for all training graphs, while the other GNN parameters are trained to predict the class labels. Our experimental results demonstrate that SSRead significantly improves the classification performance and interpretability of GNN classifiers while being compatible with a variety of aggregation functions, GNN architectures, and learning frameworks.},
  keywords = {Aggregates,Conferences,Costs,Deep learning,Global structural information,Graph classification,Graph neural networks,Learnable graph readout,Prototypes,Semantics,Training},
  file = {C:\Users\cleme\Zotero\storage\TRWBVVL6\Lee et al. - 2021 - Learnable Structural Semantic Readout for Graph Cl.pdf}
}

@article{leitgebDopplerOpticalCoherence2014,
  title = {Doppler {{Optical Coherence Tomography}}},
  author = {Leitgeb, Rainer A. and Werkmeister, Ren{\'e} M. and Blatter, Cedric and Schmetterer, Leopold},
  year = {2014},
  month = jul,
  journal = {Progress in Retinal and Eye Research},
  volume = {41},
  number = {100},
  pages = {26--43},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2014.03.004},
  urldate = {2019-11-28},
  abstract = {Optical Coherence Tomography (OCT) has revolutionized ophthalmology. Since its introduction in the early 1990s it has continuously improved in terms of speed, resolution and sensitivity. The technique has also seen a variety of extensions aiming to assess functional aspects of the tissue in addition to morphology. One of these approaches is Doppler OCT (DOCT), which aims to visualize and quantify blood flow. Such extensions were already implemented in time domain systems, but have gained importance with the introduction of Fourier domain OCT. Nowadays phase-sensitive detection techniques are most widely used to extract blood velocity and blood flow from tissues. A common problem with the technique is that the Doppler angle is not known and several approaches have been realized to obtain absolute velocity and flow data from the retina. Additional studies are required to elucidate which of these techniques is most promising. In the recent years, however, several groups have shown that data can be obtained with high validity and reproducibility. In addition, several groups have published values for total retinal blood flow. Another promising application relates to non-invasive angiography. As compared to standard techniques such as fluorescein and indocyanine-green angiography the technique offers two major advantages: no dye is required and depth resolution is required is provided. As such Doppler OCT has the potential to improve our abilities to diagnose and monitor ocular vascular diseases.},
  pmcid = {PMC4073226},
  pmid = {24704352}
}

@article{leitgebDopplerOpticalCoherence2014a,
  title = {Doppler {{Optical Coherence Tomography}}},
  author = {Leitgeb, Rainer A. and Werkmeister, Ren{\'e} M. and Blatter, Cedric and Schmetterer, Leopold},
  year = {2014},
  month = jul,
  journal = {Progress in Retinal and Eye Research},
  volume = {41},
  number = {100},
  pages = {26--43},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2014.03.004},
  urldate = {2019-11-28},
  abstract = {Optical Coherence Tomography (OCT) has revolutionized ophthalmology. Since its introduction in the early 1990s it has continuously improved in terms of speed, resolution and sensitivity. The technique has also seen a variety of extensions aiming to assess functional aspects of the tissue in addition to morphology. One of these approaches is Doppler OCT (DOCT), which aims to visualize and quantify blood flow. Such extensions were already implemented in time domain systems, but have gained importance with the introduction of Fourier domain OCT. Nowadays phase-sensitive detection techniques are most widely used to extract blood velocity and blood flow from tissues. A common problem with the technique is that the Doppler angle is not known and several approaches have been realized to obtain absolute velocity and flow data from the retina. Additional studies are required to elucidate which of these techniques is most promising. In the recent years, however, several groups have shown that data can be obtained with high validity and reproducibility. In addition, several groups have published values for total retinal blood flow. Another promising application relates to non-invasive angiography. As compared to standard techniques such as fluorescein and indocyanine-green angiography the technique offers two major advantages: no dye is required and depth resolution is required is provided. As such Doppler OCT has the potential to improve our abilities to diagnose and monitor ocular vascular diseases.},
  pmcid = {PMC4073226},
  pmid = {24704352},
  file = {C:\Users\cleme\Zotero\storage\S3JNIH9Z\Leitgeb et al. - 2014 - Doppler Optical Coherence Tomography.pdf}
}

@article{leitgebRealtimeMeasurementVitro2004,
  title = {Real-Time Measurement of in Vitro Flow by {{Fourier-domain}} Color {{Doppler}} Optical Coherence Tomography},
  author = {Leitgeb, Rainer A. and Schmetterer, Leopold and Hitzenberger, Christoph K. and Fercher, Adolf F. and Berisha, Fatma and Wojtkowski, Maciej and Bajraszewski, Tomasz},
  year = {2004},
  month = jan,
  journal = {Optics Letters},
  volume = {29},
  number = {2},
  pages = {171--173},
  issn = {1539-4794},
  doi = {10.1364/OL.29.000171},
  urldate = {2019-11-14},
  abstract = {The possibility of measuring a full Doppler flow depth profile in parallel by use of frequency-domain optical coherence tomography is demonstrated. The method is based on a local phase analysis of the backscattered signal and allows for imaging of bidirectional Doppler flow. The Doppler frequency limit is 5 kHz for the presented measurements and is set by half of the frame rate of the CCD detector array. We measured the flow of 0.3-{\textmu}m microspheres suspended in distilled water at controlled flow rates and in vitro human blood flow through a 200-{\textmu}m capillary with a real-time color-encoded Doppler tomogram rate of 2--3/s.},
  copyright = {\&\#169; 2004 Optical Society of America},
  langid = {english},
  keywords = {Doppler effect,Fourier transforms,High speed imaging,Optical coherence tomography,Speckle noise,Speckle patterns}
}

@article{leitgebRealtimeMeasurementVitro2004a,
  title = {Real-Time Measurement of in Vitro Flow by {{Fourier-domain}} Color {{Doppler}} Optical Coherence Tomography},
  author = {Leitgeb, Rainer A. and Schmetterer, Leopold and Hitzenberger, Christoph K. and Fercher, Adolf F. and Berisha, Fatma and Wojtkowski, Maciej and Bajraszewski, Tomasz},
  year = {2004},
  month = jan,
  journal = {Optics Letters},
  volume = {29},
  number = {2},
  pages = {171--173},
  issn = {1539-4794},
  doi = {10.1364/OL.29.000171},
  urldate = {2019-11-14},
  abstract = {The possibility of measuring a full Doppler flow depth profile in parallel by use of frequency-domain optical coherence tomography is demonstrated. The method is based on a local phase analysis of the backscattered signal and allows for imaging of bidirectional Doppler flow. The Doppler frequency limit is 5 kHz for the presented measurements and is set by half of the frame rate of the CCD detector array. We measured the flow of 0.3-{\textmu}m microspheres suspended in distilled water at controlled flow rates and in vitro human blood flow through a 200-{\textmu}m capillary with a real-time color-encoded Doppler tomogram rate of 2--3/s.},
  copyright = {\&\#169; 2004 Optical Society of America},
  langid = {english},
  keywords = {Doppler effect,Fourier transforms,High speed imaging,Optical coherence tomography,Speckle noise,Speckle patterns},
  file = {C:\Users\cleme\Zotero\storage\ZKVVYQ3L\abstract.html}
}

@misc{lemaitreClassificationSDOCTVolumes2016,
  type = {Research {{Article}}},
  title = {Classification of {{SD-OCT Volumes Using Local Binary Patterns}}: {{Experimental Validation}} for {{DME Detection}}},
  shorttitle = {Classification of {{SD-OCT Volumes Using Local Binary Patterns}}},
  author = {Lema{\^i}tre, Guillaume and Rastgoo, Mojdeh and Massich, Joan and Cheung, Carol Y. and Wong, Tien Y. and Lamoureux, Ecosse and Milea, Dan and M{\'e}riaudeau, Fabrice and Sidib{\'e}, D{\'e}sir{\'e}},
  year = {2016},
  doi = {10.1155/2016/3298606},
  urldate = {2019-11-22},
  abstract = {This paper addresses the problem of automatic classification of Spectral Domain OCT (SD-OCT) data for automatic identification of patients with DME versus normal subjects. Optical Coherence Tomography (OCT) has been a valuable diagnostic tool for DME, which is among the most common causes of irreversible vision loss in individuals with diabetes. Here, a classification framework with five distinctive steps is proposed and we present an extensive study of each step. Our method considers combination of various preprocessing steps in conjunction with Local Binary Patterns (LBP) features and different mapping strategies. Using linear and nonlinear classifiers, we tested the developed framework on a balanced cohort of 32 patients. Experimental results show that the proposed method outperforms the previous studies by achieving a Sensitivity (SE) and a Specificity (SP) of 81.2\% and 93.7\%, respectively. Our study concludes that the 3D features and high-level representation of 2D features using patches achieve the best results. However, the effects of preprocessing are inconsistent with different classifiers and feature configurations.},
  langid = {english}
}

@misc{lemaitreClassificationSDOCTVolumes2016a,
  type = {Research Article},
  title = {Classification of {{SD-OCT Volumes Using Local Binary Patterns}}: {{Experimental Validation}} for {{DME Detection}}},
  shorttitle = {Classification of {{SD-OCT Volumes Using Local Binary Patterns}}},
  author = {Lema{\^i}tre, Guillaume and Rastgoo, Mojdeh and Massich, Joan and Cheung, Carol Y. and Wong, Tien Y. and Lamoureux, Ecosse and Milea, Dan and M{\'e}riaudeau, Fabrice and Sidib{\'e}, D{\'e}sir{\'e}},
  year = {2016},
  journal = {Journal of Ophthalmology},
  doi = {10.1155/2016/3298606},
  urldate = {2019-11-22},
  abstract = {This paper addresses the problem of automatic classification of Spectral Domain OCT (SD-OCT) data for automatic identification of patients with DME versus normal subjects. Optical Coherence Tomography (OCT) has been a valuable diagnostic tool for DME, which is among the most common causes of irreversible vision loss in individuals with diabetes. Here, a classification framework with five distinctive steps is proposed and we present an extensive study of each step. Our method considers combination of various preprocessing steps in conjunction with Local Binary Patterns (LBP) features and different mapping strategies. Using linear and nonlinear classifiers, we tested the developed framework on a balanced cohort of 32 patients. Experimental results show that the proposed method outperforms the previous studies by achieving a Sensitivity (SE) and a Specificity (SP) of 81.2\% and 93.7\%, respectively. Our study concludes that the 3D features and high-level representation of 2D features using patches achieve the best results. However, the effects of preprocessing are inconsistent with different classifiers and feature configurations.},
  howpublished = {https://www.hindawi.com/journals/joph/2016/3298606/},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\WAP4KJZN\\Lemaître et al. - 2016 - Classification of SD-OCT Volumes Using Local Binar.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZFKITM2P\\3298606.html}
}

@article{leopoldPixelBNNAugmentingPixelCNN2019,
  title = {{{PixelBNN}}: {{Augmenting}} the {{PixelCNN}} with {{Batch Normalization}} and the {{Presentation}} of a {{Fast Architecture}} for {{Retinal Vessel Segmentation}}},
  shorttitle = {{{PixelBNN}}},
  author = {Leopold, Henry A. and Orchard, Jeff and Zelek, John S. and Lakshminarayanan, Vasudevan},
  year = {2019},
  month = feb,
  journal = {Journal of Imaging},
  volume = {5},
  number = {2},
  pages = {26},
  doi = {10.3390/jimaging5020026},
  urldate = {2019-12-29},
  abstract = {Analysis of retinal fundus images is essential for eye-care physicians in the diagnosis, care and treatment of patients. Accurate fundus and/or retinal vessel maps give rise to longitudinal studies able to utilize multimedia image registration and disease/condition status measurements, as well as applications in surgery preparation and biometrics. The segmentation of retinal morphology has numerous applications in assessing ophthalmologic and cardiovascular disease pathologies. Computer-aided segmentation of the vasculature has proven to be a challenge, mainly due to inconsistencies such as noise and variations in hue and brightness that can greatly reduce the quality of fundus images. The goal of this work is to collate different key performance indicators (KPIs) and state-of-the-art methods applied to this task, frame computational efficiency\&ndash;performance trade-offs under varying degrees of information loss using common datasets, and introduce PixelBNN, a highly efficient deep method for automating the segmentation of fundus morphologies. The model was trained, tested and cross tested on the DRIVE, STARE and CHASE\_DB1 retinal vessel segmentation datasets. Performance was evaluated using G-mean, Mathews Correlation Coefficient and F1-score, with the main success measure being computation speed. The network was 8.5\&times; faster than the current state-of-the-art at test time and performed comparatively well, considering a 5\&times; to 19\&times; reduction in information from resizing images during preprocessing.},
  langid = {english},
  keywords = {convolutional networks,deep learning,image segmentation,ophthalmic diagnosis,ophthalmology,retina,retinal vessels}
}

@article{leopoldPixelBNNAugmentingPixelCNN2019a,
  title = {{{PixelBNN}}: {{Augmenting}} the {{PixelCNN}} with {{Batch Normalization}} and the {{Presentation}} of a {{Fast Architecture}} for {{Retinal Vessel Segmentation}}},
  shorttitle = {{{PixelBNN}}},
  author = {Leopold, Henry A. and Orchard, Jeff and Zelek, John S. and Lakshminarayanan, Vasudevan},
  year = {2019},
  month = feb,
  journal = {Journal of Imaging},
  volume = {5},
  number = {2},
  pages = {26},
  doi = {10.3390/jimaging5020026},
  urldate = {2019-12-29},
  abstract = {Analysis of retinal fundus images is essential for eye-care physicians in the diagnosis, care and treatment of patients. Accurate fundus and/or retinal vessel maps give rise to longitudinal studies able to utilize multimedia image registration and disease/condition status measurements, as well as applications in surgery preparation and biometrics. The segmentation of retinal morphology has numerous applications in assessing ophthalmologic and cardiovascular disease pathologies. Computer-aided segmentation of the vasculature has proven to be a challenge, mainly due to inconsistencies such as noise and variations in hue and brightness that can greatly reduce the quality of fundus images. The goal of this work is to collate different key performance indicators (KPIs) and state-of-the-art methods applied to this task, frame computational efficiency\&ndash;performance trade-offs under varying degrees of information loss using common datasets, and introduce PixelBNN, a highly efficient deep method for automating the segmentation of fundus morphologies. The model was trained, tested and cross tested on the DRIVE, STARE and CHASE\_DB1 retinal vessel segmentation datasets. Performance was evaluated using G-mean, Mathews Correlation Coefficient and F1-score, with the main success measure being computation speed. The network was 8.5\&times; faster than the current state-of-the-art at test time and performed comparatively well, considering a 5\&times; to 19\&times; reduction in information from resizing images during preprocessing.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {convolutional networks,deep learning,image segmentation,ophthalmic diagnosis,ophthalmology,retina,retinal vessels},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\NC7VPZAR\\Leopold et al. - 2019 - PixelBNN Augmenting the PixelCNN with Batch Norma.pdf;C\:\\Users\\cleme\\Zotero\\storage\\TP2HIBLP\\26.html}
}

@inproceedings{lepetit-aimonLargeReceptiveField2018,
  title = {Large {{Receptive Field Fully Convolutional Network}} for {{Semantic Segmentation}} of {{Retinal Vasculature}} in {{Fundus Images}}},
  booktitle = {Computational {{Pathology}} and {{Ophthalmic Medical Image Analysis}}},
  author = {{Lepetit-Aimon}, Gabriel and Duval, Renaud and Cheriet, Farida},
  editor = {Stoyanov, Danail and Taylor, Zeike and Ciompi, Francesco and Xu, Yanwu and Martel, Anne and {Maier-Hein}, Lena and Rajpoot, Nasir and {van der Laak}, Jeroen and Veta, Mitko and McKenna, Stephen and Snead, David and Trucco, Emanuele and Garvin, Mona K. and Chen, Xin Jan and Bogunovic, Hrvoje},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {201--209},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-00949-6_24},
  abstract = {Analysis of the retinal vasculature morphology from fundus images, using measures such as arterio-venous ratio, is a promising lead for the early diagnosis of cardiovascular risks. The accuracy of these measures relies on the robustness of the vessels segmentation and classification. However, algorithms based on prior topological knowledge have difficulty modelling the abnormal structure of pathological vasculatures, while patch-trained Fully Convolutional Neural Networks (FCNNs) struggle to learn the wide and extensive topology of the vessels because of their narrow receptive fields.},
  isbn = {978-3-030-00949-6},
  langid = {english},
  keywords = {Convolutional neural network,Deep learning,Retinal vessel classification,Retinal vessel segmentation}
}

@inproceedings{lepetit-aimonLargeReceptiveField2018a,
  title = {Large {{Receptive Field Fully Convolutional Network}} for {{Semantic Segmentation}} of {{Retinal Vasculature}} in {{Fundus Images}}},
  booktitle = {Computational {{Pathology}} and {{Ophthalmic Medical Image Analysis}}},
  author = {{Lepetit-Aimon}, Gabriel and Duval, Renaud and Cheriet, Farida},
  editor = {Stoyanov, Danail and Taylor, Zeike and Ciompi, Francesco and Xu, Yanwu and Martel, Anne and {Maier-Hein}, Lena and Rajpoot, Nasir and {van der Laak}, Jeroen and Veta, Mitko and McKenna, Stephen and Snead, David and Trucco, Emanuele and Garvin, Mona K. and Chen, Xin Jan and Bogunovic, Hrvoje},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {201--209},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-00949-6_24},
  abstract = {Analysis of the retinal vasculature morphology from fundus images, using measures such as arterio-venous ratio, is a promising lead for the early diagnosis of cardiovascular risks. The accuracy of these measures relies on the robustness of the vessels segmentation and classification. However, algorithms based on prior topological knowledge have difficulty modelling the abnormal structure of pathological vasculatures, while patch-trained Fully Convolutional Neural Networks (FCNNs) struggle to learn the wide and extensive topology of the vessels because of their narrow receptive fields.},
  isbn = {978-3-030-00949-6},
  langid = {english},
  keywords = {Convolutional neural network,Deep learning,Retinal vessel classification,Retinal vessel segmentation},
  file = {C:\Users\cleme\Zotero\storage\M6CBNQ86\Lepetit-Aimon et al. - 2018 - Large Receptive Field Fully Convolutional Network .pdf}
}

@misc{lepetit-aimonMAPLESDRMESSIDORAnatomical2024,
  title = {{{MAPLES-DR}}: {{MESSIDOR Anatomical}} and {{Pathological Labels}} for {{Explainable Screening}} of {{Diabetic Retinopathy}}},
  shorttitle = {{{MAPLES-DR}}},
  author = {{Lepetit-Aimon}, Gabriel and Playout, Cl{\'e}ment and Boucher, Marie Carole and Duval, Renaud and Brent, Michael H. and Cheriet, Farida},
  year = {2024},
  month = jan,
  number = {arXiv:2402.04258},
  eprint = {2402.04258},
  primaryclass = {cs, eess, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.04258},
  urldate = {2024-06-22},
  abstract = {Reliable automatic diagnosis of Diabetic Retinopathy (DR) and Macular Edema (ME) is an invaluable asset in improving the rate of monitored patients among at-risk populations and in enabling earlier treatments before the pathology progresses and threatens vision. However, the explainability of screening models is still an open question, and specifically designed datasets are required to support the research. We present MAPLES-DR (MESSIDOR Anatomical and Pathological Labels for Explainable Screening of Diabetic Retinopathy), which contains, for 198 images of the MESSIDOR public fundus dataset, new diagnoses for DR and ME as well as new pixel-wise segmentation maps for 10 anatomical and pathological biomarkers related to DR. This paper documents the design choices and the annotation procedure that produced MAPLES-DR, discusses the interobserver variability and the overall quality of the annotations, and provides guidelines on using the dataset in a machine learning context.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Electrical Engineering and Systems Science - Image and Video Processing,Quantitative Biology - Quantitative Methods},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\59LW6EKJ\\Lepetit-Aimon et al. - 2024 - MAPLES-DR MESSIDOR Anatomical and Pathological La.pdf;C\:\\Users\\cleme\\Zotero\\storage\\4Q2MAPJN\\2402.html}
}

@article{lewisGeostationaryDopplerRadar2011,
  title = {Geostationary {{Doppler Radar}} and {{Tropical Cyclone Surveillance}}},
  author = {Lewis, William E. and Im, Eastwood and Tanelli, Simone and Haddad, Ziad and Tripoli, Gregory J. and Smith, Eric A.},
  year = {2011},
  month = jul,
  journal = {Journal of Atmospheric and Oceanic Technology},
  volume = {28},
  number = {10},
  pages = {1185--1191},
  issn = {0739-0572},
  doi = {10.1175/JTECH-D-11-00060.1},
  urldate = {2019-06-10},
  abstract = {The potential usefulness of spaceborne Doppler radar as a tropical cyclone observing tool is assessed by conducting a high-resolution simulation of an intense hurricane and generating synthetic observations of reflectivity and radial velocity. The ground-based velocity track display (GBVTD) technique is used to process the radial velocity observations and generate retrievals of meteorologically relevant metrics such as the maximum wind (MW), radius of maximum wind (RMW), and radius of 64-kt wind (R64). Results indicate that the performance of the retrieved metrics compares favorably with the current state-of-the-art satellite methods for intensity estimation and somewhat better than current methods for structure (i.e., wind radii).}
}

@article{lewisGeostationaryDopplerRadar2011a,
  title = {Geostationary {{Doppler Radar}} and {{Tropical Cyclone Surveillance}}},
  author = {Lewis, William E. and Im, Eastwood and Tanelli, Simone and Haddad, Ziad and Tripoli, Gregory J. and Smith, Eric A.},
  year = {2011},
  month = jul,
  journal = {Journal of Atmospheric and Oceanic Technology},
  volume = {28},
  number = {10},
  pages = {1185--1191},
  issn = {0739-0572},
  doi = {10.1175/JTECH-D-11-00060.1},
  urldate = {2019-06-10},
  abstract = {The potential usefulness of spaceborne Doppler radar as a tropical cyclone observing tool is assessed by conducting a high-resolution simulation of an intense hurricane and generating synthetic observations of reflectivity and radial velocity. The ground-based velocity track display (GBVTD) technique is used to process the radial velocity observations and generate retrievals of meteorologically relevant metrics such as the maximum wind (MW), radius of maximum wind (RMW), and radius of 64-kt wind (R64). Results indicate that the performance of the retrieved metrics compares favorably with the current state-of-the-art satellite methods for intensity estimation and somewhat better than current methods for structure (i.e., wind radii).},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\U7FLWCDA\\Lewis et al. - 2011 - Geostationary Doppler Radar and Tropical Cyclone S.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UW78YQ9G\\JTECH-D-11-00060.html}
}

@article{liangNOTALLPATCHES2022,
  title = {{{NOT ALL PATCHES ARE WHAT YOU NEED}}: {{EXPEDITING VISION TRANSFORMERS VIA TOKEN REORGANIZATIONS}}},
  author = {Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  year = {2022},
  langid = {english}
}

@book{liangNotAllPatches2022,
  title = {Not {{All Patches}} Are {{What You Need}}: {{Expediting Vision Transformers}} via {{Token Reorganizations}}},
  shorttitle = {Not {{All Patches}} Are {{What You Need}}},
  author = {Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  year = {2022},
  month = feb,
  abstract = {Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50\% while its recognition accuracy is decreased by only 0.3\% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1\% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit}
}

@inproceedings{liangNotAllPatches2022a,
  title = {Not {{All Patches}} Are {{What You Need}}: {{Expediting Vision Transformers}} via {{Token Reorganizations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  year = {2022},
  langid = {english}
}

@inproceedings{liangNotAllPatches2022b,
  title = {Not {{All Patches}} Are {{What You Need}}: {{Expediting Vision Transformers}} via {{Token Reorganizations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  year = {2022},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\VDYTVCMW\Liang et al. - Not All Patches are What You Need Expediting Visi.pdf}
}

@book{liangNotAllPatches2022c,
  title = {Not {{All Patches}} Are {{What You Need}}: {{Expediting Vision Transformers}} via {{Token Reorganizations}}},
  shorttitle = {Not {{All Patches}} Are {{What You Need}}},
  author = {Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  year = {2022},
  month = feb,
  abstract = {Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50\% while its recognition accuracy is decreased by only 0.3\% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1\% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit},
  file = {C:\Users\cleme\Zotero\storage\E86YE778\Liang et al. - 2022 - Not All Patches are What You Need Expediting Visi.pdf}
}

@article{liangNOTALLPATCHES2022d,
  title = {{{NOT ALL PATCHES ARE WHAT YOU NEED}}: {{EXPEDITING VISION TRANSFORMERS VIA TOKEN REORGANIZATIONS}}},
  author = {Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  year = {2022},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\REMC4FZG\Liang et al. - 2022 - NOT ALL PATCHES ARE WHAT YOU NEED EXPEDITING VISI.pdf}
}

@article{liApplicationsDeepLearning2021,
  title = {Applications of Deep Learning in Fundus Images: {{A}} Review},
  shorttitle = {Applications of Deep Learning in Fundus Images},
  author = {Li, Tao and Bo, Wang and Hu, Chunyu and Kang, Hong and Liu, Hanruo and Wang, Kai and Fu, Huazhu},
  year = {2021},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {69},
  pages = {101971},
  issn = {1361-8415},
  doi = {10.1016/j.media.2021.101971},
  urldate = {2021-10-02},
  abstract = {The use of fundus images for the early screening of eye diseases is of great clinical importance. Due to its powerful performance, deep learning is becoming more and more popular in related applications, such as lesion segmentation, biomarkers segmentation, disease diagnosis and image synthesis. Therefore, it is very necessary to summarize the recent developments in deep learning for fundus images with a review paper. In this review, we introduce 143 application papers with a carefully designed hierarchy. Moreover, 33 publicly available datasets are presented. Summaries and analyses are provided for each task. Finally, limitations common to all tasks are revealed and possible solutions are given. We will also release and regularly update the state-of-the-art results and newly-released datasets at https://github.com/nkicsl/Fundus\_Review to adapt to the rapid development of this field.},
  langid = {english},
  keywords = {Deep learning,Eye diseases,Fundus images}
}

@article{liApplicationsDeepLearning2021a,
  title = {Applications of Deep Learning in Fundus Images: {{A}} Review},
  shorttitle = {Applications of Deep Learning in Fundus Images},
  author = {Li, Tao and Bo, Wang and Hu, Chunyu and Kang, Hong and Liu, Hanruo and Wang, Kai and Fu, Huazhu},
  year = {2021},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {69},
  pages = {101971},
  issn = {1361-8415},
  doi = {10.1016/j.media.2021.101971},
  urldate = {2021-10-02},
  abstract = {The use of fundus images for the early screening of eye diseases is of great clinical importance. Due to its powerful performance, deep learning is becoming more and more popular in related applications, such as lesion segmentation, biomarkers segmentation, disease diagnosis and image synthesis. Therefore, it is very necessary to summarize the recent developments in deep learning for fundus images with a review paper. In this review, we introduce 143 application papers with a carefully designed hierarchy. Moreover, 33 publicly available datasets are presented. Summaries and analyses are provided for each task. Finally, limitations common to all tasks are revealed and possible solutions are given. We will also release and regularly update the state-of-the-art results and newly-released datasets at https://github.com/nkicsl/Fundus\_Review to adapt to the rapid development of this field.},
  langid = {english},
  keywords = {Deep learning,Eye diseases,Fundus images},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\NBDZPIER\\Li et al. - 2021 - Applications of deep learning in fundus images A .pdf;C\:\\Users\\cleme\\Zotero\\storage\\ES42EWAV\\S1361841521000177.html}
}

@article{liDeepLearningBased2019,
  title = {Deep Learning Based Early Stage Diabetic Retinopathy Detection Using Optical Coherence Tomography},
  author = {Li, Xuechen and Shen, Linlin and Shen, Meixiao and Tan, Fan and Qiu, Connor S.},
  year = {2019},
  month = dec,
  journal = {Neurocomputing},
  volume = {369},
  pages = {134--144},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.08.079},
  urldate = {2022-07-13},
  abstract = {Diabetic retinopathy (DR) is one of the leading causes of preventable blindness globally. Performing retinal examinations on all diabetic patients is an unmet need, and detection at an early stage can provide better control of the disease. The objective of this study is to provide an optical coherence tomography (OCT) image based diagnostic technology for automated early DR diagnosis, including at both grades 0 and 1. This work can help ophthalmologists with evaluation and treatment, reducing the rate of vision loss, and enabling timely and accurate diagnosis. In this work, we developed and evaluated a novel deep network -- OCTD\_Net, for early-stage DR detection. While one of the networks extracted features from the original OCT image, the other extracted retinal layer information. The accuracy, sensitivity and specificity was 0.92, 0.90 and 0.95, respectively. Our analysis of retinal layers and the features learned by the proposed network suggests that grade 1 DR patients present with significant changes in the thickness and reflection of certain retinal layers. However, grade 0 DR patients do not have such significant changes. The heatmaps of the trained network also suggest that patients with early DR showed different textures around the myoid and ellipsoid zones, inner nuclear layers, and photoreceptor outer segments, which should all receive dedicated attention for early DR diagnosis.},
  langid = {english},
  keywords = {Computer-aided diagnosis,Deep learning,Diabetic retinopathy,Optical coherence tomography}
}

@article{liDeepLearningBased2019a,
  title = {Deep Learning Based Early Stage Diabetic Retinopathy Detection Using Optical Coherence Tomography},
  author = {Li, Xuechen and Shen, Linlin and Shen, Meixiao and Tan, Fan and Qiu, Connor S.},
  year = {2019},
  month = dec,
  journal = {Neurocomputing},
  volume = {369},
  pages = {134--144},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.08.079},
  urldate = {2022-07-13},
  abstract = {Diabetic retinopathy (DR) is one of the leading causes of preventable blindness globally. Performing retinal examinations on all diabetic patients is an unmet need, and detection at an early stage can provide better control of the disease. The objective of this study is to provide an optical coherence tomography (OCT) image based diagnostic technology for automated early DR diagnosis, including at both grades 0 and 1. This work can help ophthalmologists with evaluation and treatment, reducing the rate of vision loss, and enabling timely and accurate diagnosis. In this work, we developed and evaluated a novel deep network -- OCTD\_Net, for early-stage DR detection. While one of the networks extracted features from the original OCT image, the other extracted retinal layer information. The accuracy, sensitivity and specificity was 0.92, 0.90 and 0.95, respectively. Our analysis of retinal layers and the features learned by the proposed network suggests that grade 1 DR patients present with significant changes in the thickness and reflection of certain retinal layers. However, grade 0 DR patients do not have such significant changes. The heatmaps of the trained network also suggest that patients with early DR showed different textures around the myoid and ellipsoid zones, inner nuclear layers, and photoreceptor outer segments, which should all receive dedicated attention for early DR diagnosis.},
  langid = {english},
  keywords = {Computer-aided diagnosis,Deep learning,Diabetic retinopathy,Optical coherence tomography},
  file = {C:\Users\cleme\Zotero\storage\VY4T5IDF\Li et al. - 2019 - Deep learning based early stage diabetic retinopat.pdf}
}

@article{liDeepLearningbasedAutomated2019,
  title = {Deep Learning-Based Automated Detection of Retinal Diseases Using Optical Coherence Tomography Images},
  author = {Li, Feng and Chen, Hua and Liu, Zheng and Zhang, Xue-dian and Jiang, Min-shan and Jiang, Min-shan and Wu, Zhi-zheng and Zhou, Kai-qian},
  year = {2019},
  month = dec,
  journal = {Biomedical Optics Express},
  volume = {10},
  number = {12},
  pages = {6204--6226},
  publisher = {Optical Society of America},
  issn = {2156-7085},
  doi = {10.1364/BOE.10.006204},
  urldate = {2021-11-11},
  abstract = {Retinal disease classification is a significant problem in computer-aided diagnosis (CAD) for medical applications. This paper is focused on a 4-class classification problem to automatically detect choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL in optical coherence tomography (OCT) images. The proposed classification algorithm adopted an ensemble of four classification model instances to identify retinal OCT images, each of which was based on an improved residual neural network (ResNet50). The experiment followed a patient-level 10-fold cross-validation process, on development retinal OCT image dataset. The proposed approach achieved 0.973 (95\&\#x0025; confidence interval [CI], 0.971\&\#x2013;0.975) classification accuracy, 0.963 (95\&\#x0025; CI, 0.960\&\#x2013;0.966) sensitivity, and 0.985 (95\&\#x0025; CI, 0.983\&\#x2013;0.987) specificity at the B-scan level, achieving a matching or exceeding performance to that of ophthalmologists with significant clinical experience. Other performance measures used in the study were the area under receiver operating characteristic curve (AUC) and kappa value. The observations of the study implied that multi-ResNet50 ensembling was a useful technique when the availability of medical images was limited. In addition, we performed qualitative evaluation of model predictions, and occlusion testing to understand the decision-making process of our model. The paper provided an analytical discussion on misclassification and pathology regions identified by the occlusion testing also. Finally, we explored the effect of the integration of retinal OCT images and medical history data from patients on model performance.},
  copyright = {\&\#169; 2019 Optical Society of America},
  langid = {english}
}

@article{liDeepLearningbasedAutomated2019a,
  title = {Deep Learning-Based Automated Detection of Retinal Diseases Using Optical Coherence Tomography Images},
  author = {Li, Feng and Chen, Hua and Liu, Zheng and Zhang, Xue-dian and Jiang, Min-shan and Jiang, Min-shan and Wu, Zhi-zheng and Zhou, Kai-qian},
  year = {2019},
  month = dec,
  journal = {Biomedical Optics Express},
  volume = {10},
  number = {12},
  pages = {6204--6226},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.10.006204},
  urldate = {2022-07-10},
  abstract = {Retinal disease classification is a significant problem in computer-aided diagnosis (CAD) for medical applications. This paper is focused on a 4-class classification problem to automatically detect choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL in optical coherence tomography (OCT) images. The proposed classification algorithm adopted an ensemble of four classification model instances to identify retinal OCT images, each of which was based on an improved residual neural network (ResNet50). The experiment followed a patient-level 10-fold cross-validation process, on development retinal OCT image dataset. The proposed approach achieved 0.973 (95\&\#x0025; confidence interval [CI], 0.971\&\#x2013;0.975) classification accuracy, 0.963 (95\&\#x0025; CI, 0.960\&\#x2013;0.966) sensitivity, and 0.985 (95\&\#x0025; CI, 0.983\&\#x2013;0.987) specificity at the B-scan level, achieving a matching or exceeding performance to that of ophthalmologists with significant clinical experience. Other performance measures used in the study were the area under receiver operating characteristic curve (AUC) and kappa value. The observations of the study implied that multi-ResNet50 ensembling was a useful technique when the availability of medical images was limited. In addition, we performed qualitative evaluation of model predictions, and occlusion testing to understand the decision-making process of our model. The paper provided an analytical discussion on misclassification and pathology regions identified by the occlusion testing also. Finally, we explored the effect of the integration of retinal OCT images and medical history data from patients on model performance.},
  copyright = {\&\#169; 2019 Optical Society of America},
  langid = {english}
}

@article{liDeepLearningbasedAutomated2019b,
  title = {Deep Learning-Based Automated Detection of Retinal Diseases Using Optical Coherence Tomography Images},
  author = {Li, Feng and Chen, Hua and Liu, Zheng and Zhang, Xue-dian and Jiang, Min-shan and Wu, Zhi-zheng and Zhou, Kai-qian},
  year = {2019},
  month = dec,
  journal = {Biomedical Optics Express},
  volume = {10},
  number = {12},
  pages = {6204},
  issn = {2156-7085, 2156-7085},
  doi = {10.1364/BOE.10.006204},
  urldate = {2022-07-10},
  abstract = {Retinal disease classification is a significant problem in computer-aided diagnosis (CAD) for medical applications. This paper is focused on a 4-class classification problem to automatically detect choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL in optical coherence tomography (OCT) images. The proposed classification algorithm adopted an ensemble of four classification model instances to identify retinal OCT images, each of which was based on an improved residual neural network (ResNet50). The experiment followed a patient-level 10-fold cross-validation process, on development retinal OCT image dataset. The proposed approach achieved 0.973 (95\% confidence interval [CI], 0.971--0.975) classification accuracy, 0.963 (95\% CI, 0.960--0.966) sensitivity, and 0.985 (95\% CI, 0.983--0.987) specificity at the B-scan level, achieving a matching or exceeding performance to that of ophthalmologists with significant clinical experience. Other performance measures used in the study were the area under receiver operating characteristic curve (AUC) and kappa value. The observations of the study implied that multi-ResNet50 ensembling was a useful technique when the availability of medical images was limited. In addition, we performed qualitative evaluation of model predictions, and occlusion testing to understand the decision-making process of our model. The paper provided an analytical discussion on misclassification and pathology regions identified by the occlusion testing also. Finally, we explored the effect of the integration of retinal OCT images and medical history data from patients on model performance.},
  langid = {english}
}

@article{liDeepLearningbasedAutomated2019c,
  title = {Deep Learning-Based Automated Detection of Retinal Diseases Using Optical Coherence Tomography Images},
  author = {Li, Feng and Chen, Hua and Liu, Zheng and Zhang, Xue-dian and Jiang, Min-shan and Jiang, Min-shan and Wu, Zhi-zheng and Zhou, Kai-qian},
  year = {2019},
  month = dec,
  journal = {Biomedical Optics Express},
  volume = {10},
  number = {12},
  pages = {6204--6226},
  publisher = {Optical Society of America},
  issn = {2156-7085},
  doi = {10.1364/BOE.10.006204},
  urldate = {2021-11-11},
  abstract = {Retinal disease classification is a significant problem in computer-aided diagnosis (CAD) for medical applications. This paper is focused on a 4-class classification problem to automatically detect choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL in optical coherence tomography (OCT) images. The proposed classification algorithm adopted an ensemble of four classification model instances to identify retinal OCT images, each of which was based on an improved residual neural network (ResNet50). The experiment followed a patient-level 10-fold cross-validation process, on development retinal OCT image dataset. The proposed approach achieved 0.973 (95\&\#x0025; confidence interval [CI], 0.971\&\#x2013;0.975) classification accuracy, 0.963 (95\&\#x0025; CI, 0.960\&\#x2013;0.966) sensitivity, and 0.985 (95\&\#x0025; CI, 0.983\&\#x2013;0.987) specificity at the B-scan level, achieving a matching or exceeding performance to that of ophthalmologists with significant clinical experience. Other performance measures used in the study were the area under receiver operating characteristic curve (AUC) and kappa value. The observations of the study implied that multi-ResNet50 ensembling was a useful technique when the availability of medical images was limited. In addition, we performed qualitative evaluation of model predictions, and occlusion testing to understand the decision-making process of our model. The paper provided an analytical discussion on misclassification and pathology regions identified by the occlusion testing also. Finally, we explored the effect of the integration of retinal OCT images and medical history data from patients on model performance.},
  copyright = {\&\#169; 2019 Optical Society of America},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\MZ8PLYJ9\\Li et al. - 2019 - Deep learning-based automated detection of retinal.pdf;C\:\\Users\\cleme\\Zotero\\storage\\E86K9F6A\\fulltext.html}
}

@article{liDeepLearningbasedAutomated2019d,
  title = {Deep Learning-Based Automated Detection of Retinal Diseases Using Optical Coherence Tomography Images},
  author = {Li, Feng and Chen, Hua and Liu, Zheng and Zhang, Xue-dian and Jiang, Min-shan and Wu, Zhi-zheng and Zhou, Kai-qian},
  year = {2019},
  month = dec,
  journal = {Biomedical Optics Express},
  volume = {10},
  number = {12},
  pages = {6204},
  issn = {2156-7085, 2156-7085},
  doi = {10.1364/BOE.10.006204},
  urldate = {2022-07-10},
  abstract = {Retinal disease classification is a significant problem in computer-aided diagnosis (CAD) for medical applications. This paper is focused on a 4-class classification problem to automatically detect choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL in optical coherence tomography (OCT) images. The proposed classification algorithm adopted an ensemble of four classification model instances to identify retinal OCT images, each of which was based on an improved residual neural network (ResNet50). The experiment followed a patient-level 10-fold cross-validation process, on development retinal OCT image dataset. The proposed approach achieved 0.973 (95\% confidence interval [CI], 0.971--0.975) classification accuracy, 0.963 (95\% CI, 0.960--0.966) sensitivity, and 0.985 (95\% CI, 0.983--0.987) specificity at the B-scan level, achieving a matching or exceeding performance to that of ophthalmologists with significant clinical experience. Other performance measures used in the study were the area under receiver operating characteristic curve (AUC) and kappa value. The observations of the study implied that multi-ResNet50 ensembling was a useful technique when the availability of medical images was limited. In addition, we performed qualitative evaluation of model predictions, and occlusion testing to understand the decision-making process of our model. The paper provided an analytical discussion on misclassification and pathology regions identified by the occlusion testing also. Finally, we explored the effect of the integration of retinal OCT images and medical history data from patients on model performance.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\MZNC7ZGW\Li et al. - 2019 - Deep learning-based automated detection of retinal.pdf}
}

@article{liDeepLearningbasedAutomated2019e,
  title = {Deep Learning-Based Automated Detection of Retinal Diseases Using Optical Coherence Tomography Images},
  author = {Li, Feng and Chen, Hua and Liu, Zheng and Zhang, Xue-dian and Jiang, Min-shan and Jiang, Min-shan and Wu, Zhi-zheng and Zhou, Kai-qian},
  year = {2019},
  month = dec,
  journal = {Biomedical Optics Express},
  volume = {10},
  number = {12},
  pages = {6204--6226},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.10.006204},
  urldate = {2022-07-10},
  abstract = {Retinal disease classification is a significant problem in computer-aided diagnosis (CAD) for medical applications. This paper is focused on a 4-class classification problem to automatically detect choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL in optical coherence tomography (OCT) images. The proposed classification algorithm adopted an ensemble of four classification model instances to identify retinal OCT images, each of which was based on an improved residual neural network (ResNet50). The experiment followed a patient-level 10-fold cross-validation process, on development retinal OCT image dataset. The proposed approach achieved 0.973 (95\&\#x0025; confidence interval [CI], 0.971\&\#x2013;0.975) classification accuracy, 0.963 (95\&\#x0025; CI, 0.960\&\#x2013;0.966) sensitivity, and 0.985 (95\&\#x0025; CI, 0.983\&\#x2013;0.987) specificity at the B-scan level, achieving a matching or exceeding performance to that of ophthalmologists with significant clinical experience. Other performance measures used in the study were the area under receiver operating characteristic curve (AUC) and kappa value. The observations of the study implied that multi-ResNet50 ensembling was a useful technique when the availability of medical images was limited. In addition, we performed qualitative evaluation of model predictions, and occlusion testing to understand the decision-making process of our model. The paper provided an analytical discussion on misclassification and pathology regions identified by the occlusion testing also. Finally, we explored the effect of the integration of retinal OCT images and medical history data from patients on model performance.},
  copyright = {\&\#169; 2019 Optical Society of America},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\TQ24GUTM\\Li et al. - 2019 - Deep learning-based automated detection of retinal.pdf;C\:\\Users\\cleme\\Zotero\\storage\\YGG25IYC\\fulltext.html}
}

@article{liDeepRetinaLayerSegmentation2020,
  title = {{{DeepRetina}}: {{Layer Segmentation}} of {{Retina}} in {{OCT Images Using Deep Learning}}},
  shorttitle = {{{DeepRetina}}},
  author = {Li, Qiaoliang and Li, Shiyu and He, Zhuoying and Guan, Huimin and Chen, Runmin and Xu, Ying and Wang, Tao and Qi, Suwen and Mei, Jun and Wang, Wei},
  year = {2020},
  month = dec,
  journal = {Translational Vision Science \& Technology},
  volume = {9},
  number = {2},
  pages = {61},
  issn = {2164-2591},
  doi = {10.1167/tvst.9.2.61},
  urldate = {2022-07-08},
  abstract = {To automate the segmentation of retinal layers, we propose DeepRetina, a method based on deep neural networks. DeepRetina uses the improved Xception65 to extract and learn the characteristics of retinal layers. The Xception65-extracted feature maps are inputted to an atrous spatial pyramid pooling module to obtain multiscale feature information. This information is then recovered to capture clearer retinal layer boundaries in the encoder-decoder module, thus completing retinal layer auto-segmentation of the retinal optical coherence tomography (OCT) images. We validated this method using a retinal OCT image database containing 280 volumes (40 B-scans per volume) to demonstrate its effectiveness. The results showed that the method exhibits excellent performance in terms of the mean intersection over union and sensitivity (Se), which are as high as 90.41 and 92.15\%, respectively. The intersection over union and Se values of the nerve fiber layer, ganglion cell layer, inner plexiform layer, inner nuclear layer, outer plexiform layer, outer nuclear layer, outer limiting membrane, photoreceptor inner segment, photoreceptor outer segment, and pigment epithelium layer were found to be above 88\%. DeepRetina can automate the segmentation of retinal layers and has great potential for the early diagnosis of fundus retinal diseases. In addition, our approach will provide a segmentation model framework for other types of tissues and cells in clinical practice. Automating the segmentation of retinal layers can help effectively diagnose and monitor clinical retinal diseases. In addition, it requires only a small amount of manual segmentation, significantly improving work efficiency.}
}

@article{liDeepRetinaLayerSegmentation2020a,
  title = {{{DeepRetina}}: {{Layer Segmentation}} of {{Retina}} in {{OCT Images Using Deep Learning}}},
  shorttitle = {{{DeepRetina}}},
  author = {Li, Qiaoliang and Li, Shiyu and He, Zhuoying and Guan, Huimin and Chen, Runmin and Xu, Ying and Wang, Tao and Qi, Suwen and Mei, Jun and Wang, Wei},
  year = {2020},
  month = dec,
  journal = {Translational Vision Science \& Technology},
  volume = {9},
  number = {2},
  pages = {61},
  issn = {2164-2591},
  doi = {10.1167/tvst.9.2.61},
  urldate = {2022-07-08},
  abstract = {To automate the segmentation of retinal layers, we propose DeepRetina, a method based on deep neural networks.    DeepRetina uses the improved Xception65 to extract and learn the characteristics of retinal layers. The Xception65-extracted feature maps are inputted to an atrous spatial pyramid pooling module to obtain multiscale feature information. This information is then recovered to capture clearer retinal layer boundaries in the encoder-decoder module, thus completing retinal layer auto-segmentation of the retinal optical coherence tomography (OCT) images.    We validated this method using a retinal OCT image database containing 280 volumes (40 B-scans per volume) to demonstrate its effectiveness. The results showed that the method exhibits excellent performance in terms of the mean intersection over union and sensitivity (Se), which are as high as 90.41 and 92.15\%, respectively. The intersection over union and Se values of the nerve fiber layer, ganglion cell layer, inner plexiform layer, inner nuclear layer, outer plexiform layer, outer nuclear layer, outer limiting membrane, photoreceptor inner segment, photoreceptor outer segment, and pigment epithelium layer were found to be above 88\%.    DeepRetina can automate the segmentation of retinal layers and has great potential for the early diagnosis of fundus retinal diseases. In addition, our approach will provide a segmentation model framework for other types of tissues and cells in clinical practice.    Automating the segmentation of retinal layers can help effectively diagnose and monitor clinical retinal diseases. In addition, it requires only a small amount of manual segmentation, significantly improving work efficiency.}
}

@article{liDiagnosticAssessmentDeep2019,
  title = {Diagnostic Assessment of Deep Learning Algorithms for Diabetic Retinopathy Screening},
  author = {Li, Tao and Gao, Yingqi and Wang, Kai and Guo, Song and Liu, Hanruo and Kang, Hong},
  year = {2019},
  month = oct,
  journal = {Information Sciences},
  volume = {501},
  pages = {511--522},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.06.011},
  urldate = {2020-02-18},
  abstract = {Diabetic retinopathy (DR), the leading cause of blindness for working-age adults, is generally intervened by early screening to reduce vision loss. A series of automated deep-learning-based algorithms for DR screening have been proposed and achieved high sensitivity and specificity (\,{$>$}\,90\%). However, these deep learning models do not perform well in clinical applications due to the limitations of the existing publicly available fundus image datasets. In order to evaluate these methods in clinical situations, we collected 13,673 fundus images from 9598 patients. These images were divided into six classes by seven graders according to image quality and DR level. Moreover, 757 images with DR were selected to annotate four types of DR-related lesions. Finally, we evaluated state-of-the-art deep learning algorithms on collected images, including image classification, semantic segmentation and object detection. Although we obtain an accuracy of 0.8284 for DR classification, these algorithms perform poorly on lesion segmentation and detection, indicating that lesion segmentation and detection are quite challenging. In summary, we are providing a new dataset named DDR for assessing deep learning models and further exploring the clinical applications, particularly for lesion recognition.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy,Fundus image,Image classification,Semantic segmentation}
}

@article{liDiagnosticAssessmentDeep2019a,
  title = {Diagnostic Assessment of Deep Learning Algorithms for Diabetic Retinopathy Screening},
  author = {Li, Tao and Gao, Yingqi and Wang, Kai and Guo, Song and Liu, Hanruo and Kang, Hong},
  year = {2019},
  month = oct,
  journal = {Information Sciences},
  volume = {501},
  pages = {511--522},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.06.011},
  urldate = {2021-10-04},
  abstract = {Diabetic retinopathy (DR), the leading cause of blindness for working-age adults, is generally intervened by early screening to reduce vision loss. A series of automated deep-learning-based algorithms for DR screening have been proposed and achieved high sensitivity and specificity (\,{$>$}\,90\%). However, these deep learning models do not perform well in clinical applications due to the limitations of the existing publicly available fundus image datasets. In order to evaluate these methods in clinical situations, we collected 13,673 fundus images from 9598 patients. These images were divided into six classes by seven graders according to image quality and DR level. Moreover, 757 images with DR were selected to annotate four types of DR-related lesions. Finally, we evaluated state-of-the-art deep learning algorithms on collected images, including image classification, semantic segmentation and object detection. Although we obtain an accuracy of 0.8284 for DR classification, these algorithms perform poorly on lesion segmentation and detection, indicating that lesion segmentation and detection are quite challenging. In summary, we are providing a new dataset named DDR for assessing deep learning models and further exploring the clinical applications, particularly for lesion recognition.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy,Fundus image,Image classification,Semantic segmentation}
}

@article{liDiagnosticAssessmentDeep2019b,
  title = {Diagnostic Assessment of Deep Learning Algorithms for Diabetic Retinopathy Screening},
  author = {Li, Tao and Gao, Yingqi and Wang, Kai and Guo, Song and Liu, Hanruo and Kang, Hong},
  year = {2019},
  month = oct,
  journal = {Information Sciences},
  volume = {501},
  pages = {511--522},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.06.011},
  urldate = {2022-10-23},
  abstract = {Diabetic retinopathy (DR), the leading cause of blindness for working-age adults, is generally intervened by early screening to reduce vision loss. A series of automated deep-learning-based algorithms for DR screening have been proposed and achieved high sensitivity and specificity (\,{$>$}\,90\%). However, these deep learning models do not perform well in clinical applications due to the limitations of the existing publicly available fundus image datasets. In order to evaluate these methods in clinical situations, we collected 13,673 fundus images from 9598 patients. These images were divided into six classes by seven graders according to image quality and DR level. Moreover, 757 images with DR were selected to annotate four types of DR-related lesions. Finally, we evaluated state-of-the-art deep learning algorithms on collected images, including image classification, semantic segmentation and object detection. Although we obtain an accuracy of 0.8284 for DR classification, these algorithms perform poorly on lesion segmentation and detection, indicating that lesion segmentation and detection are quite challenging. In summary, we are providing a new dataset named DDR for assessing deep learning models and further exploring the clinical applications, particularly for lesion recognition.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy,Fundus image,Image classification,Semantic segmentation}
}

@article{liDiagnosticAssessmentDeep2019c,
  title = {Diagnostic Assessment of Deep Learning Algorithms for Diabetic Retinopathy Screening},
  author = {Li, Tao and Gao, Yingqi and Wang, Kai and Guo, Song and Liu, Hanruo and Kang, Hong},
  year = {2019},
  month = oct,
  journal = {Information Sciences},
  volume = {501},
  pages = {511--522},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.06.011},
  urldate = {2021-10-04},
  abstract = {Diabetic retinopathy (DR), the leading cause of blindness for working-age adults, is generally intervened by early screening to reduce vision loss. A series of automated deep-learning-based algorithms for DR screening have been proposed and achieved high sensitivity and specificity (\,{$>$}\,90\%). However, these deep learning models do not perform well in clinical applications due to the limitations of the existing publicly available fundus image datasets. In order to evaluate these methods in clinical situations, we collected 13,673 fundus images from 9598 patients. These images were divided into six classes by seven graders according to image quality and DR level. Moreover, 757 images with DR were selected to annotate four types of DR-related lesions. Finally, we evaluated state-of-the-art deep learning algorithms on collected images, including image classification, semantic segmentation and object detection. Although we obtain an accuracy of 0.8284 for DR classification, these algorithms perform poorly on lesion segmentation and detection, indicating that lesion segmentation and detection are quite challenging. In summary, we are providing a new dataset named DDR for assessing deep learning models and further exploring the clinical applications, particularly for lesion recognition.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy,Fundus image,Image classification,Semantic segmentation},
  file = {C:\Users\cleme\Zotero\storage\CUS8UVBZ\S0020025519305377.html}
}

@article{liDiagnosticAssessmentDeep2019d,
  title = {Diagnostic Assessment of Deep Learning Algorithms for Diabetic Retinopathy Screening},
  author = {Li, Tao and Gao, Yingqi and Wang, Kai and Guo, Song and Liu, Hanruo and Kang, Hong},
  year = {2019},
  month = oct,
  journal = {Information Sciences},
  volume = {501},
  pages = {511--522},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.06.011},
  urldate = {2020-02-18},
  abstract = {Diabetic retinopathy (DR), the leading cause of blindness for working-age adults, is generally intervened by early screening to reduce vision loss. A series of automated deep-learning-based algorithms for DR screening have been proposed and achieved high sensitivity and specificity (\,{$>$}\,90\%). However, these deep learning models do not perform well in clinical applications due to the limitations of the existing publicly available fundus image datasets. In order to evaluate these methods in clinical situations, we collected 13,673 fundus images from 9598 patients. These images were divided into six classes by seven graders according to image quality and DR level. Moreover, 757 images with DR were selected to annotate four types of DR-related lesions. Finally, we evaluated state-of-the-art deep learning algorithms on collected images, including image classification, semantic segmentation and object detection. Although we obtain an accuracy of 0.8284 for DR classification, these algorithms perform poorly on lesion segmentation and detection, indicating that lesion segmentation and detection are quite challenging. In summary, we are providing a new dataset named DDR for assessing deep learning models and further exploring the clinical applications, particularly for lesion recognition.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy,Fundus image,Image classification,Semantic segmentation},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\ZDMUIUT3\\Li et al. - 2019 - Diagnostic assessment of deep learning algorithms .pdf;C\:\\Users\\cleme\\Zotero\\storage\\GW9WRALP\\S0020025519305377.html;C\:\\Users\\cleme\\Zotero\\storage\\JBXQ2ZPJ\\li2019.html}
}

@article{liDiagnosticAssessmentDeep2019e,
  title = {Diagnostic Assessment of Deep Learning Algorithms for Diabetic Retinopathy Screening},
  author = {Li, Tao and Gao, Yingqi and Wang, Kai and Guo, Song and Liu, Hanruo and Kang, Hong},
  year = {2019},
  month = oct,
  journal = {Information Sciences},
  volume = {501},
  pages = {511--522},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.06.011},
  urldate = {2022-10-23},
  abstract = {Diabetic retinopathy (DR), the leading cause of blindness for working-age adults, is generally intervened by early screening to reduce vision loss. A series of automated deep-learning-based algorithms for DR screening have been proposed and achieved high sensitivity and specificity (\,{$>$}\,90\%). However, these deep learning models do not perform well in clinical applications due to the limitations of the existing publicly available fundus image datasets. In order to evaluate these methods in clinical situations, we collected 13,673 fundus images from 9598 patients. These images were divided into six classes by seven graders according to image quality and DR level. Moreover, 757 images with DR were selected to annotate four types of DR-related lesions. Finally, we evaluated state-of-the-art deep learning algorithms on collected images, including image classification, semantic segmentation and object detection. Although we obtain an accuracy of 0.8284 for DR classification, these algorithms perform poorly on lesion segmentation and detection, indicating that lesion segmentation and detection are quite challenging. In summary, we are providing a new dataset named DDR for assessing deep learning models and further exploring the clinical applications, particularly for lesion recognition.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy,Fundus image,Image classification,Semantic segmentation},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\8CWWVUTK\\10.1016@j.ins.2019.06.011.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\IUSY2TS5\\S0020025519305377.html}
}

@article{liDigitalTechnologyTelemedicine2021,
  title = {Digital Technology, Tele-Medicine and Artificial Intelligence in Ophthalmology: {{A}} Global Perspective},
  shorttitle = {Digital Technology, Tele-Medicine and Artificial Intelligence in Ophthalmology},
  author = {Li, Ji-Peng Olivia and Liu, Hanruo and Ting, Darren S. J. and Jeon, Sohee and Chan, R. V. Paul and Kim, Judy E. and Sim, Dawn A. and Thomas, Peter B. M. and Lin, Haotian and Chen, Youxin and Sakomoto, Taiji and Loewenstein, Anat and Lam, Dennis S. C. and Pasquale, Louis R. and Wong, Tien Y. and Lam, Linda A. and Ting, Daniel S. W.},
  year = {2021},
  month = may,
  journal = {Progress in Retinal and Eye Research},
  volume = {82},
  pages = {100900},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2020.100900},
  urldate = {2022-07-10},
  abstract = {The simultaneous maturation of multiple digital and telecommunications technologies in 2020 has created an unprecedented opportunity for ophthalmology to adapt to new models of care using tele-health supported by digital innovations. These digital innovations include artificial intelligence (AI), 5th generation (5G) telecommunication networks and the Internet of Things (IoT), creating an inter-dependent ecosystem offering opportunities to develop new models of eye care addressing the challenges of COVID-19 and beyond. Ophthalmology has thrived in some of these areas partly due to its many image-based investigations. Tele-health and AI provide synchronous solutions to challenges facing ophthalmologists and healthcare providers worldwide. This article reviews how countries across the world have utilised these digital innovations to tackle diabetic retinopathy, retinopathy of prematurity, age-related macular degeneration, glaucoma, refractive error correction, cataract and other anterior segment disorders. The review summarises the digital strategies that countries are developing and discusses technologies that may increasingly enter the clinical workflow and processes of ophthalmologists. Furthermore as countries around the world have initiated a series of escalating containment and mitigation measures during the COVID-19 pandemic, the delivery of eye care services globally has been significantly impacted. As ophthalmic services adapt and form a ``new normal'', the rapid adoption of some of telehealth and digital innovation during the pandemic is also discussed. Finally, challenges for validation and clinical implementation are considered, as well as recommendations on future directions.},
  langid = {english},
  keywords = {Artificial intelligence,COVID-19,Deep learning,Diabetic retinopathy screening,Digital innovations,Digital technology,Digital transformation,Tele-ophthalmology,Tele-screening,Telemedicine}
}

@article{liDigitalTechnologyTelemedicine2021a,
  title = {Digital Technology, Tele-Medicine and Artificial Intelligence in Ophthalmology: {{A}} Global Perspective},
  shorttitle = {Digital Technology, Tele-Medicine and Artificial Intelligence in Ophthalmology},
  author = {Li, Ji-Peng Olivia and Liu, Hanruo and Ting, Darren S. J. and Jeon, Sohee and Chan, R. V. Paul and Kim, Judy E. and Sim, Dawn A. and Thomas, Peter B. M. and Lin, Haotian and Chen, Youxin and Sakomoto, Taiji and Loewenstein, Anat and Lam, Dennis S. C. and Pasquale, Louis R. and Wong, Tien Y. and Lam, Linda A. and Ting, Daniel S. W.},
  year = {2021},
  month = may,
  journal = {Progress in Retinal and Eye Research},
  volume = {82},
  pages = {100900},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2020.100900},
  urldate = {2022-07-10},
  abstract = {The simultaneous maturation of multiple digital and telecommunications technologies in 2020 has created an unprecedented opportunity for ophthalmology to adapt to new models of care using tele-health supported by digital innovations. These digital innovations include artificial intelligence (AI), 5th generation (5G) telecommunication networks and the Internet of Things (IoT), creating an inter-dependent ecosystem offering opportunities to develop new models of eye care addressing the challenges of COVID-19 and beyond. Ophthalmology has thrived in some of these areas partly due to its many image-based investigations. Tele-health and AI provide synchronous solutions to challenges facing ophthalmologists and healthcare providers worldwide. This article reviews how countries across the world have utilised these digital innovations to tackle diabetic retinopathy, retinopathy of prematurity, age-related macular degeneration, glaucoma, refractive error correction, cataract and other anterior segment disorders. The review summarises the digital strategies that countries are developing and discusses technologies that may increasingly enter the clinical workflow and processes of ophthalmologists. Furthermore as countries around the world have initiated a series of escalating containment and mitigation measures during the COVID-19 pandemic, the delivery of eye care services globally has been significantly impacted. As ophthalmic services adapt and form a ``new normal'', the rapid adoption of some of telehealth and digital innovation during the pandemic is also discussed. Finally, challenges for validation and clinical implementation are considered, as well as recommendations on future directions.},
  langid = {english},
  keywords = {Artificial intelligence,COVID-19,Deep learning,Diabetic retinopathy screening,Digital innovations,Digital technology,Digital transformation,Tele-ophthalmology,Tele-screening,Telemedicine},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\ATNLJTX9\\oliviali2020.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\BU9NR86J\\S1350946220300720.html}
}

@article{liEfficacyDeepLearning2018,
  title = {Efficacy of a {{Deep Learning System}} for {{Detecting Glaucomatous Optic Neuropathy Based}} on {{Color Fundus Photographs}}},
  author = {Li, Zhixi and He, Yifan and Keel, Stuart and Meng, Wei and Chang, Robert T. and He, Mingguang},
  year = {2018},
  month = aug,
  journal = {Ophthalmology},
  volume = {125},
  number = {8},
  pages = {1199--1206},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2018.01.023},
  urldate = {2019-11-20},
  abstract = {Purpose To assess the performance of a deep learning algorithm for detecting referable glaucomatous optic neuropathy (GON) based on color fundus photographs. Design A deep learning system for the classification of GON was developed for automated classification of GON on color fundus photographs. Participants We retrospectively included 48\,116 fundus photographs for the development and validation of a deep learning algorithm. Methods This study recruited 21 trained ophthalmologists to classify the photographs. Referable GON was defined as vertical cup-to-disc ratio of 0.7 or more and other typical changes of GON. The reference standard was made until 3 graders achieved agreement. A separate validation dataset of 8000 fully gradable fundus photographs was used to assess the performance of this algorithm. Main Outcome Measures The area under receiver operator characteristic curve (AUC) with sensitivity and specificity was applied to evaluate the efficacy of the deep learning algorithm detecting referable GON. Results In the validation dataset, this deep learning system achieved an AUC of 0.986 with sensitivity of 95.6\% and specificity of 92.0\%. The most common reasons for false-negative grading (n = 87) were GON with coexisting eye conditions (n = 44 [50.6\%]), including pathologic or high myopia (n = 37 [42.6\%]), diabetic retinopathy (n = 4 [4.6\%]), and age-related macular degeneration (n = 3 [3.4\%]). The leading reason for false-positive results (n = 480) was having other eye conditions (n = 458 [95.4\%]), mainly including physiologic cupping (n = 267 [55.6\%]). Misclassification as false-positive results amidst a normal-appearing fundus occurred in only 22 eyes (4.6\%). Conclusions A deep learning system can detect referable GON with high sensitivity and specificity. Coexistence of high or pathologic myopia is the most common cause resulting in false-negative results. Physiologic cupping and pathologic myopia were the most common reasons for false-positive results.},
  langid = {english}
}

@article{liEfficacyDeepLearning2018a,
  title = {Efficacy of a {{Deep Learning System}} for {{Detecting Glaucomatous Optic Neuropathy Based}} on {{Color Fundus Photographs}}},
  author = {Li, Zhixi and He, Yifan and Keel, Stuart and Meng, Wei and Chang, Robert T. and He, Mingguang},
  year = {2018},
  month = aug,
  journal = {Ophthalmology},
  volume = {125},
  number = {8},
  pages = {1199--1206},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2018.01.023},
  urldate = {2019-11-20},
  abstract = {Purpose To assess the performance of a deep learning algorithm for detecting referable glaucomatous optic neuropathy (GON) based on color fundus photographs. Design A deep learning system for the classification of GON was developed for automated classification of GON on color fundus photographs. Participants We retrospectively included 48\,116 fundus photographs for the development and validation of a deep learning algorithm. Methods This study recruited 21 trained ophthalmologists to classify the photographs. Referable GON was defined as vertical cup-to-disc ratio of 0.7 or more and other typical changes of GON. The reference standard was made until 3 graders achieved agreement. A separate validation dataset of 8000 fully gradable fundus photographs was used to assess the performance of this algorithm. Main Outcome Measures The area under receiver operator characteristic curve (AUC) with sensitivity and specificity was applied to evaluate the efficacy of the deep learning algorithm detecting referable GON. Results In the validation dataset, this deep learning system achieved an AUC of 0.986 with sensitivity of 95.6\% and specificity of 92.0\%. The most common reasons for false-negative grading (n~= 87) were GON with coexisting eye conditions (n~= 44 [50.6\%]), including pathologic or high myopia (n~= 37 [42.6\%]), diabetic retinopathy (n~= 4 [4.6\%]), and age-related macular degeneration (n~= 3 [3.4\%]). The leading reason for false-positive results (n~= 480) was having other eye conditions (n~= 458 [95.4\%]), mainly including physiologic cupping (n~=~267 [55.6\%]). Misclassification as false-positive results amidst a normal-appearing fundus occurred in only 22 eyes (4.6\%). Conclusions A deep learning system can detect referable GON with high sensitivity and specificity. Coexistence of high or pathologic myopia is the most common cause resulting in false-negative results. Physiologic cupping and pathologic myopia were the most common reasons for false-positive results.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\3FEEN3ID\\Li et al. - 2018 - Efficacy of a Deep Learning System for Detecting G.pdf;C\:\\Users\\cleme\\Zotero\\storage\\GDAJVDKG\\S0161642017335650.html}
}

@article{liHyperbandNovelBanditBased,
  title = {Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter Optimization}}},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  pages = {52},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  langid = {english}
}

@article{liHyperbandNovelBanditBased2018,
  title = {Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter Optimization}}},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  number = {18},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  langid = {english}
}

@article{liHyperbandNovelBanditBased2018a,
  title = {Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter Optimization}}},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  number = {18},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\A88QXYH8\Li et al. - Hyperband A Novel Bandit-Based Approach to Hyperp.pdf}
}

@article{liHyperbandNovelBanditBaseda,
  title = {Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter Optimization}}},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  pages = {52},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\HUYZDAE6\Li et al. - Hyperband A Novel Bandit-Based Approach to Hyperp.pdf}
}

@article{liImageProjectionNetwork2020,
  title = {Image {{Projection Network}}: {{3D}} to {{2D Image Segmentation}} in {{OCTA Images}}},
  shorttitle = {Image {{Projection Network}}},
  author = {Li, Mingchao and Chen, Yerui and Ji, Zexuan and Xie, Keren and Yuan, Songtao and Chen, Qiang and Li, Shuo},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {39},
  number = {11},
  pages = {3343--3354},
  issn = {1558-254X},
  doi = {10.1109/TMI.2020.2992244},
  abstract = {We present an image projection network (IPN), which is a novel end-to-end architecture and can achieve 3D-to-2D image segmentation in optical coherence tomography angiography (OCTA) images. Our key insight is to build a projection learning module (PLM) which uses a unidirectional pooling layer to conduct effective features selection and dimension reduction concurrently. By combining multiple PLMs, the proposed network can input 3D OCTA data, and output 2D segmentation results such as retinal vessel segmentation. It provides a new idea for the quantification of retinal indicators: without retinal layer segmentation and without projection maps. We tested the performance of our network for two crucial retinal image segmentation issues: retinal vessel (RV) segmentation and foveal avascular zone (FAZ) segmentation. The experimental results on 316 OCTA volumes demonstrate that the IPN is an effective implementation of 3D-to-2D segmentation networks, and the uses of multi-modality information and volumetric information make IPN perform better than the baseline methods.},
  keywords = {3D to 2D,Biomedical imaging,biomedical volumetric image segmentation,End to end,image projection network,Image segmentation,optical coherence tomography angiography,Retinal vessels,Semantics,Three-dimensional displays,Two dimensional displays}
}

@article{liImageProjectionNetwork2020a,
  title = {Image {{Projection Network}}: {{3D}} to {{2D Image Segmentation}} in {{OCTA Images}}},
  shorttitle = {Image {{Projection Network}}},
  author = {Li, Mingchao and Chen, Yerui and Ji, Zexuan and Xie, Keren and Yuan, Songtao and Chen, Qiang and Li, Shuo},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {39},
  number = {11},
  pages = {3343--3354},
  issn = {1558-254X},
  doi = {10.1109/TMI.2020.2992244},
  abstract = {We present an image projection network (IPN), which is a novel end-to-end architecture and can achieve 3D-to-2D image segmentation in optical coherence tomography angiography (OCTA) images. Our key insight is to build a projection learning module (PLM) which uses a unidirectional pooling layer to conduct effective features selection and dimension reduction concurrently. By combining multiple PLMs, the proposed network can input 3D OCTA data, and output 2D segmentation results such as retinal vessel segmentation. It provides a new idea for the quantification of retinal indicators: without retinal layer segmentation and without projection maps. We tested the performance of our network for two crucial retinal image segmentation issues: retinal vessel (RV) segmentation and foveal avascular zone (FAZ) segmentation. The experimental results on 316 OCTA volumes demonstrate that the IPN is an effective implementation of 3D-to-2D segmentation networks, and the uses of multi-modality information and volumetric information make IPN perform better than the baseline methods.},
  keywords = {3D to 2D,Biomedical imaging,biomedical volumetric image segmentation,End to end,image projection network,Image segmentation,optical coherence tomography angiography,Retinal vessels,Semantics,Three-dimensional displays,Two dimensional displays},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\ANAETSVG\\Li et al. - 2020 - Image Projection Network 3D to 2D Image Segmentat.pdf;C\:\\Users\\cleme\\Zotero\\storage\\JFMTIKBQ\\10.1109@TMI.2020.2992244.pdf.pdf}
}

@article{liIntegratingHandcraftedDeep2019,
  title = {Integrating {{Handcrafted}} and {{Deep Features}} for {{Optical Coherence Tomography Based Retinal Disease Classification}}},
  author = {Li, Xuechen and Shen, Linlin and Shen, Meixiao and Qiu, Connor S.},
  year = {2019},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {7},
  pages = {33771--33777},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2891975},
  abstract = {Deep neural networks (DNNs) have been widely applied to the automatic analysis of medical images for disease diagnosis and to help human experts by efficiently processing immense amounts of images. While the handcrafted feature has been used for eye disease detection or classification since the 1990s, DNN was recently adopted in this area and showed a very promising performance. Since handcrafted and deep feature can extract complementary information, we propose, in this paper, three different integration frameworks to combine handcrafted and deep feature for optical coherence tomography image-based eye disease classification. In addition, to integrate the handcrafted feature at the input and fully connected layers using existing networks, such as VGG, DenseNet, and Xception, a novel ribcage network (RC Net) is also proposed for feature integration at middle layers. For RC Net, two ``rib'' channels are designed to independently process deep and handcrafted features, and another so-called ``spine'' channel is designed for the integration. While dense blocks are the main components of the three channels, sum operation is proposed for the feature map integration. Our experimental results showed that the deep networks achieved better classification accuracy after the integration of the handcrafted features, e.g., scale-invariant feature transform and Gabor. The RC Net showed the best performance among all the proposed feature integration methods.},
  keywords = {Artificial intelligence,deep learning,Diseases,Feature extraction,feature integration,Medical diagnostic imaging,optical coherence tomography,Retina,Testing,Training}
}

@article{liIntegratingHandcraftedDeep2019a,
  title = {Integrating {{Handcrafted}} and {{Deep Features}} for {{Optical Coherence Tomography Based Retinal Disease Classification}}},
  author = {Li, Xuechen and Shen, Linlin and Shen, Meixiao and Qiu, Connor S.},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {33771--33777},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2891975},
  abstract = {Deep neural networks (DNNs) have been widely applied to the automatic analysis of medical images for disease diagnosis and to help human experts by efficiently processing immense amounts of images. While the handcrafted feature has been used for eye disease detection or classification since the 1990s, DNN was recently adopted in this area and showed a very promising performance. Since handcrafted and deep feature can extract complementary information, we propose, in this paper, three different integration frameworks to combine handcrafted and deep feature for optical coherence tomography image-based eye disease classification. In addition, to integrate the handcrafted feature at the input and fully connected layers using existing networks, such as VGG, DenseNet, and Xception, a novel ribcage network (RC Net) is also proposed for feature integration at middle layers. For RC Net, two ``rib'' channels are designed to independently process deep and handcrafted features, and another so-called ``spine'' channel is designed for the integration. While dense blocks are the main components of the three channels, sum operation is proposed for the feature map integration. Our experimental results showed that the deep networks achieved better classification accuracy after the integration of the handcrafted features, e.g., scale-invariant feature transform and Gabor. The RC Net showed the best performance among all the proposed feature integration methods.},
  keywords = {Artificial intelligence,deep learning,Diseases,Feature extraction,feature integration,Medical diagnostic imaging,optical coherence tomography,Retina,Testing,Training},
  file = {C:\Users\cleme\Zotero\storage\A4HYBQ5S\10.1109@ACCESS.2019.2891975.pdf.pdf}
}

@article{liInterpretableDeepLearning2021,
  title = {Interpretable {{Deep Learning}}: {{Interpretation}}, {{Interpretability}}, {{Trustworthiness}}, and {{Beyond}}},
  shorttitle = {Interpretable {{Deep Learning}}},
  author = {Li, Xuhong and Xiong, Haoyi and Li, Xingjian and Wu, Xuanyu and Zhang, Xiao and Liu, Ji and Bian, Jiang and Dou, Dejing},
  year = {2021},
  month = may,
  journal = {arXiv:2103.10689 [cs]},
  eprint = {2103.10689},
  primaryclass = {cs},
  urldate = {2021-11-15},
  abstract = {Deep neural networks have been well-known for their superb performance in handling various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal the ways that deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we introduce and clarify two basic concepts-interpretations and interpretability-that people usually get confused. First of all, to address the research efforts in interpretations, we elaborate the design of several recent interpretation algorithms, from different perspectives, through proposing a new taxonomy. Then, to understand the results of interpretation, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the existing work in evaluating models' interpretability using "trustworthy" interpretation algorithms. Finally, we review and discuss the connections between deep models' interpretations and other factors, such as adversarial robustness and data augmentations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{liInterpretableDeepLearning2021a,
  title = {Interpretable {{Deep Learning}}: {{Interpretation}}, {{Interpretability}}, {{Trustworthiness}}, and {{Beyond}}},
  shorttitle = {Interpretable {{Deep Learning}}},
  author = {Li, Xuhong and Xiong, Haoyi and Li, Xingjian and Wu, Xuanyu and Zhang, Xiao and Liu, Ji and Bian, Jiang and Dou, Dejing},
  year = {2021},
  month = may,
  journal = {arXiv:2103.10689 [cs]},
  eprint = {2103.10689},
  primaryclass = {cs},
  urldate = {2021-11-15},
  abstract = {Deep neural networks have been well-known for their superb performance in handling various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal the ways that deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we introduce and clarify two basic concepts-interpretations and interpretability-that people usually get confused. First of all, to address the research efforts in interpretations, we elaborate the design of several recent interpretation algorithms, from different perspectives, through proposing a new taxonomy. Then, to understand the results of interpretation, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the existing work in evaluating models' interpretability using "trustworthy" interpretation algorithms. Finally, we review and discuss the connections between deep models' interpretations and other factors, such as adversarial robustness and data augmentations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\NKWC599Y\\Li et al. - 2021 - Interpretable Deep Learning Interpretation, Inter.pdf;C\:\\Users\\cleme\\Zotero\\storage\\HL45UDTS\\2103.html}
}

@article{linardatosExplainableAIReview2020,
  title = {Explainable {{AI}}: {{A Review}} of {{Machine Learning Interpretability Methods}}},
  shorttitle = {Explainable {{AI}}},
  author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  year = {2020},
  month = dec,
  journal = {Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies},
  volume = {23},
  number = {1},
  pages = {18},
  issn = {1099-4300},
  doi = {10.3390/e23010018},
  urldate = {2023-05-04},
  abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into ``black box'' approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
  pmcid = {PMC7824368},
  pmid = {33375658}
}

@article{linardatosExplainableAIReview2020a,
  title = {Explainable {{AI}}: {{A Review}} of {{Machine Learning Interpretability Methods}}},
  shorttitle = {Explainable {{AI}}},
  author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  year = {2020},
  month = dec,
  journal = {Entropy},
  volume = {23},
  number = {1},
  pages = {18},
  issn = {1099-4300},
  doi = {10.3390/e23010018},
  urldate = {2023-05-04},
  abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into ``black box'' approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
  pmcid = {PMC7824368},
  pmid = {33375658},
  file = {C:\Users\cleme\Zotero\storage\II8ESYUA\Linardatos et al. - 2020 - Explainable AI A Review of Machine Learning Inter.pdf}
}

@article{liptonMythosModelInterpretability2018,
  title = {The {{Mythos}} of {{Model Interpretability}}: {{In}} Machine Learning, the Concept of Interpretability Is Both Important and Slippery.},
  shorttitle = {The {{Mythos}} of {{Model Interpretability}}},
  author = {Lipton, Zachary C.},
  year = {2018},
  month = jun,
  journal = {Queue},
  volume = {16},
  number = {3},
  pages = {31--57},
  issn = {1542-7730},
  doi = {10.1145/3236386.3241340},
  urldate = {2023-05-16},
  abstract = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?}
}

@article{liptonMythosModelInterpretability2018a,
  title = {The {{Mythos}} of {{Model Interpretability}}: {{In}} Machine Learning, the Concept of Interpretability Is Both Important and Slippery.},
  shorttitle = {The {{Mythos}} of {{Model Interpretability}}},
  author = {Lipton, Zachary C.},
  year = {2018},
  month = jun,
  journal = {Queue},
  volume = {16},
  number = {3},
  pages = {31--57},
  issn = {1542-7730},
  doi = {10.1145/3236386.3241340},
  urldate = {2023-05-16},
  abstract = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
  file = {C:\Users\cleme\Zotero\storage\TD5ICKRT\Lipton - 2018 - The Mythos of Model Interpretability In machine l.pdf}
}

@article{liRegistrationOCTFundus2010,
  title = {Registration of {{OCT}} Fundus Images with Color Fundus Photographs Based on Blood Vessel Ridges},
  author = {Li, Ying and Gregori, Giovanni and Knighton, Robert W. and Lujan, Brandon J. and Rosenfeld, Philip J.},
  year = {2010},
  month = dec,
  journal = {Optics Express},
  volume = {19},
  number = {1},
  pages = {7--16},
  issn = {1094-4087},
  doi = {10.1364/OE.19.000007},
  urldate = {2019-07-23},
  abstract = {This paper proposes an algorithm to register OCT fundus images (OFIs) with color fundus photographs (CFPs). This makes it possible to correlate retinal features across the different imaging modalities. Blood vessel ridges are taken as features for registration. A specially defined distance, incorporating information of normal direction of blood vessel ridge pixels, is designed to calculate the distance between each pair of pixels to be matched in the pair image. Based on this distance a similarity function between the pair image is defined. Brute force search is used for a coarse registration and then an Iterative Closest Point (ICP) algorithm for a more accurate registration. The registration algorithm was tested on a sample set containing images of both normal eyes and eyes with pathologies. Three transformation models (similarity, affine and quadratic models) were tested on all image pairs respectively. The experimental results showed that the registration algorithm worked well. The average root mean square errors for the affine model are 31 {\textmu}m (normal) and 59 {\textmu}m (eyes with disease). The proposed algorithm can be easily adapted to registration for other modality retinal images.},
  pmcid = {PMC3368356},
  pmid = {21263537}
}

@article{liRegistrationOCTFundus2010a,
  title = {Registration of {{OCT}} Fundus Images with Color Fundus Photographs Based on Blood Vessel Ridges},
  author = {Li, Ying and Gregori, Giovanni and Knighton, Robert W. and Lujan, Brandon J. and Rosenfeld, Philip J.},
  year = {2010},
  month = dec,
  journal = {Optics Express},
  volume = {19},
  number = {1},
  pages = {7--16},
  issn = {1094-4087},
  doi = {10.1364/OE.19.000007},
  urldate = {2019-07-23},
  abstract = {This paper proposes an algorithm to register OCT fundus images (OFIs) with color fundus photographs (CFPs). This makes it possible to correlate retinal features across the different imaging modalities. Blood vessel ridges are taken as features for registration. A specially defined distance, incorporating information of normal direction of blood vessel ridge pixels, is designed to calculate the distance between each pair of pixels to be matched in the pair image. Based on this distance a similarity function between the pair image is defined. Brute force search is used for a coarse registration and then an Iterative Closest Point (ICP) algorithm for a more accurate registration. The registration algorithm was tested on a sample set containing images of both normal eyes and eyes with pathologies. Three transformation models (similarity, affine and quadratic models) were tested on all image pairs respectively. The experimental results showed that the registration algorithm worked well. The average root mean square errors for the affine model are 31 {\textmu}m (normal) and 59 {\textmu}m (eyes with disease). The proposed algorithm can be easily adapted to registration for other modality retinal images.},
  pmcid = {PMC3368356},
  pmid = {21263537},
  file = {C:\Users\cleme\Zotero\storage\SJV5D2DH\Li et al. - 2010 - Registration of OCT fundus images with color fundu.pdf}
}

@article{liRegistrationOCTFundus2011,
  title = {Registration of {{OCT}} Fundus Images with Color Fundus Photographs Based on Blood Vessel Ridges},
  author = {Li, Ying and Gregori, Giovanni and Knighton, Robert W. and Lujan, Brandon J. and Rosenfeld, Philip J.},
  year = {2011},
  month = jan,
  journal = {Optics Express},
  volume = {19},
  number = {1},
  pages = {7--16},
  issn = {1094-4087},
  doi = {10.1364/OE.19.000007},
  urldate = {2019-12-11},
  abstract = {This paper proposes an algorithm to register OCT fundus images (OFIs) with color fundus photographs (CFPs). This makes it possible to correlate retinal features across the different imaging modalities. Blood vessel ridges are taken as features for registration. A specially defined distance, incorporating information of normal direction of blood vessel ridge pixels, is designed to calculate the distance between each pair of pixels to be matched in the pair image. Based on this distance a similarity function between the pair image is defined. Brute force search is used for a coarse registration and then an Iterative Closest Point (ICP) algorithm for a more accurate registration. The registration algorithm was tested on a sample set containing images of both normal eyes and eyes with pathologies. Three transformation models (similarity, affine and quadratic models) were tested on all image pairs respectively. The experimental results showed that the registration algorithm worked well. The average root mean square errors for the affine model are 31 {\textmu}m (normal) and 59 {\textmu}m (eyes with disease). The proposed algorithm can be easily adapted to registration for other modality retinal images.},
  copyright = {\&\#169; 2011 OSA},
  langid = {english},
  keywords = {Blood,Eye movements,Image registration,Ophthalmic imaging,Spectral domain optical coherence tomography,Visibility}
}

@article{liRegistrationOCTFundus2011a,
  title = {Registration of {{OCT}} Fundus Images with Color Fundus Photographs Based on Blood Vessel Ridges},
  author = {Li, Ying and Gregori, Giovanni and Knighton, Robert W. and Lujan, Brandon J. and Rosenfeld, Philip J.},
  year = {2011},
  month = jan,
  journal = {Optics Express},
  volume = {19},
  number = {1},
  pages = {7--16},
  issn = {1094-4087},
  doi = {10.1364/OE.19.000007},
  urldate = {2019-12-11},
  abstract = {This paper proposes an algorithm to register OCT fundus images (OFIs) with color fundus photographs (CFPs). This makes it possible to correlate retinal features across the different imaging modalities. Blood vessel ridges are taken as features for registration. A specially defined distance, incorporating information of normal direction of blood vessel ridge pixels, is designed to calculate the distance between each pair of pixels to be matched in the pair image. Based on this distance a similarity function between the pair image is defined. Brute force search is used for a coarse registration and then an Iterative Closest Point (ICP) algorithm for a more accurate registration. The registration algorithm was tested on a sample set containing images of both normal eyes and eyes with pathologies. Three transformation models (similarity, affine and quadratic models) were tested on all image pairs respectively. The experimental results showed that the registration algorithm worked well. The average root mean square errors for the affine model are 31 {\textmu}m (normal) and 59 {\textmu}m (eyes with disease). The proposed algorithm can be easily adapted to registration for other modality retinal images.},
  copyright = {\&\#169; 2011 OSA},
  langid = {english},
  keywords = {Blood,Eye movements,Image registration,Ophthalmic imaging,Spectral domain optical coherence tomography,Visibility},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\RAFIJKC8\\Li et al. - 2011 - Registration of OCT fundus images with color fundu.pdf;C\:\\Users\\cleme\\Zotero\\storage\\EC53FBML\\abstract.html;C\:\\Users\\cleme\\Zotero\\storage\\QYUXCB39\\li2010.html}
}

@inproceedings{liRobustSourceFreeDomain2024,
  title = {Robust {{Source-Free Domain Adaptation}} for {{Fundus Image Segmentation}}},
  booktitle = {2024 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Li, Lingrui and Zhou, Yanfeng and Yang, Ge},
  year = {2024},
  month = jan,
  pages = {7825--7834},
  publisher = {IEEE Computer Society},
  doi = {10.1109/WACV57701.2024.00766},
  urldate = {2024-06-22},
  abstract = {Unsupervised Domain Adaptation (UDA) is a learning technique that transfers knowledge learned in the source domain from labelled training data to the target domain with only unlabelled data. It is of significant importance to medical image segmentation because of the usual lack of labelled training data. Although extensive efforts have been made to optimize UDA techniques to improve the accuracy of segmentation models in the target domain, few studies have addressed the robustness of these models under UDA. In this study, we propose a two-stage training strategy for robust domain adaptation. In the source training stage, we utilize adversarial sample augmentation to enhance the robustness and generalization capability of the source model. And in the target training stage, we propose a novel robust pseudo-label and pseudo-boundary (PLPB) method, which effectively utilizes unlabeled target data to generate pseudo labels and pseudo boundaries that enable model self-adaptation without requiring source data. Extensive experimental results on cross-domain fundus image segmentation confirm the effectiveness and versatility of our method. Source code of this study is openly accessible at https://github.com/LinGrayy/PLPB.},
  isbn = {9798350318920},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\NBMXUY5G\Li et al. - 2024 - Robust Source-Free Domain Adaptation for Fundus Im.pdf}
}

@inproceedings{liTyphoonCloudPrediction2018,
  title = {Typhoon {{Cloud Prediction Via Generative Adversarial Networks}}},
  booktitle = {{{IGARSS}} 2018 - 2018 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Li, H. and Yu, X. and Ren, P.},
  year = {2018},
  month = jul,
  pages = {3023--3026},
  doi = {10.1109/IGARSS.2018.8518069},
  abstract = {We present a novel typhoon cloud prediction method via generative adversarial networks (GANs). Specifically, we develop a adversarial prediction model consisting of a generator and a discriminator. The generator generates the continuous future cloud images by learning the evolution trend of typhoon clouds from multiple continuous historical typhoon cloud images. In this way, the generator completes the visual predictions for typhoon clouds. On the other hand, the discriminator distinguishes the generated future cloud images from the real ones. Furthermore, we adopt a gradient difference loss function and a total variation loss function to improve the quality of the generated cloud images. The proposed method effectively predicts the whole spatial-temporal evolution of the typhoon clouds, resulting in a visual complement for classic typhoon prediction methods. The effectiveness of the proposed method has been evaluated in the real satellite cloud images.},
  keywords = {adversarial prediction model,atmospheric techniques,classic typhoon prediction methods,clouds,Clouds,continuous future cloud images,data visualisation,generated cloud images,generated future cloud images,generative adversarial networks,generative adversarial networks (GANs),Generators,geophysical image processing,geophysical signal processing,gradient difference loss,Mathematical model,multiple continuous historical typhoon cloud images,prediction theory,Predictive models,satellite cloud images,storms,Testing,total variation loss,Training,Tropical cyclones,Typhoon cloud prediction,typhoon cloud prediction method,typhoon clouds,visual predictions}
}

@inproceedings{liTyphoonCloudPrediction2018a,
  title = {Typhoon {{Cloud Prediction Via Generative Adversarial Networks}}},
  booktitle = {{{IGARSS}} 2018 - 2018 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Li, H. and Yu, X. and Ren, P.},
  year = {2018},
  month = jul,
  pages = {3023--3026},
  doi = {10.1109/IGARSS.2018.8518069},
  abstract = {We present a novel typhoon cloud prediction method via generative adversarial networks (GANs). Specifically, we develop a adversarial prediction model consisting of a generator and a discriminator. The generator generates the continuous future cloud images by learning the evolution trend of typhoon clouds from multiple continuous historical typhoon cloud images. In this way, the generator completes the visual predictions for typhoon clouds. On the other hand, the discriminator distinguishes the generated future cloud images from the real ones. Furthermore, we adopt a gradient difference loss function and a total variation loss function to improve the quality of the generated cloud images. The proposed method effectively predicts the whole spatial-temporal evolution of the typhoon clouds, resulting in a visual complement for classic typhoon prediction methods. The effectiveness of the proposed method has been evaluated in the real satellite cloud images.},
  keywords = {adversarial prediction model,atmospheric techniques,classic typhoon prediction methods,clouds,Clouds,continuous future cloud images,data visualisation,generated cloud images,generated future cloud images,generative adversarial networks,generative adversarial networks (GANs),Generators,geophysical image processing,geophysical signal processing,gradient difference loss,Mathematical model,multiple continuous historical typhoon cloud images,prediction theory,Predictive models,satellite cloud images,storms,Testing,total variation loss,Training,Tropical cyclones,Typhoon cloud prediction,typhoon cloud prediction method,typhoon clouds,visual predictions}
}

@inproceedings{liTyphoonCloudPrediction2018b,
  title = {Typhoon {{Cloud Prediction Via Generative Adversarial Networks}}},
  booktitle = {{{IGARSS}} 2018 - 2018 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Li, H. and Yu, X. and Ren, P.},
  year = {2018},
  month = jul,
  pages = {3023--3026},
  doi = {10.1109/IGARSS.2018.8518069},
  abstract = {We present a novel typhoon cloud prediction method via generative adversarial networks (GANs). Specifically, we develop a adversarial prediction model consisting of a generator and a discriminator. The generator generates the continuous future cloud images by learning the evolution trend of typhoon clouds from multiple continuous historical typhoon cloud images. In this way, the generator completes the visual predictions for typhoon clouds. On the other hand, the discriminator distinguishes the generated future cloud images from the real ones. Furthermore, we adopt a gradient difference loss function and a total variation loss function to improve the quality of the generated cloud images. The proposed method effectively predicts the whole spatial-temporal evolution of the typhoon clouds, resulting in a visual complement for classic typhoon prediction methods. The effectiveness of the proposed method has been evaluated in the real satellite cloud images.},
  keywords = {adversarial prediction model,atmospheric techniques,classic typhoon prediction methods,clouds,Clouds,continuous future cloud images,data visualisation,generated cloud images,generated future cloud images,generative adversarial networks,generative adversarial networks (GANs),Generators,geophysical image processing,geophysical signal processing,gradient difference loss,Mathematical model,multiple continuous historical typhoon cloud images,prediction theory,Predictive models,satellite cloud images,storms,Testing,total variation loss,Training,Tropical cyclones,Typhoon cloud prediction,typhoon cloud prediction method,typhoon clouds,visual predictions},
  file = {C:\Users\cleme\Zotero\storage\TL7ZWURQ\8518069.html}
}

@inproceedings{liTyphoonCloudPrediction2018c,
  title = {Typhoon {{Cloud Prediction Via Generative Adversarial Networks}}},
  booktitle = {{{IGARSS}} 2018 - 2018 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Li, H. and Yu, X. and Ren, P.},
  year = {2018},
  month = jul,
  pages = {3023--3026},
  doi = {10.1109/IGARSS.2018.8518069},
  abstract = {We present a novel typhoon cloud prediction method via generative adversarial networks (GANs). Specifically, we develop a adversarial prediction model consisting of a generator and a discriminator. The generator generates the continuous future cloud images by learning the evolution trend of typhoon clouds from multiple continuous historical typhoon cloud images. In this way, the generator completes the visual predictions for typhoon clouds. On the other hand, the discriminator distinguishes the generated future cloud images from the real ones. Furthermore, we adopt a gradient difference loss function and a total variation loss function to improve the quality of the generated cloud images. The proposed method effectively predicts the whole spatial-temporal evolution of the typhoon clouds, resulting in a visual complement for classic typhoon prediction methods. The effectiveness of the proposed method has been evaluated in the real satellite cloud images.},
  keywords = {adversarial prediction model,atmospheric techniques,classic typhoon prediction methods,clouds,Clouds,continuous future cloud images,data visualisation,generated cloud images,generated future cloud images,generative adversarial networks,generative adversarial networks (GANs),Generators,geophysical image processing,geophysical signal processing,gradient difference loss,Mathematical model,multiple continuous historical typhoon cloud images,prediction theory,Predictive models,satellite cloud images,storms,Testing,total variation loss,Training,Tropical cyclones,Typhoon cloud prediction,typhoon cloud prediction method,typhoon clouds,visual predictions},
  file = {C:\Users\cleme\Zotero\storage\EXD6Y5V5\8518069.html}
}

@article{liuAutomatedMacularPathology2011,
  title = {Automated Macular Pathology Diagnosis in Retinal {{OCT}} Images Using Multi-Scale Spatial Pyramid and Local Binary Patterns in Texture and Shape Encoding},
  author = {Liu, Yu-Ying and Chen, Mei and Ishikawa, Hiroshi and Wollstein, Gadi and Schuman, Joel S. and Rehg, James M.},
  year = {2011},
  month = oct,
  journal = {Medical Image Analysis},
  volume = {15},
  number = {5},
  pages = {748--759},
  issn = {1361-8423},
  doi = {10.1016/j.media.2011.06.005},
  abstract = {We address a novel problem domain in the analysis of optical coherence tomography (OCT) images: the diagnosis of multiple macular pathologies in retinal OCT images. The goal is to identify the presence of normal macula and each of three types of macular pathologies, namely, macular edema, macular hole, and age-related macular degeneration, in the OCT slice centered at the fovea. We use a machine learning approach based on global image descriptors formed from a multi-scale spatial pyramid. Our local features are dimension-reduced local binary pattern histograms, which are capable of encoding texture and shape information in retinal OCT images and their edge maps, respectively. Our representation operates at multiple spatial scales and granularities, leading to robust performance. We use 2-class support vector machine classifiers to identify the presence of normal macula and each of the three pathologies. To further discriminate sub-types within a pathology, we also build a classifier to differentiate full-thickness holes from pseudo-holes within the macular hole category. We conduct extensive experiments on a large dataset of 326 OCT scans from 136 subjects. The results show that the proposed method is very effective (all AUC{$>$}0.93).},
  langid = {english},
  pmcid = {PMC3164533},
  pmid = {21737338},
  keywords = {Algorithms,Area Under Curve,Artificial Intelligence,Automated,Computer-Assisted,Diagnosis,Humans,Image Enhancement,Image Interpretation,Macula Lutea,Optical Coherence,Pattern Recognition,Retinal Diseases,Retinoscopy,Sensitivity and Specificity,Tomography}
}

@article{liuAutomatedMacularPathology2011a,
  title = {Automated Macular Pathology Diagnosis in Retinal {{OCT}} Images Using Multi-Scale Spatial Pyramid and Local Binary Patterns in Texture and Shape Encoding},
  author = {Liu, Yu-Ying and Chen, Mei and Ishikawa, Hiroshi and Wollstein, Gadi and Schuman, Joel S. and Rehg, James M.},
  year = {2011},
  month = oct,
  journal = {Medical Image Analysis},
  volume = {15},
  number = {5},
  pages = {748--759},
  issn = {1361-8423},
  doi = {10.1016/j.media.2011.06.005},
  abstract = {We address a novel problem domain in the analysis of optical coherence tomography (OCT) images: the diagnosis of multiple macular pathologies in retinal OCT images. The goal is to identify the presence of normal macula and each of three types of macular pathologies, namely, macular edema, macular hole, and age-related macular degeneration, in the OCT slice centered at the fovea. We use a machine learning approach based on global image descriptors formed from a multi-scale spatial pyramid. Our local features are dimension-reduced local binary pattern histograms, which are capable of encoding texture and shape information in retinal OCT images and their edge maps, respectively. Our representation operates at multiple spatial scales and granularities, leading to robust performance. We use 2-class support vector machine classifiers to identify the presence of normal macula and each of the three pathologies. To further discriminate sub-types within a pathology, we also build a classifier to differentiate full-thickness holes from pseudo-holes within the macular hole category. We conduct extensive experiments on a large dataset of 326 OCT scans from 136 subjects. The results show that the proposed method is very effective (all AUC{$>$}0.93).},
  langid = {english},
  pmcid = {PMC3164533},
  pmid = {21737338},
  keywords = {Algorithms,Area Under Curve,Artificial Intelligence,Diagnosis Computer-Assisted,Humans,Image Enhancement,Image Interpretation Computer-Assisted,Macula Lutea,Pattern Recognition Automated,Retinal Diseases,Retinoscopy,Sensitivity and Specificity,Tomography Optical Coherence},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\K8MTRIWX\\Liu et al. - 2011 - Automated macular pathology diagnosis in retinal O.pdf;C\:\\Users\\cleme\\Zotero\\storage\\NYM8SR5Z\\liu2011.pdf}
}

@article{liuDeepDRiDDiabeticRetinopathy2022,
  title = {{{DeepDRiD}}: {{Diabetic Retinopathy}}---{{Grading}} and {{Image Quality Estimation Challenge}}},
  shorttitle = {{{DeepDRiD}}},
  author = {Liu, Ruhan and Wang, Xiangning and Wu, Qiang and Dai, Ling and Fang, Xi and Yan, Tao and Son, Jaemin and Tang, Shiqi and Li, Jiang and Gao, Zijian and Galdran, Adrian and Poorneshwaran, J. M. and Liu, Hao and Wang, Jie and Chen, Yerui and Porwal, Prasanna and Tan, Gavin Siew Wei and Yang, Xiaokang and Dai, Chao and Song, Haitao and Chen, Mingang and Li, Huating and Jia, Weiping and Shen, Dinggang and Sheng, Bin and Zhang, Ping},
  year = {2022},
  month = jun,
  journal = {Patterns},
  volume = {3},
  number = {6},
  publisher = {Elsevier},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2022.100512},
  urldate = {2024-06-10},
  langid = {english},
  keywords = {and tested for one domain/problem,artificial intelligence,challenge,deep learning,diabetic retinopathy,DSML2: Proof-of-concept Data science output has been formulated,fundus image,image quality analysis,implemented,retinal image,screening,ultra-widefield},
  file = {C:\Users\cleme\Zotero\storage\I5WVG7QF\Liu et al. - 2022 - DeepDRiD Diabetic Retinopathy—Grading and Image Q.pdf}
}

@article{liuGraphLSurvScalableSurvival2023,
  title = {{{GraphLSurv}}: {{A}} Scalable Survival Prediction Network with Adaptive and Sparse Structure Learning for Histopathological Whole-Slide Images},
  shorttitle = {{{GraphLSurv}}},
  author = {Liu, Pei and Ji, Luping and Ye, Feng and Fu, Bo},
  year = {2023},
  month = apr,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {231},
  pages = {107433},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2023.107433},
  urldate = {2023-10-02},
  abstract = {Background and Objective Predicting patients' survival from gigapixel Whole-Slide Images (WSIs) has always been a challenging task. To learn effective WSI representations for survival prediction, existing deep learning methods have explored utilizing graphs to describe the complex structure inner WSIs, where graph node is respective to WSI patch. However, these graphs are often densely-connected or static, leading to some redundant or missing patch correlations. Moreover, these methods cannot be directly scaled to the very-large WSI with more than 10,000 patches. To address these, this paper proposes a scalable graph convolution network, GraphLSurv, which can efficiently learn adaptive and sparse structures to better characterize WSIs for survival prediction. Methods GraphLSurv has three highlights in methodology: (1) it generates adaptive and sparse structures for patches so that latent patch correlations could be captured and adjusted dynamically according to prediction tasks; (2) based on the generated structure and a given graph, GraphLSurv further aggregates local microenvironmental cues into a non-local embedding using the proposed hybrid message passing network; (3) to make this network suitable for very large-scale graphs, it adopts an anchor-based technique to reduce theorical computation complexity. Results The experiments on 2268 WSIs show that GraphLSurv achieves a concordance-index of 0.66132 and 0.68348, with an improvement of 3.79\% and 3.41\% compared to existing methods, on NLST and TCGA-BRCA, respectively. Conclusions GraphLSurv could often perform better than previous methods, which suggests that GraphLSurv could provide an important and effective means for WSI survival prediction. Moreover, this work empirically shows that adaptive and sparse structures could be more suitable than static or dense ones for modeling WSIs.},
  keywords = {Computational pathology,Graph convolution network,Survival prediction,Whole slide image}
}

@article{liuGraphLSurvScalableSurvival2023a,
  title = {{{GraphLSurv}}: {{A}} Scalable Survival Prediction Network with Adaptive and Sparse Structure Learning for Histopathological Whole-Slide Images},
  shorttitle = {{{GraphLSurv}}},
  author = {Liu, Pei and Ji, Luping and Ye, Feng and Fu, Bo},
  year = {2023},
  month = apr,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {231},
  pages = {107433},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2023.107433},
  urldate = {2023-10-02},
  abstract = {Background and Objective Predicting patients' survival from gigapixel Whole-Slide Images (WSIs) has always been a challenging task. To learn effective WSI representations for survival prediction, existing deep learning methods have explored utilizing graphs to describe the complex structure inner WSIs, where graph node is respective to WSI patch. However, these graphs are often densely-connected or static, leading to some redundant or missing patch correlations. Moreover, these methods cannot be directly scaled to the very-large WSI with more than 10,000 patches. To address these, this paper proposes a scalable graph convolution network, GraphLSurv, which can efficiently learn adaptive and sparse structures to better characterize WSIs for survival prediction. Methods GraphLSurv has three highlights in methodology: (1) it generates adaptive and sparse structures for patches so that latent patch correlations could be captured and adjusted dynamically according to prediction tasks; (2) based on the generated structure and a given graph, GraphLSurv further aggregates local microenvironmental cues into a non-local embedding using the proposed hybrid message passing network; (3) to make this network suitable for very large-scale graphs, it adopts an anchor-based technique to reduce theorical computation complexity. Results The experiments on 2268 WSIs show that GraphLSurv achieves a concordance-index of 0.66132 and 0.68348, with an improvement of 3.79\% and 3.41\% compared to existing methods, on NLST and TCGA-BRCA, respectively. Conclusions GraphLSurv could often perform better than previous methods, which suggests that GraphLSurv could provide an important and effective means for WSI survival prediction. Moreover, this work empirically shows that adaptive and sparse structures could be more suitable than static or dense ones for modeling WSIs.},
  keywords = {Computational pathology,Graph convolution network,Survival prediction,Whole slide image},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\BJVET866\\Liu et al. - 2023 - GraphLSurv A scalable survival prediction network.pdf;C\:\\Users\\cleme\\Zotero\\storage\\KPBPNYNB\\S0169260723001001.html}
}

@article{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.14030 [cs]},
  eprint = {2103.14030},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting selfattention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The code and models will be made publicly available at https:// github.com/microsoft/Swin-Transformer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{liuSwinTransformerHierarchical2021a,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.14030 [cs]},
  eprint = {2103.14030},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting selfattention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The code and models will be made publicly available at https:// github.com/microsoft/Swin-Transformer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\MJET5P7C\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf}
}

@article{liuTMMNetsTransferredMulti2023,
  title = {{{TMM-Nets}}: {{Transferred Multi-}} to {{Mono-Modal Generation}} for {{Lupus Retinopathy Diagnosis}}},
  shorttitle = {{{TMM-Nets}}},
  author = {Liu, Ruhan and Wang, Tianqin and Li, Huating and Zhang, Ping and Li, Jing and Yang, Xiaokang and Shen, Dinggang and Sheng, Bin},
  year = {2023},
  month = apr,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {42},
  number = {4},
  pages = {1083--1094},
  issn = {1558-254X},
  doi = {10.1109/TMI.2022.3223683},
  urldate = {2024-06-10},
  abstract = {Rare diseases, which are severely underrepresented in basic and clinical research, can particularly benefit from machine learning techniques. However, current learning-based approaches usually focus on either mono-modal image data or matched multi-modal data, whereas the diagnosis of rare diseases necessitates the aggregation of unstructured and unmatched multi-modal image data due to their rare and diverse nature. In this study, we therefore propose diagnosis-guided multi-to-mono modal generation networks (TMM-Nets) along with training and testing procedures. TMM-Nets can transfer data from multiple sources to a single modality for diagnostic data structurization. To demonstrate their potential in the context of rare diseases, TMM-Nets were deployed to diagnose the lupus retinopathy (LR-SLE), leveraging unmatched regular and ultra-wide-field fundus images for transfer learning. The TMM-Nets encoded the transfer learning from diabetic retinopathy to LR-SLE based on the similarity of the fundus lesions. In addition, a lesion-aware multi-scale attention mechanism was developed for clinical alerts, enabling TMM-Nets not only to inform patient care, but also to provide insights consistent with those of clinicians. An adversarial strategy was also developed to refine multi- to mono-modal image generation based on diagnostic results and the data distribution to enhance the data augmentation performance. Compared to the baseline model, the TMM-Nets showed 35.19\% and 33.56\% F1 score improvements on the test and external validation sets, respectively. In addition, the TMM-Nets can be used to develop diagnostic models for other rare diseases.},
  keywords = {Biomedical imaging,Data models,generating adversarial training,Image synthesis,Lesions,Lupus retinopathy,Retinopathy,Training,Transfer learning,unmatched multi-modal data,UWF-FFA,UWF-FP}
}

@misc{liuTodyNetTemporalDynamic2023,
  title = {{{TodyNet}}: {{Temporal Dynamic Graph Neural Network}} for {{Multivariate Time Series Classification}}},
  shorttitle = {{{TodyNet}}},
  author = {Liu, Huaiyuan and Liu, Xianzhang and Yang, Donghua and Liang, Zhiyu and Wang, Hongzhi and Cui, Yong and Gu, Jun},
  year = {2023},
  month = apr,
  number = {arXiv:2304.05078},
  eprint = {2304.05078},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.05078},
  urldate = {2023-10-05},
  abstract = {Multivariate time series classification (MTSC) is an important data mining task, which can be effectively solved by popular deep learning technology. Unfortunately, the existing deep learning-based methods neglect the hidden dependencies in different dimensions and also rarely consider the unique dynamic features of time series, which lack sufficient feature extraction capability to obtain satisfactory classification accuracy. To address this problem, we propose a novel temporal dynamic graph neural network (TodyNet) that can extract hidden spatio-temporal dependencies without undefined graph structure. It enables information flow among isolated but implicit interdependent variables and captures the associations between different time slots by dynamic graph mechanism, which further improves the classification performance of the model. Meanwhile, the hierarchical representations of graphs cannot be learned due to the limitation of GNNs. Thus, we also design a temporal graph pooling layer to obtain a global graph-level representation for graph learning with learnable temporal parameters. The dynamic graph, graph information propagation, and temporal convolution are jointly learned in an end-to-end framework. The experiments on 26 UEA benchmark datasets illustrate that the proposed TodyNet outperforms existing deep learning-based methods in the MTSC tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{liuTodyNetTemporalDynamic2023a,
  title = {{{TodyNet}}: {{Temporal Dynamic Graph Neural Network}} for {{Multivariate Time Series Classification}}},
  shorttitle = {{{TodyNet}}},
  author = {Liu, Huaiyuan and Liu, Xianzhang and Yang, Donghua and Liang, Zhiyu and Wang, Hongzhi and Cui, Yong and Gu, Jun},
  year = {2023},
  month = apr,
  number = {arXiv:2304.05078},
  eprint = {2304.05078},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.05078},
  urldate = {2023-10-05},
  abstract = {Multivariate time series classification (MTSC) is an important data mining task, which can be effectively solved by popular deep learning technology. Unfortunately, the existing deep learning-based methods neglect the hidden dependencies in different dimensions and also rarely consider the unique dynamic features of time series, which lack sufficient feature extraction capability to obtain satisfactory classification accuracy. To address this problem, we propose a novel temporal dynamic graph neural network (TodyNet) that can extract hidden spatio-temporal dependencies without undefined graph structure. It enables information flow among isolated but implicit interdependent variables and captures the associations between different time slots by dynamic graph mechanism, which further improves the classification performance of the model. Meanwhile, the hierarchical representations of graphs cannot be learned due to the limitation of GNNs. Thus, we also design a temporal graph pooling layer to obtain a global graph-level representation for graph learning with learnable temporal parameters. The dynamic graph, graph information propagation, and temporal convolution are jointly learned in an end-to-end framework. The experiments on 26 UEA benchmark datasets illustrate that the proposed TodyNet outperforms existing deep learning-based methods in the MTSC tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\RZZGYT49\\Liu et al. - 2023 - TodyNet Temporal Dynamic Graph Neural Network for.pdf;C\:\\Users\\cleme\\Zotero\\storage\\PUA94KYM\\2304.html}
}

@book{longFullyConvolutionalNetworks2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  month = jun,
  pages = {3440},
  doi = {10.1109/CVPR.2015.7298965}
}

@inproceedings{longFullyConvolutionalNetworks2015a,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  month = jun,
  pages = {3431--3440},
  doi = {10.1109/CVPR.2015.7298965},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ``fully convolutional'' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  langid = {english}
}

@inproceedings{longFullyConvolutionalNetworks2015b,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  month = jun,
  pages = {3431--3440},
  doi = {10.1109/CVPR.2015.7298965},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ``fully convolutional'' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\J63U4RL8\Long et al. - Fully Convolutional Networks for Semantic Segmenta.pdf}
}

@book{longFullyConvolutionalNetworks2015c,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  month = jun,
  pages = {3440},
  doi = {10.1109/CVPR.2015.7298965},
  file = {C:\Users\cleme\Zotero\storage\KA4I29WA\Long et al. - 2015 - Fully convolutional networks for semantic segmenta.pdf}
}

@inproceedings{loshchilovDecoupledWeightDecay2018,
  title = {Decoupled {{Weight Decay Regularization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2018},
  month = sep,
  urldate = {2021-03-01},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case...},
  langid = {english}
}

@inproceedings{loshchilovDecoupledWeightDecay2018a,
  title = {Decoupled {{Weight Decay Regularization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2018},
  month = sep,
  urldate = {2021-03-01},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case...},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\4SACFSHA\\Loshchilov et Hutter - 2018 - Decoupled Weight Decay Regularization.pdf;C\:\\Users\\cleme\\Zotero\\storage\\VE7HILSF\\forum.html}
}

@article{loshchilovDECOUPLEDWEIGHTDECAY2019,
  title = {{{DECOUPLED WEIGHT DECAY REGULARIZATION}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  pages = {18},
  langid = {english}
}

@article{loshchilovDECOUPLEDWEIGHTDECAY2019a,
  title = {{{DECOUPLED WEIGHT DECAY REGULARIZATION}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  pages = {18},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\2XZ3KSEH\Loshchilov et Hutter - 2019 - DECOUPLED WEIGHT DECAY REGULARIZATION.pdf}
}

@article{loweDistinctiveImageFeatures2004,
  title = {Distinctive {{Image Features}} from {{Scale-Invariant Keypoints}}},
  author = {Lowe, David G.},
  year = {2004},
  month = nov,
  journal = {International Journal of Computer Vision},
  volume = {60},
  number = {2},
  pages = {91--110},
  issn = {1573-1405},
  doi = {10.1023/B:VISI.0000029664.99615.94},
  urldate = {2019-12-10},
  abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  langid = {english},
  keywords = {image matching,invariant features,object recognition,scale invariance}
}

@article{loweDistinctiveImageFeatures2004a,
  title = {Distinctive {{Image Features}} from {{Scale-Invariant Keypoints}}},
  author = {Lowe, David G.},
  year = {2004},
  month = nov,
  journal = {International Journal of Computer Vision},
  volume = {60},
  number = {2},
  pages = {91--110},
  issn = {1573-1405},
  doi = {10.1023/B:VISI.0000029664.99615.94},
  urldate = {2019-12-10},
  abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  langid = {english},
  keywords = {image matching,invariant features,object recognition,scale invariance},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\GNP4CAH6\\Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf;C\:\\Users\\cleme\\Zotero\\storage\\WF5IHQNX\\lowe2004.pdf}
}

@article{lowellOpticNerveHead2004,
  title = {Optic {{Nerve Head Segmentation}}},
  author = {Lowell, J. and Hunter, A. and Steel, D. and Basu, A. and Ryder, R. and Fletcher, E. and Kennedy, L.},
  year = {2004},
  month = feb,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {23},
  number = {2},
  pages = {256--264},
  issn = {0278-0062},
  doi = {10.1109/TMI.2003.823261},
  urldate = {2019-12-26},
  abstract = {Reliable and efficient optic disk localization and segmentation are important tasks in automated retinal screening. General-purpose edge detection algorithms often fail to segment the optic disk due to fuzzy boundaries, inconsistent image contrast or missing edge features. This paper presents an algorithm for the localization and segmentation of the optic nerve head boundary in low-resolution images (about 20{\textmu} per pixel). Optic disk localization is achieved using specialized template matching, and segmentation by a deformable contour model. The latter uses a global elliptical model and a local deformable model with variable edge-strength dependent stiffness. The algorithm is evaluated against a randomly-selected database of 100 images from a diabetic screening programme. Ten images were classified as unusable; the others were of highly variable quality. The localization algorithm succeeded on all bar one usable image; the contour estimation algorithm was qualitatively assessed by an ophthalmologist as having Excellent, Good or Fair performance in 83\% of cases, and performs well even on blurred images.},
  langid = {english}
}

@article{lowellOpticNerveHead2004a,
  title = {Optic {{Nerve Head Segmentation}}},
  author = {Lowell, J. and Hunter, A. and Steel, D. and Basu, A. and Ryder, R. and Fletcher, E. and Kennedy, L.},
  year = {2004},
  month = feb,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {23},
  number = {2},
  pages = {256--264},
  issn = {0278-0062},
  doi = {10.1109/TMI.2003.823261},
  urldate = {2019-12-26},
  abstract = {Reliable and efficient optic disk localization and segmentation are important tasks in automated retinal screening. General-purpose edge detection algorithms often fail to segment the optic disk due to fuzzy boundaries, inconsistent image contrast or missing edge features. This paper presents an algorithm for the localization and segmentation of the optic nerve head boundary in low-resolution images (about 20{\textmu} per pixel). Optic disk localization is achieved using specialized template matching, and segmentation by a deformable contour model. The latter uses a global elliptical model and a local deformable model with variable edge-strength dependent stiffness. The algorithm is evaluated against a randomly-selected database of 100 images from a diabetic screening programme. Ten images were classified as unusable; the others were of highly variable quality. The localization algorithm succeeded on all bar one usable image; the contour estimation algorithm was qualitatively assessed by an ophthalmologist as having Excellent, Good or Fair performance in 83\% of cases, and performs well even on blurred images.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\VZGUS8DS\\Lowell et al. - 2004 - Optic Nerve Head Segmentation.pdf;C\:\\Users\\cleme\\Zotero\\storage\\L9P9CFAY\\lowell2004.html}
}

@misc{LucasRodesGuirao_MasterThesisReportPdfGoogle,
  title = {{{LucasRodesGuirao}}\_{{MasterThesisReport}}.Pdf - {{Google~Drive}}},
  urldate = {2019-02-14},
  howpublished = {https://drive.google.com/file/d/1O-lt1BUkMN-u5fW9wUW4oJpWmgtRu1z1/view},
  file = {C:\Users\cleme\Zotero\storage\WE4EHSWQ\view.html}
}

@misc{LucasRodesGuiraoMasterThesisReportPdf,
  title = {{{LucasRodesGuirao}}\_{{MasterThesisReport}}.Pdf - {{Google Drive}}},
  urldate = {2019-02-14}
}

@article{luDeeplearningBasedMulticlass2019,
  title = {Deep-Learning Based Multiclass Retinal Fluid Segmentation and Detection in Optical Coherence Tomography Images Using a Fully Convolutional Neural Network},
  author = {Lu, Donghuan and Heisler, Morgan and Lee, Sieun and Ding, Gavin Weiguang and Navajas, Eduardo and Sarunic, Marinko V. and Beg, Mirza Faisal},
  year = {2019},
  month = may,
  journal = {Medical Image Analysis},
  volume = {54},
  pages = {100--110},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.02.011},
  urldate = {2022-07-09},
  abstract = {As a non-invasive imaging modality, optical coherence tomography (OCT) can provide micrometer-resolution 3D images of retinal structures. These images can help reveal disease-related alterations below the surface of the retina, such as the presence of edema, or accumulation of fluid which can distort vision, and are an indication of disruptions in the vasculature of the retina. In this paper, a new framework is proposed for multiclass fluid segmentation and detection in the retinal OCT images. Based on the intensity of OCT images and retinal layer segmentations provided by a graph-cut algorithm, a fully convolutional neural network was trained to recognize and label the fluid pixels. Random forest classification was performed on the segmented fluid regions to detect and reject the falsely labeled fluid regions. The proposed framework won the first place in the MICCAI RETOUCH challenge in 2017 on both the segmentation performance (mean Dice: 0.7667) and the detection performance (mean AUC: 1.00) tasks.},
  langid = {english},
  keywords = {Fully convolutional network,Multiclass segmentation and detection,Optical coherence tomography,Retinal fluid}
}

@article{luDeeplearningBasedMulticlass2019a,
  title = {Deep-Learning Based Multiclass Retinal Fluid Segmentation and Detection in Optical Coherence Tomography Images Using a Fully Convolutional Neural Network},
  author = {Lu, Donghuan and Heisler, Morgan and Lee, Sieun and Ding, Gavin Weiguang and Navajas, Eduardo and Sarunic, Marinko V. and Beg, Mirza Faisal},
  year = {2019},
  month = may,
  journal = {Medical Image Analysis},
  volume = {54},
  pages = {100--110},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.02.011},
  urldate = {2022-07-09},
  abstract = {As a non-invasive imaging modality, optical coherence tomography (OCT) can provide micrometer-resolution 3D images of retinal structures. These images can help reveal disease-related alterations below the surface of the retina, such as the presence of edema, or accumulation of fluid which can distort vision, and are an indication of disruptions in the vasculature of the retina. In this paper, a new framework is proposed for multiclass fluid segmentation and detection in the retinal OCT images. Based on the intensity of OCT images and retinal layer segmentations provided by a graph-cut algorithm, a fully convolutional neural network was trained to recognize and label the fluid pixels. Random forest classification was performed on the segmented fluid regions to detect and reject the falsely labeled fluid regions. The proposed framework won the first place in the MICCAI RETOUCH challenge in 2017 on both the segmentation performance (mean Dice: 0.7667) and the detection performance (mean AUC: 1.00) tasks.},
  langid = {english},
  keywords = {Fully convolutional network,Multiclass segmentation and detection,Optical coherence tomography,Retinal fluid},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\LX9CESZ3\\lu2019.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UFZKMYDN\\S1361841519300167.html}
}

@article{luExpressivePowerNeural,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
  langid = {english}
}

@inproceedings{luExpressivePowerNeural2017,
  title = {The Expressive Power of Neural Networks: A View from the Width},
  shorttitle = {The Expressive Power of Neural Networks},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  year = {2017},
  month = dec,
  series = {{{NIPS}}'17},
  pages = {6232--6240},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-06-19},
  abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
  isbn = {978-1-5108-6096-4}
}

@inproceedings{luExpressivePowerNeural2017a,
  title = {The Expressive Power of Neural Networks: A View from the Width},
  shorttitle = {The Expressive Power of Neural Networks},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  year = {2017},
  month = dec,
  series = {{{NIPS}}'17},
  pages = {6232--6240},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-06-19},
  abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
  isbn = {978-1-5108-6096-4},
  file = {C:\Users\cleme\Zotero\storage\FQ7VK25X\Lu et al. - 2017 - The expressive power of neural networks a view fr.pdf}
}

@article{luExpressivePowerNeurala,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\AYMQDALL\Lu et al. - The Expressive Power of Neural Networks A View fr.pdf}
}

@article{lundbergUnifiedApproachInterpreting,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  pages = {10},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\RKFAZBBH\Lundberg et Lee - A Unified Approach to Interpreting Model Predictio.pdf}
}

@inproceedings{luoDeepNeuralNetworks2022,
  title = {Deep {{Neural Networks Learn Meta-Structures}} from {{Noisy Labels}} in {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Luo, Yaoru and Liu, Guole and Guo, Yuanhao and Yang, Ge},
  year = {2022},
  month = jun,
  volume = {36},
  pages = {1908--1916},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i2.20085},
  urldate = {2023-06-19},
  abstract = {How deep neural networks (DNNs) learn from noisy labels has been studied extensively in image classification but much less in image segmentation. So far, our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. In this study, we address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We generate extremely noisy labels by randomly sampling a small fraction (e.g., 10\%) or flipping a large fraction (e.g., 90\%) of the ground truth labels. When trained with these noisy labels, DNNs provide largely the same segmentation performance as trained by the original ground truth. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We refer to these hidden structures in labels as meta-structures. When DNNs are trained by labels with different perturbations to the meta-structure, we find consistent degradation in their segmentation performance. In contrast, incorporation of meta-structure information substantially improves performance of an unsupervised segmentation model developed for binary semantic segmentation. We define meta-structures mathematically as spatial density distributions and show both theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.}
}

@article{luoDeepNeuralNetworks2022a,
  title = {Deep {{Neural Networks Learn Meta-Structures}} from {{Noisy Labels}} in {{Semantic Segmentation}}},
  author = {Luo, Yaoru and Liu, Guole and Guo, Yuanhao and Yang, Ge},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {2},
  pages = {1908--1916},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i2.20085},
  urldate = {2023-06-28},
  abstract = {How deep neural networks (DNNs) learn from noisy labels has been studied extensively in image classification but much less in image segmentation. So far, our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. In this study, we address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We generate extremely noisy labels by randomly sampling a small fraction (e.g., 10\%) or flipping a large fraction (e.g., 90\%) of the ground truth labels. When trained with these noisy labels, DNNs provide largely the same segmentation performance as trained by the original ground truth. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We refer to these hidden structures in labels as meta-structures. When DNNs are trained by labels with different perturbations to the meta-structure, we find consistent degradation in their segmentation performance. In contrast, incorporation of meta-structure information substantially improves performance of an unsupervised segmentation model developed for binary semantic segmentation. We define meta-structures mathematically as spatial density distributions and show both theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Computer Vision (CV)}
}

@article{luoDeepNeuralNetworks2022b,
  title = {Deep {{Neural Networks Learn Meta-Structures}} from {{Noisy Labels}} in {{Semantic Segmentation}}},
  author = {Luo, Yaoru and Liu, Guole and Guo, Yuanhao and Yang, Ge},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {2},
  pages = {1908--1916},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i2.20085},
  urldate = {2023-06-28},
  abstract = {How deep neural networks (DNNs) learn from noisy labels has been studied extensively in image classification but much less in image segmentation. So far, our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. In this study, we address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We generate extremely noisy labels by randomly sampling a small fraction (e.g., 10\%) or flipping a large fraction (e.g., 90\%) of the ground truth labels. When trained with these noisy labels, DNNs provide largely the same segmentation performance as trained by the original ground truth. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We refer to these hidden structures in labels as meta-structures. When DNNs are trained by labels with different perturbations to the meta-structure, we find consistent degradation in their segmentation performance. In contrast, incorporation of meta-structure information substantially improves performance of an unsupervised segmentation model developed for binary semantic segmentation. We define meta-structures mathematically as spatial density distributions and show both theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Computer Vision (CV)},
  file = {C:\Users\cleme\Zotero\storage\IEZ8M3FW\Luo et al. - 2022 - Deep Neural Networks Learn Meta-Structures from No.pdf}
}

@inproceedings{luoDeepNeuralNetworks2022c,
  title = {Deep {{Neural Networks Learn Meta-Structures}} from {{Noisy Labels}} in {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Luo, Yaoru and Liu, Guole and Guo, Yuanhao and Yang, Ge},
  year = {2022},
  month = jun,
  volume = {36},
  pages = {1908--1916},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i2.20085},
  urldate = {2023-06-19},
  abstract = {How deep neural networks (DNNs) learn from noisy labels has been studied extensively in image classification but much less in image segmentation. So far, our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. In this study, we address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We generate extremely noisy labels by randomly sampling a small fraction (e.g., 10\%) or flipping a large fraction (e.g., 90\%) of the ground truth labels. When trained with these noisy labels, DNNs provide largely the same segmentation performance as trained by the original ground truth. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We refer to these hidden structures in labels as meta-structures. When DNNs are trained by labels with different perturbations to the meta-structure, we find consistent degradation in their segmentation performance. In contrast, incorporation of meta-structure information substantially improves performance of an unsupervised segmentation model developed for binary semantic segmentation. We define meta-structures mathematically as spatial density distributions and show both theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.},
  file = {C:\Users\cleme\Zotero\storage\HRXZZK34\Luo et al. - 2022 - Deep Neural Networks Learn Meta-Structures from No.pdf}
}

@article{luoElucidatingMetaStructuresNoisy2022,
  title = {Elucidating {{Meta-Structures}} of {{Noisy Labels}} in {{Semantic Segmentation}} by {{Deep Neural Networks}}},
  author = {Luo, Yaoru and Liu, Guole and Guo, Yuanhao and Yang, Ge},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2205.00160},
  urldate = {2023-06-19},
  abstract = {Supervised training of deep neural networks (DNNs) by noisy labels has been studied extensively in image classification but much less in image segmentation. Our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. We address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We classify segmentation labels according to their noise transition matrices (NTMs) and compare performance of DNNs trained by different types of labels. When we randomly sample a small fraction (e.g., 10\%) or flip a large fraction (e.g., 90\%) of the ground-truth labels to train DNNs, their segmentation performance remains largely unchanged. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We call these hidden structures meta-structures. When labels with different perturbations to the meta-structures are used to train DNNs, their performance in feature extraction and segmentation degrades consistently. In contrast, addition of meta-structure information substantially improves performance of an unsupervised model in binary semantic segmentation. We formulate meta-structures mathematically as spatial density distributions. We show theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{luoElucidatingMetaStructuresNoisy2022a,
  title = {Elucidating {{Meta-Structures}} of {{Noisy Labels}} in {{Semantic Segmentation}} by {{Deep Neural Networks}}},
  author = {Luo, Yaoru and Liu, Guole and Guo, Yuanhao and Yang, Ge},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2205.00160},
  urldate = {2023-06-19},
  abstract = {Supervised training of deep neural networks (DNNs) by noisy labels has been studied extensively in image classification but much less in image segmentation. Our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. We address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We classify segmentation labels according to their noise transition matrices (NTMs) and compare performance of DNNs trained by different types of labels. When we randomly sample a small fraction (e.g., 10\%) or flip a large fraction (e.g., 90\%) of the ground-truth labels to train DNNs, their segmentation performance remains largely unchanged. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We call these hidden structures meta-structures. When labels with different perturbations to the meta-structures are used to train DNNs, their performance in feature extraction and segmentation degrades consistently. In contrast, addition of meta-structure information substantially improves performance of an unsupervised model in binary semantic segmentation. We formulate meta-structures mathematically as spatial density distributions. We show theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {C:\Users\cleme\Zotero\storage\SQZTUC8D\Luo et al. - 2022 - Elucidating Meta-Structures of Noisy Labels in Sem.pdf}
}

@article{luoUnderstandingEffectiveReceptive,
  title = {Understanding the {{Effective Receptive Field}} in {{Deep Convolutional Neural Networks}}},
  author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
  langid = {english}
}

@inproceedings{luoUnderstandingEffectiveReceptive2016,
  title = {Understanding the Effective Receptive Field in Deep Convolutional Neural Networks},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  year = {2016},
  month = dec,
  series = {{{NIPS}}'16},
  pages = {4905--4913},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-02-17},
  abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
  isbn = {978-1-5108-3881-9}
}

@inproceedings{luoUnderstandingEffectiveReceptive2016a,
  title = {Understanding the Effective Receptive Field in Deep Convolutional Neural Networks},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  year = {2016},
  month = dec,
  series = {{{NIPS}}'16},
  pages = {4905--4913},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-02-17},
  abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
  isbn = {978-1-5108-3881-9},
  file = {C:\Users\cleme\Zotero\storage\X8XBC8SG\Luo et al. - 2016 - Understanding the effective receptive field in dee.pdf}
}

@article{luoUnderstandingEffectiveReceptivea,
  title = {Understanding the {{Effective Receptive Field}} in {{Deep Convolutional Neural Networks}}},
  author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
  langid = {english}
}

@article{luoUnderstandingEffectiveReceptiveb,
  title = {Understanding the {{Effective Receptive Field}} in {{Deep Convolutional Neural Networks}}},
  author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\MQQVFI7Z\Luo et al. - Understanding the Effective Receptive Field in Dee.pdf}
}

@article{luoUnderstandingEffectiveReceptivec,
  title = {Understanding the {{Effective Receptive Field}} in {{Deep Convolutional Neural Networks}}},
  author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\D6MRBN9N\Luo et al. - Understanding the Effective Receptive Field in Dee.pdf}
}

@article{madryDeepLearningModels2018,
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  author = {Madry, A. and Makelov, A. and Schmidt, L. and Tsipras, D. and Vladu, A.},
  year = {2018},
  journal = {arXiv},
  urldate = {2023-05-11},
  abstract = {{\copyright} Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved. Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
  copyright = {Creative Commons Attribution-Noncommercial-Share Alike},
  langid = {english}
}

@inproceedings{madryDeepLearningModels2018a,
  title = {Towards {{Deep Learning Models Resistant}} to {{Adversarial Attacks}}},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2018, {{Vancouver}}, {{BC}}, {{Canada}}, {{April}} 30 - {{May}} 3, 2018, {{Conference Track Proceedings}}},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2018},
  publisher = {OpenReview.net},
  urldate = {2023-11-30}
}

@inproceedings{madryDeepLearningModels2018b,
  title = {Towards {{Deep Learning Models Resistant}} to {{Adversarial Attacks}}},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2018, {{Vancouver}}, {{BC}}, {{Canada}}, {{April}} 30 - {{May}} 3, 2018, {{Conference Track Proceedings}}},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2018},
  publisher = {OpenReview.net},
  urldate = {2023-11-30}
}

@article{madryDeepLearningModels2018c,
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  author = {Madry, A. and Makelov, A. and Schmidt, L. and Tsipras, D. and Vladu, A.},
  year = {2018},
  journal = {arXiv},
  urldate = {2023-05-11},
  abstract = {{\copyright} Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved. Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
  copyright = {Creative Commons Attribution-Noncommercial-Share Alike},
  langid = {english},
  annotation = {Accepted: 2021-11-05T14:56:05Z},
  file = {C:\Users\cleme\Zotero\storage\2H3PVSUJ\Madry et al. - 2018 - Towards deep learning models resistant to adversar.pdf}
}

@article{mahmoudDiabeticRetinopathyProgression2022,
  title = {Diabetic {{Retinopathy Progression Prediction Using}} a {{Deep Learning Model}}},
  author = {Mahmoud, Hosni and A, Hanan},
  year = {2022},
  month = nov,
  journal = {Axioms},
  volume = {11},
  number = {11},
  pages = {614},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-1680},
  doi = {10.3390/axioms11110614},
  urldate = {2023-10-05},
  abstract = {Diabetes is an illness that happens with a high level of glucose in the body, and can harm the retina, causing permanent loss vision or diabetic retinopathy. The fundus oculi method comprises detecting the eyes to perform a pathology test. In this research, we implement a method to predict the progress of diabetic retinopathy. There is a research gap that exists for the detection of diabetic retinopathy progression employing deep learning models. Therefore, in this research, we introduce a recurrent CNN (R-CNN) model to detect upcoming visual field inspections to predict diabetic retinopathy progression. A benchmark dataset of 7000 eyes from healthy and diabetic retinopathy progress cases over the years are utilized in this research. Approximately 80\% of ocular cases from the dataset is utilized for the training stage, 10\% of cases are used for validation, and 10\% are used for testing. Six successive visual field tests are used as input and the seventh test is compared with the output of the R-CNN. The precision of the R-CNN is compared with the regression model and the Hidden Markov (HMM) method. The average prediction precision of the R-CNN is considerably greater than both regression and HMM. In the pointwise classification, R-CNN depicts the least classification mean square error among the compared models in most of the tests. Also, R-CNN is found to be the minimum model affected by the deterioration of reliability and diabetic retinopathy severity. Correctly predicting a progressive visual field test with the R-CNN model can aid physicians in making decisions concerning diabetic retinopathy.},
  langid = {english},
  keywords = {deep learning architecture,diabetic retinopathy progression,retinopathy}
}

@article{mahmoudDiabeticRetinopathyProgression2022a,
  title = {Diabetic {{Retinopathy Progression Prediction Using}} a {{Deep Learning Model}}},
  author = {Mahmoud, Hosni and A, Hanan},
  year = {2022},
  month = nov,
  journal = {Axioms},
  volume = {11},
  number = {11},
  pages = {614},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-1680},
  doi = {10.3390/axioms11110614},
  urldate = {2023-10-05},
  abstract = {Diabetes is an illness that happens with a high level of glucose in the body, and can harm the retina, causing permanent loss vision or diabetic retinopathy. The fundus oculi method comprises detecting the eyes to perform a pathology test. In this research, we implement a method to predict the progress of diabetic retinopathy. There is a research gap that exists for the detection of diabetic retinopathy progression employing deep learning models. Therefore, in this research, we introduce a recurrent CNN (R-CNN) model to detect upcoming visual field inspections to predict diabetic retinopathy progression. A benchmark dataset of 7000 eyes from healthy and diabetic retinopathy progress cases over the years are utilized in this research. Approximately 80\% of ocular cases from the dataset is utilized for the training stage, 10\% of cases are used for validation, and 10\% are used for testing. Six successive visual field tests are used as input and the seventh test is compared with the output of the R-CNN. The precision of the R-CNN is compared with the regression model and the Hidden Markov (HMM) method. The average prediction precision of the R-CNN is considerably greater than both regression and HMM. In the pointwise classification, R-CNN depicts the least classification mean square error among the compared models in most of the tests. Also, R-CNN is found to be the minimum model affected by the deterioration of reliability and diabetic retinopathy severity. Correctly predicting a progressive visual field test with the R-CNN model can aid physicians in making decisions concerning diabetic retinopathy.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning architecture,diabetic retinopathy progression,retinopathy},
  file = {C:\Users\cleme\Zotero\storage\YJW22A2C\Mahmoud et A - 2022 - Diabetic Retinopathy Progression Prediction Using .pdf}
}

@misc{maier-heinMetricsReloadedPitfalls2022,
  title = {Metrics Reloaded: {{Pitfalls}} and Recommendations for Image Analysis Validation},
  shorttitle = {Metrics Reloaded},
  author = {{Maier-Hein}, Lena and Reinke, Annika and Godau, Patrick and Tizabi, Minu D. and B{\"u}ttner, Florian and Christodoulou, Evangelia and Glocker, Ben and Isensee, Fabian and Kleesiek, Jens and Kozubek, Michal and Reyes, Mauricio and Riegler, Michael A. and Wiesenfarth, Manuel and Kavur, Emre and Sudre, Carole H. and Baumgartner, Michael and Eisenmann, Matthias and {Heckmann-N{\"o}tzel}, Doreen and R{\"a}dsch, A. Tim and Acion, Laura and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Benis, Arriel and Blaschko, Matthew and Cardoso, M. Jorge and Cheplygina, Veronika and Cimini, Beth A. and Collins, Gary S. and Farahani, Keyvan and Ferrer, Luciana and Galdran, Adrian and {van Ginneken}, Bram and Haase, Robert and Hashimoto, Daniel A. and Hoffman, Michael M. and Huisman, Merel and Jannin, Pierre and Kahn, Charles E. and Kainmueller, Dagmar and Kainz, Bernhard and Karargyris, Alexandros and Karthikesalingam, Alan and Kenngott, Hannes and Kofler, Florian and {Kopp-Schneider}, Annette and Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A. and Litjens, Geert and Madani, Amin and {Maier-Hein}, Klaus and Martel, Anne L. and Mattson, Peter and Meijering, Erik and Menze, Bjoern and Moons, Karel G. M. and M{\"u}ller, Henning and Nichyporuk, Brennan and Nickel, Felix and Petersen, Jens and Rajpoot, Nasir and Rieke, Nicola and {Saez-Rodriguez}, Julio and S{\'a}nchez, Clara I. and Shetty, Shravya and {van Smeden}, Maarten and Summers, Ronald M. and Taha, Abdel A. and Tiulpin, Aleksei and Tsaftaris, Sotirios A. and Van Calster, Ben and Varoquaux, Ga{\"e}l and J{\"a}ger, Paul F.},
  year = {2022},
  month = jun,
  doi = {10.48550/arXiv.2206.01653},
  urldate = {2023-04-23},
  abstract = {Increasing evidence shows that flaws in machine learning (ML) algorithm validation are an underestimated global problem. Particularly in automatic biomedical image analysis, chosen performance metrics often do not reflect the domain interest, thus failing to adequately measure scientific progress and hindering translation of ML techniques into practice. To overcome this, our large international expert consortium created Metrics Reloaded, a comprehensive framework guiding researchers in the problem-aware selection of metrics. Following the convergence of ML methodology across application domains, Metrics Reloaded fosters the convergence of validation methodology. The framework was developed in a multi-stage Delphi process and is based on the novel concept of a problem fingerprint - a structured representation of the given problem that captures all aspects that are relevant for metric selection, from the domain interest to the properties of the target structure(s), data set and algorithm output. Based on the problem fingerprint, users are guided through the process of choosing and applying appropriate validation metrics while being made aware of potential pitfalls. Metrics Reloaded targets image analysis problems that can be interpreted as a classification task at image, object or pixel level, namely image-level classification, object detection, semantic segmentation, and instance segmentation tasks. To improve the user experience, we implemented the framework in the Metrics Reloaded online tool, which also provides a point of access to explore weaknesses, strengths and specific recommendations for the most common validation metrics. The broad applicability of our framework across domains is demonstrated by an instantiation for various biological and medical image analysis use cases.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{maier-heinMetricsReloadedPitfalls2022a,
  title = {Metrics Reloaded: {{Pitfalls}} and Recommendations for Image Analysis Validation},
  shorttitle = {Metrics Reloaded},
  author = {{Maier-Hein}, Lena and Reinke, Annika and Godau, Patrick and Tizabi, Minu D. and B{\"u}ttner, Florian and Christodoulou, Evangelia and Glocker, Ben and Isensee, Fabian and Kleesiek, Jens and Kozubek, Michal and Reyes, Mauricio and Riegler, Michael A. and Wiesenfarth, Manuel and Kavur, Emre and Sudre, Carole H. and Baumgartner, Michael and Eisenmann, Matthias and {Heckmann-N{\"o}tzel}, Doreen and R{\"a}dsch, A. Tim and Acion, Laura and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Benis, Arriel and Blaschko, Matthew and Cardoso, M. Jorge and Cheplygina, Veronika and Cimini, Beth A. and Collins, Gary S. and Farahani, Keyvan and Ferrer, Luciana and Galdran, Adrian and {van Ginneken}, Bram and Haase, Robert and Hashimoto, Daniel A. and Hoffman, Michael M. and Huisman, Merel and Jannin, Pierre and Kahn, Charles E. and Kainmueller, Dagmar and Kainz, Bernhard and Karargyris, Alexandros and Karthikesalingam, Alan and Kenngott, Hannes and Kofler, Florian and {Kopp-Schneider}, Annette and Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A. and Litjens, Geert and Madani, Amin and {Maier-Hein}, Klaus and Martel, Anne L. and Mattson, Peter and Meijering, Erik and Menze, Bjoern and Moons, Karel G. M. and M{\"u}ller, Henning and Nichyporuk, Brennan and Nickel, Felix and Petersen, Jens and Rajpoot, Nasir and Rieke, Nicola and {Saez-Rodriguez}, Julio and S{\'a}nchez, Clara I. and Shetty, Shravya and {van Smeden}, Maarten and Summers, Ronald M. and Taha, Abdel A. and Tiulpin, Aleksei and Tsaftaris, Sotirios A. and Van Calster, Ben and Varoquaux, Ga{\"e}l and J{\"a}ger, Paul F.},
  year = {2022},
  month = jun,
  journal = {arXiv e-prints},
  doi = {10.48550/arXiv.2206.01653},
  urldate = {2023-04-23},
  abstract = {Increasing evidence shows that flaws in machine learning (ML) algorithm validation are an underestimated global problem. Particularly in automatic biomedical image analysis, chosen performance metrics often do not reflect the domain interest, thus failing to adequately measure scientific progress and hindering translation of ML techniques into practice. To overcome this, our large international expert consortium created Metrics Reloaded, a comprehensive framework guiding researchers in the problem-aware selection of metrics. Following the convergence of ML methodology across application domains, Metrics Reloaded fosters the convergence of validation methodology. The framework was developed in a multi-stage Delphi process and is based on the novel concept of a problem fingerprint - a structured representation of the given problem that captures all aspects that are relevant for metric selection, from the domain interest to the properties of the target structure(s), data set and algorithm output. Based on the problem fingerprint, users are guided through the process of choosing and applying appropriate validation metrics while being made aware of potential pitfalls. Metrics Reloaded targets image analysis problems that can be interpreted as a classification task at image, object or pixel level, namely image-level classification, object detection, semantic segmentation, and instance segmentation tasks. To improve the user experience, we implemented the framework in the Metrics Reloaded online tool, which also provides a point of access to explore weaknesses, strengths and specific recommendations for the most common validation metrics. The broad applicability of our framework across domains is demonstrated by an instantiation for various biological and medical image analysis use cases.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ADS Bibcode: 2022arXiv220601653M},
  file = {C:\Users\cleme\Zotero\storage\ERLKIIUZ\Maier-Hein et al. - 2022 - Metrics reloaded Pitfalls and recommendations for.pdf}
}

@article{maintzSurveyMedicalImage1998,
  title = {A Survey of Medical Image Registration},
  author = {Maintz, J. B. Antoine and Viergever, Max A.},
  year = {1998},
  month = mar,
  journal = {Medical Image Analysis},
  volume = {2},
  number = {1},
  pages = {1--36},
  issn = {1361-8415},
  doi = {10.1016/S1361-8415(01)80026-8},
  urldate = {2019-11-15},
  abstract = {The purpose of this paper is to present a survey of recent (published in 1993 or later) publications concerning medical image registration techniques. These publications will be classified according to a model based on nine salient criteria, the main dichotomy of which is extrinsic versus intrinsic methods. The statistics of the classification show definite trends in the evolving registration techniques, which will be discussed. At this moment, the bulk of interesting intrinsic methods is based on either segmented points or surfaces, or on techniques endeavouring to use the full information content of the images involved.},
  langid = {english},
  keywords = {matching,registration},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\KIZWX6KX\\Maintz et Viergever - 1998 - A survey of medical image registration.pdf;C\:\\Users\\cleme\\Zotero\\storage\\97ZVIMYB\\S1361841501800268.html}
}

@inproceedings{makhzaniAdversarialAutoencoders2016,
  title = {Adversarial {{Autoencoders}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian},
  year = {2016},
  urldate = {2019-06-13}
}

@inproceedings{makhzaniAdversarialAutoencoders2016a,
  title = {Adversarial {{Autoencoders}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian},
  year = {2016},
  urldate = {2019-06-13}
}

@article{malocaUnravelingDeepLearning2021,
  title = {Unraveling the Deep Learning Gearbox in Optical Coherence Tomography Image Segmentation towards Explainable Artificial Intelligence},
  author = {Maloca, Peter M. and M{\"u}ller, Philipp L. and Lee, Aaron Y. and Tufail, Adnan and Balaskas, Konstantinos and Niklaus, Stephanie and Kaiser, Pascal and Suter, Susanne and {Zarranz-Ventura}, Javier and Egan, Catherine and Scholl, Hendrik P. N. and Schnitzer, Tobias K. and Singer, Thomas and Hasler, Pascal W. and Denk, Nora},
  year = {2021},
  month = feb,
  journal = {Communications Biology},
  volume = {4},
  number = {1},
  pages = {1--12},
  publisher = {Nature Publishing Group},
  issn = {2399-3642},
  doi = {10.1038/s42003-021-01697-y},
  urldate = {2021-11-11},
  abstract = {Machine learning has greatly facilitated the analysis of medical data, while the internal operations usually remain intransparent. To better comprehend these opaque procedures, a convolutional neural network for optical coherence tomography image segmentation was enhanced with a Traceable Relevance Explainability (T-REX) technique. The proposed application was based on three components: ground truth generation by multiple graders, calculation of Hamming distances among graders and the machine learning algorithm, as well as a smart data visualization (`neural recording'). An overall average variability of 1.75\% between the human graders and the algorithm was found, slightly minor to 2.02\% among human graders. The ambiguity in ground truth had noteworthy impact on machine learning results, which could be visualized. The convolutional neural network balanced between graders and allowed for modifiable predictions dependent on the compartment. Using the proposed T-REX setup, machine learning processes could be rendered more transparent and understandable, possibly leading to optimized applications.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Medical research,Molecular medicine}
}

@article{malocaUnravelingDeepLearning2021a,
  title = {Unraveling the Deep Learning Gearbox in Optical Coherence Tomography Image Segmentation towards Explainable Artificial Intelligence},
  author = {Maloca, Peter M. and M{\"u}ller, Philipp L. and Lee, Aaron Y. and Tufail, Adnan and Balaskas, Konstantinos and Niklaus, Stephanie and Kaiser, Pascal and Suter, Susanne and {Zarranz-Ventura}, Javier and Egan, Catherine and Scholl, Hendrik P. N. and Schnitzer, Tobias K. and Singer, Thomas and Hasler, Pascal W. and Denk, Nora},
  year = {2021},
  month = feb,
  journal = {Communications Biology},
  volume = {4},
  number = {1},
  pages = {1--12},
  publisher = {Nature Publishing Group},
  issn = {2399-3642},
  doi = {10.1038/s42003-021-01697-y},
  urldate = {2021-11-11},
  abstract = {Machine learning has greatly facilitated the analysis of medical data, while the internal operations usually remain intransparent. To better comprehend these opaque procedures, a convolutional neural network for optical coherence tomography image segmentation was enhanced with a Traceable Relevance Explainability (T-REX) technique. The proposed application was based on three components: ground truth generation by multiple graders, calculation of Hamming distances among graders and the machine learning algorithm, as well as a smart data visualization (`neural recording'). An overall average variability of 1.75\% between the human graders and the algorithm was found, slightly minor to 2.02\% among human graders. The ambiguity in ground truth had noteworthy impact on machine learning results, which could be visualized. The convolutional neural network balanced between graders and allowed for modifiable predictions dependent on the compartment. Using the proposed T-REX setup, machine learning processes could be rendered more transparent and understandable, possibly leading to optimized applications.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Medical research,Molecular medicine},
  annotation = {Bandiera\_abtest: a\\
Cc\_license\_type: cc\_by\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Medical research;Molecular medicine\\
Subject\_term\_id: medical-research;molecular-medicine},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\GUYMVQB7\\Maloca et al. - 2021 - Unraveling the deep learning gearbox in optical co.pdf;C\:\\Users\\cleme\\Zotero\\storage\\WHAK4Q6A\\s42003-021-01697-y.html}
}

@article{mao2023tjdr,
  title = {{{TJDR}}: A High-Quality Diabetic Retinopathy Pixel-Level Annotation Dataset},
  author = {Mao, Jingxin and Ma, Xiaoyu and Bi, Yanlong and Zhang, Rongqing},
  year = {2023},
  journal = {arXiv preprint arXiv:2312.15389},
  eprint = {2312.15389},
  archiveprefix = {arXiv}
}

@article{maoMedGCNGraphConvolutional2019,
  title = {{{MedGCN}}: {{Graph Convolutional Networks}} for {{Multiple Medical Tasks}}},
  shorttitle = {{{MedGCN}}},
  author = {Mao, Chengsheng and Yao, Liang and Luo, Yuan},
  year = {2019},
  month = mar,
  journal = {arXiv:1904.00326 [cs, stat]},
  eprint = {1904.00326},
  primaryclass = {cs, stat},
  urldate = {2019-10-28},
  abstract = {Laboratory testing and medication prescription are two of the most important routines in daily clinical practice. Developing an artificial intelligence system that can automatically make lab test imputations and medication recommendations can save cost on potentially redundant lab tests and inform physicians in more effective prescription. We present an intelligent model that can automatically recommend the patients' medications based on their incomplete lab tests, and can even accurately estimate the lab values that have not been taken. We model the complex relations between multiple types of medical entities with their inherent features in a heterogeneous graph. Then we learn a distributed representation for each entity in the graph based on graph convolutional networks to make the representations integrate information from multiple types of entities. Since the entity representations incorporate multiple types of medical information, they can be used for multiple medical tasks. In our experiments, we construct a graph to associate patients, encounters, lab tests and medications, and conduct the two tasks: medication recommendation and lab test imputation. The experimental results demonstrate that our model can outperform the state-of-the-art models in both tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{maoMedGCNGraphConvolutional2019a,
  title = {{{MedGCN}}: {{Graph Convolutional Networks}} for {{Multiple Medical Tasks}}},
  shorttitle = {{{MedGCN}}},
  author = {Mao, Chengsheng and Yao, Liang and Luo, Yuan},
  year = {2019},
  month = mar,
  journal = {arXiv:1904.00326 [cs, stat]},
  eprint = {1904.00326},
  primaryclass = {cs, stat},
  urldate = {2019-10-28},
  abstract = {Laboratory testing and medication prescription are two of the most important routines in daily clinical practice. Developing an artificial intelligence system that can automatically make lab test imputations and medication recommendations can save cost on potentially redundant lab tests and inform physicians in more effective prescription. We present an intelligent model that can automatically recommend the patients' medications based on their incomplete lab tests, and can even accurately estimate the lab values that have not been taken. We model the complex relations between multiple types of medical entities with their inherent features in a heterogeneous graph. Then we learn a distributed representation for each entity in the graph based on graph convolutional networks to make the representations integrate information from multiple types of entities. Since the entity representations incorporate multiple types of medical information, they can be used for multiple medical tasks. In our experiments, we construct a graph to associate patients, encounters, lab tests and medications, and conduct the two tasks: medication recommendation and lab test imputation. The experimental results demonstrate that our model can outperform the state-of-the-art models in both tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9CAWUMJV\\Mao et al. - 2019 - MedGCN Graph Convolutional Networks for Multiple .pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZGHHGH99\\1904.html}
}

@article{marcinkevicsInterpretableExplainableMachine,
  title = {Interpretable and Explainable Machine Learning: {{A}} Methods-Centric Overview with Concrete Examples},
  shorttitle = {Interpretable and Explainable Machine Learning},
  author = {Marcinkevi{\v c}s, Ri{\v c}ards and Vogt, Julia E.},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {n/a},
  number = {n/a},
  pages = {e1493},
  issn = {1942-4795},
  doi = {10.1002/widm.1493},
  urldate = {2023-05-05},
  abstract = {Interpretability and explainability are crucial for machine learning (ML) and statistical applications in medicine, economics, law, and natural sciences and form an essential principle for ML model design and development. Although interpretability and explainability have escaped a precise and universal definition, many models and techniques motivated by these properties have been developed over the last 30 years, with the focus currently shifting toward deep learning. We will consider concrete examples of state-of-the-art, including specially tailored rule-based, sparse, and additive classification models, interpretable representation learning, and methods for explaining black-box models post hoc. The discussion will emphasize the need for and relevance of interpretability and explainability, the divide between them, and the inductive biases behind the presented ``zoo'' of interpretable models and explanation methods. This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Explainable AI Technologies {$>$} Machine Learning Commercial, Legal, and Ethical Issues {$>$} Social Considerations},
  langid = {english},
  keywords = {explainability,interpretability,machine learning,neural networks}
}

@article{marcinkevicsInterpretableExplainableMachinea,
  title = {Interpretable and Explainable Machine Learning: {{A}} Methods-Centric Overview with Concrete Examples},
  shorttitle = {Interpretable and Explainable Machine Learning},
  author = {Marcinkevi{\v c}s, Ri{\v c}ards and Vogt, Julia E.},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {n/a},
  number = {n/a},
  pages = {e1493},
  issn = {1942-4795},
  doi = {10.1002/widm.1493},
  urldate = {2023-05-05},
  abstract = {Interpretability and explainability are crucial for machine learning (ML) and statistical applications in medicine, economics, law, and natural sciences and form an essential principle for ML model design and development. Although interpretability and explainability have escaped a precise and universal definition, many models and techniques motivated by these properties have been developed over the last 30 years, with the focus currently shifting toward deep learning. We will consider concrete examples of state-of-the-art, including specially tailored rule-based, sparse, and additive classification models, interpretable representation learning, and methods for explaining black-box models post hoc. The discussion will emphasize the need for and relevance of interpretability and explainability, the divide between them, and the inductive biases behind the presented ``zoo'' of interpretable models and explanation methods. This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Explainable AI Technologies {$>$} Machine Learning Commercial, Legal, and Ethical Issues {$>$} Social Considerations},
  langid = {english},
  keywords = {explainability,interpretability,machine learning,neural networks},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9BGLN694\\Marcinkevičs et Vogt - Interpretable and explainable machine learning A .pdf;C\:\\Users\\cleme\\Zotero\\storage\\A3TM8U62\\widm.html}
}

@article{maROSERetinalOCTAngiography2021,
  title = {{{ROSE}}: {{A Retinal OCT-Angiography Vessel Segmentation Dataset}} and {{New Model}}},
  shorttitle = {{{ROSE}}},
  author = {Ma, Yuhui and Hao, Huaying and Xie, Jianyang and Fu, Huazhu and Zhang, Jiong and Yang, Jianlong and Wang, Zhen and Liu, Jiang and Zheng, Yalin and Zhao, Yitian},
  year = {2021},
  month = mar,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {40},
  number = {3},
  pages = {928--939},
  issn = {1558-254X},
  doi = {10.1109/TMI.2020.3042802},
  urldate = {2023-10-05},
  abstract = {Optical Coherence Tomography Angiography (OCTA) is a non-invasive imaging technique that has been increasingly used to image the retinal vasculature at capillary level resolution. However, automated segmentation of retinal vessels in OCTA has been under-studied due to various challenges such as low capillary visibility and high vessel complexity, despite its significance in understanding many vision-related diseases. In addition, there is no publicly available OCTA dataset with manually graded vessels for training and validation of segmentation algorithms. To address these issues, for the first time in the field of retinal image analysis we construct a dedicated Retinal OCTA SEgmentation dataset (ROSE), which consists of 229 OCTA images with vessel annotations at either centerline-level or pixel level. This dataset with the source code has been released for public access to assist researchers in the community in undertaking research in related topics. Secondly, we introduce a novel split-based coarse-to-fine vessel segmentation network for OCTA images (OCTA-Net), with the ability to detect thick and thin vessels separately. In the OCTA-Net, a split-based coarse segmentation module is first utilized to produce a preliminary confidence map of vessels, and a split-based refined segmentation module is then used to optimize the shape/contour of the retinal microvasculature. We perform a thorough evaluation of the state-of-the-art vessel segmentation models and our OCTA-Net on the constructed ROSE dataset. The experimental results demonstrate that our OCTA-Net yields better vessel segmentation performance in OCTA than both traditional and other deep learning methods. In addition, we provide a fractal dimension analysis on the segmented microvasculature, and the statistical analysis demonstrates significant differences between the healthy control and Alzheimer's Disease group. This consolidates that the analysis of retinal microvasculature may offer a new scheme to study various neurodegenerative diseases.}
}

@article{maROSERetinalOCTAngiography2021a,
  title = {{{ROSE}}: {{A Retinal OCT-Angiography Vessel Segmentation Dataset}} and {{New Model}}},
  shorttitle = {{{ROSE}}},
  author = {Ma, Yuhui and Hao, Huaying and Xie, Jianyang and Fu, Huazhu and Zhang, Jiong and Yang, Jianlong and Wang, Zhen and Liu, Jiang and Zheng, Yalin and Zhao, Yitian},
  year = {2021},
  month = mar,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {40},
  number = {3},
  pages = {928--939},
  issn = {1558-254X},
  doi = {10.1109/TMI.2020.3042802},
  urldate = {2023-10-05},
  abstract = {Optical Coherence Tomography Angiography (OCTA) is a non-invasive imaging technique that has been increasingly used to image the retinal vasculature at capillary level resolution. However, automated segmentation of retinal vessels in OCTA has been under-studied due to various challenges such as low capillary visibility and high vessel complexity, despite its significance in understanding many vision-related diseases. In addition, there is no publicly available OCTA dataset with manually graded vessels for training and validation of segmentation algorithms. To address these issues, for the first time in the field of retinal image analysis we construct a dedicated Retinal OCTA SEgmentation dataset (ROSE), which consists of 229 OCTA images with vessel annotations at either centerline-level or pixel level. This dataset with the source code has been released for public access to assist researchers in the community in undertaking research in related topics. Secondly, we introduce a novel split-based coarse-to-fine vessel segmentation network for OCTA images (OCTA-Net), with the ability to detect thick and thin vessels separately. In the OCTA-Net, a split-based coarse segmentation module is first utilized to produce a preliminary confidence map of vessels, and a split-based refined segmentation module is then used to optimize the shape/contour of the retinal microvasculature. We perform a thorough evaluation of the state-of-the-art vessel segmentation models and our OCTA-Net on the constructed ROSE dataset. The experimental results demonstrate that our OCTA-Net yields better vessel segmentation performance in OCTA than both traditional and other deep learning methods. In addition, we provide a fractal dimension analysis on the segmented microvasculature, and the statistical analysis demonstrates significant differences between the healthy control and Alzheimer's Disease group. This consolidates that the analysis of retinal microvasculature may offer a new scheme to study various neurodegenerative diseases.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\48WHXDTZ\\Ma et al. - 2021 - ROSE A Retinal OCT-Angiography Vessel Segmentatio.pdf;C\:\\Users\\cleme\\Zotero\\storage\\QGPAJ8NU\\9284503.html}
}

@inproceedings{masciStackedConvolutionalAutoEncoders2011,
  title = {Stacked {{Convolutional Auto-Encoders}} for {{Hierarchical Feature Extraction}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2011},
  author = {Masci, Jonathan and Meier, Ueli and Cire{\c s}an, Dan and Schmidhuber, J{\"u}rgen},
  editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {52--59},
  publisher = {Springer Berlin Heidelberg},
  abstract = {We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.},
  isbn = {978-3-642-21735-7},
  langid = {english},
  keywords = {auto-encoder,classification,convolutional neural network,unsupervised learning}
}

@inproceedings{masciStackedConvolutionalAutoEncoders2011a,
  title = {Stacked {{Convolutional Auto-Encoders}} for {{Hierarchical Feature Extraction}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2011},
  author = {Masci, Jonathan and Meier, Ueli and Cire{\c s}an, Dan and Schmidhuber, J{\"u}rgen},
  editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {52--59},
  publisher = {Springer Berlin Heidelberg},
  abstract = {We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.},
  isbn = {978-3-642-21735-7},
  langid = {english},
  keywords = {auto-encoder,classification,convolutional neural network,unsupervised learning},
  file = {C:\Users\cleme\Zotero\storage\MSAAZ9UM\Masci et al. - 2011 - Stacked Convolutional Auto-Encoders for Hierarchic.pdf}
}

@inproceedings{matsoukasItTimeReplace2021,
  title = {Is It {{Time}} to {{Replace CNNs}} with {{Transformers}} for {{Medical Images}}?},
  booktitle = {{{ICCV}}: {{Workshop}} on {{Computer Vision}} for {{Automated Medical Diagnosis}}},
  author = {Matsoukas, Christos and Haslum, Johan Fredin and S{\"o}derberg, Magnus and Smith, Kevin},
  year = {2021},
  volume = {abs/2108.09038},
  eprint = {2108.09038},
  urldate = {2021-11-08},
  archiveprefix = {arXiv}
}

@inproceedings{matsoukasItTimeReplace2021a,
  title = {Is It {{Time}} to {{Replace CNNs}} with {{Transformers}} for {{Medical Images}}?},
  booktitle = {{{ICCV}}: {{Workshop}} on {{Computer Vision}} for {{Automated Medical Diagnosis}}},
  author = {Matsoukas, Christos and Haslum, Johan Fredin and S{\"o}derberg, Magnus and Smith, Kevin},
  year = {2021},
  volume = {abs/2108.09038},
  eprint = {2108.09038},
  urldate = {2021-11-08},
  archiveprefix = {arXiv}
}

@inproceedings{meenumohanFastMethodRetinal2022,
  title = {A {{Fast Method}} for {{Retinal Disease Classification}} from {{OCT Images Using Depthwise Separable Convolution}}},
  booktitle = {Artificial {{Intelligence}} and {{Technologies}}},
  author = {Meenu Mohan, S. and Aji, S.},
  editor = {Raje, Rajeev R. and Hussain, Farookh and Kannan, R. Jagadeesh},
  year = {2022},
  series = {Lecture {{Notes}} in {{Electrical Engineering}}},
  pages = {153--163},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-16-6448-9_18},
  abstract = {Retinal diseases are the commonest reason for losing eye sight at an early age. The retina is the most important part of forming visual images, which is a thin layer of tissue situated at the backside wall of the eye. These diseases can be identified accurately from SD-OCT scanned images. OCT is a non-contact imaging technology that is used to detect abnormalities among retinal layers. Currently, the classification performed by an ophthalmologist is relatively time consuming. Among the several retinal OCT classification techniques, a recent advancement is deep learning techniques for retinal image classification. Among the deep learning models used is the convolutional neural network (CNN), which can instinctively learn a series of hierarchical features. Therefore, the proposed method uses a variety of standard spatial convolution, which is depthwise separable convolution (DSC). DSC reduces the complexity of the network by reducing trainable parameters and by reducing time and space complexity. The method was tested on the public dataset OCT2017. Experimental results show the proposed method gives a promising result with better execution time compared to the standard CNN.},
  isbn = {9789811664489},
  langid = {english},
  keywords = {Depthwise separable convolution,Retinal diseases classification,Spectral domain-optical coherence tomography}
}

@article{melinscakAnnotatedRetinalOptical2021,
  title = {Annotated Retinal Optical Coherence Tomography Images ({{AROI}}) Database for Joint Retinal Layer and Fluid Segmentation},
  author = {Melin{\v s}{\v c}ak, Martina and Radmilovi{\'c}, Marin and Vatavuk, Zoran and Lon{\v c}ari{\'c}, Sven},
  year = {2021},
  month = oct,
  journal = {Automatika},
  volume = {62},
  number = {3-4},
  pages = {375--385},
  publisher = {Taylor \& Francis},
  issn = {0005-1144},
  doi = {10.1080/00051144.2021.1973298},
  urldate = {2022-06-22},
  abstract = {Optical coherence tomography (OCT) images of the retina provide a structural representation and give an insight into the pathological changes present in age-related macular degeneration (AMD). Due to the three-dimensionality and complexity of the images, manual analysis of pathological features is difficult, time-consuming, and prone to subjectivity. Computer analysis of 3D OCT images is necessary to enable automated quantitative measuring of the features, objectively and repeatedly. As supervised and semi-supervised learning-based automatic segmentation depends on the training data and quality of annotations, we have created a new database of annotated retinal OCT images -- the AROI database. It consists of 1136 images with annotations for pathological changes (fluid accumulation and related findings) and basic structures (layers) in patients with AMD. Inter- and intra-observer errors have been calculated in order to enable the validation of developed algorithms in relation to human variability. Also, we have performed the automatic segmentation with standard U-net architecture and two state-of-the-art architectures for medical image segmentation to set a baseline for further algorithm development and to get insight into challenges for automatic segmentation. To facilitate and encourage further research in the field, we have made the AROI database openly available.},
  keywords = {age-related macular degeneration,Annotated retinal OCT images,automatic image segmentation,deep learning,images database}
}

@article{melinscakAnnotatedRetinalOptical2021a,
  title = {Annotated Retinal Optical Coherence Tomography Images ({{AROI}}) Database for Joint Retinal Layer and Fluid Segmentation},
  author = {Melin{\v s}{\v c}ak, Martina and Radmilovi{\'c}, Marin and Vatavuk, Zoran and Lon{\v c}ari{\'c}, Sven},
  year = {2021},
  month = oct,
  journal = {Automatika},
  volume = {62},
  number = {3-4},
  pages = {375--385},
  publisher = {Taylor \& Francis},
  issn = {0005-1144},
  doi = {10.1080/00051144.2021.1973298},
  urldate = {2022-06-22},
  abstract = {Optical coherence tomography (OCT) images of the retina provide a structural representation and give an insight into the pathological changes present in age-related macular degeneration (AMD). Due to the three-dimensionality and complexity of the images, manual analysis of pathological features is difficult, time-consuming, and prone to subjectivity. Computer analysis of 3D OCT images is necessary to enable automated quantitative measuring of the features, objectively and repeatedly. As supervised and semi-supervised learning-based automatic segmentation depends on the training data and quality of annotations, we have created a new database of annotated retinal OCT images -- the AROI database. It consists of 1136 images with annotations for pathological changes (fluid accumulation and related findings) and basic structures (layers) in patients with AMD. Inter- and intra-observer errors have been calculated in order to enable the validation of developed algorithms in relation to human variability. Also, we have performed the automatic segmentation with standard U-net architecture and two state-of-the-art architectures for medical image segmentation to set a baseline for further algorithm development and to get insight into challenges for automatic segmentation. To facilitate and encourage further research in the field, we have made the AROI database openly available.},
  keywords = {age-related macular degeneration,Annotated retinal OCT images,automatic image segmentation,deep learning,images database},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\KKTTPR4N\\Melinščak et al. - 2021 - Annotated retinal optical coherence tomography ima.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6NRB9WDK\\00051144.2021.html}
}

@article{messinaSurveyDeepLearning2022,
  title = {A {{Survey}} on {{Deep Learning}} and {{Explainability}} for {{Automatic Report Generation}} from {{Medical Images}}},
  author = {Messina, Pablo and Pino, Pablo and Parra, Denis and Soto, Alvaro and Besa, Cecilia and Uribe, Sergio and And{\'i}a, Marcelo and Tejos, Cristian and Prieto, Claudia and Capurro, Daniel},
  year = {2022},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {10s},
  pages = {203:1--203:40},
  issn = {0360-0300},
  doi = {10.1145/3522747},
  urldate = {2023-05-04},
  abstract = {Every year physicians face an increasing demand of image-based diagnosis from patients, a problem that can be addressed with recent artificial intelligence methods. In this context, we survey works in the area of automatic report generation from medical images, with emphasis on methods using deep neural networks, with respect to (1) Datasets, (2) Architecture Design, (3) Explainability, and (4) Evaluation Metrics. Our survey identifies interesting developments but also remaining challenges. Among them, the current evaluation of generated reports is especially weak, since it mostly relies on traditional Natural Language Processing (NLP) metrics, which do not accurately capture medical correctness.},
  keywords = {deep learning,explainable artificial intelligence,medical image captioning,medical images,Medical report generation,natural language report},
  file = {C:\Users\cleme\Zotero\storage\2AR9W5V4\Messina et al. - 2022 - A Survey on Deep Learning and Explainability for A.pdf}
}

@article{micheliNeuralNetworkGraphs2009,
  title = {Neural {{Network}} for {{Graphs}}: {{A Contextual Constructive Approach}}},
  shorttitle = {Neural {{Network}} for {{Graphs}}},
  author = {Micheli, Alessio},
  year = {2009},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {3},
  pages = {498--511},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2010350},
  abstract = {This paper presents a new approach for learning in structured domains (SDs) using a constructive neural network for graphs (NN4G). The new model allows the extension of the input domain for supervised neural networks to a general class of graphs including both acyclic/cyclic, directed/undirected labeled graphs. In particular, the model can realize adaptive contextual transductions, learning the mapping from graphs for both classification and regression tasks. In contrast to previous neural networks for structures that had a recursive dynamics, NN4G is based on a constructive feedforward architecture with state variables that uses neurons with no feedback connections. The neurons are applied to the input graphs by a general traversal process that relaxes the constraints of previous approaches derived by the causality assumption over hierarchical input data. Moreover, the incremental approach eliminates the need to introduce cyclic dependencies in the definition of the system state variables. In the traversal process, the NN4G units exploit (local) contextual information of the graphs vertices. In spite of the simplicity of the approach, we show that, through the compositionality of the contextual information developed by the learning, the model can deal with contextual information that is incrementally extended according to the graphs topology. The effectiveness and the generality of the new approach are investigated by analyzing its theoretical properties and providing experimental results.},
  keywords = {Cascade correlation,constructive feedforward architecture,Context modeling,contextual transductions,Data structures,Feedforward neural networks,graph patterns,graph theory,graphs topology,input graphs,Kernel,learning (artificial intelligence),learning in structured domains (SDs),Machine learning,mathematics computing,neural nets,neural network for graphs,Neural networks,neural networks for structured data,Neurofeedback,Neurons,NN4G,Recurrent neural networks,recursive neural networks,State feedback,supervised neural networks}
}

@article{micheliNeuralNetworkGraphs2009a,
  title = {Neural {{Network}} for {{Graphs}}: {{A Contextual Constructive Approach}}},
  shorttitle = {Neural {{Network}} for {{Graphs}}},
  author = {Micheli, Alessio},
  year = {2009},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {3},
  pages = {498--511},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2010350},
  abstract = {This paper presents a new approach for learning in structured domains (SDs) using a constructive neural network for graphs (NN4G). The new model allows the extension of the input domain for supervised neural networks to a general class of graphs including both acyclic/cyclic, directed/undirected labeled graphs. In particular, the model can realize adaptive contextual transductions, learning the mapping from graphs for both classification and regression tasks. In contrast to previous neural networks for structures that had a recursive dynamics, NN4G is based on a constructive feedforward architecture with state variables that uses neurons with no feedback connections. The neurons are applied to the input graphs by a general traversal process that relaxes the constraints of previous approaches derived by the causality assumption over hierarchical input data. Moreover, the incremental approach eliminates the need to introduce cyclic dependencies in the definition of the system state variables. In the traversal process, the NN4G units exploit (local) contextual information of the graphs vertices. In spite of the simplicity of the approach, we show that, through the compositionality of the contextual information developed by the learning, the model can deal with contextual information that is incrementally extended according to the graphs topology. The effectiveness and the generality of the new approach are investigated by analyzing its theoretical properties and providing experimental results.},
  keywords = {Cascade correlation,constructive feedforward architecture,Context modeling,contextual transductions,Data structures,Feedforward neural networks,graph patterns,graph theory,graphs topology,input graphs,Kernel,learning (artificial intelligence),learning in structured domains (SDs),Machine learning,mathematics computing,neural nets,neural network for graphs,Neural networks,neural networks for structured data,Neurofeedback,Neurons,NN4G,Recurrent neural networks,recursive neural networks,State feedback,supervised neural networks},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HV37Y44B\\Micheli - 2009 - Neural Network for Graphs A Contextual Constructi.pdf;C\:\\Users\\cleme\\Zotero\\storage\\L2VL4QXI\\4773279.html}
}

@article{midenaComparison50Handheld2023,
  title = {Comparison of 50{$^\circ$} Handheld Fundus Camera versus Ultra-Widefield Table-Top Fundus Camera for Diabetic Retinopathy Detection and Grading},
  author = {Midena, Edoardo and Zennaro, Luca and Lapo, Cristian and Torresin, Tommaso and Midena, Giulia and Frizziero, Luisa},
  year = {2023},
  month = oct,
  journal = {Eye},
  volume = {37},
  number = {14},
  pages = {2994--2999},
  publisher = {Nature Publishing Group},
  issn = {1476-5454},
  doi = {10.1038/s41433-023-02458-3},
  urldate = {2024-02-22},
  abstract = {To compare the performance of a handheld fundus camera with standard 50{$^\circ$} visual field to ultra-widefield (UWF) table-top fundus camera in diabetic retinopathy (DR) detection and grading.},
  copyright = {2023 The Author(s), under exclusive licence to The Royal College of Ophthalmologists},
  langid = {english},
  keywords = {Eye manifestations,Retinal diseases},
  file = {C:\Users\cleme\Zotero\storage\I2YJD3NJ\Midena et al. - 2023 - Comparison of 50° handheld fundus camera versus ul.pdf}
}

@article{midenaOCTHyperreflectiveRetinal2021,
  title = {{{OCT Hyperreflective Retinal Foci}} in {{Diabetic Retinopathy}}: {{A Semi-Automatic Detection Comparative Study}}},
  shorttitle = {{{OCT Hyperreflective Retinal Foci}} in {{Diabetic Retinopathy}}},
  author = {Midena, Edoardo and Torresin, Tommaso and Velotta, Erika and Pilotto, Elisabetta and Parrozzani, Raffaele and Frizziero, Luisa},
  year = {2021},
  journal = {Frontiers in Immunology},
  volume = {12},
  issn = {1664-3224},
  urldate = {2022-07-09},
  abstract = {Optical coherence tomography (OCT) allows us to identify, into retinal layers, new morphological entities, which can be considered clinical biomarkers of retinal diseases. According to the literature, solitary, small ({$<$}30 {\textmu}m), medium level hyperreflective (similar to retinal fiber layer) retinal foci (HRF) may represent aggregates of activated microglial cells and an in vivo biomarker of retinal inflammation. The identification and quantification of this imaging biomarker allows for estimating the level and possibly the amount of intraretinal inflammation in major degenerative retinal disorders, whose inflammatory component has already been demonstrated (diabetic retinopathy, age-related macular degeneration, radiation retinopathy). Currently, diabetic retinopathy (DR) probably represents the best clinical model to apply this analysis in the definition of this clinical biomarker. However, the main limitation to the clinical use of HRF is related to the technical difficulty of counting them: a time-consuming methodology, which also needs trained examiners. To contribute to solve this limitation, we developed and validated a new method for the semi-automatic detection of HRF in OCT scans. OCT scans of patients affected by DR, were analyzed. HRF were manually counted in High Resolution spectral domain OCT images. Then, the same OCT scans underwent semi-automatic HRF counting, using an ImageJ software with four different settings profiles. Statistical analysis showed an excellent intraclass correlation coefficient (ICC) between the manual count and each of the four semi-automated methods. The use of the second setting profile allows to obtain at the Bland--Altman graph a bias of -0.2 foci and a limit of agreement of {\textpm}16.3 foci. This validation approach opens the way not only to the reliable and daily clinical applicable quantification of HRF, but also to a better knowledge of the inflammatory component---including its progression and regression changes---of diabetic retinopathy.}
}

@article{midenaOCTHyperreflectiveRetinal2021a,
  title = {{{OCT Hyperreflective Retinal Foci}} in {{Diabetic Retinopathy}}: {{A Semi-Automatic Detection Comparative Study}}},
  shorttitle = {{{OCT Hyperreflective Retinal Foci}} in {{Diabetic Retinopathy}}},
  author = {Midena, Edoardo and Torresin, Tommaso and Velotta, Erika and Pilotto, Elisabetta and Parrozzani, Raffaele and Frizziero, Luisa},
  year = {2021},
  journal = {Frontiers in Immunology},
  volume = {12},
  issn = {1664-3224},
  urldate = {2022-07-09},
  abstract = {Optical coherence tomography (OCT) allows us to identify, into retinal layers, new morphological entities, which can be considered clinical biomarkers of retinal diseases. According to the literature, solitary, small ({$<$}30 {\textmu}m), medium level hyperreflective (similar to retinal fiber layer) retinal foci (HRF) may represent aggregates of activated microglial cells and an in vivo biomarker of retinal inflammation. The identification and quantification of this imaging biomarker allows for estimating the level and possibly the amount of intraretinal inflammation in major degenerative retinal disorders, whose inflammatory component has already been demonstrated (diabetic retinopathy, age-related macular degeneration, radiation retinopathy). Currently, diabetic retinopathy (DR) probably represents the best clinical model to apply this analysis in the definition of this clinical biomarker. However, the main limitation to the clinical use of HRF is related to the technical difficulty of counting them: a time-consuming methodology, which also needs trained examiners. To contribute to solve this limitation, we developed and validated a new method for the semi-automatic detection of HRF in OCT scans. OCT scans of patients affected by DR, were analyzed. HRF were manually counted in High Resolution spectral domain OCT images. Then, the same OCT scans underwent semi-automatic HRF counting, using an ImageJ software with four different settings profiles. Statistical analysis showed an excellent intraclass correlation coefficient (ICC) between the manual count and each of the four semi-automated methods. The use of the second setting profile allows to obtain at the Bland--Altman graph a bias of -0.2 foci and a limit of agreement of {\textpm}16.3 foci. This validation approach opens the way not only to the reliable and daily clinical applicable quantification of HRF, but also to a better knowledge of the inflammatory component---including its progression and regression changes---of diabetic retinopathy.},
  file = {C:\Users\cleme\Zotero\storage\G5JB3PN8\Midena et al. - 2021 - OCT Hyperreflective Retinal Foci in Diabetic Retin.pdf}
}

@article{midenaUltrawidefieldFundusPhotography2022,
  title = {Ultra-Wide-Field Fundus Photography Compared to Ophthalmoscopy in Diagnosing and Classifying Major Retinal Diseases},
  author = {Midena, E. and Marchione, G. and Di Giorgio, S. and Rotondi, G. and Longhin, E. and Frizziero, L. and Pilotto, E. and Parrozzani, R. and Midena, G.},
  year = {2022},
  month = nov,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {19287},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-23170-4},
  urldate = {2024-02-22},
  abstract = {To analyze the performance of ultra-wide-field (UWF) fundus photography compared with ophthalmoscopy in identifying and classifying retinal diseases. Patients examined for presumed major retinal disorders were consecutively enrolled. Each patient underwent indirect ophthalmoscopic evaluation, with scleral depression and/or fundus biomicroscopy, when clinically indicated, and mydriatic UWF fundus imaging by means of CLARUS 500™ fundus camera. Each eye was classified by a clinical grader and two image graders in the following groups: normal retina, diabetic retinopathy, vascular abnormalities, macular degenerations and dystrophies, retinal and choroidal tumors, peripheral degenerative lesions and retinal detachment and myopic alterations. 7024 eyes of new patients were included. The inter-grader agreement for images classification was perfect (kappa\,=\,0.998, 95\% Confidence Interval (95\%CI)\,=\,0.997--0.999), as the two methods concordance for retinal diseases diagnosis (kappa\,=\,0.997, 95\%CI\,=\,0.996--0.999) without statistically significant difference. UWF fundus imaging might be an alternative to ophthalmoscopy, since it allows to accurately classify major retinal diseases, widening the range of disorders possibly diagnosed with teleophthalmology. Although the clinician should be aware of the possibility that a minority of the most peripheral lesions may be not entirely visualized, it might be considered a first line diagnostic modality, in the context of a full ophthalmological examination.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Biomarkers,Eye diseases,Health services,Medical imaging,Retinal diseases},
  file = {C:\Users\cleme\Zotero\storage\W7ASZ67K\Midena et al. - 2022 - Ultra-wide-field fundus photography compared to op.pdf}
}

@article{midenaUltrawidefieldFundusPhotography2022a,
  title = {Ultra-Wide-Field Fundus Photography Compared to Ophthalmoscopy in Diagnosing and Classifying Major Retinal Diseases},
  author = {Midena, E. and Marchione, G. and Di Giorgio, S. and Rotondi, G. and Longhin, E. and Frizziero, L. and Pilotto, E. and Parrozzani, R. and Midena, G.},
  year = {2022},
  month = nov,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {19287},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-23170-4},
  urldate = {2024-02-22},
  abstract = {To analyze the performance of ultra-wide-field (UWF) fundus photography compared with ophthalmoscopy in identifying and classifying retinal diseases. Patients examined for presumed major retinal disorders were consecutively enrolled. Each patient underwent indirect ophthalmoscopic evaluation, with scleral depression and/or fundus biomicroscopy, when clinically indicated, and mydriatic UWF fundus imaging by means of CLARUS 500™ fundus camera. Each eye was classified by a clinical grader and two image graders in the following groups: normal retina, diabetic retinopathy, vascular abnormalities, macular degenerations and dystrophies, retinal and choroidal tumors, peripheral degenerative lesions and retinal detachment and myopic alterations. 7024 eyes of new patients were included. The inter-grader agreement for images classification was perfect (kappa\,=\,0.998, 95\% Confidence Interval (95\%CI)\,=\,0.997--0.999), as the two methods concordance for retinal diseases diagnosis (kappa\,=\,0.997, 95\%CI\,=\,0.996--0.999) without statistically significant difference. UWF fundus imaging might be an alternative to ophthalmoscopy, since it allows to accurately classify major retinal diseases, widening the range of disorders possibly diagnosed with teleophthalmology. Although the clinician should be aware of the possibility that a minority of the most peripheral lesions may be not entirely visualized, it might be considered a first line diagnostic modality, in the context of a full ophthalmological examination.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Biomarkers,Eye diseases,Health services,Medical imaging,Retinal diseases},
  file = {C:\Users\cleme\Zotero\storage\8PRZKIFG\Midena et al. - 2022 - Ultra-wide-field fundus photography compared to op.pdf}
}

@article{minhExplainableArtificialIntelligence2022,
  title = {Explainable Artificial Intelligence: A Comprehensive Review},
  shorttitle = {Explainable Artificial Intelligence},
  author = {Minh, Dang and Wang, H. Xiang and Li, Y. Fen and Nguyen, Tan N.},
  year = {2022},
  month = jun,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {5},
  pages = {3503--3568},
  issn = {1573-7462},
  doi = {10.1007/s10462-021-10088-y},
  urldate = {2023-05-05},
  abstract = {Thanks to the exponential growth in computing power and vast amounts of data, artificial intelligence (AI) has witnessed remarkable developments in recent years, enabling it to be ubiquitously adopted in our daily lives. Even though AI-powered systems have brought competitive advantages, the black-box nature makes them lack transparency and prevents them from explaining their decisions. This issue has motivated the introduction of explainable artificial intelligence (XAI), which promotes AI algorithms that can show their internal process and explain how they made decisions. The number of XAI research has increased significantly in recent years, but there lacks a unified and comprehensive review of the latest XAI progress. This review aims to bridge the gap by discovering the critical perspectives of the rapidly growing body of research associated with XAI. After offering the readers a solid XAI background, we analyze and review various XAI methods, which are grouped into (i) pre-modeling explainability, (ii) interpretable model, and (iii) post-modeling explainability. We also pay attention to the current methods that dedicate to interpret and analyze deep learning methods. In addition, we systematically discuss various XAI challenges, such as the trade-off between the performance and the explainability, evaluation methods, security, and policy. Finally, we show the standard approaches that are leveraged to deal with the mentioned challenges.},
  langid = {english},
  keywords = {Black-box models,Deep learning,Explainable artificial intelligence,Interpretability,Machine learning}
}

@article{minhExplainableArtificialIntelligence2022a,
  title = {Explainable Artificial Intelligence: A Comprehensive Review},
  shorttitle = {Explainable Artificial Intelligence},
  author = {Minh, Dang and Wang, H. Xiang and Li, Y. Fen and Nguyen, Tan N.},
  year = {2022},
  month = jun,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {5},
  pages = {3503--3568},
  issn = {1573-7462},
  doi = {10.1007/s10462-021-10088-y},
  urldate = {2023-05-05},
  abstract = {Thanks to the exponential growth in computing power and vast amounts of data, artificial intelligence (AI) has witnessed remarkable developments in recent years, enabling it to be ubiquitously adopted in our daily lives. Even though AI-powered systems have brought competitive advantages, the black-box nature makes them lack transparency and prevents them from explaining their decisions. This issue has motivated the introduction of explainable artificial intelligence (XAI), which promotes AI algorithms that can show their internal process and explain how they made decisions. The number of XAI research has increased significantly in recent years, but there lacks a unified and comprehensive review of the latest XAI progress. This review aims to bridge the gap by discovering the critical perspectives of the rapidly growing body of research associated with XAI. After offering the readers a solid XAI background, we analyze and review various XAI methods, which are grouped into (i) pre-modeling explainability, (ii) interpretable model, and (iii) post-modeling explainability. We also pay attention to the current methods that dedicate to interpret and analyze deep learning methods. In addition, we systematically discuss various XAI challenges, such as the trade-off between the performance and the explainability, evaluation methods, security, and policy. Finally, we show the standard approaches that are leveraged to deal with the mentioned challenges.},
  langid = {english},
  keywords = {Black-box models,Deep learning,Explainable artificial intelligence,Interpretability,Machine learning},
  file = {C:\Users\cleme\Zotero\storage\95CHWC4F\Minh et al. - 2022 - Explainable artificial intelligence a comprehensi.pdf}
}

@inproceedings{miriMultimodalGraphTheoreticApproach2015,
  title = {Multimodal {{Graph-Theoretic Approach}} for {{Segmentation}} of the {{Internal Limiting Membrane}} at the {{Optic Nerve Head}}},
  author = {Miri, Mohammad Saleh and Robles, V{\'i}ctor A. Due{\~n}as and Abr{\`a}moff, Michael David and Kwon, Young Hyun and Garvin, Mona Kathryn},
  year = {2015},
  doi = {10.17077/omia.1027},
  abstract = {In this work, we present a multimodal multiresolution graph- based method to segment the top surface of the retina called the inter- nal limiting membrane (ILM) within optic-nerve-head-centered spectral- domain optical coherence tomography (SD-OCT) volumes. Having a pre- cise ILM surface is crucial as this surface is utilized for measuring several structural parameters such as Bruch's membrane opening-minimum rim width (BMO-MRW) and cup volume. The proposed method addresses the common current segmentation errors due to the presence of retinal blood vessels, deep cupping, or a very steep slope of the ILM. In order to resolve these issues, the volume is resampled using a set of gradient vector flow (GVF) based columns. The GVF field is computed according to an initial surface segmentation which is obtained through a multires- olution framework. The retinal blood vessel information (obtained from corresponding registered fundus photographs) along with shape prior in- formation are incorporated in a graph-theoretic approach to compute the ILM segmentation. The method is tested on the SD-OCT volumes from 44 glaucoma subjects and significantly smaller errors were obtained than that from current approaches.},
  keywords = {biologic segmentation,Blood Flow Velocity,Column (database),Eye,Glaucoma,Gradient,Graph - visual representation,Graph theory,Inner Limiting Membrane,Multimodal interaction,Nerve Tissue,Optic Nerve (GCHQ),Optical Coherence,photograph,Physiological Sexual Disorders,Raw image format,Registration,Structure of blood vessel of retina,Tomography,width,X-Ray Computed Tomography}
}

@inproceedings{miriMultimodalGraphTheoreticApproach2015a,
  title = {Multimodal {{Graph-Theoretic Approach}} for {{Segmentation}} of the {{Internal Limiting Membrane}} at the {{Optic Nerve Head}}},
  author = {Miri, Mohammad Saleh and Robles, V{\'i}ctor A. Due{\~n}as and Abr{\`a}moff, Michael David and Kwon, Young Hyun and Garvin, Mona Kathryn},
  year = {2015},
  doi = {10.17077/omia.1027},
  abstract = {In this work, we present a multimodal multiresolution graph- based method to segment the top surface of the retina called the inter- nal limiting membrane (ILM) within optic-nerve-head-centered spectral- domain optical coherence tomography (SD-OCT) volumes. Having a pre- cise ILM surface is crucial as this surface is utilized for measuring several structural parameters such as Bruch's membrane opening-minimum rim width (BMO-MRW) and cup volume. The proposed method addresses the common current segmentation errors due to the presence of retinal blood vessels, deep cupping, or a very steep slope of the ILM. In order to resolve these issues, the volume is resampled using a set of gradient vector flow (GVF) based columns. The GVF field is computed according to an initial surface segmentation which is obtained through a multires- olution framework. The retinal blood vessel information (obtained from corresponding registered fundus photographs) along with shape prior in- formation are incorporated in a graph-theoretic approach to compute the ILM segmentation. The method is tested on the SD-OCT volumes from 44 glaucoma subjects and significantly smaller errors were obtained than that from current approaches.},
  keywords = {biologic segmentation,Blood Flow Velocity,Column (database),Eye,Glaucoma,Gradient,Graph - visual representation,Graph theory,Inner Limiting Membrane,Multimodal interaction,Nerve Tissue,Optic Nerve (GCHQ),photograph,Physiological Sexual Disorders,Raw image format,Registration,Structure of blood vessel of retina,Tomography Optical Coherence,width,X-Ray Computed Tomography},
  file = {C:\Users\cleme\Zotero\storage\XGM5KTXE\viewcontent.pdf}
}

@article{miriMultimodalMachinelearningGraphbased2016,
  title = {A Multimodal Machine-Learning Graph-Based Approach for Segmenting Glaucomatous Optic Nerve Head Structures from {{SD-OCT}} Volumes and Fundus Photographs},
  author = {Miri, Mohammad Saleh},
  year = {2016},
  month = may,
  journal = {Theses and Dissertations},
  pages = {178},
  doi = {10.17077/etd.23kdq0ph},
  file = {C:\Users\cleme\Zotero\storage\NYVPMI5K\Miri - A multimodal machine-learning graph-based approach.pdf}
}

@article{miriMultimodalRegistrationSDOCT2016,
  title = {Multimodal Registration of {{SD-OCT}} Volumes and Fundus Photographs Using Histograms of Oriented Gradients},
  author = {Miri, Mohammad Saleh and Abr{\`a}moff, Michael D. and Kwon, Young H. and Garvin, Mona K.},
  year = {2016},
  month = nov,
  journal = {Biomedical Optics Express},
  volume = {7},
  number = {12},
  pages = {5252--5267},
  issn = {2156-7085},
  doi = {10.1364/BOE.7.005252},
  urldate = {2019-07-23},
  abstract = {With availability of different retinal imaging modalities such as fundus photography and spectral domain optical coherence tomography (SD-OCT), having a robust and accurate registration scheme to enable utilization of this complementary information is beneficial. The few existing fundus-OCT registration approaches contain a vessel segmentation step, as the retinal blood vessels are the most dominant structures that are in common between the pair of images. However, errors in the vessel segmentation from either modality may cause corresponding errors in the registration. In this paper, we propose a feature-based registration method for registering fundus photographs and SD-OCT projection images that benefits from vasculature structural information without requiring blood vessel segmentation. In particular, after a preprocessing step, a set of control points (CPs) are identified by looking for the corners in the images. Next, each CP is represented by a feature vector which encodes the local structural information via computing the histograms of oriented gradients (HOG) from the neighborhood of each CP. The best matching CPs are identified by calculating the distance of their corresponding feature vectors. After removing the incorrect matches the best affine transform that registers fundus photographs to SD-OCT projection images is computed using the random sample consensus (RANSAC) method. The proposed method was tested on 44 pairs of fundus and SD-OCT projection images of glaucoma patients and the result showed that the proposed method successfully registers the multimodal images and produced a registration error of 25.34 {\textpm} 12.34 {$\mu$}m (0.84 {\textpm} 0.41 pixels).},
  pmcid = {PMC5175567},
  pmid = {28018740}
}

@article{miriMultimodalRegistrationSDOCT2016a,
  title = {Multimodal Registration of {{SD-OCT}} Volumes and Fundus Photographs Using Histograms of Oriented Gradients},
  author = {Miri, Mohammad Saleh and Abr{\`a}moff, Michael D. and Kwon, Young H. and Garvin, Mona K.},
  year = {2016},
  month = nov,
  journal = {Biomedical Optics Express},
  volume = {7},
  number = {12},
  pages = {5252--5267},
  issn = {2156-7085},
  doi = {10.1364/BOE.7.005252},
  urldate = {2019-07-23},
  abstract = {With availability of different retinal imaging modalities such as fundus photography and spectral domain optical coherence tomography (SD-OCT), having a robust and accurate registration scheme to enable utilization of this complementary information is beneficial. The few existing fundus-OCT registration approaches contain a vessel segmentation step, as the retinal blood vessels are the most dominant structures that are in common between the pair of images. However, errors in the vessel segmentation from either modality may cause corresponding errors in the registration. In this paper, we propose a feature-based registration method for registering fundus photographs and SD-OCT projection images that benefits from vasculature structural information without requiring blood vessel segmentation. In particular, after a preprocessing step, a set of control points (CPs) are identified by looking for the corners in the images. Next, each CP is represented by a feature vector which encodes the local structural information via computing the histograms of oriented gradients (HOG) from the neighborhood of each CP. The best matching CPs are identified by calculating the distance of their corresponding feature vectors. After removing the incorrect matches the best affine transform that registers fundus photographs to SD-OCT projection images is computed using the random sample consensus (RANSAC) method. The proposed method was tested on 44 pairs of fundus and SD-OCT projection images of glaucoma patients and the result showed that the proposed method successfully registers the multimodal images and produced a registration error of 25.34 {\textpm} 12.34 {$\mu$}m (0.84 {\textpm} 0.41 pixels).},
  pmcid = {PMC5175567},
  pmid = {28018740},
  file = {C:\Users\cleme\Zotero\storage\4SVCNU7E\Miri et al. - 2016 - Multimodal registration of SD-OCT volumes and fund.pdf}
}

@article{miriMultimodalSegmentationOptic2015,
  title = {Multimodal {{Segmentation}} of {{Optic Disc}} and {{Cup From SD-OCT}} and {{Color Fundus Photographs Using}} a {{Machine-Learning Graph-Based Approach}}},
  author = {Miri, M. S. and Abr{\`a}moff, M. D. and Lee, K. and Niemeijer, M. and Wang, J. and Kwon, Y. H. and Garvin, M. K.},
  year = {2015},
  month = sep,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {34},
  number = {9},
  pages = {1854--1866},
  issn = {0278-0062},
  doi = {10.1109/TMI.2015.2412881},
  abstract = {In this work, a multimodal approach is proposed to use the complementary information from fundus photographs and spectral domain optical coherence tomography (SD-OCT) volumes in order to segment the optic disc and cup boundaries. The problem is formulated as an optimization problem where the optimal solution is obtained using a machine-learning theoretical graph-based method. In particular, first the fundus photograph is registered to the 2D projection of the SD-OCT volume. Three in-region cost functions are designed using a random forest classifier corresponding to three regions of cup, rim, and background. Next, the volumes are resampled to create radial scans in which the Bruch's Membrane Opening (BMO) endpoints are easier to detect. Similar to in-region cost function design, the disc-boundary cost function is designed using a random forest classifier for which the features are created by applying the Haar Stationary Wavelet Transform (SWT) to the radial projection image. A multisurface graph-based approach utilizes the in-region and disc-boundary cost images to segment the boundaries of optic disc and cup under feasibility constraints. The approach is evaluated on 25 multimodal image pairs from 25 subjects in a leave-one-out fashion (by subject). The performances of the graph-theoretic approach using three sets of cost functions are compared: 1) using unimodal (OCT only) in-region costs, 2) using multimodal in-region costs, and 3) using multimodal in-region and disc-boundary costs. Results show that the multimodal approaches outperform the unimodal approach in segmenting the optic disc and cup.},
  keywords = {2D projection,Algorithms,biomedical optical imaging,Biomedical optical imaging,Bruch membrane opening endpoints,Bruch's membrane opening,color fundus photographs,Cost function,cup boundaries,Diagnostic Techniques,disc-boundary cost function,eye,Feature extraction,graph theory,Haar stationary wavelet transform,Haar transforms,Humans,image classification,Image color analysis,image registration,image sampling,image segmentation,Image segmentation,Imaging,in-region cost functions,learning (artificial intelligence),Machine Learning,machine-learning theoretical graph-based method,medical image processing,multimodal,multimodal image pairs,Multimodal Imaging,multimodal segmentation,multisurface graph-based approach,Ophthalmological,ophthalmology,optic disc,optic disc boundaries,Optic Disk,Optical imaging,optical tomography,optimization problem,radial projection image,radial scans,random forest classifier,retina,SD-OCT,SD-OCT volume,segmentation,spectral domain optical coherence tomography volumes,Three-Dimensional,Urban areas,wavelet transforms}
}

@article{miriMultimodalSegmentationOptic2015a,
  title = {Multimodal {{Segmentation}} of {{Optic Disc}} and {{Cup From SD-OCT}} and {{Color Fundus Photographs Using}} a {{Machine-Learning Graph-Based Approach}}},
  author = {Miri, M. S. and Abr{\`a}moff, M. D. and Lee, K. and Niemeijer, M. and Wang, J. and Kwon, Y. H. and Garvin, M. K.},
  year = {2015},
  month = sep,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {34},
  number = {9},
  pages = {1854--1866},
  issn = {0278-0062},
  doi = {10.1109/TMI.2015.2412881},
  abstract = {In this work, a multimodal approach is proposed to use the complementary information from fundus photographs and spectral domain optical coherence tomography (SD-OCT) volumes in order to segment the optic disc and cup boundaries. The problem is formulated as an optimization problem where the optimal solution is obtained using a machine-learning theoretical graph-based method. In particular, first the fundus photograph is registered to the 2D projection of the SD-OCT volume. Three in-region cost functions are designed using a random forest classifier corresponding to three regions of cup, rim, and background. Next, the volumes are resampled to create radial scans in which the Bruch's Membrane Opening (BMO) endpoints are easier to detect. Similar to in-region cost function design, the disc-boundary cost function is designed using a random forest classifier for which the features are created by applying the Haar Stationary Wavelet Transform (SWT) to the radial projection image. A multisurface graph-based approach utilizes the in-region and disc-boundary cost images to segment the boundaries of optic disc and cup under feasibility constraints. The approach is evaluated on 25 multimodal image pairs from 25 subjects in a leave-one-out fashion (by subject). The performances of the graph-theoretic approach using three sets of cost functions are compared: 1) using unimodal (OCT only) in-region costs, 2) using multimodal in-region costs, and 3) using multimodal in-region and disc-boundary costs. Results show that the multimodal approaches outperform the unimodal approach in segmenting the optic disc and cup.},
  keywords = {2D projection,Algorithms,biomedical optical imaging,Biomedical optical imaging,Bruch membrane opening endpoints,Bruch's membrane opening,color fundus photographs,Cost function,cup boundaries,Diagnostic Techniques Ophthalmological,disc-boundary cost function,eye,Feature extraction,graph theory,Haar stationary wavelet transform,Haar transforms,Humans,image classification,Image color analysis,image registration,image sampling,image segmentation,Image segmentation,Imaging Three-Dimensional,in-region cost functions,learning (artificial intelligence),Machine Learning,machine-learning theoretical graph-based method,medical image processing,multimodal,multimodal image pairs,Multimodal Imaging,multimodal segmentation,multisurface graph-based approach,ophthalmology,optic disc,optic disc boundaries,Optic Disk,Optical imaging,optical tomography,optimization problem,radial projection image,radial scans,random forest classifier,retina,SD-OCT,SD-OCT volume,segmentation,spectral domain optical coherence tomography volumes,Urban areas,wavelet transforms}
}

@article{mishraMultiLevelDualAttentionBased2019,
  title = {Multi-{{Level Dual-Attention Based CNN}} for {{Macular Optical Coherence Tomography Classification}}},
  author = {Mishra, Sapna S. and Mandal, Bappaditya and Puhan, N. B.},
  year = {2019},
  month = dec,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {12},
  pages = {1793--1797},
  issn = {1558-2361},
  doi = {10.1109/LSP.2019.2949388},
  abstract = {In this letter, we propose a multi-level dual-attention model to classify two common macular diseases, age-related macular degeneration (AMD) and diabetic macular edema (DME) from normal macular eye conditions using optical coherence tomography (OCT) imaging technique. Our approach unifies the dual-attention mechanism at multi-levels of the pre-trained deep convolutional neural network (CNN). It provides a focused learning mechanism by taking into account both multi-level features based attention focusing on the salient coarser features and self-attention mechanism attending higher entropy regions of the finer features. Our proposed method enables the network to automatically focus on the relevant parts of the input images at different levels of feature subspaces. This leads to a more locally deformation-aware feature generation and classification. The proposed approach does not require pre-processing steps such as extraction of region of interest, denoising, and retinal flattening, making the network more robust and fully automatic. Experimental results on two macular OCT databases show the superior performance of our proposed approach as compared to the current state-of-the-art methodologies.},
  keywords = {age-related macular degeneration (AMD),Attention mechanism,Dams,Databases,diabetic macular edema (DME),Diseases,Feature extraction,Integrated circuits,multi-level dual-attention,optical coherence tomography (OCT),Protocols,Retina}
}

@article{mishraMultiLevelDualAttentionBased2019a,
  title = {Multi-{{Level Dual-Attention Based CNN}} for {{Macular Optical Coherence Tomography Classification}}},
  author = {Mishra, Sapna S. and Mandal, Bappaditya and Puhan, N. B.},
  year = {2019},
  month = dec,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {12},
  pages = {1793--1797},
  issn = {1558-2361},
  doi = {10.1109/LSP.2019.2949388},
  abstract = {In this letter, we propose a multi-level dual-attention model to classify two common macular diseases, age-related macular degeneration (AMD) and diabetic macular edema (DME) from normal macular eye conditions using optical coherence tomography (OCT) imaging technique. Our approach unifies the dual-attention mechanism at multi-levels of the pre-trained deep convolutional neural network (CNN). It provides a focused learning mechanism by taking into account both multi-level features based attention focusing on the salient coarser features and self-attention mechanism attending higher entropy regions of the finer features. Our proposed method enables the network to automatically focus on the relevant parts of the input images at different levels of feature subspaces. This leads to a more locally deformation-aware feature generation and classification. The proposed approach does not require pre-processing steps such as extraction of region of interest, denoising, and retinal flattening, making the network more robust and fully automatic. Experimental results on two macular OCT databases show the superior performance of our proposed approach as compared to the current state-of-the-art methodologies.},
  keywords = {age-related macular degeneration (AMD),Attention mechanism,Dams,Databases,diabetic macular edema (DME),Diseases,Feature extraction,Integrated circuits,multi-level dual-attention,optical coherence tomography (OCT),Protocols,Retina}
}

@article{mohseniMultidisciplinarySurveyFramework2021,
  title = {A {{Multidisciplinary Survey}} and {{Framework}} for {{Design}} and {{Evaluation}} of {{Explainable AI Systems}}},
  author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
  year = {2021},
  month = sep,
  journal = {ACM Transactions on Interactive Intelligent Systems},
  volume = {11},
  number = {3-4},
  pages = {24:1--24:45},
  issn = {2160-6455},
  doi = {10.1145/3387166},
  urldate = {2023-05-04},
  abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence (AI) applications used in everyday life. Explainable AI (XAI) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
  keywords = {Explainable artificial intelligence (XAI),explanation,human-computer interaction (HCI),machine learning,transparency},
  file = {C:\Users\cleme\Zotero\storage\L6A2LJ3V\Mohseni et al. - 2021 - A Multidisciplinary Survey and Framework for Desig.pdf}
}

@article{montavonExplainingNonlinearClassification2017,
  title = {Explaining Nonlinear Classification Decisions with Deep {{Taylor}} Decomposition},
  author = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2017},
  month = may,
  journal = {Pattern Recognition},
  volume = {65},
  pages = {211--222},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2016.11.008},
  urldate = {2021-03-03},
  abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
  langid = {english},
  keywords = {Deep neural networks,Heatmapping,Image recognition,Relevance propagation,Taylor decomposition}
}

@article{montavonExplainingNonlinearClassification2017a,
  title = {Explaining Nonlinear Classification Decisions with Deep {{Taylor}} Decomposition},
  author = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2017},
  month = may,
  journal = {Pattern Recognition},
  volume = {65},
  pages = {211--222},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2016.11.008},
  urldate = {2021-03-03},
  abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
  langid = {english},
  keywords = {Deep neural networks,Heatmapping,Image recognition,Relevance propagation,Taylor decomposition},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5IQXP4QT\\Montavon et al. - 2017 - Explaining nonlinear classification decisions with.pdf;C\:\\Users\\cleme\\Zotero\\storage\\FHECUIBR\\S0031320316303582.html}
}

@incollection{montavonLayerWiseRelevancePropagation2019,
  title = {Layer-{{Wise Relevance Propagation}}: {{An Overview}}},
  shorttitle = {Layer-{{Wise Relevance Propagation}}},
  booktitle = {Explainable {{AI}}: {{Interpreting}}, {{Explaining}} and {{Visualizing Deep Learning}}},
  author = {Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  editor = {Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and M{\"u}ller, Klaus-Robert},
  year = {2019},
  volume = {11700},
  pages = {193--209},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-28954-6_10},
  urldate = {2022-06-15},
  abstract = {For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a `deep Taylor decomposition', (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.},
  isbn = {978-3-030-28953-9 978-3-030-28954-6},
  langid = {english}
}

@incollection{montavonLayerWiseRelevancePropagation2019a,
  title = {Layer-{{Wise Relevance Propagation}}: {{An Overview}}},
  shorttitle = {Layer-{{Wise Relevance Propagation}}},
  booktitle = {Explainable {{AI}}: {{Interpreting}}, {{Explaining}} and {{Visualizing Deep Learning}}},
  author = {Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  editor = {Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and M{\"u}ller, Klaus-Robert},
  year = {2019},
  volume = {11700},
  pages = {193--209},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-28954-6_10},
  urldate = {2022-06-15},
  abstract = {For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a `deep Taylor decomposition', (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.},
  isbn = {978-3-030-28953-9 978-3-030-28954-6},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\8BF3AWRP\Montavon et al. - 2019 - Layer-Wise Relevance Propagation An Overview.pdf}
}

@article{montavonMethodsInterpretingUnderstanding2018,
  title = {Methods for Interpreting and Understanding Deep Neural Networks},
  author = {Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2018},
  month = feb,
  journal = {Digital Signal Processing},
  volume = {73},
  pages = {1--15},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2017.10.011},
  urldate = {2023-06-28},
  abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
  langid = {english},
  keywords = {Activation maximization,Deep neural networks,Layer-wise relevance propagation,Sensitivity analysis,Taylor decomposition}
}

@article{montavonMethodsInterpretingUnderstanding2018a,
  title = {Methods for Interpreting and Understanding Deep Neural Networks},
  author = {Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2018},
  month = feb,
  journal = {Digital Signal Processing},
  volume = {73},
  pages = {1--15},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2017.10.011},
  urldate = {2023-06-28},
  abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
  langid = {english},
  keywords = {Activation maximization,Deep neural networks,Layer-wise relevance propagation,Sensitivity analysis,Taylor decomposition},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\M9UWSJE4\\Montavon et al. - 2018 - Methods for interpreting and understanding deep ne.pdf;C\:\\Users\\cleme\\Zotero\\storage\\FFDWZVDN\\S1051200417302385.html}
}

@misc{monteiroStochasticSegmentationNetworks2020,
  title = {Stochastic {{Segmentation Networks}}: {{Modelling Spatially Correlated Aleatoric Uncertainty}}},
  shorttitle = {Stochastic {{Segmentation Networks}}},
  author = {Monteiro, Miguel and Folgoc, Lo{\"i}c Le and {de Castro}, Daniel Coelho and Pawlowski, Nick and Marques, Bernardo and Kamnitsas, Konstantinos and {van der Wilk}, Mark and Glocker, Ben},
  year = {2020},
  month = dec,
  number = {arXiv:2006.06015},
  eprint = {2006.06015},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.06015},
  urldate = {2024-06-22},
  abstract = {In image segmentation, there is often more than one plausible solution for a given input. In medical imaging, for example, experts will often disagree about the exact location of object boundaries. Estimating this inherent uncertainty and predicting multiple plausible hypotheses is of great interest in many applications, yet this ability is lacking in most current deep learning methods. In this paper, we introduce stochastic segmentation networks (SSNs), an efficient probabilistic method for modelling aleatoric uncertainty with any image segmentation network architecture. In contrast to approaches that produce pixel-wise estimates, SSNs model joint distributions over entire label maps and thus can generate multiple spatially coherent hypotheses for a single image. By using a low-rank multivariate normal distribution over the logit space to model the probability of the label map given the image, we obtain a spatially consistent probability distribution that can be efficiently computed by a neural network without any changes to the underlying architecture. We tested our method on the segmentation of real-world medical data, including lung nodules in 2D CT and brain tumours in 3D multimodal MRI scans. SSNs outperform state-of-the-art for modelling correlated uncertainty in ambiguous images while being much simpler, more flexible, and more efficient.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\FTREVZCG\\Monteiro et al. - 2020 - Stochastic Segmentation Networks Modelling Spatia.pdf;C\:\\Users\\cleme\\Zotero\\storage\\X3I2XP36\\2006.html}
}

@article{montufarNumberLinearRegions,
  title = {On the {{Number}} of {{Linear Regions}} of {{Deep Neural Networks}}},
  author = {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
  langid = {english}
}

@inproceedings{montufarNumberLinearRegions2014,
  title = {On the Number of Linear Regions of Deep Neural Networks},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Mont{\'u}far, Guido and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  series = {{{NIPS}}'14},
  pages = {2924--2932},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  urldate = {2023-02-22},
  abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
  keywords = {deep learning,input space partition,maxout,neural network,rectifier}
}

@inproceedings{montufarNumberLinearRegions2014a,
  title = {On the Number of Linear Regions of Deep Neural Networks},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Mont{\'u}far, Guido and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  series = {{{NIPS}}'14},
  pages = {2924--2932},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  urldate = {2023-02-22},
  abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
  keywords = {deep learning,input space partition,maxout,neural network,rectifier}
}

@article{montufarNumberLinearRegionsa,
  title = {On the {{Number}} of {{Linear Regions}} of {{Deep Neural Networks}}},
  author = {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\CVZ35PCI\Montufar et al. - On the Number of Linear Regions of Deep Neural Net.pdf}
}

@article{mookiahLocalConfigurationPattern2015,
  title = {Local Configuration Pattern Features for Age-Related Macular Degeneration Characterization and Classification},
  author = {Mookiah, Muthu Rama Krishnan and Acharya, U. Rajendra and Fujita, Hamido and Koh, Joel E. W. and Tan, Jen Hong and Noronha, Kevin and Bhandary, Sulatha V. and Chua, Chua Kuang and Lim, Choo Min and Laude, Augustinus and Tong, Louis},
  year = {2015},
  month = aug,
  journal = {Computers in Biology and Medicine},
  volume = {63},
  pages = {208--218},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2015.05.019},
  urldate = {2019-11-19},
  abstract = {Age-related Macular Degeneration (AMD) is an irreversible and chronic medical condition characterized by drusen, Choroidal Neovascularization (CNV) and Geographic Atrophy (GA). AMD is one of the major causes of visual loss among elderly people. It is caused by the degeneration of cells in the macula which is responsible for central vision. AMD can be dry or wet type, however dry AMD is most common. It is classified into early, intermediate and late AMD. The early detection and treatment may help one to stop the progression of the disease. Automated AMD diagnosis may reduce the screening time of the clinicians. In this work, we have introduced LCP to characterize normal and AMD classes using fundus images. Linear Configuration Coefficients (CC) and Pattern Occurrence (PO) features are extracted from fundus images. These extracted features are ranked using p-value of the t-test and fed to various supervised classifiers viz. Decision Tree (DT), Nearest Neighbour (k-NN), Naive Bayes (NB), Probabilistic Neural Network (PNN) and Support Vector Machine (SVM) to classify normal and AMD classes. The performance of the system is evaluated using both private (Kasturba Medical Hospital, Manipal, India) and public domain datasets viz. Automated Retinal Image Analysis (ARIA) and STructured Analysis of the Retina (STARE) using ten-fold cross validation. The proposed approach yielded best performance with a highest average accuracy of 97.78\%, sensitivity of 98.00\% and specificity of 97.50\% for STARE dataset using 22 significant features. Hence, this system can be used as an aiding tool to the clinicians during mass eye screening programs to diagnose AMD.},
  langid = {english},
  keywords = {Age-related macular degeneration,Fundus imaging,Local configuration pattern,Retina,Support vector machine}
}

@article{mookiahLocalConfigurationPattern2015a,
  title = {Local Configuration Pattern Features for Age-Related Macular Degeneration Characterization and Classification},
  author = {Mookiah, Muthu Rama Krishnan and Acharya, U. Rajendra and Fujita, Hamido and Koh, Joel E. W. and Tan, Jen Hong and Noronha, Kevin and Bhandary, Sulatha V. and Chua, Chua Kuang and Lim, Choo Min and Laude, Augustinus and Tong, Louis},
  year = {2015},
  month = aug,
  journal = {Computers in Biology and Medicine},
  volume = {63},
  pages = {208--218},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2015.05.019},
  urldate = {2019-11-19},
  abstract = {Age-related Macular Degeneration (AMD) is an irreversible and chronic medical condition characterized by drusen, Choroidal Neovascularization (CNV) and Geographic Atrophy (GA). AMD is one of the major causes of visual loss among elderly people. It is caused by the degeneration of cells in the macula which is responsible for central vision. AMD can be dry or wet type, however dry AMD is most common. It is classified into early, intermediate and late AMD. The early detection and treatment may help one to stop the progression of the disease. Automated AMD diagnosis may reduce the screening time of the clinicians. In this work, we have introduced LCP to characterize normal and AMD classes using fundus images. Linear Configuration Coefficients (CC) and Pattern Occurrence (PO) features are extracted from fundus images. These extracted features are ranked using p-value of the t-test and fed to various supervised classifiers viz. Decision Tree (DT), Nearest Neighbour (k-NN), Naive Bayes (NB), Probabilistic Neural Network (PNN) and Support Vector Machine (SVM) to classify normal and AMD classes. The performance of the system is evaluated using both private (Kasturba Medical Hospital, Manipal, India) and public domain datasets viz. Automated Retinal Image Analysis (ARIA) and STructured Analysis of the Retina (STARE) using ten-fold cross validation. The proposed approach yielded best performance with a highest average accuracy of 97.78\%, sensitivity of 98.00\% and specificity of 97.50\% for STARE dataset using 22 significant features. Hence, this system can be used as an aiding tool to the clinicians during mass eye screening programs to diagnose AMD.},
  langid = {english},
  keywords = {Age-related macular degeneration,Fundus imaging,Local configuration pattern,Retina,Support vector machine},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\KN8UXWPV\\mookiah2015.pdf;C\:\\Users\\cleme\\Zotero\\storage\\TLA4RCQ2\\Mookiah et al. - 2015 - Local configuration pattern features for age-relat.pdf;C\:\\Users\\cleme\\Zotero\\storage\\8INZTC3L\\S0010482515002012.html}
}

@article{morinEstimatingRainfallIntensities2003,
  title = {Estimating {{Rainfall Intensities}} from {{Weather Radar Data}}: {{The Scale-Dependency Problem}}},
  shorttitle = {Estimating {{Rainfall Intensities}} from {{Weather Radar Data}}},
  author = {Morin, Efrat and Krajewski, Witold F. and Goodrich, David C. and Gao, Xiaogang and Sorooshian, Soroosh},
  year = {2003},
  month = oct,
  journal = {Journal of Hydrometeorology},
  volume = {4},
  number = {5},
  pages = {782--797},
  issn = {1525-755X, 1525-7541},
  doi = {10.1175/1525-7541(2003)004<0782:ERIFWR>2.0.CO;2},
  urldate = {2019-06-10},
  abstract = {Meteorological radar is a remote sensing system that provides rainfall estimations at high spatial and temporal resolutions. The radar-based rainfall intensities (R) are calculated from the observed radar reflectivities (Z ). Often, rain gauge rainfall observations are used in combination with the radar data to find the optimal parameters in the Z--R transformation equation. The scale dependency of the power-law Z--R parameters when estimated from radar reflectivity and rain gauge intensity data is explored herein. The multiplicative (a) and exponent (b) parameters are said to be ``scale dependent'' if applying the observed and calculated rainfall intensities to objective function at different scale results in different ``optimal'' parameters. Radar and gauge data were analyzed from convective storms over a midsize, semiarid, and well-equipped watershed. Using the root-mean-square difference (rmsd) objective function, a significant scale dependency was observed. Increased time- and space scales resulted in a considerable increase of the a parameter and decrease of the b parameter. Two sources of uncertainties related to scale dependency were examined: 1) observational uncertainties, which were studied both experimentally and with simplified models that allow representation of observation errors; and 2) model uncertainties. It was found that observational errors are mainly (but not only) associated with positive bias of the b parameter that is reduced with integration, at least for small scales. Model errors also result in scale dependency, but the trend is less systematic, as in the case of observational errors. It is concluded that identification of optimal scale for Z--R relationship determination requires further knowledge of reflectivity and rainintensity error structure.},
  langid = {english}
}

@article{morinEstimatingRainfallIntensities2003a,
  title = {Estimating {{Rainfall Intensities}} from {{Weather Radar Data}}: {{The Scale-Dependency Problem}}},
  shorttitle = {Estimating {{Rainfall Intensities}} from {{Weather Radar Data}}},
  author = {Morin, Efrat and Krajewski, Witold F. and Goodrich, David C. and Gao, Xiaogang and Sorooshian, Soroosh},
  year = {2003},
  month = oct,
  journal = {Journal of Hydrometeorology},
  volume = {4},
  number = {5},
  pages = {782--797},
  issn = {1525-755X, 1525-7541},
  doi = {10.1175/1525-7541(2003)004<0782:ERIFWR>2.0.CO;2},
  urldate = {2019-06-10},
  abstract = {Meteorological radar is a remote sensing system that provides rainfall estimations at high spatial and temporal resolutions. The radar-based rainfall intensities (R) are calculated from the observed radar reflectivities (Z ). Often, rain gauge rainfall observations are used in combination with the radar data to find the optimal parameters in the Z--R transformation equation. The scale dependency of the power-law Z--R parameters when estimated from radar reflectivity and rain gauge intensity data is explored herein. The multiplicative (a) and exponent (b) parameters are said to be ``scale dependent'' if applying the observed and calculated rainfall intensities to objective function at different scale results in different ``optimal'' parameters. Radar and gauge data were analyzed from convective storms over a midsize, semiarid, and well-equipped watershed. Using the root-mean-square difference (rmsd) objective function, a significant scale dependency was observed. Increased time- and space scales resulted in a considerable increase of the a parameter and decrease of the b parameter. Two sources of uncertainties related to scale dependency were examined: 1) observational uncertainties, which were studied both experimentally and with simplified models that allow representation of observation errors; and 2) model uncertainties. It was found that observational errors are mainly (but not only) associated with positive bias of the b parameter that is reduced with integration, at least for small scales. Model errors also result in scale dependency, but the trend is less systematic, as in the case of observational errors. It is concluded that identification of optimal scale for Z--R relationship determination requires further knowledge of reflectivity and rainintensity error structure.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\IVD7TFCS\Morin et al. - 2003 - Estimating Rainfall Intensities from Weather Radar.pdf}
}

@article{moritaRealTimeExtractionImportant2019,
  title = {Real-{{Time Extraction}} of {{Important Surgical Phases}} in {{Cataract Surgery Videos}}},
  author = {Morita, Shoji and Tabuchi, Hitoshi and Masumoto, Hiroki and Yamauchi, Tomofusa and Kamiura, Naotake},
  year = {2019},
  month = nov,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {1--8},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-53091-8},
  urldate = {2020-05-06},
  abstract = {The present study aimed to conduct a real-time automatic analysis of two important surgical phases, which are continuous curvilinear capsulorrhexis (CCC), nuclear extraction, and three other surgical phases of cataract surgery using artificial intelligence technology. A total of 303 cases of cataract surgery registered in the clinical database of the Ophthalmology Department of Tsukazaki Hospital were used as a dataset. Surgical videos were downsampled to a resolution of 299\,{\texttimes}\,168 at 1 FPS to image each frame. Next, based on the start and end times of each surgical phase recorded by an ophthalmologist, the obtained images were labeled correctly. Using the data, a neural network model, known as InceptionV3, was developed to identify the given surgical phase for each image. Then, the obtained images were processed in chronological order using the neural network model, where the moving average of the output result of five consecutive images was derived. The class with the maximum output value was defined as the surgical phase. For each surgical phase, the time at which a phase was first identified was defined as the start time, and the time at which a phase was last identified was defined as the end time. The performance was evaluated by finding the mean absolute error between the start and end times of each important phase recorded by the ophthalmologist as well as the start and end times determined by the model. The correct response rate of the cataract surgical phase classification was 90.7\% for CCC, 94.5\% for nuclear extraction, and 97.9\% for other phases, with a mean correct response rate of 96.5\%. The errors between each phase's start and end times recorded by the ophthalmologist and those determined by the neural network model were as follows: CCC's start and end times, 3.34\,seconds and 4.43\,seconds, respectively and nuclear extraction's start and end times, 7.21\,seconds and 6.04\,seconds, respectively, with a mean of 5.25\,seconds. The neural network model used in this study was able to perform the classification of the surgical phase by only referring to the last 5\,seconds of video images. Therefore, our method has performed like a real-time classification.},
  copyright = {2019 The Author(s)},
  langid = {english}
}

@article{moritaRealTimeExtractionImportant2019a,
  title = {Real-{{Time Extraction}} of {{Important Surgical Phases}} in {{Cataract Surgery Videos}}},
  author = {Morita, Shoji and Tabuchi, Hitoshi and Masumoto, Hiroki and Yamauchi, Tomofusa and Kamiura, Naotake},
  year = {2019},
  month = nov,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {1--8},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-53091-8},
  urldate = {2020-05-06},
  abstract = {The present study aimed to conduct a real-time automatic analysis of two important surgical phases, which are continuous curvilinear capsulorrhexis (CCC), nuclear extraction, and three other surgical phases of cataract surgery using artificial intelligence technology. A total of 303 cases of cataract surgery registered in the clinical database of the Ophthalmology Department of Tsukazaki Hospital were used as a dataset. Surgical videos were downsampled to a resolution of 299\,{\texttimes}\,168 at 1 FPS to image each frame. Next, based on the start and end times of each surgical phase recorded by an ophthalmologist, the obtained images were labeled correctly. Using the data, a neural network model, known as InceptionV3, was developed to identify the given surgical phase for each image. Then, the obtained images were processed in chronological order using the neural network model, where the moving average of the output result of five consecutive images was derived. The class with the maximum output value was defined as the surgical phase. For each surgical phase, the time at which a phase was first identified was defined as the start time, and the time at which a phase was last identified was defined as the end time. The performance was evaluated by finding the mean absolute error between the start and end times of each important phase recorded by the ophthalmologist as well as the start and end times determined by the model. The correct response rate of the cataract surgical phase classification was 90.7\% for CCC, 94.5\% for nuclear extraction, and 97.9\% for other phases, with a mean correct response rate of 96.5\%. The errors between each phase's start and end times recorded by the ophthalmologist and those determined by the neural network model were as follows: CCC's start and end times, 3.34\,seconds and 4.43\,seconds, respectively and nuclear extraction's start and end times, 7.21\,seconds and 6.04\,seconds, respectively, with a mean of 5.25\,seconds. The neural network model used in this study was able to perform the classification of the surgical phase by only referring to the last 5\,seconds of video images. Therefore, our method has performed like a real-time classification.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\BGF8KN6B\\Morita et al. - 2019 - Real-Time Extraction of Important Surgical Phases .pdf;C\:\\Users\\cleme\\Zotero\\storage\\DQSTCC4N\\s41598-019-53091-8.html}
}

@article{muchuchutiRetinalDiseaseDetection2023,
  title = {Retinal {{Disease Detection Using Deep Learning Techniques}}: {{A Comprehensive Review}}},
  shorttitle = {Retinal {{Disease Detection Using Deep Learning Techniques}}},
  author = {Muchuchuti, Stewart and Viriri, Serestina},
  year = {2023},
  month = apr,
  journal = {Journal of Imaging},
  volume = {9},
  number = {4},
  pages = {84},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-433X},
  doi = {10.3390/jimaging9040084},
  urldate = {2023-06-28},
  abstract = {Millions of people are affected by retinal abnormalities worldwide. Early detection and treatment of these abnormalities could arrest further progression, saving multitudes from avoidable blindness. Manual disease detection is time-consuming, tedious and lacks repeatability. There have been efforts to automate ocular disease detection, riding on the successes of the application of Deep Convolutional Neural Networks (DCNNs) and vision transformers (ViTs) for Computer-Aided Diagnosis (CAD). These models have performed well, however, there remain challenges owing to the complex nature of retinal lesions. This work reviews the most common retinal pathologies, provides an overview of prevalent imaging modalities and presents a critical evaluation of current deep-learning research for the detection and grading of glaucoma, diabetic retinopathy, Age-Related Macular Degeneration and multiple retinal diseases. The work concluded that CAD, through deep learning, will increasingly be vital as an assistive technology. As future work, there is a need to explore the potential impact of using ensemble CNN architectures in multiclass, multilabel tasks. Efforts should also be expended on the improvement of model explainability to win the trust of clinicians and patients.},
  langid = {english},
  keywords = {convolutional neural networks,deep learning,diabetic retinopathy,glaucoma,hypertensive retinopathy,macula degeneration,retinal disease classification}
}

@article{muchuchutiRetinalDiseaseDetection2023a,
  title = {Retinal {{Disease Detection Using Deep Learning Techniques}}: {{A Comprehensive Review}}},
  shorttitle = {Retinal {{Disease Detection Using Deep Learning Techniques}}},
  author = {Muchuchuti, Stewart and Viriri, Serestina},
  year = {2023},
  month = apr,
  journal = {Journal of Imaging},
  volume = {9},
  number = {4},
  pages = {84},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-433X},
  doi = {10.3390/jimaging9040084},
  urldate = {2023-06-28},
  abstract = {Millions of people are affected by retinal abnormalities worldwide. Early detection and treatment of these abnormalities could arrest further progression, saving multitudes from avoidable blindness. Manual disease detection is time-consuming, tedious and lacks repeatability. There have been efforts to automate ocular disease detection, riding on the successes of the application of Deep Convolutional Neural Networks (DCNNs) and vision transformers (ViTs) for Computer-Aided Diagnosis (CAD). These models have performed well, however, there remain challenges owing to the complex nature of retinal lesions. This work reviews the most common retinal pathologies, provides an overview of prevalent imaging modalities and presents a critical evaluation of current deep-learning research for the detection and grading of glaucoma, diabetic retinopathy, Age-Related Macular Degeneration and multiple retinal diseases. The work concluded that CAD, through deep learning, will increasingly be vital as an assistive technology. As future work, there is a need to explore the potential impact of using ensemble CNN architectures in multiclass, multilabel tasks. Efforts should also be expended on the improvement of model explainability to win the trust of clinicians and patients.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {convolutional neural networks,deep learning,diabetic retinopathy,glaucoma,hypertensive retinopathy,macula degeneration,retinal disease classification},
  file = {C:\Users\cleme\Zotero\storage\BNYCQYJ7\Muchuchuti et Viriri - 2023 - Retinal Disease Detection Using Deep Learning Tech.pdf}
}

@article{muhammadHybridDeepLearning2017,
  title = {Hybrid Deep Learning on Single Wide-Field Optical Coherence Tomography Scans Accurately Classifies Glaucoma Suspects},
  author = {Muhammad, Hassan and Fuchs, Thomas J. and De Cuir, Nicole and De Moraes, Carlos G and Blumberg, Dana M and Liebmann, Jeffrey M and Ritch, Robert and Hood, Donald C.},
  year = {2017},
  month = dec,
  journal = {Journal of glaucoma},
  volume = {26},
  number = {12},
  pages = {1086--1094},
  issn = {1057-0829},
  doi = {10.1097/IJG.0000000000000765},
  urldate = {2019-11-22},
  abstract = {Purpose Existing summary statistics based upon optical coherence tomography (OCT) scans and/or visual fields (VF) are suboptimal for distinguishing between healthy and glaucomatous eyes in the clinic. This study evaluates the extent to which a hybrid deep learning method (HDLM), combined with a single wide-field OCT protocol, can distinguish eyes previously classified as either healthy suspects or mild glaucoma. Patients and Methods 102 eyes from 102 patients, with or suspected open-angle glaucoma, had previously been classified by two glaucoma experts as either glaucomatous (57 eyes) or healthy/suspects (45 eyes). The HDLM had access only to information from a single, wide-field (9{\texttimes}12mm) swept-source OCT scan per patient. Convolutional neural networks were used to extract rich features from maps derived from these scans. Random forest classifier was used to train a model based on these features to predict the existence of glaucomatous damage. The algorithm was compared against traditional OCT and VF metrics. Results The accuracy of the HDLM ranged from 63.7\% to 93.1\% depending upon the input map. The RNFL probability map had the best accuracy (93.1\%), with 4 false positives, and 3 false negatives. In comparison, the accuracy of the OCT and 24-2 and 10-2 VF metrics ranged from 66.7\% to 87.3\%. The OCT quadrants analysis had the best accuracy (87.3\%) of the metrics, with 4 FP and 9 FN. Conclusion The HDLM protocol outperforms standard OCT and VF clinical metrics in distinguishing healthy suspect eyes from eyes with early glaucoma. It should be possible to further improve this algorithm and with improvement it might be useful for screening},
  pmcid = {PMC5716847},
  pmid = {29045329}
}

@article{muhammadHybridDeepLearning2017a,
  title = {Hybrid Deep Learning on Single Wide-Field Optical Coherence Tomography Scans Accurately Classifies Glaucoma Suspects},
  author = {Muhammad, Hassan and Fuchs, Thomas J. and De Cuir, Nicole and De Moraes, Carlos G and Blumberg, Dana M and Liebmann, Jeffrey M and Ritch, Robert and Hood, Donald C.},
  year = {2017},
  month = dec,
  journal = {Journal of glaucoma},
  volume = {26},
  number = {12},
  pages = {1086--1094},
  issn = {1057-0829},
  doi = {10.1097/IJG.0000000000000765},
  urldate = {2019-11-22},
  abstract = {Purpose Existing summary statistics based upon optical coherence tomography (OCT) scans and/or visual fields (VF) are suboptimal for distinguishing between healthy and glaucomatous eyes in the clinic. This study evaluates the extent to which a hybrid deep learning method (HDLM), combined with a single wide-field OCT protocol, can distinguish eyes previously classified as either healthy suspects or mild glaucoma. Patients and Methods 102 eyes from 102 patients, with or suspected open-angle glaucoma, had previously been classified by two glaucoma experts as either glaucomatous (57 eyes) or healthy/suspects (45 eyes). The HDLM had access only to information from a single, wide-field (9{\texttimes}12mm) swept-source OCT scan per patient. Convolutional neural networks were used to extract rich features from maps derived from these scans. Random forest classifier was used to train a model based on these features to predict the existence of glaucomatous damage. The algorithm was compared against traditional OCT and VF metrics. Results The accuracy of the HDLM ranged from 63.7\% to 93.1\% depending upon the input map. The RNFL probability map had the best accuracy (93.1\%), with 4 false positives, and 3 false negatives. In comparison, the accuracy of the OCT and 24-2 and 10-2 VF metrics ranged from 66.7\% to 87.3\%. The OCT quadrants analysis had the best accuracy (87.3\%) of the metrics, with 4 FP and 9 FN. Conclusion The HDLM protocol outperforms standard OCT and VF clinical metrics in distinguishing healthy suspect eyes from eyes with early glaucoma. It should be possible to further improve this algorithm and with improvement it might be useful for screening},
  pmcid = {PMC5716847},
  pmid = {29045329},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\EX35HBJ8\\Muhammad et al. - 2017 - Hybrid deep learning on single wide-field optical .pdf;C\:\\Users\\cleme\\Zotero\\storage\\QYR8NWKU\\muhammad2017.pdf}
}

@techreport{mukherjeeSearchOptimalPreprocessing2021,
  type = {Preprint},
  title = {In {{Search}} for the {{Optimal Preprocessing Technique}} for {{Deep Learning Based Diabetic Retinopathy Stage Classification}} from {{Fundus Images}}},
  author = {Mukherjee, Nilarun and Sengupta, Souvik},
  year = {2021},
  month = aug,
  institution = {In Review},
  doi = {10.21203/rs.3.rs-654484/v1},
  urldate = {2023-03-16},
  abstract = {Background: Diabetic retinopathy (DR) is a complication of diabetes mellitus, which if left untreated may lead to complete vision loss. Early diagnosis and treatment is the key to prevent further complications of DR. Computer-aided diagnosis is a very effective method to support ophthalmologists, as manual inspection of pathological changes in retina images are time consuming and expensive. In recent times, Machine Learning and Deep Learning techniques have subsided conventional rule based approaches for detection, segmentation and classification of DR stages and lesions in fundus images. Method: In this paper, we present a comparative study of the different state-of-the-art preprocessing methods that have been used in deep learning based DR classification tasks in recent times and also propose a new unsupervised learning based retinal region extraction technique and new combinations of preprocessing pipelines designed on top of it. Efficacy of different existing and new combinations of the preprocessing methods are analyzed using two publicly available retinal datasets (EyePACS and APTOS) for different DR stage classification tasks, such as referable DR, DR screening, and five-class DR grading, using a benchmark deep learning model (ResNet-50). Results: It has been observed that the proposed preprocessing strategy composed of region of interest extraction through K-means clustering followed by contrast and edge enhancement using Graham's method and z-score intensity normalization achieved the highest accuracy of 98.5\%, 96.51\% and 90.59\% in DR-screening, referable-DR, and DR gradation tasks respectively and also achieved the best quadratic weighted kappa score of 0.945 in DR grading task. It achieved best AUC-ROC of 0.98 and 0.9981 in DR grading and DR screening tasks respectively. Conclusion: It is evident from the results that the proposed preprocessing pipeline composed of the proposed ROI extraction through K-means clustering, followed by edge and contrast enhancement using Graham's method and then z-score intensity normalization outperforms all other existing preprocessing pipelines and has proven to be the most effective preprocessing strategy in helping the baseline CNN model to extract meaningful deep features.},
  langid = {english}
}

@techreport{mukherjeeSearchOptimalPreprocessing2021a,
  type = {Preprint},
  title = {In {{Search}} for the {{Optimal Preprocessing Technique}} for {{Deep Learning Based Diabetic Retinopathy Stage Classification}} from {{Fundus Images}}},
  author = {Mukherjee, Nilarun and Sengupta, Souvik},
  year = {2021},
  month = aug,
  institution = {In Review},
  doi = {10.21203/rs.3.rs-654484/v1},
  urldate = {2023-03-16},
  abstract = {Background: Diabetic retinopathy (DR) is a complication of diabetes mellitus, which if left untreated may lead to complete vision loss. Early diagnosis and treatment is the key to prevent further complications of DR. Computer-aided diagnosis is a very effective method to support ophthalmologists, as manual inspection of pathological changes in retina images are time consuming and expensive. In recent times, Machine Learning and Deep Learning techniques have subsided conventional rule based approaches for detection, segmentation and classification of DR stages and lesions in fundus images. Method: In this paper, we present a comparative study of the different state-of-the-art preprocessing methods that have been used in deep learning based DR classification tasks in recent times and also propose a new unsupervised learning based retinal region extraction technique and new combinations of preprocessing pipelines designed on top of it. Efficacy of different existing and new combinations of the preprocessing methods are analyzed using two publicly available retinal datasets (EyePACS and APTOS) for different DR stage classification tasks, such as referable DR, DR screening, and five-class DR grading, using a benchmark deep learning model (ResNet-50). Results: It has been observed that the proposed preprocessing strategy composed of region of interest extraction through K-means clustering followed by contrast and edge enhancement using Graham's method and z-score intensity normalization achieved the highest accuracy of 98.5\%, 96.51\% and 90.59\% in DR-screening, referable-DR, and DR gradation tasks respectively and also achieved the best quadratic weighted kappa score of 0.945 in DR grading task. It achieved best AUC-ROC of 0.98 and 0.9981 in DR grading and DR screening tasks respectively. Conclusion: It is evident from the results that the proposed preprocessing pipeline composed of the proposed ROI extraction through K-means clustering, followed by edge and contrast enhancement using Graham's method and then z-score intensity normalization outperforms all other existing preprocessing pipelines and has proven to be the most effective preprocessing strategy in helping the baseline CNN model to extract meaningful deep features.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\2SFMR5Y4\Mukherjee et Sengupta - 2021 - In Search for the Optimal Preprocessing Technique .pdf}
}

@article{muramatsuDetectionRetinalNerve2010,
  title = {Detection of Retinal Nerve Fiber Layer Defects on Retinal Fundus Images for Early Diagnosis of Glaucoma},
  author = {Muramatsu, Chisako and Hayashi, Yoshinori and Sawada, Akira and Hatanaka, Yuji and Hara, Takeshi and M.d, Tetsuya Yamamoto and Fujita, Hiroshi},
  year = {2010},
  month = jan,
  journal = {Journal of Biomedical Optics},
  volume = {15},
  number = {1},
  pages = {016021},
  issn = {1083-3668, 1560-2281},
  doi = {10.1117/1.3322388},
  urldate = {2019-12-02},
  abstract = {Retinal nerve fiber layer defect (NFLD) is a major sign of glaucoma, which is the second leading cause of blindness in the world. Early detection of NFLDs is critical for improved prognosis of this progressive, blinding disease. We have investigated a computerized scheme for detection of NFLDs on retinal fundus images. In this study, 162 images, including 81 images with 99 NFLDs, were used. After major blood vessels were removed, the images were transformed so that the curved paths of retinal nerves become approximately straight on the basis of ellipses, and the Gabor filters were applied for enhancement of NFLDs. Bandlike regions darker than the surrounding pixels were detected as candidates of NFLDs. For each candidate, image features were determined and the likelihood of a true NFLD was determined by using the linear discriminant analysis and an artificial neural network (ANN). The sensitivity for detecting the NFLDs was 91\% at 1.0 false positive per image by using the ANN. The proposed computerized system for the detection of NFLDs can be useful to physicians in the diagnosis of glaucoma in a mass screening.}
}

@article{muramatsuDetectionRetinalNerve2010a,
  title = {Detection of Retinal Nerve Fiber Layer Defects on Retinal Fundus Images for Early Diagnosis of Glaucoma},
  author = {Muramatsu, Chisako and Hayashi, Yoshinori and Sawada, Akira and Hatanaka, Yuji and Hara, Takeshi and M.d, Tetsuya Yamamoto and Fujita, Hiroshi},
  year = {2010},
  month = jan,
  journal = {Journal of Biomedical Optics},
  volume = {15},
  number = {1},
  pages = {016021},
  issn = {1083-3668, 1560-2281},
  doi = {10.1117/1.3322388},
  urldate = {2019-12-02},
  abstract = {Retinal nerve fiber layer defect (NFLD) is a major sign of glaucoma, which is the second leading cause of blindness in the world. Early detection of NFLDs is critical for improved prognosis of this progressive, blinding disease. We have investigated a computerized scheme for detection of NFLDs on retinal fundus images. In this study, 162 images, including 81 images with 99 NFLDs, were used. After major blood vessels were removed, the images were transformed so that the curved paths of retinal nerves become approximately straight on the basis of ellipses, and the Gabor filters were applied for enhancement of NFLDs. Bandlike regions darker than the surrounding pixels were detected as candidates of NFLDs. For each candidate, image features were determined and the likelihood of a true NFLD was determined by using the linear discriminant analysis and an artificial neural network (ANN). The sensitivity for detecting the NFLDs was 91\% at 1.0 false positive per image by using the ANN. The proposed computerized system for the detection of NFLDs can be useful to physicians in the diagnosis of glaucoma in a mass screening.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\3YW3IRZK\\Muramatsu et al. - 2010 - Detection of retinal nerve fiber layer defects on .pdf;C\:\\Users\\cleme\\Zotero\\storage\\LUSQXYQS\\muramatsu2010.pdf;C\:\\Users\\cleme\\Zotero\\storage\\PUM7CIP8\\1.3322388.html}
}

@article{najmanWatershedContinuousFunction1994,
  title = {Watershed of a Continuous Function},
  author = {Najman, Laurent and Schmitt, Michel},
  year = {1994},
  month = jul,
  journal = {Signal Processing},
  series = {Mathematical {{Morphology}} and Its {{Applications}} to {{Signal Processing}}},
  volume = {38},
  number = {1},
  pages = {99--112},
  issn = {0165-1684},
  doi = {10.1016/0165-1684(94)90059-0},
  urldate = {2023-06-23},
  abstract = {The notion of watershed, used in morphological segmentation, has only a digital definition. In this paper, we propose to extend this definition to the continuous plane. Using this continuous definition, we present the watershed differences with classical edge detectors. We then exhibit a metric in the plane for which the watershed is a skeleton by influence zones and show the lower semicontinuous behaviour of the associated skeleton. This theoretical approach suggests an algorithm for solving the eikonal equation: {\textbardbl}{\textbardbl} = g. Finally, we end with some new watershed algorithms, which present the advantage of allowing the use of markers and/or anchor points, thus opening the way towards grey-tone skeletons. Zusammenfassung Der Begriff Wassersheide, der in der morphologischen Segmentation verwendet wird, hat nur eine diskrete Definition. In diesem Artikel schlagen wir vor, diese Definition auf die kontinuierlich Ebene auszudehnen. Indem wir die kontinuierliche Definition verwenden, stellen wird die Unterschiede zwischen Wasserscheide und klassischen Kantendetektoren vor. Wir zeigen dann eine Metrik in der Ebene, f{\"u}r die die Wasserscheide ein Skelett von Einflu{$\beta$}zonen ist und zeigen das untergeordnete halbkontinuierliche Verhalten von damit verbunden Skeletten. Dieser theoretische Ansatz schl{\"a}gt einen Algorithmus f{\"u}r die L{\"o}sung der Eikonal-Gleichung {\textbardbl}{\textbardbl} = g vor. Schlie{$\beta$}lich gelangen wir zu einem neuen Wasserscheiden-Algorithmus, der den Vorteil hat, die Benutzung von Markierungen und/oder Ankerpunkten zu erlauben und daher den Weg zu Graustufenskeletten {\"o}ffnet. R{\'e}sum{\'e} La notion de ligne de partage des eaux, utilis{\'e}e en segmentation morphologique dispose uniquement d'une d{\'e}finition digitale. Dans cet article, nous proposons d'{\'e}tendre la d{\'e}finition de la ligne de partage des eaux au plan continu. En utilisant cette d{\'e}finition continue, nous comparons la ligne de partage des eaux avec les extracteurs de contours classiques, et montrons leurs diff{\'e}rences. Nous introduisons ensuite une m{\'e}trique pour laquelle la ligne de partage des eaux est un squelette par zones d'influence, ce qui nous permet de montrer son comportement semi-continu. Cette approche th{\'e}orique nous sugg{\`e}re un nouvel algorithme pour r{\'e}soudre l'{\'e}quation eikonal: trouver f telle que {\textbardbl}{\textbardbl} = g.},
  langid = {english}
}

@article{najmanWatershedContinuousFunction1994a,
  title = {Watershed of a Continuous Function},
  author = {Najman, Laurent and Schmitt, Michel},
  year = {1994},
  month = jul,
  journal = {Signal Processing},
  series = {Mathematical {{Morphology}} and Its {{Applications}} to {{Signal Processing}}},
  volume = {38},
  number = {1},
  pages = {99--112},
  issn = {0165-1684},
  doi = {10.1016/0165-1684(94)90059-0},
  urldate = {2023-06-23},
  abstract = {The notion of watershed, used in morphological segmentation, has only a digital definition. In this paper, we propose to extend this definition to the continuous plane. Using this continuous definition, we present the watershed differences with classical edge detectors. We then exhibit a metric in the plane for which the watershed is a skeleton by influence zones and show the lower semicontinuous behaviour of the associated skeleton. This theoretical approach suggests an algorithm for solving the eikonal equation: {\textbardbl}∇{\textflorin}{\textbardbl} = g. Finally, we end with some new watershed algorithms, which present the advantage of allowing the use of markers and/or anchor points, thus opening the way towards grey-tone skeletons. Zusammenfassung Der Begriff Wassersheide, der in der morphologischen Segmentation verwendet wird, hat nur eine diskrete Definition. In diesem Artikel schlagen wir vor, diese Definition auf die kontinuierlich Ebene auszudehnen. Indem wir die kontinuierliche Definition verwenden, stellen wird die Unterschiede zwischen Wasserscheide und klassischen Kantendetektoren vor. Wir zeigen dann eine Metrik in der Ebene, f{\"u}r die die Wasserscheide ein Skelett von Einflu{$\beta$}zonen ist und zeigen das untergeordnete halbkontinuierliche Verhalten von damit verbunden Skeletten. Dieser theoretische Ansatz schl{\"a}gt einen Algorithmus f{\"u}r die L{\"o}sung der Eikonal-Gleichung {\textbardbl}∇{\textflorin}{\textbardbl} = g vor. Schlie{$\beta$}lich gelangen wir zu einem neuen Wasserscheiden-Algorithmus, der den Vorteil hat, die Benutzung von Markierungen und/oder Ankerpunkten zu erlauben und daher den Weg zu Graustufenskeletten {\"o}ffnet. R{\'e}sum{\'e} La notion de ligne de partage des eaux, utilis{\'e}e en segmentation morphologique dispose uniquement d'une d{\'e}finition digitale. Dans cet article, nous proposons d'{\'e}tendre la d{\'e}finition de la ligne de partage des eaux au plan continu. En utilisant cette d{\'e}finition continue, nous comparons la ligne de partage des eaux avec les extracteurs de contours classiques, et montrons leurs diff{\'e}rences. Nous introduisons ensuite une m{\'e}trique pour laquelle la ligne de partage des eaux est un squelette par zones d'influence, ce qui nous permet de montrer son comportement semi-continu. Cette approche th{\'e}orique nous sugg{\`e}re un nouvel algorithme pour r{\'e}soudre l'{\'e}quation eikonal: trouver {\textflorin} telle que {\textbardbl}∇{\textflorin}{\textbardbl} = g.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\YHAXZ7TH\\Najman et Schmitt - 1994 - Watershed of a continuous function.pdf;C\:\\Users\\cleme\\Zotero\\storage\\48XYQWHA\\0165168494900590.html}
}

@article{namaziLapToolNetContextualDetector2019,
  title = {{{LapTool-Net}}: {{A Contextual Detector}} of {{Surgical Tools}} in {{Laparoscopic Videos Based}} on {{Recurrent Convolutional Neural Networks}}},
  shorttitle = {{{LapTool-Net}}},
  author = {Namazi, Babak and Sankaranarayanan, Ganesh and Devarajan, Venkat},
  year = {2019},
  month = may,
  journal = {arXiv:1905.08983 [cs]},
  eprint = {1905.08983},
  primaryclass = {cs},
  urldate = {2020-05-06},
  abstract = {We propose a new multilabel classifier, called LapTool-Net to detect the presence of surgical tools in each frame of a laparoscopic video. The novelty of LapTool-Net is the exploitation of the correlations among the usage of different tools and, the tools and tasks - i.e., the context of the tools' usage. Towards this goal, the pattern in the co-occurrence of the tools is utilized for designing a decision policy for a multilabel classifier based on a Recurrent Convolutional Neural Network (RCNN) architecture to simultaneously extract the spatio-temporal features. In contrast to the previous multilabel classification methods, the RCNN and the decision model are trained in an end-to-end manner using a multi-task learning scheme. To overcome the high imbalance and avoid overfitting caused by the lack of variety in the training data, a high down-sampling rate is chosen based on the more frequent combinations. Furthermore, at the post-processing step, the predictions for all the frames of a video are corrected by designing a bi-directional RNN to model the long-term tasks' order. LapTool-Net was trained using a publicly available dataset of laparoscopic cholecystectomy. The results show LapTool-Net outperformed existing methods significantly, even while using fewer training samples and a shallower architecture.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{namaziLapToolNetContextualDetector2019a,
  title = {{{LapTool-Net}}: {{A Contextual Detector}} of {{Surgical Tools}} in {{Laparoscopic Videos Based}} on {{Recurrent Convolutional Neural Networks}}},
  shorttitle = {{{LapTool-Net}}},
  author = {Namazi, Babak and Sankaranarayanan, Ganesh and Devarajan, Venkat},
  year = {2019},
  month = may,
  journal = {arXiv:1905.08983 [cs]},
  eprint = {1905.08983},
  primaryclass = {cs},
  urldate = {2020-05-06},
  abstract = {We propose a new multilabel classifier, called LapTool-Net to detect the presence of surgical tools in each frame of a laparoscopic video. The novelty of LapTool-Net is the exploitation of the correlations among the usage of different tools and, the tools and tasks - i.e., the context of the tools' usage. Towards this goal, the pattern in the co-occurrence of the tools is utilized for designing a decision policy for a multilabel classifier based on a Recurrent Convolutional Neural Network (RCNN) architecture to simultaneously extract the spatio-temporal features. In contrast to the previous multilabel classification methods, the RCNN and the decision model are trained in an end-to-end manner using a multi-task learning scheme. To overcome the high imbalance and avoid overfitting caused by the lack of variety in the training data, a high down-sampling rate is chosen based on the more frequent combinations. Furthermore, at the post-processing step, the predictions for all the frames of a video are corrected by designing a bi-directional RNN to model the long-term tasks' order. LapTool-Net was trained using a publicly available dataset of laparoscopic cholecystectomy. The results show LapTool-Net outperformed existing methods significantly, even while using fewer training samples and a shallower architecture.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\FQKPBXXE\Namazi et al. - 2019 - LapTool-Net A Contextual Detector of Surgical Too.pdf}
}

@article{nautaAnecdotalEvidenceQuantitative2023,
  title = {From {{Anecdotal Evidence}} to {{Quantitative Evaluation Methods}}: {{A Systematic Review}} on {{Evaluating Explainable AI}}},
  shorttitle = {From {{Anecdotal Evidence}} to {{Quantitative Evaluation Methods}}},
  author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl{\"o}tterer, J{\"o}rg and {van Keulen}, Maurice and Seifert, Christin},
  year = {2023},
  month = feb,
  journal = {ACM Computing Surveys},
  issn = {0360-0300},
  doi = {10.1145/3583558},
  urldate = {2023-05-17},
  abstract = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the last 7 years at major AI and ML conferences that introduce an XAI method. We find that 1 in 3 papers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training in order to optimize for accuracy and interpretability simultaneously.},
  keywords = {evaluation,explainability,explainable AI,explainable artificial intelligence,interpretability,interpretable machine learning,quantitative evaluation methods,XAI}
}

@article{nautaAnecdotalEvidenceQuantitative2023a,
  title = {From {{Anecdotal Evidence}} to {{Quantitative Evaluation Methods}}: {{A Systematic Review}} on {{Evaluating Explainable AI}}},
  shorttitle = {From {{Anecdotal Evidence}} to {{Quantitative Evaluation Methods}}},
  author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl{\"o}tterer, J{\"o}rg and {van Keulen}, Maurice and Seifert, Christin},
  year = {2023},
  month = feb,
  journal = {ACM Computing Surveys},
  issn = {0360-0300},
  doi = {10.1145/3583558},
  urldate = {2023-05-17},
  abstract = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the last 7 years at major AI and ML conferences that introduce an XAI method. We find that 1 in 3 papers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training in order to optimize for accuracy and interpretability simultaneously.},
  keywords = {evaluation,explainability,explainable AI,explainable artificial intelligence,interpretability,interpretable machine learning,quantitative evaluation methods,XAI},
  annotation = {Just Accepted},
  file = {C:\Users\cleme\Zotero\storage\UV8ZPWUW\Nauta et al. - 2023 - From Anecdotal Evidence to Quantitative Evaluation.pdf}
}

@article{nayakAutomaticIdentificationDiabetic2009,
  title = {Automatic Identification of Diabetic Maculopathy Stages Using Fundus Images},
  author = {Nayak, J. and Bhat, P. S. and Acharya, U. R.},
  year = {2009},
  month = jan,
  journal = {Journal of Medical Engineering \& Technology},
  volume = {33},
  number = {2},
  pages = {119--129},
  issn = {0309-1902},
  doi = {10.1080/03091900701349602},
  urldate = {2019-11-19},
  abstract = {Diabetes mellitus is a major cause of visual impairment and blindness. Twenty years after the onset of diabetes, almost all patients with type 1 diabetes and over 60\% of patients with type 2 diabetes will have some degree of retinopathy. Prolonged diabetes retinopathy leads to maculopathy, which impairs the normal vision depending on the severity of damage of the macula. This paper presents a computer-based intelligent system for the identification of clinically significant maculopathy, non-clinically significant maculopathy and normal fundus eye images. Features are extracted from these raw fundus images which are then fed to the classifier. Our protocol uses feed-forward architecture in an artificial neural network classifier for classification of different stages. Three different kinds of eye disease conditions were tested in 350 subjects. We demonstrated a sensitivity of more than 95\% for these classifiers with a specificity of 100\%, and results are very promising. Our systems are ready to run clinically on large amounts of datasets.},
  keywords = {Closing,Feed-forward,Fundus,Maculopathy,Neural network,Opening,Retinopathy}
}

@article{nayakAutomaticIdentificationDiabetic2009a,
  title = {Automatic Identification of Diabetic Maculopathy Stages Using Fundus Images},
  author = {Nayak, J. and Bhat, P. S. and Acharya, U. R.},
  year = {2009},
  month = jan,
  journal = {Journal of Medical Engineering \& Technology},
  volume = {33},
  number = {2},
  pages = {119--129},
  issn = {0309-1902},
  doi = {10.1080/03091900701349602},
  urldate = {2019-11-19},
  abstract = {Diabetes mellitus is a major cause of visual impairment and blindness. Twenty years after the onset of diabetes, almost all patients with type 1 diabetes and over 60\% of patients with type 2 diabetes will have some degree of retinopathy. Prolonged diabetes retinopathy leads to maculopathy, which impairs the normal vision depending on the severity of damage of the macula. This paper presents a computer-based intelligent system for the identification of clinically significant maculopathy, non-clinically significant maculopathy and normal fundus eye images. Features are extracted from these raw fundus images which are then fed to the classifier. Our protocol uses feed-forward architecture in an artificial neural network classifier for classification of different stages. Three different kinds of eye disease conditions were tested in 350 subjects. We demonstrated a sensitivity of more than 95\% for these classifiers with a specificity of 100\%, and results are very promising. Our systems are ready to run clinically on large amounts of datasets.},
  keywords = {Closing,Feed-forward,Fundus,Maculopathy,Neural network,Opening,Retinopathy},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\6ZQ8EP9I\\nayak2009.pdf;C\:\\Users\\cleme\\Zotero\\storage\\LYNJGEJH\\03091900701349602.html}
}

@article{nesperOCTAngiographyVisiblelight2017,
  title = {{{OCT}} Angiography and Visible-Light {{OCT}} in Diabetic Retinopathy},
  author = {Nesper, Peter L. and Soetikno, Brian T. and Zhang, Hao F. and Fawzi, Amani A.},
  year = {2017},
  month = oct,
  journal = {Vision Research},
  series = {Diabetic {{Retinopathy}} - an {{Overview}}},
  volume = {139},
  pages = {191--203},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2017.05.006},
  urldate = {2022-06-27},
  abstract = {In recent years, advances in optical coherence tomography (OCT) techniques have increased our understanding of diabetic retinopathy, an important microvascular complication of diabetes. OCT angiography is a non-invasive method that visualizes the retinal vasculature by detecting motion contrast from flowing blood. Visible-light OCT shows promise as a novel technique for quantifying retinal hypoxia by measuring the retinal oxygen delivery and metabolic rates. In this article, we discuss recent insights provided by these techniques into the vascular pathophysiology of diabetic retinopathy. The next milestones for these modalities are large multicenter studies to establish consensus on the most reliable and consistent outcome parameters to study diabetic retinopathy.},
  langid = {english},
  keywords = {Diabetic retinopathy,Imaging,OCT,OCT angiography,Retina,Visible-light OCT}
}

@article{nesperOCTAngiographyVisiblelight2017a,
  title = {{{OCT}} Angiography and Visible-Light {{OCT}} in Diabetic Retinopathy},
  author = {Nesper, Peter L. and Soetikno, Brian T. and Zhang, Hao F. and Fawzi, Amani A.},
  year = {2017},
  month = oct,
  journal = {Vision Research},
  series = {Diabetic {{Retinopathy}} - an {{Overview}}},
  volume = {139},
  pages = {191--203},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2017.05.006},
  urldate = {2022-06-27},
  abstract = {In recent years, advances in optical coherence tomography (OCT) techniques have increased our understanding of diabetic retinopathy, an important microvascular complication of diabetes. OCT angiography is a non-invasive method that visualizes the retinal vasculature by detecting motion contrast from flowing blood. Visible-light OCT shows promise as a novel technique for quantifying retinal hypoxia by measuring the retinal oxygen delivery and metabolic rates. In this article, we discuss recent insights provided by these techniques into the vascular pathophysiology of diabetic retinopathy. The next milestones for these modalities are large multicenter studies to establish consensus on the most reliable and consistent outcome parameters to study diabetic retinopathy.},
  langid = {english},
  keywords = {Diabetic retinopathy,Imaging,OCT,OCT angiography,Retina,Visible-light OCT},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\GJSJV993\\Nesper et al. - 2017 - OCT angiography and visible-light OCT in diabetic .pdf;C\:\\Users\\cleme\\Zotero\\storage\\NG9VD85L\\S0042698917300950.html}
}

@article{neyshaburExploringGeneralizationDeep,
  title = {Exploring {{Generalization}} in {{Deep Learning}}},
  author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
  abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
  langid = {english}
}

@article{neyshaburExploringGeneralizationDeepa,
  title = {Exploring {{Generalization}} in {{Deep Learning}}},
  author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
  abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\MKHBD6UR\Neyshabur et al. - Exploring Generalization in Deep Learning.pdf}
}

@article{nguyenEffectiveRetinalBlood2013,
  title = {An Effective Retinal Blood Vessel Segmentation Method Using Multi-Scale Line Detection},
  author = {Nguyen, Uyen T. V. and Bhuiyan, Alauddin and Park, Laurence A. F. and Ramamohanarao, Kotagiri},
  year = {2013},
  month = mar,
  journal = {Pattern Recognition},
  volume = {46},
  number = {3},
  pages = {703--715},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2012.08.009},
  urldate = {2019-11-19},
  abstract = {Changes in retinal blood vessel features are precursors of serious diseases such as cardiovascular disease and stroke. Therefore, analysis of retinal vascular features can assist in detecting these changes and allow the patient to take action while the disease is still in its early stages. Automation of this process would help to reduce the cost associated with trained graders and remove the issue of inconsistency introduced by manual grading. Among different retinal analysis tasks, retinal blood vessel extraction plays an extremely important role as it is the first essential step before any measurement can be made. In this paper, we present an effective method for automatically extracting blood vessels from colour retinal images. The proposed method is based on the fact that by changing the length of a basic line detector, line detectors at varying scales are achieved. To maintain the strength and eliminate the drawbacks of each individual line detector, the line responses at varying scales are linearly combined to produce the final segmentation for each retinal image. The performance of the proposed method was evaluated both quantitatively and qualitatively on three publicly available DRIVE, STARE, and REVIEW datasets. On DRIVE and STARE datasets, the proposed method achieves high local accuracy (a measure to assess the accuracy at regions around the vessels) while retaining comparable accuracy compared to other existing methods. Visual inspection on the segmentation results shows that the proposed method produces accurate segmentation on central reflex vessels while keeping close vessels well separated. On REVIEW dataset, the vessel width measurements obtained using the segmentations produced by the proposed method are highly accurate and close to the measurements provided by the experts. This has demonstrated the high segmentation accuracy of the proposed method and its applicability for automatic vascular calibre measurement. Other advantages of the proposed method include its efficiency with fast segmentation time, its simplicity and scalability to deal with high resolution retinal images.},
  langid = {english},
  keywords = {Central reflex,Line detector,Retinal image,Vessel extraction}
}

@article{nguyenEffectiveRetinalBlood2013a,
  title = {An Effective Retinal Blood Vessel Segmentation Method Using Multi-Scale Line Detection},
  author = {Nguyen, Uyen T. V. and Bhuiyan, Alauddin and Park, Laurence A. F. and Ramamohanarao, Kotagiri},
  year = {2013},
  month = mar,
  journal = {Pattern Recognition},
  volume = {46},
  number = {3},
  pages = {703--715},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2012.08.009},
  urldate = {2019-11-19},
  abstract = {Changes in retinal blood vessel features are precursors of serious diseases such as cardiovascular disease and stroke. Therefore, analysis of retinal vascular features can assist in detecting these changes and allow the patient to take action while the disease is still in its early stages. Automation of this process would help to reduce the cost associated with trained graders and remove the issue of inconsistency introduced by manual grading. Among different retinal analysis tasks, retinal blood vessel extraction plays an extremely important role as it is the first essential step before any measurement can be made. In this paper, we present an effective method for automatically extracting blood vessels from colour retinal images. The proposed method is based on the fact that by changing the length of a basic line detector, line detectors at varying scales are achieved. To maintain the strength and eliminate the drawbacks of each individual line detector, the line responses at varying scales are linearly combined to produce the final segmentation for each retinal image. The performance of the proposed method was evaluated both quantitatively and qualitatively on three publicly available DRIVE, STARE, and REVIEW datasets. On DRIVE and STARE datasets, the proposed method achieves high local accuracy (a measure to assess the accuracy at regions around the vessels) while retaining comparable accuracy compared to other existing methods. Visual inspection on the segmentation results shows that the proposed method produces accurate segmentation on central reflex vessels while keeping close vessels well separated. On REVIEW dataset, the vessel width measurements obtained using the segmentations produced by the proposed method are highly accurate and close to the measurements provided by the experts. This has demonstrated the high segmentation accuracy of the proposed method and its applicability for automatic vascular calibre measurement. Other advantages of the proposed method include its efficiency with fast segmentation time, its simplicity and scalability to deal with high resolution retinal images.},
  langid = {english},
  keywords = {Central reflex,Line detector,Retinal image,Vessel extraction},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\INXKH6AL\\Nguyen et al. - 2013 - An effective retinal blood vessel segmentation met.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RKUWIPM5\\nguyen2013.pdf;C\:\\Users\\cleme\\Zotero\\storage\\796QRVXA\\S003132031200355X.html}
}

@article{niemeijerAutomatedMeasurementArteriolartovenular2011,
  title = {Automated Measurement of the Arteriolar-to-Venular Width Ratio in Digital Color Fundus Photographs},
  author = {Niemeijer, Meindert and Xu, Xiayu and Dumitrescu, Alina V. and Gupta, Priya and {van Ginneken}, Bram and Folk, James C. and Abramoff, Michael D.},
  year = {2011},
  month = nov,
  journal = {IEEE transactions on medical imaging},
  volume = {30},
  number = {11},
  pages = {1941--1950},
  issn = {1558-254X},
  doi = {10.1109/TMI.2011.2159619},
  abstract = {A decreased ratio of the width of retinal arteries to veins [arteriolar-to-venular diameter ratio (AVR)], is well established as predictive of cerebral atrophy, stroke and other cardiovascular events in adults. Tortuous and dilated arteries and veins, as well as decreased AVR are also markers for plus disease in retinopathy of prematurity. This work presents an automated method to estimate the AVR in retinal color images by detecting the location of the optic disc, determining an appropriate region of interest (ROI), classifying vessels as arteries or veins, estimating vessel widths, and calculating the AVR. After vessel segmentation and vessel width determination, the optic disc is located and the system eliminates all vessels outside the AVR measurement ROI. A skeletonization operation is applied to the remaining vessels after which vessel crossings and bifurcation points are removed, leaving a set of vessel segments consisting of only vessel centerline pixels. Features are extracted from each centerline pixel in order to assign these a soft label indicating the likelihood that the pixel is part of a vein. As all centerline pixels in a connected vessel segment should be the same type, the median soft label is assigned to each centerline pixel in the segment. Next, artery vein pairs are matched using an iterative algorithm, and the widths of the vessels are used to calculate the AVR. We trained and tested the algorithm on a set of 65 high resolution digital color fundus photographs using a reference standard that indicates for each major vessel in the image whether it is an artery or vein. We compared the AVR values produced by our system with those determined by a semi-automated reference system. We obtained a mean unsigned error of 0.06 (SD 0.04) in 40 images with a mean AVR of 0.67. A second observer using the semi-automated system obtained the same mean unsigned error of 0.06 (SD 0.05) on the set of images with a mean AVR of 0.66. The testing data and reference standard used in this study has been made publicly available.},
  langid = {english},
  pmid = {21690008},
  keywords = {Adult,Color,Computer-Assisted,Diabetic Retinopathy,Fundus Oculi,Humans,Hypertensive Retinopathy,Optic Disk,Photography,Radiographic Image Interpretation,Reference Standards,Retina,Retinal Artery,Retinal Vein,Retinal Vessels,Vascular Resistance}
}

@article{niemeijerAutomatedMeasurementArteriolartovenular2011a,
  title = {Automated Measurement of the Arteriolar-to-Venular Width Ratio in Digital Color Fundus Photographs},
  author = {Niemeijer, Meindert and Xu, Xiayu and Dumitrescu, Alina V. and Gupta, Priya and {van Ginneken}, Bram and Folk, James C. and Abramoff, Michael D.},
  year = {2011},
  month = nov,
  journal = {IEEE transactions on medical imaging},
  volume = {30},
  number = {11},
  pages = {1941--1950},
  issn = {1558-254X},
  doi = {10.1109/TMI.2011.2159619},
  abstract = {A decreased ratio of the width of retinal arteries to veins [arteriolar-to-venular diameter ratio (AVR)], is well established as predictive of cerebral atrophy, stroke and other cardiovascular events in adults. Tortuous and dilated arteries and veins, as well as decreased AVR are also markers for plus disease in retinopathy of prematurity. This work presents an automated method to estimate the AVR in retinal color images by detecting the location of the optic disc, determining an appropriate region of interest (ROI), classifying vessels as arteries or veins, estimating vessel widths, and calculating the AVR. After vessel segmentation and vessel width determination, the optic disc is located and the system eliminates all vessels outside the AVR measurement ROI. A skeletonization operation is applied to the remaining vessels after which vessel crossings and bifurcation points are removed, leaving a set of vessel segments consisting of only vessel centerline pixels. Features are extracted from each centerline pixel in order to assign these a soft label indicating the likelihood that the pixel is part of a vein. As all centerline pixels in a connected vessel segment should be the same type, the median soft label is assigned to each centerline pixel in the segment. Next, artery vein pairs are matched using an iterative algorithm, and the widths of the vessels are used to calculate the AVR. We trained and tested the algorithm on a set of 65 high resolution digital color fundus photographs using a reference standard that indicates for each major vessel in the image whether it is an artery or vein. We compared the AVR values produced by our system with those determined by a semi-automated reference system. We obtained a mean unsigned error of 0.06 (SD 0.04) in 40 images with a mean AVR of 0.67. A second observer using the semi-automated system obtained the same mean unsigned error of 0.06 (SD 0.05) on the set of images with a mean AVR of 0.66. The testing data and reference standard used in this study has been made publicly available.},
  langid = {english},
  pmid = {21690008},
  keywords = {Adult,Color,Diabetic Retinopathy,Fundus Oculi,Humans,Hypertensive Retinopathy,Optic Disk,Photography,Radiographic Image Interpretation Computer-Assisted,Reference Standards,Retina,Retinal Artery,Retinal Vein,Retinal Vessels,Vascular Resistance},
  file = {C:\Users\cleme\Zotero\storage\NSA2J6BK\niemeijer2011.html}
}

@article{niemeijerRetinopathyOnlineChallenge2010,
  title = {Retinopathy {{Online Challenge}}: {{Automatic Detection}} of {{Microaneurysms}} in {{Digital Color Fundus Photographs}}},
  shorttitle = {Retinopathy {{Online Challenge}}},
  author = {Niemeijer, M. and {van Ginneken}, B. and Cree, M. J. and Mizutani, A. and Quellec, G. and Sanchez, C. I. and Zhang, B. and Hornero, R. and Lamard, M. and Muramatsu, C. and Wu, X. and Cazuguel, G. and You, J. and Mayo, A. and Li, Q. and Hatanaka, Y. and Cochener, B. and Roux, C. and Karray, F. and Garcia, M. and Fujita, H. and Abramoff, M. D.},
  year = {2010},
  month = jan,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {29},
  number = {1},
  pages = {185--195},
  doi = {10.1109/TMI.2009.2033909},
  abstract = {The detection of microaneurysms in digital color fundus photographs is a critical first step in automated screening for diabetic retinopathy (DR), a common complication of diabetes. To accomplish this detection numerous methods have been published in the past but none of these was compared with each other on the same data. In this work we present the results of the first international microaneurysm detection competition, organized in the context of the Retinopathy Online Challenge (ROC), a multiyear online competition for various aspects of DR detection. For this competition, we compare the results of five different methods, produced by five different teams of researchers on the same set of data. The evaluation was performed in a uniform manner using an algorithm presented in this work. The set of data used for the competition consisted of 50 training images with available reference standard and 50 test images where the reference standard was withheld by the organizers (M. Niemeijer, B. van Ginneken, and M. D. AbrA{\textquestiondown}moff). The results obtained on the test data was submitted through a website after which standardized evaluation software was used to determine the performance of each of the methods. A human expert detected microaneurysms in the test set to allow comparison with the performance of the automatic methods. The overall results show that microaneurysm detection is a challenging task for both the automatic methods as well as the human expert. There is room for improvement as the best performing system does not reach the performance of the human expert. The data associated with the ROC microaneurysm detection competition will remain publicly available and the website will continue accepting submissions.},
  keywords = {Algorithms,Aneurysm,automated screening,automatic microaneurysm detection,Bayes Theorem,Biomedical engineering,Biomedical imaging,Blindness,Cities and towns,Computer aided detection,computer aided diagnosis,Databases,Diabetes,diabetic retinopathy,Diagnostic Techniques,digital color fundus photograph,digital photography,diseases,eye,Factual,False Positive Reactions,Fundus Oculi,fundus photographs,Humans,medical image processing,microaneurysms,multiyear online competition,Ophthalmological,Photography,retina,Retinal Diseases,Retinal Vessels,Retinopathy,Retinopathy Online Challenge,Retinopathy Online Challenge (ROC) competition,ROC Curve,Statistics,Systems engineering and theory,Telecommunications,USA Councils}
}

@article{niemeijerRetinopathyOnlineChallenge2010a,
  title = {Retinopathy {{Online Challenge}}: {{Automatic Detection}} of {{Microaneurysms}} in {{Digital Color Fundus Photographs}}},
  shorttitle = {Retinopathy {{Online Challenge}}},
  author = {Niemeijer, Meindert and {van Ginneken}, Bram and Cree, Michael J. and Mizutani, Atsushi and Quellec, Gw{\'E}nol{\'E} and Sanchez, Clara I. and Zhang, Bob and Hornero, Roberto and Lamard, Mathieu and Muramatsu, Chisako and Wu, Xiangqian and Cazuguel, Guy and You, Jane and Mayo, Agust{\'I}n and Li, Qin and Hatanaka, Yuji and Cochener, B{\'e}atrice and Roux, Christian and Karray, Fakhri and Garcia, Mar{\'I}a and Fujita, Hiroshi and Abramoff, Michael D.},
  year = {2010},
  month = jan,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {29},
  number = {1},
  pages = {185--195},
  issn = {1558-254X},
  doi = {10.1109/TMI.2009.2033909},
  abstract = {The detection of microaneurysms in digital color fundus photographs is a critical first step in automated screening for diabetic retinopathy (DR), a common complication of diabetes. To accomplish this detection numerous methods have been published in the past but none of these was compared with each other on the same data. In this work we present the results of the first international microaneurysm detection competition, organized in the context of the Retinopathy Online Challenge (ROC), a multiyear online competition for various aspects of DR detection. For this competition, we compare the results of five different methods, produced by five different teams of researchers on the same set of data. The evaluation was performed in a uniform manner using an algorithm presented in this work. The set of data used for the competition consisted of 50 training images with available reference standard and 50 test images where the reference standard was withheld by the organizers (M. Niemeijer, B. van Ginneken, and M. D. AbrA{\textquestiondown}moff). The results obtained on the test data was submitted through a website after which standardized evaluation software was used to determine the performance of each of the methods. A human expert detected microaneurysms in the test set to allow comparison with the performance of the automatic methods. The overall results show that microaneurysm detection is a challenging task for both the automatic methods as well as the human expert. There is room for improvement as the best performing system does not reach the performance of the human expert. The data associated with the ROC microaneurysm detection competition will remain publicly available and the website will continue accepting submissions.},
  keywords = {Algorithms,Aneurysm,automated screening,automatic microaneurysm detection,Bayes Theorem,Biomedical engineering,Biomedical imaging,Blindness,Cities and towns,Computer aided detection,computer aided diagnosis,Databases,Diabetes,diabetic retinopathy,Diagnostic Techniques,digital color fundus photograph,digital photography,diseases,eye,Factual,False Positive Reactions,Fundus Oculi,fundus photographs,Humans,medical image processing,microaneurysms,multiyear online competition,Ophthalmological,Photography,retina,Retinal Diseases,Retinal Vessels,Retinopathy,Retinopathy Online Challenge,Retinopathy Online Challenge (ROC) competition,ROC Curve,Statistics,Systems engineering and theory,Telecommunications,USA Councils}
}

@article{niemeijerRetinopathyOnlineChallenge2010b,
  title = {Retinopathy Online Challenge: {{Automatic}} Detection of Microaneurysms in Digital Color Fundus Photographs},
  shorttitle = {Retinopathy Online Challenge},
  author = {Niemeijer, Meindert and {van Ginneken}, Bram and Cree, Michael J. and Mizutani, Atsushi and Quellec, Gw{\'e}nol{\'e} and Sanchez, Clara I. and Zhang, Bob and Hornero, Roberto and Lamard, Mathieu and Muramatsu, Chisako and Wu, Xiangqian and Cazuguel, Guy and You, Jane and Mayo, Agust{\'i}n and Li, Qin and Hatanaka, Yuji and Cochener, B{\'e}atrice and Roux, Christian and Karray, Fakhri and Garcia, Mar{\'i}a and Fujita, Hiroshi and Abramoff, Michael D.},
  year = {2010},
  month = jan,
  journal = {IEEE transactions on medical imaging},
  volume = {29},
  number = {1},
  pages = {185--195},
  issn = {1558-254X},
  doi = {10.1109/TMI.2009.2033909},
  abstract = {The detection of microaneurysms in digital color fundus photographs is a critical first step in automated screening for diabetic retinopathy (DR), a common complication of diabetes. To accomplish this detection numerous methods have been published in the past but none of these was compared with each other on the same data. In this work we present the results of the first international microaneurysm detection competition, organized in the context of the Retinopathy Online Challenge (ROC), a multiyear online competition for various aspects of DR detection. For this competition, we compare the results of five different methods, produced by five different teams of researchers on the same set of data. The evaluation was performed in a uniform manner using an algorithm presented in this work. The set of data used for the competition consisted of 50 training images with available reference standard and 50 test images where the reference standard was withheld by the organizers (M. Niemeijer, B. van Ginneken, and M. D. Abr{\`a}moff). The results obtained on the test data was submitted through a website after which standardized evaluation software was used to determine the performance of each of the methods. A human expert detected microaneurysms in the test set to allow comparison with the performance of the automatic methods. The overall results show that microaneurysm detection is a challenging task for both the automatic methods as well as the human expert. There is room for improvement as the best performing system does not reach the performance of the human expert. The data associated with the ROC microaneurysm detection competition will remain publicly available and the website will continue accepting submissions.},
  langid = {english},
  pmid = {19822469},
  keywords = {Algorithms,Aneurysm,Bayes Theorem,Databases,Diagnostic Techniques,Factual,False Positive Reactions,Fundus Oculi,Humans,Ophthalmological,Photography,Retinal Diseases,Retinal Vessels,ROC Curve}
}

@article{niemeijerRetinopathyOnlineChallenge2010c,
  title = {Retinopathy Online Challenge: Automatic Detection of Microaneurysms in Digital Color Fundus Photographs},
  shorttitle = {Retinopathy Online Challenge},
  author = {Niemeijer, Meindert and {van Ginneken}, Bram and Cree, Michael J. and Mizutani, Atsushi and Quellec, Gw{\'e}nol{\'e} and Sanchez, Clara I. and Zhang, Bob and Hornero, Roberto and Lamard, Mathieu and Muramatsu, Chisako and Wu, Xiangqian and Cazuguel, Guy and You, Jane and Mayo, Agust{\'i}n and Li, Qin and Hatanaka, Yuji and Cochener, B{\'e}atrice and Roux, Christian and Karray, Fakhri and Garcia, Mar{\'i}a and Fujita, Hiroshi and Abramoff, Michael D.},
  year = {2010},
  month = jan,
  journal = {IEEE transactions on medical imaging},
  volume = {29},
  number = {1},
  pages = {185--195},
  issn = {1558-254X},
  doi = {10.1109/TMI.2009.2033909},
  abstract = {The detection of microaneurysms in digital color fundus photographs is a critical first step in automated screening for diabetic retinopathy (DR), a common complication of diabetes. To accomplish this detection numerous methods have been published in the past but none of these was compared with each other on the same data. In this work we present the results of the first international microaneurysm detection competition, organized in the context of the Retinopathy Online Challenge (ROC), a multiyear online competition for various aspects of DR detection. For this competition, we compare the results of five different methods, produced by five different teams of researchers on the same set of data. The evaluation was performed in a uniform manner using an algorithm presented in this work. The set of data used for the competition consisted of 50 training images with available reference standard and 50 test images where the reference standard was withheld by the organizers (M. Niemeijer, B. van Ginneken, and M. D. Abr{\`a}moff). The results obtained on the test data was submitted through a website after which standardized evaluation software was used to determine the performance of each of the methods. A human expert detected microaneurysms in the test set to allow comparison with the performance of the automatic methods. The overall results show that microaneurysm detection is a challenging task for both the automatic methods as well as the human expert. There is room for improvement as the best performing system does not reach the performance of the human expert. The data associated with the ROC microaneurysm detection competition will remain publicly available and the website will continue accepting submissions.},
  langid = {english},
  pmid = {19822469},
  keywords = {Algorithms,Aneurysm,Bayes Theorem,Databases Factual,Diagnostic Techniques Ophthalmological,False Positive Reactions,Fundus Oculi,Humans,Photography,Retinal Diseases,Retinal Vessels,ROC Curve},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\N4YIT9IA\\Niemeijer et al. - 2010 - Retinopathy online challenge automatic detection .pdf;C\:\\Users\\cleme\\Zotero\\storage\\DXKMNZTT\\niemeijer2010.html}
}

@article{niemeijerRetinopathyOnlineChallenge2010d,
  title = {Retinopathy {{Online Challenge}}: {{Automatic Detection}} of {{Microaneurysms}} in {{Digital Color Fundus Photographs}}},
  shorttitle = {Retinopathy {{Online Challenge}}},
  author = {Niemeijer, M. and van Ginneken, B. and Cree, M. J. and Mizutani, A. and Quellec, G. and Sanchez, C. I. and Zhang, B. and Hornero, R. and Lamard, M. and Muramatsu, C. and Wu, X. and Cazuguel, G. and You, J. and Mayo, A. and Li, Q. and Hatanaka, Y. and Cochener, B. and Roux, C. and Karray, F. and Garcia, M. and Fujita, H. and Abramoff, M. D.},
  year = {2010},
  month = jan,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {29},
  number = {1},
  pages = {185--195},
  doi = {10.1109/TMI.2009.2033909},
  abstract = {The detection of microaneurysms in digital color fundus photographs is a critical first step in automated screening for diabetic retinopathy (DR), a common complication of diabetes. To accomplish this detection numerous methods have been published in the past but none of these was compared with each other on the same data. In this work we present the results of the first international microaneurysm detection competition, organized in the context of the Retinopathy Online Challenge (ROC), a multiyear online competition for various aspects of DR detection. For this competition, we compare the results of five different methods, produced by five different teams of researchers on the same set of data. The evaluation was performed in a uniform manner using an algorithm presented in this work. The set of data used for the competition consisted of 50 training images with available reference standard and 50 test images where the reference standard was withheld by the organizers (M. Niemeijer, B. van Ginneken, and M. D. AbrA{\textquestiondown}moff). The results obtained on the test data was submitted through a website after which standardized evaluation software was used to determine the performance of each of the methods. A human expert detected microaneurysms in the test set to allow comparison with the performance of the automatic methods. The overall results show that microaneurysm detection is a challenging task for both the automatic methods as well as the human expert. There is room for improvement as the best performing system does not reach the performance of the human expert. The data associated with the ROC microaneurysm detection competition will remain publicly available and the website will continue accepting submissions.},
  keywords = {Algorithms,Aneurysm,automated screening,automatic microaneurysm detection,Bayes Theorem,Biomedical engineering,Biomedical imaging,Blindness,Cities and towns,Computer aided detection,computer aided diagnosis,Databases Factual,Diabetes,diabetic retinopathy,Diagnostic Techniques Ophthalmological,digital color fundus photograph,digital photography,diseases,eye,False Positive Reactions,Fundus Oculi,fundus photographs,Humans,medical image processing,microaneurysms,multiyear online competition,Photography,retina,Retinal Diseases,Retinal Vessels,Retinopathy,Retinopathy Online Challenge,Retinopathy Online Challenge (ROC) competition,ROC Curve,Statistics,Systems engineering and theory,Telecommunications,USA Councils},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\BG8C3FEX\\Niemeijer et al. - 2010 - Retinopathy Online Challenge Automatic Detection .pdf;C\:\\Users\\cleme\\Zotero\\storage\\N6RCUTQW\\5282586.html}
}

@article{niemeijerRetinopathyOnlineChallenge2010e,
  title = {Retinopathy {{Online Challenge}}: {{Automatic Detection}} of {{Microaneurysms}} in {{Digital Color Fundus Photographs}}},
  shorttitle = {Retinopathy {{Online Challenge}}},
  author = {Niemeijer, Meindert and {van Ginneken}, Bram and Cree, Michael J. and Mizutani, Atsushi and Quellec, Gw{\'E}nol{\'E} and Sanchez, Clara I. and Zhang, Bob and Hornero, Roberto and Lamard, Mathieu and Muramatsu, Chisako and Wu, Xiangqian and Cazuguel, Guy and You, Jane and Mayo, Agust{\'I}n and Li, Qin and Hatanaka, Yuji and Cochener, {\relax B{\'e}}atrice and Roux, Christian and Karray, Fakhri and Garcia, Mar{\'I}a and Fujita, Hiroshi and Abramoff, Michael D.},
  year = {2010},
  month = jan,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {29},
  number = {1},
  pages = {185--195},
  issn = {1558-254X},
  doi = {10.1109/TMI.2009.2033909},
  abstract = {The detection of microaneurysms in digital color fundus photographs is a critical first step in automated screening for diabetic retinopathy (DR), a common complication of diabetes. To accomplish this detection numerous methods have been published in the past but none of these was compared with each other on the same data. In this work we present the results of the first international microaneurysm detection competition, organized in the context of the Retinopathy Online Challenge (ROC), a multiyear online competition for various aspects of DR detection. For this competition, we compare the results of five different methods, produced by five different teams of researchers on the same set of data. The evaluation was performed in a uniform manner using an algorithm presented in this work. The set of data used for the competition consisted of 50 training images with available reference standard and 50 test images where the reference standard was withheld by the organizers (M. Niemeijer, B. van Ginneken, and M. D. AbrA{\textquestiondown}moff). The results obtained on the test data was submitted through a website after which standardized evaluation software was used to determine the performance of each of the methods. A human expert detected microaneurysms in the test set to allow comparison with the performance of the automatic methods. The overall results show that microaneurysm detection is a challenging task for both the automatic methods as well as the human expert. There is room for improvement as the best performing system does not reach the performance of the human expert. The data associated with the ROC microaneurysm detection competition will remain publicly available and the website will continue accepting submissions.},
  keywords = {Algorithms,Aneurysm,automated screening,automatic microaneurysm detection,Bayes Theorem,Biomedical engineering,Biomedical imaging,Blindness,Cities and towns,Computer aided detection,computer aided diagnosis,Databases Factual,Diabetes,diabetic retinopathy,Diagnostic Techniques Ophthalmological,digital color fundus photograph,digital photography,diseases,eye,False Positive Reactions,Fundus Oculi,fundus photographs,Humans,medical image processing,microaneurysms,multiyear online competition,Photography,retina,Retinal Diseases,Retinal Vessels,Retinopathy,Retinopathy Online Challenge,Retinopathy Online Challenge (ROC) competition,ROC Curve,Statistics,Systems engineering and theory,Telecommunications,USA Councils},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\P45CFL5H\\Niemeijer et al. - 2010 - Retinopathy Online Challenge Automatic Detection .pdf;C\:\\Users\\cleme\\Zotero\\storage\\4T9JF3YV\\5282586.html;C\:\\Users\\cleme\\Zotero\\storage\\TZTGGP44\\niemeijer2010.html}
}

@inproceedings{NIPS2017_3f5ee243,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.}
}

@inproceedings{NIPS2017_3f5ee243,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.}
}

@inproceedings{NIPS2017_3f5ee243,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.}
}

@inproceedings{NIPS2017_3f5ee243,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.}
}

@inproceedings{niuRegistrationSDOCTEnface2014,
  title = {Registration of {{SD-OCT}} En-Face Images with Color Fundus Photographs Based on Local Patch Matching},
  booktitle = {Proceedings of the {{Ophthalmic Medical Image Analysis First International Workshop}}},
  author = {Niu, Sijie and Chen, Qiang and Shen, Honglie and {de Sisternes}, Luis and Rubin, Daniel L.},
  year = {2014},
  month = sep,
  pages = {25--32},
  publisher = {University of Iowa},
  address = {Boston, MA, USA},
  doi = {10.17077/omia.1005},
  urldate = {2019-08-01},
  abstract = {Registration of multi-modal retinal images is very significant to integrate information gained from different modalities for a reliable diagnosis of retinal diseases by ophthalmologists. However, accurate image registration is a challenging, we propose an algorithm for registration of summed-voxel projection images (SVPIs) with color fundus photographs (CFPs) based on local patch matching. SVPIs are evenly split into 16 local image blocks for extracting matching point pairs by searching local maximization of the similarity function. These matching point pairs are used for a coarse registration and then a search region of feature matching points is redefined for a more accurate registration. The performance of our registration algorithm is tested on a series of datasets including 3 normal eyes and 20 eyes with age-related macular degeneration. The experiment demonstrates that the proposed method can achieve accurate registration results (the average of root mean square error is 128{$\mu$}m).},
  langid = {english}
}

@inproceedings{niuRegistrationSDOCTEnface2014a,
  title = {Registration of {{SD-OCT}} En-Face Images with Color Fundus Photographs Based on Local Patch Matching},
  booktitle = {Proceedings of the {{Ophthalmic Medical Image Analysis First International Workshop}}},
  author = {Niu, Sijie and Chen, Qiang and Shen, Honglie and {de Sisternes}, Luis and Rubin, Daniel L.},
  year = {2014},
  month = sep,
  pages = {25--32},
  publisher = {University of Iowa},
  address = {Boston, MA, USA},
  doi = {10.17077/omia.1005},
  urldate = {2019-08-01},
  abstract = {Registration of multi-modal retinal images is very significant to integrate information gained from different modalities for a reliable diagnosis of retinal diseases by ophthalmologists. However, accurate image registration is a challenging, we propose an algorithm for registration of summed-voxel projection images (SVPIs) with color fundus photographs (CFPs) based on local patch matching. SVPIs are evenly split into 16 local image blocks for extracting matching point pairs by searching local maximization of the similarity function. These matching point pairs are used for a coarse registration and then a search region of feature matching points is redefined for a more accurate registration. The performance of our registration algorithm is tested on a series of datasets including 3 normal eyes and 20 eyes with age-related macular degeneration. The experiment demonstrates that the proposed method can achieve accurate registration results (the average of root mean square error is 128{$\mu$}m).},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\7G4GFVLC\Niu et al. - 2014 - Registration of SD-OCT en-face images with color f.pdf}
}

@incollection{nocedalCalculatingDerivatives2006,
  title = {Calculating {{Derivatives}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {193--219},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_8},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Adjacency Graph,Automatic Differentiation,Computational Graph,Intersection Graph,Reverse Mode}
}

@incollection{nocedalCalculatingDerivatives2006a,
  title = {Calculating {{Derivatives}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {193--219},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_8},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Adjacency Graph,Automatic Differentiation,Computational Graph,Intersection Graph,Reverse Mode},
  file = {C:\Users\cleme\Zotero\storage\K8U9IF4K\Nocedal et Wright - 2006 - Calculating Derivatives.pdf}
}

@incollection{nocedalConjugateGradientMethods2006,
  title = {Conjugate {{Gradient Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {101--134},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_5},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english}
}

@incollection{nocedalConjugateGradientMethods2006a,
  title = {Conjugate {{Gradient Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {101--134},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_5},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\BNZ7C9V4\Nocedal et Wright - 2006 - Conjugate Gradient Methods.pdf}
}

@incollection{nocedalDerivativeFreeOptimization2006,
  title = {Derivative-{{Free Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {220--244},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_9},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Interpolation Condition,Interpolation Point,Line Search,Quadratic Model,Search Direction}
}

@incollection{nocedalDerivativeFreeOptimization2006a,
  title = {Derivative-{{Free Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {220--244},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_9},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Interpolation Condition,Interpolation Point,Line Search,Quadratic Model,Search Direction},
  file = {C:\Users\cleme\Zotero\storage\UPW8SBML\Nocedal et Wright - 2006 - Derivative-Free Optimization.pdf}
}

@incollection{nocedalFundamentalsAlgorithmsNonlinear2006,
  title = {Fundamentals of {{Algorithms}} for {{Nonlinear Constrained Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {421--447},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_15},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Exact Penalty Function,Line Search Method,Merit Function,Nonlinear Programming Problem,Sequential Quadratic Programming Method}
}

@incollection{nocedalFundamentalsAlgorithmsNonlinear2006a,
  title = {Fundamentals of {{Algorithms}} for {{Nonlinear Constrained Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {421--447},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_15},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Exact Penalty Function,Line Search Method,Merit Function,Nonlinear Programming Problem,Sequential Quadratic Programming Method}
}

@incollection{nocedalFundamentalsUnconstrainedOptimization2006,
  title = {Fundamentals of {{Unconstrained Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {10--29},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_2},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Global Minimizer,Line Search,Local Minimizer,Search Direction,Step Length}
}

@incollection{nocedalFundamentalsUnconstrainedOptimization2006a,
  title = {Fundamentals of {{Unconstrained Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {10--29},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_2},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Global Minimizer,Line Search,Local Minimizer,Search Direction,Step Length},
  file = {C:\Users\cleme\Zotero\storage\UPJK5XE4\Nocedal et Wright - 2006 - Fundamentals of Unconstrained Optimization.pdf}
}

@incollection{nocedalInteriorPointMethodsNonlinear2006,
  title = {Interior-{{Point Methods}} for {{Nonlinear Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {563--597},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_19},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Barrier Parameter,Barrier Problem,Line Search,Merit Function,Strict Complementarity Condition}
}

@incollection{nocedalInteriorPointMethodsNonlinear2006a,
  title = {Interior-{{Point Methods}} for {{Nonlinear Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {563--597},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_19},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Barrier Parameter,Barrier Problem,Line Search,Merit Function,Strict Complementarity Condition},
  file = {C:\Users\cleme\Zotero\storage\P4HIH224\Nocedal et Wright - 2006 - Interior-Point Methods for Nonlinear Programming.pdf}
}

@incollection{nocedalIntroduction2006,
  title = {Introduction},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {1--9},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_1},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Constrain Optimization Problem,Feasible Point,Linear Programming Problem,Robust Optimization,Stochastic Optimization}
}

@incollection{nocedalIntroduction2006a,
  title = {Introduction},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {1--9},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_1},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Constrain Optimization Problem,Feasible Point,Linear Programming Problem,Robust Optimization,Stochastic Optimization},
  file = {C:\Users\cleme\Zotero\storage\2N3ZTDA7\Nocedal et Wright - 2006 - Introduction.pdf}
}

@incollection{nocedalLargeScaleUnconstrainedOptimization2006,
  title = {Large-{{Scale Unconstrained Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {164--192},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_7},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Conjugate Gradient Method,Lanczos Method,Line Search,Newton Step,Vector Pair}
}

@incollection{nocedalLargeScaleUnconstrainedOptimization2006a,
  title = {Large-{{Scale Unconstrained Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {164--192},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_7},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Conjugate Gradient Method,Lanczos Method,Line Search,Newton Step,Vector Pair},
  file = {C:\Users\cleme\Zotero\storage\DRSR7D73\Nocedal et Wright - 2006 - Large-Scale Unconstrained Optimization.pdf}
}

@incollection{nocedalLeastSquaresProblems2006,
  title = {Least-{{Squares Problems}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {245--269},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_10},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Cholesky Factorization,Line Search,Newton Method,Superlinear Convergence,Trust Region}
}

@incollection{nocedalLeastSquaresProblems2006a,
  title = {Least-{{Squares Problems}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {245--269},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_10},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Cholesky Factorization,Line Search,Newton Method,Superlinear Convergence,Trust Region},
  file = {C:\Users\cleme\Zotero\storage\G9WKAM2J\Nocedal et Wright - 2006 - Least-Squares Problems.pdf}
}

@incollection{nocedalLinearProgrammingInteriorPoint2006,
  title = {Linear {{Programming}}: {{Interior-Point Methods}}},
  shorttitle = {Linear {{Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {392--420},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_14},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Central Path,Duality Measure,Nonnegative Orthant,Pairwise Product,Potential Reduction Method}
}

@incollection{nocedalLinearProgrammingInteriorPoint2006a,
  title = {Linear {{Programming}}: {{Interior-Point Methods}}},
  shorttitle = {Linear {{Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {392--420},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_14},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Central Path,Duality Measure,Nonnegative Orthant,Pairwise Product,Potential Reduction Method},
  file = {C:\Users\cleme\Zotero\storage\DR378FBI\Nocedal et Wright - 2006 - Linear Programming Interior-Point Methods.pdf}
}

@incollection{nocedalLinearProgrammingSimplex2006,
  title = {Linear {{Programming}}: {{The Simplex Method}}},
  shorttitle = {Linear {{Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {355--391},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_13},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Basis Matrix,Dual Problem,Equality Constraint,Feasible Point,Simplex Method}
}

@incollection{nocedalLinearProgrammingSimplex2006a,
  title = {Linear {{Programming}}: {{The Simplex Method}}},
  shorttitle = {Linear {{Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {355--391},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_13},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Basis Matrix,Dual Problem,Equality Constraint,Feasible Point,Simplex Method},
  file = {C:\Users\cleme\Zotero\storage\SJI7U9D5\Nocedal et Wright - 2006 - Linear Programming The Simplex Method.pdf}
}

@incollection{nocedalLineSearchMethods2006,
  title = {Line {{Search Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {30--65},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_3},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english}
}

@incollection{nocedalLineSearchMethods2006a,
  title = {Line {{Search Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {30--65},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_3},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\QIR4A6C7\Nocedal et Wright - 2006 - Line Search Methods.pdf}
}

@incollection{nocedalNonlinearEquations2006,
  title = {Nonlinear {{Equations}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {270--302},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_11},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Continuation Method,Line Search,Merit Function,Trust Region,Unconstrained Optimization}
}

@incollection{nocedalNonlinearEquations2006a,
  title = {Nonlinear {{Equations}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {270--302},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_11},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Continuation Method,Line Search,Merit Function,Trust Region,Unconstrained Optimization},
  file = {C:\Users\cleme\Zotero\storage\ZIMDUUVD\Nocedal et Wright - 2006 - Nonlinear Equations.pdf}
}

@incollection{nocedalPenaltyAugmentedLagrangian2006,
  title = {Penalty and {{Augmented Lagrangian Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {497--528},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_17},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Augmented Lagrangian Method,Penalty Function,Penalty Parameter,Sequential Quadratic Programming,Sequential Quadratic Programming Method}
}

@incollection{nocedalPenaltyAugmentedLagrangian2006a,
  title = {Penalty and {{Augmented Lagrangian Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {497--528},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_17},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Augmented Lagrangian Method,Penalty Function,Penalty Parameter,Sequential Quadratic Programming,Sequential Quadratic Programming Method},
  file = {C:\Users\cleme\Zotero\storage\TNHLZGHF\Nocedal et Wright - 2006 - Penalty and Augmented Lagrangian Methods.pdf}
}

@incollection{nocedalQuadraticProgramming2006,
  title = {Quadratic {{Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {448--492},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_16},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Conjugate Gradient Iteration,Convex Quadratic Program,Gradient Projection Method,Quadratic Program,Unique Global Solution}
}

@incollection{nocedalQuadraticProgramming2006a,
  title = {Quadratic {{Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {448--492},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_16},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Conjugate Gradient Iteration,Convex Quadratic Program,Gradient Projection Method,Quadratic Program,Unique Global Solution},
  file = {C:\Users\cleme\Zotero\storage\PRS6SYZU\Nocedal et Wright - 2006 - Quadratic Programming.pdf}
}

@incollection{nocedalQuasiNewtonMethods2006,
  title = {Quasi-{{Newton Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {135--163},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_6},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english}
}

@incollection{nocedalQuasiNewtonMethods2006a,
  title = {Quasi-{{Newton Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {135--163},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_6},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\R2F4JWWV\Nocedal et Wright - 2006 - Quasi-Newton Methods.pdf}
}

@incollection{nocedalSequentialQuadraticProgramming2006,
  title = {Sequential {{Quadratic Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {529--562},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_18},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Gradient Projection Method,Merit Function,Penalty Parameter,Sequential Quadratic Programming,Trust Region}
}

@incollection{nocedalSequentialQuadraticProgramming2006a,
  title = {Sequential {{Quadratic Programming}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {529--562},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_18},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Gradient Projection Method,Merit Function,Penalty Parameter,Sequential Quadratic Programming,Trust Region},
  file = {C:\Users\cleme\Zotero\storage\ZLEEDTDN\Nocedal et Wright - 2006 - Sequential Quadratic Programming.pdf}
}

@incollection{nocedalTheoryConstrainedOptimization2006,
  title = {Theory of {{Constrained Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {304--354},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_12},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english}
}

@incollection{nocedalTheoryConstrainedOptimization2006a,
  title = {Theory of {{Constrained Optimization}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {304--354},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_12},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\UCH4VGWF\Nocedal et Wright - 2006 - Theory of Constrained Optimization.pdf}
}

@incollection{nocedalTrustRegionMethods2006,
  title = {Trust-{{Region Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {66--100},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_4},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Global Convergence,Newton Step,Superlinear Convergence,Trust Region,Unconstrained Minimizer}
}

@incollection{nocedalTrustRegionMethods2006a,
  title = {Trust-{{Region Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {66--100},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-40065-5_4},
  urldate = {2023-04-06},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {Global Convergence,Newton Step,Superlinear Convergence,Trust Region,Unconstrained Minimizer},
  file = {C:\Users\cleme\Zotero\storage\HI7ITGVE\Nocedal et Wright - 2006 - Trust-Region Methods.pdf}
}

@inproceedings{nohCombiningFundusImages2020,
  title = {Combining {{Fundus Images}} and {{Fluorescein Angiography}} for {{Artery}}/{{Vein Classification Using}} the {{Hierarchical Vessel Graph Network}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2020},
  author = {Noh, Kyoung Jin and Park, Sang Jun and Lee, Soochahn},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {595--605},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-59722-1_57},
  abstract = {We present a new framework for retinal artery/vein classification from fundus images and corresponding fluorescein angiography (FA) images. While FA seem to provide the most relevant information, it is often insufficient depending on the acquisition conditions. As fundus images are often acquired by default, we combine the fundus image and FA within a parallel convolutional neural network to extract the maximum information in the generated features. Furthermore, we use these features as the input to a hierarchical graph neural network to ensure that the connectivity of vessels plays a part in the classification. We provide investigative evidence through ablative and comparative quantitative evaluations to better determine the optimal configuration in combining the fundus image and FA in a deep learning framework and demonstrate the enhancement in performance compared to previous methods.},
  isbn = {978-3-030-59722-1},
  langid = {english},
  keywords = {Artery/vein classification,Convolutional neural network,Fluorescein angiography,Fundus images,Graph neural network}
}

@inproceedings{nohCombiningFundusImages2020a,
  title = {Combining {{Fundus Images}} and {{Fluorescein Angiography}} for {{Artery}}/{{Vein Classification Using}} the {{Hierarchical Vessel Graph Network}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2020},
  author = {Noh, Kyoung Jin and Park, Sang Jun and Lee, Soochahn},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {595--605},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-59722-1_57},
  abstract = {We present a new framework for retinal artery/vein classification from fundus images and corresponding fluorescein angiography (FA) images. While FA seem to provide the most relevant information, it is often insufficient depending on the acquisition conditions. As fundus images are often acquired by default, we combine the fundus image and FA within a parallel convolutional neural network to extract the maximum information in the generated features. Furthermore, we use these features as the input to a hierarchical graph neural network to ensure that the connectivity of vessels plays a part in the classification. We provide investigative evidence through ablative and comparative quantitative evaluations to better determine the optimal configuration in combining the fundus image and FA in a deep learning framework and demonstrate the enhancement in performance compared to previous methods.},
  isbn = {978-3-030-59722-1},
  langid = {english},
  keywords = {Artery/vein classification,Convolutional neural network,Fluorescein angiography,Fundus images,Graph neural network}
}

@misc{NoisyLabelsAre2021,
  title = {Noisy {{Labels}} Are {{Treasure}}: {{Mean-Teacher-Assisted Confident Learning}} for {{Hepatic Vessel Segmentation}}},
  shorttitle = {Noisy {{Labels}} Are {{Treasure}}},
  year = {2021},
  month = sep,
  urldate = {2023-06-28},
  abstract = {Paper Info Reviews Meta-review Author Feedback Post-Rebuttal Meta-reviews Authors Zhe Xu, Donghuan Lu, Yixin Wang, Jie Luo, Jagadeesan Jayender, Kai Ma, Yefeng Zheng, Xiu Li Abstract Manually segmenting the hepatic vessels from Computer Tomography (CT) is far more expertise-demanding and laborious than other structures due to the low-contrast and complex morphology of vessels, resulting in the extreme lack of high-quality labeled data. Without sufficient high-quality annotations, the usual data-driven learning-based approaches struggle with deficient training. On the other hand, directly introducing additional data with low-quality annotations may confuse the network, leading to undesirable performance degradation. To address this issue, we propose a novel mean-teacher-assisted confident learning framework to robustly exploit the noisy labeled data for the challenging hepatic vessel segmentation task. Specifically, with the adapted confident learning assisted by a third party, i.e., the weight-averaged teacher model, the noisy labels in the additional low-quality dataset can be transformed from {\textbackslash}textit\{`encumbrance'\} to {\textbackslash}textit\{`treasure'\} via progressive pixel-wise soft-correction, thus providing productive guidance. Extensive experiments using two public datasets demonstrate the superiority of the proposed framework as well as the effectiveness of each component. Link to paper DOI: https://doi.org/10.1007/978-3-030-87193-2\_1 SharedIt: https://rdcu.be/cyhLt Link to the code repository https://github.com/lemoshu/MTCL Link to the dataset(s) https://www.ircad.fr/research/3d-ircadb-01/ http://medicaldecathlon.com/ Reviews Review \#1 Please describe the contribution of the paper This paper presents a method to make use of separate datasets in which the same regions were attempted to be segmented manually but one had significantly higher quality (or potentially slightly different labeling instructions/criteria). The method makes use of a student-teacher paradigm and presents empirical results on hepatic vessel segmentation. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. First, problem that this work is trying to address is very important and pervasive in our field. Label noise is common and often unavoidable, and with such a scarcity of segmentation label, we are often forced to utilize labels from multiple sources with varying levels of quality. Second, the paper is well-written and makes effective use of tables and figures to demonstrate the issue and support its arguments. Finally, the authors used publicly-available data and they say they will release their code which will help facilitate the replication and extension of this work. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors applied their method to only the single problem of hepatic vessel segmentation, and all empirical results are derived from a test set with only ten segmented cases. Because of this, even though the results look promising, it's difficult to be sure that this is not just a lucky coincidence. No attempt is made to demonstrate the statistical significance of the reported improvement. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The prospects for reproducibility of this paper are excellent. The paper uses publicly-available data and the authors promise to release their code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer's guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I feel that this paper would be far stronger if the same experiments had been conducted on one or two other similar problems -- perhaps ones in which HQ test sets with more than ten cases are possible. Here, you will have far more statistical power to demonstrate that your results are not just a statistical anomaly. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This work is promising, but contains unconvincing empirical evidence that it truly represents a state of the art advancement. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review \#2 Please describe the contribution of the paper To overcome the lack of high-quality labeled data, the authors propose a mean-teacher-assisted confident learning framework for hepatic vessel segmentation purposes. By encouraging consistent segmentation under different perturbations of the same image, the network can exploit image information with low-quality annotations. The teacher model serves as ``third party'' to provide guidance in identifying label noises and perform progressive pixel-wise soft-correction. Experiments using two public datasets (3DIRCADb, Medical Image Decathlon) demonstrate the effectiveness of the proposed contributions and show that the annotation quality of noisy labeled data can be improved with a small amount of high-quality labeled data only. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper addresses the not widely investigated but crucial following issue: how to robustly exploit abundant low-quality noisy labeled data for segmentation purposes? Nice set of contributions including vessel probability map use (from Sato tubeless filter) as auxiliary input modality and adaptation of confident learning in a mean-teacher learning segmentation framework Methodological contributions are assessed rigorously through a detailed ablation study Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Authors should provide more insights to explain the proposed progressive self-denoising process Lack of comparisons with respect to state-of-the-art (Huang et al. and standard U-Net only) Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance The associated code will be released after the anonymous review. In addition, the proposed method could be easily re-implemented based on provided architecture and training information. Both 3DIRCADb and Medical Image Decathlon datasets are publicly-available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer's guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The submitted paper is innovative and well written. Comments provided below could be taken into account for further improvements. Main comments : Method. The student model is optimized by minimizing the supervised loss on high-quality annotated data as well as an unsupervised consistency loss between predictions from student and teacher models on both datasets. The progressive self-denoising process which is added to the whole framework remains less clear. You should explicitly explain that \${\textbackslash}hat\{p\}\_j\$ is obtained using the teacher model. I also suggest you to provide more interpretations to let readers precisely understand: 1- the estimation of \$t\_j\$, 2- how the joint distribution matrix formulation (Eq.2) is derived from the confusion matrix (Eq.1) as well as 3- the smoothly self-denoising operation (Eq.3). Finally, I wonder why \${\textbackslash}mathcal\{L\}\_s\$ involves cross-entropy, Dice, focal and boundary losses while \${\textbackslash}mathcal\{L\}\_cl\$ uses cross-entropy and focal losses only. Experiments. The high-quality annotated dataset is randomly divided into two groups (10 cases for training, 10 for test). The article does not mention any cross-validation strategy. Do you use cross-validation to strengthen the effectiveness of the obtained results? Even if the 3DIRCADb dataset is relatively small, you should include a validation subset to find the optimal network hyper-parameters. Ablation study. What does ``MTCL(c) w/o SSDM'' means exactly? Does it correspond to hard-correction (instead of the proposed soft-correction, Eq.3) ? Comparisons. I would suggest you to use other state-of-the-art methods additionally to Huang et al. [8] and standard U-Net to improve the experimental part. Minor comments: Introduction. Instead of ``noises'', I would refer to ``wrong annotations'' when dealing with unlabeled or mislabeled pixels. Related works. You should explain more precisely why Semi-Supervised Learning (SSL) fails to exploit the potential useful information of noisy label. Methods. An Exponential Moving Average (EMA) is employed. What is the influence of the EMA decay rate \${\textbackslash}alpha\$? In the same spirit, you could explain how \$n\$ and \${\textbackslash}tau\$ parameters are selected and what is their related sensibility. Results. I would have integrated the quantitative results of sub-section ``Effectiveness of label self-denoising'' into Tab.1. Statistical analysis: performance gains could be confirmed using t-tests. Formulation. In the sentences ``Extensive experiments [{\dots}] demonstrate the superiority [{\dots}]'' (abstract), ``The results demonstrate the superiority [{\dots}] `` (Sect.1), ``The superior performance [..]'' (conclusion), you should explain with respect to what! Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? (+) Innovative contributions (+) Well-conducted ablation study (-) More insights required to explain the progressive self-denoising process What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review \#3 Please describe the contribution of the paper This paper introduces a method to train a segmentation network on a dataset containing incorrect or incomplete labels, which may happen even on public datasets. The method is based on a mean-teacher training procedure, where both a trainer and a student networks are trained simultaneously. The high quality images are used as usual, while the low quality ones are used (1) in an unsupervised way via a consistency loss over random perturbations, (2) within a novel confidence estimation and relabeling procedure. The method is evaluated in the context of liver vessel segmentation using two public datasets: one with high quality segmentation and the other with incomplete segmentations. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Datasets with bad segmentation quality are still quite common, so this method addresses an important problem which can be helpful to the community. The experimental setup is sound and seems pretty convincing, ablation studies have been performed. The authors plan to release their source code. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The method is quite complex with many parameters: three loss functions (one of them also having three sub-components), {\textbackslash}lambda\_\{cl\} mus be set to zero for the first thousands iterations, etc. I expect it to be a bit hard to re-implement. The description of the method could be improved (see comments below). The validation dataset of 10 images is quite limited. Some other baselines could have been tested (see comments below). Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for instance, providing code and data is a plus, but not a requirement for acceptance I disagree with the claim that ``The average runtime for each result, or estimated energy cost.'' is not applicable in this case. Training two networks instead of one definitely comes with a cost in computational time and memory footprint compared to a standard UNet. This should have been discussed. The code has not been attached as supplementary material. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer's guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It seems that most of the wrong pixel labels are false negative: vessels that should have been labelled but are not (due to the tedious nature of the task). There are losses (for instance Tversky loss) to accomodate for that kind of bias, but they are unfortunately not considered in the paper. Furthermore there are some other more simple baselines: train a network, then apply it on the training images, then retrain on those newly labeled images. Reporting average numbers are not enough - we need more information on the error distribution (min/max/std). Actually in this case, since there are only 10 validation cases, it would have been possible to report the numbers for each case individually. It is a bit startling to still see papers without any statistical test. I am not sure if the experiment comparing training on the HQ+LQ data vs training only on HQ data is fair. Since there are much more LQ data, it seems that training on HQ+LQ means training on LQ only. The HQ should be sampled much more often so that an HQ image appears as often as a LQ one. The decision to not resample in the z-direction is very surprising. If the slice thickness changes, the network has to unnecessarily learn structures with a different size in this direction. I do not understand the invoked rationale of ``avoiding resampling artifacts''. Furthermore this is even acknowledged by the authors when they report that 3D networks are worse than 2D ones. There is a little bit of name dropping overall in the paper (but more particularly in Section 1), which may seem a bit intimidating for some readers (``Mean-Teacher-assisted Confident Learning'', ``Classification Noise Process'', ``Smoothly Self-Denoising Module''). Adding a primer on teacher-student networks would have made the paper self-contained. I have the feeling that the subsection ``Learn from Progressively Self-Denoised Soft Labels of Set-LQ'' could have been explained more simply. In that case, I don't think the mathematical notations help a lot. For instance, you could consider moving all the equations to a supplementary material and spend more time giving a higher level, more intuitive, description of the approach. Or maybe some pseudo-code would have been clearer. The word ``treasure'' in the title and the paper is a bit clickbaity (when consdidering the overall improvement) and could be toned down. Figure 3 is not very readable, and in particular not colorblind friendly (consider using blue instead of green for instance) Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Although I do think this paper could be improved, it addresses a relevant problem and the presented method seems to provide a significant boost in terms of segmentation. I have some concerns about reproducibility (and in particular re-implementation) but the authors plan to release their code, therefore I recommend to accept it. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers' recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Strengths: First, problem that this work is trying to address is very important and pervasive in our field. Label noise is common and often unavoidable, and with such a scarcity of segmentation label, we are often forced to utilize labels from multiple sources with varying levels of quality. Second, the paper is well-written and makes effective use of tables and figures to demonstrate the issue and support its arguments. Finally, the authors used publicly-available data and they say they will release their code which will help facilitate the replication and extension of this work. Methodological contributions are assessed rigorously through a detailed ablation study Weaknesses: inclusion of typical baselines would be useful for the miccai audience A second application target would make an excellent journal extension. Overall: very strong, we justified paper that could be improved with a typical baseline. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback We are glad that reviewers found our problem and motivation ``not widely investigated but very important and promising in MIA field''. They also found our work ``innovative'', ``well-written'', ``makes effective use of tables and figures to demonstrate the issue and support its arguments'', ``nice set of contributions'' and ``methodological contributions are assessed rigorously''. We also appreciate all the constructive comments from reviewers such as some valuable suggestions about the design of other baselines. Besides, thanks AC for appreciating the second application target (i.e., using tiny HQ labeled data to improve label quality of LQ labeled data) would also be an interesting direction. Consistently, we are working on our extended journal version. Nevertheless, this work is still an early attempt and can be further improved. We feel grateful that this work is early accepted and look forward to more research on this field. back to top},
  langid = {english}
}

@misc{NoisyLabelsAre2021a,
  title = {Noisy {{Labels}} Are {{Treasure}}: {{Mean-Teacher-Assisted Confident Learning}} for {{Hepatic Vessel Segmentation}}},
  shorttitle = {Noisy {{Labels}} Are {{Treasure}}},
  year = {2021},
  month = sep,
  journal = {MICCAI 2021 - Accepted Papers and Reviews},
  urldate = {2023-06-28},
  abstract = {Paper Info Reviews Meta-review Author Feedback Post-Rebuttal Meta-reviews Authors Zhe Xu, Donghuan Lu, Yixin Wang, Jie Luo, Jagadeesan Jayender, Kai Ma, Yefeng Zheng, Xiu Li Abstract Manually segmenting the hepatic vessels from Computer Tomography (CT) is far more expertise-demanding and laborious than other structures due to the low-contrast and complex morphology of vessels, resulting in the extreme lack of high-quality labeled data. Without sufficient high-quality annotations, the usual data-driven learning-based approaches struggle with deficient training. On the other hand, directly introducing additional data with low-quality annotations may confuse the network, leading to undesirable performance degradation. To address this issue, we propose a novel mean-teacher-assisted confident learning framework to robustly exploit the noisy labeled data for the challenging hepatic vessel segmentation task. Specifically, with the adapted confident learning assisted by a third party, i.e., the weight-averaged teacher model, the noisy labels in the additional low-quality dataset can be transformed from {\textbackslash}textit\{`encumbrance'\} to {\textbackslash}textit\{`treasure'\} via progressive pixel-wise soft-correction, thus providing productive guidance. Extensive experiments using two public datasets demonstrate the superiority of the proposed framework as well as the effectiveness of each component. Link to paper DOI: https://doi.org/10.1007/978-3-030-87193-2\_1 SharedIt: https://rdcu.be/cyhLt Link to the code repository https://github.com/lemoshu/MTCL Link to the dataset(s) https://www.ircad.fr/research/3d-ircadb-01/ http://medicaldecathlon.com/ Reviews Review \#1 Please describe the contribution of the paper This paper presents a method to make use of separate datasets in which the same regions were attempted to be segmented manually but one had significantly higher quality (or potentially slightly different labeling instructions/criteria). The method makes use of a student-teacher paradigm and presents empirical results on hepatic vessel segmentation. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. First, problem that this work is trying to address is very important and pervasive in our field. Label noise is common and often unavoidable, and with such a scarcity of segmentation label, we are often forced to utilize labels from multiple sources with varying levels of quality. Second, the paper is well-written and makes effective use of tables and figures to demonstrate the issue and support its arguments. Finally, the authors used publicly-available data and they say they will release their code which will help facilitate the replication and extension of this work. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The authors applied their method to only the single problem of hepatic vessel segmentation, and all empirical results are derived from a test set with only ten segmented cases. Because of this, even though the results look promising, it's difficult to be sure that this is not just a lucky coincidence. No attempt is made to demonstrate the statistical significance of the reported improvement. Please rate the clarity and organization of this paper Excellent Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for~instance, providing code and data is a plus, but not a requirement for acceptance The prospects for reproducibility of this paper are excellent. The paper uses publicly-available data and the authors promise to release their code. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer's guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html I feel that this paper would be far stronger if the same experiments had been conducted on one or two other similar problems -- perhaps ones in which HQ test sets with more than ten cases are possible. Here, you will have far more statistical power to demonstrate that your results are not just a statistical anomaly. Please state your overall opinion of the paper borderline accept (6) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? This work is promising, but contains unconvincing empirical evidence that it truly represents a state of the art advancement. What is the ranking of this paper in your review stack? 4 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review \#2 Please describe the contribution of the paper To overcome the lack of high-quality labeled data, the authors propose a mean-teacher-assisted confident learning framework for hepatic vessel segmentation purposes. By encouraging consistent segmentation under different perturbations of the same image, the network can exploit image information with low-quality annotations. The teacher model serves as ``third party'' to provide guidance in identifying label noises and perform progressive pixel-wise soft-correction. Experiments using two public datasets (3DIRCADb, Medical Image Decathlon) demonstrate the effectiveness of the proposed contributions and show that the annotation quality of noisy labeled data can be improved with a small amount of high-quality labeled data only. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. The paper addresses the not widely investigated but crucial following issue: how to robustly exploit abundant low-quality noisy labeled data for segmentation purposes? Nice set of contributions including vessel probability map use (from Sato tubeless filter) as auxiliary input modality and adaptation of confident learning in a mean-teacher learning segmentation framework Methodological contributions are assessed rigorously through a detailed ablation study Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. Authors should provide more insights to explain the proposed progressive self-denoising process Lack of comparisons with respect to state-of-the-art (Huang et al. and standard U-Net only) Please rate the clarity and organization of this paper Very Good Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for~instance, providing code and data is a plus, but not a requirement for acceptance The associated code will be released after the anonymous review. In addition, the proposed method could be easily re-implemented based on provided architecture and training information. Both 3DIRCADb and Medical Image Decathlon datasets are publicly-available. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer's guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html The submitted paper is innovative and well written. Comments provided below could be taken into account for further improvements. Main comments : Method. The student model is optimized by minimizing the supervised loss on high-quality annotated data as well as an unsupervised consistency loss between predictions from student and teacher models on both datasets. The progressive self-denoising process which is added to the whole framework remains less clear. You should explicitly explain that \${\textbackslash}hat\{p\}\_j\$ is obtained using the teacher model. I also suggest you to provide more interpretations to let readers precisely understand: 1- the estimation of \$t\_j\$, 2- how the joint distribution matrix formulation (Eq.2) is derived from the confusion matrix (Eq.1) as well as 3- the smoothly self-denoising operation (Eq.3). Finally, I wonder why \${\textbackslash}mathcal\{L\}\_s\$ involves cross-entropy, Dice, focal and boundary losses while \${\textbackslash}mathcal\{L\}\_cl\$ uses cross-entropy and focal losses only. Experiments. The high-quality annotated dataset is randomly divided into two groups (10 cases for training, 10 for test). The article does not mention any cross-validation strategy. Do you use cross-validation to strengthen the effectiveness of the obtained results? Even if the 3DIRCADb dataset is relatively small, you should include a validation subset to find the optimal network hyper-parameters. Ablation study. What does ``MTCL(c) w/o SSDM'' means exactly? Does it correspond to hard-correction (instead of the proposed soft-correction, Eq.3) ? Comparisons. I would suggest you to use other state-of-the-art methods additionally to Huang et al. [8] and standard U-Net to improve the experimental part. Minor comments: Introduction. Instead of ``noises'', I would refer to ``wrong annotations'' when dealing with unlabeled or mislabeled pixels. Related works. You should explain more precisely why Semi-Supervised Learning (SSL) fails to exploit the potential useful information of noisy label. Methods. An Exponential Moving Average (EMA) is employed. What is the influence of the EMA decay rate \${\textbackslash}alpha\$? In the same spirit, you could explain how \$n\$ and \${\textbackslash}tau\$ parameters are selected and what is their related sensibility. Results. I would have integrated the quantitative results of sub-section ``Effectiveness of label self-denoising'' into Tab.1. Statistical analysis: performance gains could be confirmed using t-tests. Formulation. In the sentences ``Extensive experiments [{\dots}] demonstrate the superiority [{\dots}]'' (abstract), ``The results demonstrate the superiority [{\dots}] `` (Sect.1), ``The superior performance [..]'' (conclusion), you should explain with respect to what! Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? (+) Innovative contributions (+) Well-conducted ablation study (-) More insights required to explain the progressive self-denoising process What is the ranking of this paper in your review stack? 2 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Review \#3 Please describe the contribution of the paper This paper introduces a method to train a segmentation network on a dataset containing incorrect or incomplete labels, which may happen even on public datasets. The method is based on a mean-teacher training procedure, where both a trainer and a student networks are trained simultaneously. The high quality images are used as usual, while the low quality ones are used (1) in an unsupervised way via a consistency loss over random perturbations, (2) within a novel confidence estimation and relabeling procedure. The method is evaluated in the context of liver vessel segmentation using two public datasets: one with high quality segmentation and the other with incomplete segmentations. Please list the main strengths of the paper; you should write about a novel formulation, an original way to use data, demonstration of clinical feasibility, a novel application, a particularly strong evaluation, or anything else that is a strong aspect of this work. Please provide details, for instance, if a method is novel, explain what aspect is novel and why this is interesting. Datasets with bad segmentation quality are still quite common, so this method addresses an important problem which can be helpful to the community. The experimental setup is sound and seems pretty convincing, ablation studies have been performed. The authors plan to release their source code. Please list the main weaknesses of the paper. Please provide details, for instance, if you think a method is not novel, explain why and provide a reference to prior work. The method is quite complex with many parameters: three loss functions (one of them also having three sub-components), {\textbackslash}lambda\_\{cl\} mus be set to zero for the first thousands iterations, etc. I expect it to be a bit hard to re-implement. The description of the method could be improved (see comments below). The validation dataset of 10 images is quite limited. Some other baselines could have been tested (see comments below). Please rate the clarity and organization of this paper Satisfactory Please comment on the reproducibility of the paper. Note, that authors have filled out a reproducibility checklist upon submission. Please be aware that authors are not required to meet all criteria on the checklist - for~instance, providing code and data is a plus, but not a requirement for acceptance I disagree with the claim that ``The average runtime for each result, or estimated energy cost.'' is not applicable in this case. Training two networks instead of one definitely comes with a cost in computational time and memory footprint compared to a standard UNet. This should have been discussed. The code has not been attached as supplementary material. Please provide detailed and constructive comments for the authors. Please also refer to our Reviewer's guide on what makes a good review: https://miccai2021.org/en/REVIEWER-GUIDELINES.html It seems that most of the wrong pixel labels are false negative: vessels that should have been labelled but are not (due to the tedious nature of the task). There are losses (for instance Tversky loss) to accomodate for that kind of bias, but they are unfortunately not considered in the paper. Furthermore there are some other more simple baselines: train a network, then apply it on the training images, then retrain on those newly labeled images. Reporting average numbers are not enough - we need more information on the error distribution (min/max/std). Actually in this case, since there are only 10 validation cases, it would have been possible to report the numbers for each case individually. It is a bit startling to still see papers without any statistical test. I am not sure if the experiment comparing training on the HQ+LQ data vs training only on HQ data is fair. Since there are much more LQ data, it seems that training on HQ+LQ means training on LQ only. The HQ should be sampled much more often so that an HQ image appears as often as a LQ one. The decision to not resample in the z-direction is very surprising. If the slice thickness changes, the network has to unnecessarily learn structures with a different size in this direction. I do not understand the invoked rationale of ``avoiding resampling artifacts''. Furthermore this is even acknowledged by the authors when they report that 3D networks are worse than 2D ones. There is a little bit of name dropping overall in the paper (but more particularly in Section 1), which may seem a bit intimidating for some readers (``Mean-Teacher-assisted Confident Learning'', ``Classification Noise Process'', ``Smoothly Self-Denoising Module''). Adding a primer on teacher-student networks would have made the paper self-contained. I have the feeling that the subsection ``Learn from Progressively Self-Denoised Soft Labels of Set-LQ'' could have been explained more simply. In that case, I don't think the mathematical notations help a lot. For instance, you could consider moving all the equations to a supplementary material and spend more time giving a higher level, more intuitive, description of the approach. Or maybe some pseudo-code would have been clearer. The word ``treasure'' in the title and the paper is a bit clickbaity (when consdidering the overall improvement) and could be toned down. Figure 3 is not very readable, and in particular not colorblind friendly (consider using blue instead of green for instance) Please state your overall opinion of the paper Probably accept (7) Please justify your recommendation. What were the major factors that led you to your overall score for this paper? Although I do think this paper could be improved, it addresses a relevant problem and the presented method seems to provide a significant boost in terms of segmentation. I have some concerns about reproducibility (and in particular re-implementation) but the authors plan to release their code, therefore I recommend to accept it. What is the ranking of this paper in your review stack? 1 Number of papers in your stack 5 Reviewer confidence Confident but not absolutely certain Primary Meta-Review Please provide your assessment of this work, taking into account all reviews. Summarize the key strengths and weaknesses of the paper and justify your recommendation. In case you deviate from the reviewers' recommendations, explain in detail the reasons why. In case of an invitation for rebuttal, clarify which points are important to address in the rebuttal. Strengths: First, problem that this work is trying to address is very important and pervasive in our field. Label noise is common and often unavoidable, and with such a scarcity of segmentation label, we are often forced to utilize labels from multiple sources with varying levels of quality. Second, the paper is well-written and makes effective use of tables and figures to demonstrate the issue and support its arguments. Finally, the authors used publicly-available data and they say they will release their code which will help facilitate the replication and extension of this work. Methodological contributions are assessed rigorously through a detailed ablation study Weaknesses: inclusion of typical baselines would be useful for the miccai audience A second application target would make an excellent journal extension. Overall: very strong, we justified paper that could be improved with a typical baseline. What is the ranking of this paper in your stack? Use a number between 1 (best paper in your stack) and n (worst paper in your stack of n papers). 3 Author Feedback We are glad that reviewers found our problem and motivation ``not widely investigated but very important and promising in MIA field''. They also found our work ``innovative'', ``well-written'', ``makes effective use of tables and figures to demonstrate the issue and support its arguments'', ``nice set of contributions'' and ``methodological contributions are assessed rigorously''. We also appreciate all the constructive comments from reviewers such as some valuable suggestions about the design of other baselines. Besides, thanks AC for appreciating the second application target (i.e., using tiny HQ labeled data to improve label quality of LQ labeled data) would also be an interesting direction. Consistently, we are working on our extended journal version. Nevertheless, this work is still an early attempt and can be further improved. We feel grateful that this work is early accepted and look forward to more research on this field. back to top},
  howpublished = {https://miccai2021.org/2021/09/01/343-Paper0008},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\8E2CTHUL\343-Paper0008.html}
}

@article{northcuttConfidentLearningEstimating2021,
  title = {Confident {{Learning}}: {{Estimating Uncertainty}} in {{Dataset Labels}}},
  shorttitle = {Confident {{Learning}}},
  author = {Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  year = {2021},
  month = may,
  journal = {Journal of Artificial Intelligence Research},
  volume = {70},
  pages = {1373--1411},
  issn = {1076-9757},
  doi = {10.1613/jair.1.12125},
  urldate = {2023-06-23},
  abstract = {Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and experimentally performant. We present sufficient conditions where CL exactly finds label errors, and show CL performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. Uniquely, the CL framework is not coupled to a specific data modality or model (e.g., we use CL to find several label errors in the presumed error-free MNIST dataset and improve sentiment classification on text data in Amazon Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 missile images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release.}
}

@article{northcuttConfidentLearningEstimating2021a,
  title = {Confident {{Learning}}: {{Estimating Uncertainty}} in {{Dataset Labels}}},
  shorttitle = {Confident {{Learning}}},
  author = {Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  year = {2021},
  month = may,
  journal = {Journal of Artificial Intelligence Research},
  volume = {70},
  pages = {1373--1411},
  issn = {1076-9757},
  doi = {10.1613/jair.1.12125},
  urldate = {2023-06-23},
  abstract = {Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and experimentally performant. We present sufficient conditions where CL exactly finds label errors, and show CL performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. Uniquely, the CL framework is not coupled to a specific data modality or model (e.g., we use CL to find several label errors in the presumed error-free MNIST dataset and improve sentiment classification on text data in Amazon Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 missile images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release.},
  file = {C:\Users\cleme\Zotero\storage\TZ245NAK\Northcutt et al. - 2021 - Confident Learning Estimating Uncertainty in Data.pdf}
}

@article{novoselJointSegmentationRetinal2017,
  title = {Joint {{Segmentation}} of {{Retinal Layers}} and {{Focal Lesions}} in 3-{{D OCT Data}} of {{Topologically Disrupted Retinas}}},
  author = {Novosel, J. and Vermeer, K. A. and {de Jong}, J. H. and Wang, Z. and {van Vliet}, L. J.},
  year = {2017},
  month = jun,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {36},
  number = {6},
  pages = {1276--1286},
  issn = {0278-0062},
  doi = {10.1109/TMI.2017.2666045},
  abstract = {Accurate quantification of retinal structures in 3-D optical coherence tomography data of eyes with pathologies provides clinically relevant information. We present an approach to jointly segment retinal layers and lesions in eyes with topology-disrupting retinal diseases by a loosely coupled level set framework. In the new approach, lesions are modeled as an additional space-variant layer delineated by auxiliary interfaces. Furthermore, the segmentation of interfaces is steered by local differences in the signal between adjacent retinal layers, thereby allowing the approach to handle local intensity variations. The accuracy of the proposed method of both layer and lesion segmentation has been evaluated on eyes affected by central serous retinopathy and age-related macular degeneration. In addition, layer segmentation of the proposed approach was evaluated on eyes without topology-disrupting retinal diseases. Good agreement between the segmentation performed manually by a medical doctor and results obtained from the automatic segmentation was found for all data types. The mean unsigned error for all interfaces varied between 2.3 and 11.9 {$\mu$}m (0.6-3.1 pixels). Furthermore, lesion segmentation showed a Dice coefficient of 0.68 for drusen and 0.89 for fluid pockets. Overall, the method provides a flexible and accurate solution to jointly segment lesions and retinal layers.},
  keywords = {3D OCT data,3D optical coherence tomography data,age-related macular degeneration,Algorithms,Attenuation,attenuation coefficients,auxiliary interfaces,biomedical optical imaging,central serous retinopathy,clinically relevant information,data types,diabetic-macular edema,Dice coefficient,diseases,Diseases,eye,eyes,fluid pockets,focal lesions,Humans,image segmentation,Image segmentation,joint segmentation,Lesions,Level set,local intensity variations,loosely coupled level set framework,Loosely coupled level sets,Macular Degeneration,medical doctor,medical image processing,Optical Coherence,optical tomography,pathologies,Pathology,Retina,Retinal Diseases,retinal layers,retinal structures,space-variant layer delineation,Tomography,topology-disrupting retinal diseases}
}

@article{novoselJointSegmentationRetinal2017a,
  title = {Joint {{Segmentation}} of {{Retinal Layers}} and {{Focal Lesions}} in 3-{{D OCT Data}} of {{Topologically Disrupted Retinas}}},
  author = {Novosel, J. and Vermeer, K. A. and de Jong, J. H. and Wang, Z. and van Vliet, L. J.},
  year = {2017},
  month = jun,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {36},
  number = {6},
  pages = {1276--1286},
  issn = {0278-0062},
  doi = {10.1109/TMI.2017.2666045},
  abstract = {Accurate quantification of retinal structures in 3-D optical coherence tomography data of eyes with pathologies provides clinically relevant information. We present an approach to jointly segment retinal layers and lesions in eyes with topology-disrupting retinal diseases by a loosely coupled level set framework. In the new approach, lesions are modeled as an additional space-variant layer delineated by auxiliary interfaces. Furthermore, the segmentation of interfaces is steered by local differences in the signal between adjacent retinal layers, thereby allowing the approach to handle local intensity variations. The accuracy of the proposed method of both layer and lesion segmentation has been evaluated on eyes affected by central serous retinopathy and age-related macular degeneration. In addition, layer segmentation of the proposed approach was evaluated on eyes without topology-disrupting retinal diseases. Good agreement between the segmentation performed manually by a medical doctor and results obtained from the automatic segmentation was found for all data types. The mean unsigned error for all interfaces varied between 2.3 and 11.9 {$\mu$}m (0.6-3.1 pixels). Furthermore, lesion segmentation showed a Dice coefficient of 0.68 for drusen and 0.89 for fluid pockets. Overall, the method provides a flexible and accurate solution to jointly segment lesions and retinal layers.},
  keywords = {3D OCT data,3D optical coherence tomography data,age-related macular degeneration,Algorithms,Attenuation,attenuation coefficients,auxiliary interfaces,biomedical optical imaging,central serous retinopathy,clinically relevant information,data types,diabetic-macular edema,Dice coefficient,diseases,Diseases,eye,eyes,fluid pockets,focal lesions,Humans,image segmentation,Image segmentation,joint segmentation,Lesions,Level set,local intensity variations,loosely coupled level set framework,Loosely coupled level sets,Macular Degeneration,medical doctor,medical image processing,optical tomography,pathologies,Pathology,Retina,Retinal Diseases,retinal layers,retinal structures,space-variant layer delineation,Tomography Optical Coherence,topology-disrupting retinal diseases},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\TWH8AKVE\\Novosel et al. - 2017 - Joint Segmentation of Retinal Layers and Focal Les.pdf;C\:\\Users\\cleme\\Zotero\\storage\\SHYIVNLE\\7847402.html}
}

@book{NumericalOptimization2006,
  title = {Numerical {{Optimization}}},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  publisher = {Springer New York},
  doi = {10.1007/978-0-387-40065-5},
  urldate = {2023-04-06},
  isbn = {978-0-387-30303-1},
  langid = {english},
  keywords = {algorithms,Calculus of Variations,linear optimization,nonlinear optimization,operations research,optimization,quadratic programming,Quasi-Newton method}
}

@book{NumericalOptimization2006a,
  title = {Numerical {{Optimization}}},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  publisher = {Springer New York},
  doi = {10.1007/978-0-387-40065-5},
  urldate = {2023-04-06},
  isbn = {978-0-387-30303-1},
  langid = {english},
  keywords = {algorithms,Calculus of Variations,linear optimization,nonlinear optimization,operations research,optimization,quadratic programming,Quasi-Newton method},
  file = {C:\Users\cleme\Zotero\storage\46NPYJUD\2006 - Numerical Optimization.pdf}
}

@article{nunesMultiscaleElasticRegistration2004,
  title = {A Multiscale Elastic Registration Scheme for Retinal Angiograms},
  author = {Nunes, J. C. and Bouaoune, Y. and Delechelle, E. and Bunel, {\relax Ph}.},
  year = {2004},
  month = aug,
  journal = {Computer Vision and Image Understanding},
  volume = {95},
  number = {2},
  pages = {129--149},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2004.03.007},
  urldate = {2019-12-10},
  abstract = {The present paper describes a new and efficient method for registration of retinal angiogram. The presence of noise, the variations in the background, and the temporal variation of fluorescence level poses serious problems in obtaining a robust registration of the retinal image. Here, a multiscale registration scheme is proposed which comprises of three steps. The first step of this work proposes an edge preserving smoothing of the vascular tree. This morphological filtering approach is based on opening and closing with a linear rotating structuring element. For complete preservation of the linear shape of the vascular structures, a morphological reconstruction by dilation of the opened image and a reconstruction by erosion of the closed image are applied. It is proposed to compute the registration transform between two successive original frames, from their morphological gradient. Then, the second step consists in computing the morphological gradient of the two filtered images and radiometrically correcting these gradient images. To take into account the intensity variations, our model incorporates two constant multiplicative and additive factors (based on contrast and brightness) estimated employing a simple analysis of the local histograms (based on a sliding window). In the third step, the proposed method computes the registering transform through a coarse-to-fine (or multiscale) hierarchical approach. After computing the dominant registering transform (which implies the translation) between two successive frames, an elastic transform (also called local affine transform) is carried out to achieve a residual correction. The proposed method is tested by experimental studies, performed on macular fluorescein and Indo cyanine green angiographies. It has been sufficiently demonstrated that our proposed registering method is robust, accurate and fully automated, and it is not based on the extraction of the features or landmarks.},
  langid = {english},
  keywords = {Image registration,Mathematical morphology,Multiresolution,Optical flow,Retinal angiography},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\D885QSW3\\Nunes et al. - 2004 - A multiscale elastic registration scheme for retin.pdf;C\:\\Users\\cleme\\Zotero\\storage\\WJ3TT4KP\\nunes2004.pdf;C\:\\Users\\cleme\\Zotero\\storage\\KJXLKWDA\\S107731420400044X.html}
}

@misc{ObjectiveMeasuresTropical,
  title = {Objective {{Measures}} of {{Tropical Cyclone Structure}} and {{Intensity Change From Remotely Sensed Infrared Image Data}} - {{IEEE Journals}} \& {{Magazine}}},
  urldate = {2019-06-10}
}

@misc{ObjectiveMeasuresTropicala,
  title = {Objective {{Measures}} of {{Tropical Cyclone Structure}} and {{Intensity Change From Remotely Sensed Infrared Image Data}} - {{IEEE Journals}} \& {{Magazine}}},
  urldate = {2019-06-10},
  howpublished = {https://ieeexplore.ieee.org/document/4685943},
  file = {C:\Users\cleme\Zotero\storage\S5ANZKVC\4685943.html}
}

@article{OcularDiseaseIntelligent,
  title = {Ocular Disease Intelligent Recognition {{ODIR-5K}}},
  abstract = {We collected a structured ophthalmic database of 5,000 patients with age, color fundus photographs from left and right eyes and doctors' diagnostic keywords from doctors (in short, ODIR-5K). This dataset is ``real-life'' set of patient information collected by Shanggong Medical Technology Co., Ltd. from different hospitals/medical centers in China. In these institutions, fundus images are captured by various cameras in the market, such as Canon, Zeiss and Kowa, resulting into varied image resolutions. Patient identifying information will be removed. Annotations are labeled by trained human readers with quality control management. They classify patient into eight labels including normal (N), diabetes (D), glaucoma (G), cataract (C), AMD (A), hypertension (H), myopia (M) and other diseases/abnormalities (O) based on both eye images and additionally patient age. The publishing of this dataset follows the ethical and privacy rules of China. Table 1 shows one record from ODIR-5K dataset. The 5,000 patients in this challenge are divided into training, off-site testing and on-site testing subsets. Almost 4,000 cases are used in training stage while others are for testing stages (off-site and on-site). Table 2 shows the distribution of case number with respect to eight labels in different stages. Note: one patient may contains one or multiple labels. https://i.imgur.com/vXa8rU9.png https://i.imgur.com/Hs7kYUF.png}
}

@article{OcularDiseaseIntelligenta,
  title = {Ocular Disease Intelligent Recognition {{ODIR-5K}}},
  abstract = {We collected a structured ophthalmic database of 5,000 patients with age, color fundus photographs from left and right eyes and doctors' diagnostic keywords from doctors (in short, ODIR-5K). This dataset is ``real-life'' set of patient information collected by Shanggong Medical Technology Co., Ltd. from different hospitals/medical centers in China. In these institutions, fundus images are captured by various cameras in the market, such as Canon, Zeiss and Kowa, resulting into varied image resolutions. Patient identifying information will be removed. Annotations are labeled by trained human readers with quality control management. They classify patient into eight labels including normal (N), diabetes (D), glaucoma (G), cataract (C), AMD (A), hypertension (H), myopia (M) and other diseases/abnormalities (O) based on both eye images and additionally patient age. The publishing of this dataset follows the ethical and privacy rules of China. Table 1 shows one record from ODIR-5K dataset. The 5,000 patients in this challenge are divided into training, off-site testing and on-site testing subsets. Almost 4,000 cases are used in training stage while others are for testing stages (off-site and on-site). Table 2 shows the distribution of case number with respect to eight labels in different stages. Note: one patient may contains one or multiple labels. https://i.imgur.com/vXa8rU9.png https://i.imgur.com/Hs7kYUF.png}
}

@misc{OcularDiseaseRecognition,
  title = {Ocular {{Disease Recognition}}},
  urldate = {2021-10-04},
  abstract = {Right and left eye fundus photographs of 5000 patients},
  langid = {english}
}

@article{OEilHumain2019,
  title = {{{\OE}il humain}},
  year = {2019},
  month = oct,
  journal = {Wikip{\'e}dia},
  urldate = {2019-11-28},
  abstract = {L'{\oe}il humain est l'organe de la vision de l'{\^e}tre humain ; il lui permet de capter la lumi{\`e}re, pour ensuite l'analyser et interagir avec son environnement. L'{\oe}il humain permet de distinguer les formes et les couleurs. L'un des grands d{\'e}fis de la technologie sera de fabriquer des yeux {\'e}lectroniques, capables d'{\'e}galer voire de d{\'e}passer les aptitudes des yeux du monde vivant pour, par exemple, remplacer l'{\oe}il d'une personne accident{\'e}e. La science qui {\'e}tudie l'{\oe}il s'appelle l'ophtalmologie.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french}
}

@article{OEilHumain2019a,
  title = {{{\OE}il humain}},
  year = {2019},
  month = oct,
  journal = {Wikip{\'e}dia},
  urldate = {2019-11-28},
  abstract = {L'{\oe}il humain est l'organe de la vision de l'{\^e}tre humain ; il lui permet de capter la lumi{\`e}re, pour ensuite l'analyser et interagir avec son environnement. L'{\oe}il humain permet de distinguer les formes et les couleurs. L'un des grands d{\'e}fis de la technologie sera de fabriquer des yeux {\'e}lectroniques, capables d'{\'e}galer voire de d{\'e}passer les aptitudes des yeux du monde vivant pour, par exemple, remplacer l'{\oe}il d'une personne accident{\'e}e. La science qui {\'e}tudie l'{\oe}il s'appelle l'ophtalmologie.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 163177035},
  file = {C:\Users\cleme\Zotero\storage\4DI8J5PJ\index.html}
}

@inproceedings{oktayAttentionUNetLearning2022,
  title = {Attention {{U-Net}}: {{Learning Where}} to {{Look}} for the {{Pancreas}}},
  shorttitle = {Attention {{U-Net}}},
  booktitle = {Medical {{Imaging}} with {{Deep Learning}}},
  author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
  year = {2022},
  month = jul,
  urldate = {2023-05-18},
  abstract = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
  langid = {english}
}

@inproceedings{oktayAttentionUNetLearning2022a,
  title = {Attention {{U-Net}}: {{Learning Where}} to {{Look}} for the {{Pancreas}}},
  shorttitle = {Attention {{U-Net}}},
  booktitle = {Medical {{Imaging}} with {{Deep Learning}}},
  author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
  year = {2022},
  month = jul,
  urldate = {2023-05-18},
  abstract = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\BASPBVF4\Oktay et al. - 2022 - Attention U-Net Learning Where to Look for the Pa.pdf}
}

@article{olanderAdvancedDvorakTechnique2007,
  title = {The {{Advanced Dvorak Technique}}: {{Continued Development}} of an {{Objective Scheme}} to {{Estimate Tropical Cyclone Intensity Using Geostationary Infrared Satellite Imagery}}},
  shorttitle = {The {{Advanced Dvorak Technique}}},
  author = {Olander, Timothy L. and Velden, Christopher S.},
  year = {2007},
  month = apr,
  journal = {Weather and Forecasting},
  volume = {22},
  number = {2},
  pages = {287--298},
  issn = {0882-8156},
  doi = {10.1175/WAF975.1},
  urldate = {2019-06-10},
  abstract = {Tropical cyclones are becoming an increasing menace to society as populations grow in coastal regions. Forecasting the intensity of these often-temperamental weather systems can be a real challenge, especially if the true intensity at the forecast time is not well known. To address this issue, techniques to accurately estimate tropical cyclone intensity from satellites are a natural goal because in situ observations over the vast oceanic basins are scarce. The most widely utilized satellite-based method to estimate tropical cyclone intensity is the Dvorak technique, a partially subjective scheme that has been employed operationally at tropical forecast centers around the world for over 30 yr. With the recent advent of improved satellite sensors, the rapid advances in computing capacity, and accumulated experience with the behavioral characteristics of the Dvorak technique, the development of a fully automated, computer-based objective scheme to derive tropical cyclone intensity has become possible. In this paper the advanced Dvorak technique is introduced, which, as its name implies, is a derivative of the original Dvorak technique. The advanced Dvorak technique builds on the basic conceptual model and empirically derived rules of the original Dvorak technique, but advances the science and applicability in an automated environment that does not require human intervention. The algorithm is the culmination of a body of research that includes the objective Dvorak technique (ODT) and advanced objective Dvorak technique (AODT) developed at the University of Wisconsin---Madison's Cooperative Institute for Meteorological Satellite Studies. The ODT could only be applied to storms that possessed a minimum intensity of hurricane/typhoon strength. In addition, the ODT still required a storm center location to be manually selected by an analyst prior to algorithm execution. These issues were the primary motivations for the continued advancement of the algorithm (AODT). While these two objective schemes had as their primary goal to simply achieve the basic functionality and performance of the Dvorak technique in a computer-driven environment, the advanced Dvorak technique exceeds the boundaries of the original Dvorak technique through modifications based on rigorous statistical and empirical analysis. It is shown that the accuracy of the advanced Dvorak technique is statistically competitive with the original Dvorak technique, and can provide objective tropical cyclone intensity guidance for systems in all global basins.}
}

@article{olanderAdvancedDvorakTechnique2007a,
  title = {The {{Advanced Dvorak Technique}}: {{Continued Development}} of an {{Objective Scheme}} to {{Estimate Tropical Cyclone Intensity Using Geostationary Infrared Satellite Imagery}}},
  shorttitle = {The {{Advanced Dvorak Technique}}},
  author = {Olander, Timothy L. and Velden, Christopher S.},
  year = {2007},
  month = apr,
  journal = {Weather and Forecasting},
  volume = {22},
  number = {2},
  pages = {287--298},
  issn = {0882-8156},
  doi = {10.1175/WAF975.1},
  urldate = {2019-06-10},
  abstract = {Tropical cyclones are becoming an increasing menace to society as populations grow in coastal regions. Forecasting the intensity of these often-temperamental weather systems can be a real challenge, especially if the true intensity at the forecast time is not well known. To address this issue, techniques to accurately estimate tropical cyclone intensity from satellites are a natural goal because in situ observations over the vast oceanic basins are scarce. The most widely utilized satellite-based method to estimate tropical cyclone intensity is the Dvorak technique, a partially subjective scheme that has been employed operationally at tropical forecast centers around the world for over 30 yr. With the recent advent of improved satellite sensors, the rapid advances in computing capacity, and accumulated experience with the behavioral characteristics of the Dvorak technique, the development of a fully automated, computer-based objective scheme to derive tropical cyclone intensity has become possible. In this paper the advanced Dvorak technique is introduced, which, as its name implies, is a derivative of the original Dvorak technique. The advanced Dvorak technique builds on the basic conceptual model and empirically derived rules of the original Dvorak technique, but advances the science and applicability in an automated environment that does not require human intervention. The algorithm is the culmination of a body of research that includes the objective Dvorak technique (ODT) and advanced objective Dvorak technique (AODT) developed at the University of Wisconsin---Madison's Cooperative Institute for Meteorological Satellite Studies. The ODT could only be applied to storms that possessed a minimum intensity of hurricane/typhoon strength. In addition, the ODT still required a storm center location to be manually selected by an analyst prior to algorithm execution. These issues were the primary motivations for the continued advancement of the algorithm (AODT). While these two objective schemes had as their primary goal to simply achieve the basic functionality and performance of the Dvorak technique in a computer-driven environment, the advanced Dvorak technique exceeds the boundaries of the original Dvorak technique through modifications based on rigorous statistical and empirical analysis. It is shown that the accuracy of the advanced Dvorak technique is statistically competitive with the original Dvorak technique, and can provide objective tropical cyclone intensity guidance for systems in all global basins.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\AP2H3Q93\\Olander et Velden - 2007 - The Advanced Dvorak Technique Continued Developme.pdf;C\:\\Users\\cleme\\Zotero\\storage\\3XC557S9\\WAF975.html}
}

@incollection{olsenDiabeticRetinopathy2014,
  title = {Diabetic {{Retinopathy}}},
  booktitle = {Retina/{{Vitreous Preferred Practice Pattern}}},
  author = {Olsen, Timothy W. and Adelman, Ron A. and Flaxel, Christina J. and Folk, James C. and Pulido, Jose S. and Regillo, Carl D. and Hyman, Leslie},
  year = {2014},
  month = jun,
  publisher = {America Academy of Ophtalmology}
}

@incollection{olsenDiabeticRetinopathy2014a,
  title = {Diabetic {{Retinopathy}}},
  booktitle = {Retina/{{Vitreous Preferred Practice Pattern}}},
  author = {Olsen, Timothy W. and Adelman, Ron A. and Flaxel, Christina J. and Folk, James C. and Pulido, Jose S. and Regillo, Carl D. and Hyman, Leslie},
  year = {2014},
  month = jun,
  publisher = {America Academy of Ophtalmology}
}

@misc{OpticalCoherenceTomography,
  title = {Optical Coherence Tomography. - {{PubMed}} - {{NCBI}}},
  eprint = {1957169},
  eprinttype = {pubmed},
  urldate = {2019-11-14},
  pmid = {1957169}
}

@misc{OpticalCoherenceTomographya,
  title = {Optical Coherence Tomography. - {{PubMed}} - {{NCBI}}},
  eprint = {1957169},
  eprinttype = {pubmed},
  urldate = {2019-11-14},
  howpublished = {https://www.ncbi.nlm.nih.gov/pubmed/1957169},
  file = {C:\Users\cleme\Zotero\storage\CFMACAWR\1957169.html}
}

@article{ouardiniPracticalUnsupervisedAnomaly2019,
  title = {Towards {{Practical Unsupervised Anomaly Detection}} on {{Retinal Images}}},
  author = {Ouardini, Khalil and Yang, Huijuan and Unnikrishnan, Balagopal and Romain, Manon and Garcin, Camille and Zenati, Houssam and Campbell, J. Peter and Chiang, Michael F. and {Kalpathy-Cramer}, Jayashree and Chandrasekhar, Vijay and Krishnaswamy, Pavitra and Foo, Chuan-Sheng},
  year = {2019},
  month = oct,
  journal = {Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data},
  pages = {225--234},
  publisher = {Springer, Cham},
  doi = {10.1007/978-3-030-33391-1_26},
  urldate = {2021-04-12},
  abstract = {Supervised deep learning approaches provide state-of-the-art performance on medical image classification tasks for disease screening. However, these methods require large labeled datasets that...},
  langid = {english}
}

@inproceedings{ouardiniPracticalUnsupervisedAnomaly2019a,
  title = {Towards {{Practical Unsupervised Anomaly Detection}} on {{Retinal Images}}},
  booktitle = {{{DART}}/{{MIL3ID}}@{{MICCAI}}},
  author = {Ouardini, Khalil and Yang, Huijuan and Unnikrishnan, Balagopal and Romain, Manon and Garcin, Camille and Zenati, Houssam and Campbell, J. Peter and Chiang, Michael F. and {Kalpathy-Cramer}, Jayashree and Chandrasekhar, Vijay and Krishnaswamy, Pavitra and Foo, Chuan-Sheng},
  year = {2019},
  month = jan,
  urldate = {2021-04-12},
  abstract = {Supervised deep learning approaches provide state-of-the-art performance on medical image classification tasks for disease screening. However, these methods require large labeled datasets that...},
  langid = {english}
}

@inproceedings{ouardiniPracticalUnsupervisedAnomaly2019b,
  title = {Towards {{Practical Unsupervised Anomaly Detection}} on {{Retinal Images}}},
  booktitle = {{{DART}}/{{MIL3ID}}@{{MICCAI}}},
  author = {Ouardini, Khalil and Yang, Huijuan and Unnikrishnan, Balagopal and Romain, Manon and Garcin, Camille and Zenati, Houssam and Campbell, J. Peter and Chiang, Michael F. and {Kalpathy-Cramer}, Jayashree and Chandrasekhar, Vijay and Krishnaswamy, Pavitra and Foo, Chuan-Sheng},
  year = {2019},
  month = jan,
  urldate = {2021-04-12},
  abstract = {Supervised deep learning approaches provide state-of-the-art performance on medical image classification tasks for disease screening. However, these methods require large labeled datasets that...},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\ELHN6QU5\forum.html}
}

@article{ouardiniPracticalUnsupervisedAnomaly2019c,
  title = {Towards {{Practical Unsupervised Anomaly Detection}} on {{Retinal Images}}},
  author = {Ouardini, Khalil and Yang, Huijuan and Unnikrishnan, Balagopal and Romain, Manon and Garcin, Camille and Zenati, Houssam and Campbell, J. Peter and Chiang, Michael F. and {Kalpathy-Cramer}, Jayashree and Chandrasekhar, Vijay and Krishnaswamy, Pavitra and Foo, Chuan-Sheng},
  year = {2019},
  month = oct,
  journal = {Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data},
  pages = {225--234},
  publisher = {Springer, Cham},
  doi = {10.1007/978-3-030-33391-1_26},
  urldate = {2021-04-12},
  abstract = {Supervised deep learning approaches provide state-of-the-art performance on medical image classification tasks for disease screening. However, these methods require large labeled datasets that...},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\WRNP23HP\10.html}
}

@inproceedings{pallawalaAutomatedMicroaneurysmSegmentation2005,
  title = {Automated {{Microaneurysm Segmentation}} and {{Detection}} Using {{Generalized Eigenvectors}}},
  booktitle = {2005 {{Seventh IEEE Workshops}} on {{Applications}} of {{Computer Vision}} ({{WACV}}/{{MOTION}}'05) - {{Volume}} 1},
  author = {Pallawala, P. M. D. S. and Hsu, W. and Lee, M. L. and Goh, S. S.},
  year = {2005},
  month = jan,
  volume = {1},
  pages = {322--327},
  doi = {10.1109/ACVMOT.2005.26},
  abstract = {Diabetic retinopathy is a major cause of blindness and microaneurysms are the first clinically observable manifestations of diabetic retinopathy. Regular screening and timely intervention can halt or reverse the progression of this disease. This paper describes an approach that is based on the generalized eigenvectors of affinity matrix to extract microaneurysms from digital retinal images. Microaneurysms are in the low intensity regions and detection is complicated by their small sizes, the presence of retinal vessels, and their similarity to another type of retinal abnormality - haemorrhages. In order to accurately detect microaneurysms, the affinity matrix is defined to suppress larger structures such as blood vessels, haemorrhages, etc and to create uniform affinity distribution for pixels belonging to microaneurysms. The generalized eigenvector solution seeks to find the optimal segmentation for microaneurysms and provides indication to the possible locations of microaneurysms. We differentiate the true microaneurysms by studying their feature characteristics. Experiments on 70 retinal sub-images of diabetic patients indicate that we are able to achieve 93\% accuracy in the detection of microaneurysms.},
  keywords = {affinity matrix,automated microaneurysm detection,automated microaneurysm segmentation,Blindness,Diabetes,diabetic retinopathy,digital retinal images,Diseases,eigenvalues and eigenfunctions,eye,Filtering,generalized eigenvectors,Hemorrhaging,image segmentation,Image segmentation,matrix algebra,medical image processing,object detection,Pigmentation,Retina,Retinal vessels,Retinopathy}
}

@inproceedings{pallawalaAutomatedMicroaneurysmSegmentation2005a,
  title = {Automated {{Microaneurysm Segmentation}} and {{Detection}} Using {{Generalized Eigenvectors}}},
  booktitle = {2005 {{Seventh IEEE Workshops}} on {{Applications}} of {{Computer Vision}} ({{WACV}}/{{MOTION}}'05) - {{Volume}} 1},
  author = {Pallawala, P. M. D. S. and Hsu, W. and Lee, M. L. and Goh, S. S.},
  year = {2005},
  month = jan,
  volume = {1},
  pages = {322--327},
  doi = {10.1109/ACVMOT.2005.26},
  abstract = {Diabetic retinopathy is a major cause of blindness and microaneurysms are the first clinically observable manifestations of diabetic retinopathy. Regular screening and timely intervention can halt or reverse the progression of this disease. This paper describes an approach that is based on the generalized eigenvectors of affinity matrix to extract microaneurysms from digital retinal images. Microaneurysms are in the low intensity regions and detection is complicated by their small sizes, the presence of retinal vessels, and their similarity to another type of retinal abnormality - haemorrhages. In order to accurately detect microaneurysms, the affinity matrix is defined to suppress larger structures such as blood vessels, haemorrhages, etc and to create uniform affinity distribution for pixels belonging to microaneurysms. The generalized eigenvector solution seeks to find the optimal segmentation for microaneurysms and provides indication to the possible locations of microaneurysms. We differentiate the true microaneurysms by studying their feature characteristics. Experiments on 70 retinal sub-images of diabetic patients indicate that we are able to achieve 93\% accuracy in the detection of microaneurysms.},
  keywords = {affinity matrix,automated microaneurysm detection,automated microaneurysm segmentation,Blindness,Diabetes,diabetic retinopathy,digital retinal images,Diseases,eigenvalues and eigenfunctions,eye,Filtering,generalized eigenvectors,Hemorrhaging,image segmentation,Image segmentation,matrix algebra,medical image processing,object detection,Pigmentation,Retina,Retinal vessels,Retinopathy},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HMYKH87M\\Pallawala et al. - 2005 - Automated Microaneurysm Segmentation and Detection.pdf;C\:\\Users\\cleme\\Zotero\\storage\\S6DKU6JF\\4129498.html}
}

@inproceedings{panagiotakopoulosOnlineDomainAdaptation2022,
  title = {Online {{Domain Adaptation}} for {{Semantic Segmentation}} in {{Ever-Changing Conditions}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Panagiotakopoulos, Theodoros and Dovesi, Pier Luigi and {H{\"a}renstam-Nielsen}, Linus and Poggi, Matteo},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {128--146},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-19830-4_8},
  abstract = {Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between training and testing data and is, in most cases, carried out in offline manner. However, domain changes may occur continuously and unpredictably during deployment (e.g. sudden weather changes). In such conditions, deep neural networks witness dramatic drops in accuracy and offline adaptation may not be enough to contrast it. In this paper, we tackle Online Domain Adaptation (OnDA) for semantic segmentation. We design a pipeline that is robust to continuous domain shifts, either gradual or sudden, and we evaluate it in the case of rainy and foggy scenarios. Our experiments show that our framework can effectively adapt to new domains during deployment, while not being affected by catastrophic forgetting of the previous domains.},
  isbn = {978-3-031-19830-4},
  langid = {english}
}

@inproceedings{panagiotakopoulosOnlineDomainAdaptation2022a,
  title = {Online {{Domain Adaptation}} for~{{Semantic Segmentation}} in~{{Ever-Changing Conditions}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Panagiotakopoulos, Theodoros and Dovesi, Pier Luigi and {H{\"a}renstam-Nielsen}, Linus and Poggi, Matteo},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {128--146},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-19830-4_8},
  abstract = {Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between training and testing data and is, in most cases, carried out in offline manner. However, domain changes may occur continuously and unpredictably during deployment (e.g. sudden weather changes). In such conditions, deep neural networks witness dramatic drops in accuracy and offline adaptation may not be enough to contrast it. In this paper, we tackle Online Domain Adaptation (OnDA) for semantic segmentation. We design a pipeline that is robust to continuous domain shifts, either gradual or sudden, and we evaluate it in the case of rainy and foggy scenarios. Our experiments show that our framework can effectively adapt to new domains during deployment, while not being affected by catastrophic forgetting of the previous domains.},
  isbn = {978-3-031-19830-4},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\UZANLB57\Panagiotakopoulos et al. - 2022 - Online Domain Adaptation for Semantic Segmentation.pdf}
}

@article{panchalRetinalFundusMultiDisease2023,
  title = {Retinal {{Fundus Multi-Disease Image Dataset}} ({{RFMiD}}) 2.0: {{A Dataset}} of {{Frequently}} and {{Rarely Identified Diseases}}},
  shorttitle = {Retinal {{Fundus Multi-Disease Image Dataset}} ({{RFMiD}}) 2.0},
  author = {Panchal, Sachin and Naik, Ankita and Kokare, Manesh and Pachade, Samiksha and Naigaonkar, Rushikesh and Phadnis, Prerana and Bhange, Archana},
  year = {2023},
  month = feb,
  journal = {Data},
  volume = {8},
  number = {2},
  pages = {29},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2306-5729},
  doi = {10.3390/data8020029},
  urldate = {2023-06-27},
  abstract = {Irreversible vision loss is a worldwide threat. Developing a computer-aided diagnosis system to detect retinal fundus diseases is extremely useful and serviceable to ophthalmologists. Early detection, diagnosis, and correct treatment could save the eye's vision. Nevertheless, an eye may be afflicted with several diseases if proper care is not taken. A single retinal fundus image might be linked to one or more diseases. Age-related macular degeneration, cataracts, diabetic retinopathy, Glaucoma, and uncorrected refractive errors are the leading causes of visual impairment. Our research team at the center of excellence lab has generated a new dataset called the Retinal Fundus Multi-Disease Image Dataset 2.0 (RFMiD2.0). This dataset includes around 860 retinal fundus images, annotated by three eye specialists, and is a multiclass, multilabel dataset. We gathered images from a research facility in Jalna and Nanded, where patients across Maharashtra come for preventative and therapeutic eye care. Our dataset would be the second publicly available dataset consisting of the most frequent diseases, along with some rarely identified diseases. This dataset is auxiliary to the previously published RFMiD dataset. This dataset would be significant for the research and development of artificial intelligence in ophthalmology.},
  langid = {english},
  keywords = {data analysis,data annotation,multilabel classification,ocular diseases,retinal fundus image dataset}
}

@article{panchalRetinalFundusMultiDisease2023a,
  title = {Retinal {{Fundus Multi-Disease Image Dataset}} ({{RFMiD}}) 2.0: {{A Dataset}} of {{Frequently}} and {{Rarely Identified Diseases}}},
  shorttitle = {Retinal {{Fundus Multi-Disease Image Dataset}} ({{RFMiD}}) 2.0},
  author = {Panchal, Sachin and Naik, Ankita and Kokare, Manesh and Pachade, Samiksha and Naigaonkar, Rushikesh and Phadnis, Prerana and Bhange, Archana},
  year = {2023},
  month = feb,
  journal = {Data},
  volume = {8},
  number = {2},
  pages = {29},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2306-5729},
  doi = {10.3390/data8020029},
  urldate = {2023-06-27},
  abstract = {Irreversible vision loss is a worldwide threat. Developing a computer-aided diagnosis system to detect retinal fundus diseases is extremely useful and serviceable to ophthalmologists. Early detection, diagnosis, and correct treatment could save the eye's vision. Nevertheless, an eye may be afflicted with several diseases if proper care is not taken. A single retinal fundus image might be linked to one or more diseases. Age-related macular degeneration, cataracts, diabetic retinopathy, Glaucoma, and uncorrected refractive errors are the leading causes of visual impairment. Our research team at the center of excellence lab has generated a new dataset called the Retinal Fundus Multi-Disease Image Dataset 2.0 (RFMiD2.0). This dataset includes around 860 retinal fundus images, annotated by three eye specialists, and is a multiclass, multilabel dataset. We gathered images from a research facility in Jalna and Nanded, where patients across Maharashtra come for preventative and therapeutic eye care. Our dataset would be the second publicly available dataset consisting of the most frequent diseases, along with some rarely identified diseases. This dataset is auxiliary to the previously published RFMiD dataset. This dataset would be significant for the research and development of artificial intelligence in ophthalmology.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {data analysis,data annotation,multilabel classification,ocular diseases,retinal fundus image dataset},
  file = {C:\Users\cleme\Zotero\storage\FDPEUFB3\Panchal et al. - 2023 - Retinal Fundus Multi-Disease Image Dataset (RFMiD).pdf}
}

@article{pandeyTransformationalRoleGPU2022,
  title = {The Transformational Role of {{GPU}} Computing and Deep Learning in Drug Discovery},
  author = {Pandey, Mohit and Fernandez, Michael and Gentile, Francesco and Isayev, Olexandr and Tropsha, Alexander and Stern, Abraham C. and Cherkasov, Artem},
  year = {2022},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {3},
  pages = {211--221},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00463-x},
  urldate = {2023-02-15},
  abstract = {Deep learning has disrupted nearly every field of research, including those of direct importance to drug discovery, such as medicinal chemistry and pharmacology. This revolution has largely been attributed to the unprecedented advances in highly parallelizable graphics processing units (GPUs) and the development of GPU-enabled algorithms. In this Review, we present a comprehensive overview of historical trends and recent advances in GPU algorithms and discuss their immediate impact on the discovery of new drugs and drug targets. We also cover the state-of-the-art of deep learning architectures that have found practical applications in both early drug discovery and consequent hit-to-lead optimization stages, including the acceleration of molecular docking, the evaluation of off-target effects and the prediction of pharmacological properties. We conclude by discussing the impacts of GPU acceleration and deep learning models on the global democratization of the field of drug discovery that may lead to efficient exploration of the ever-expanding chemical universe to accelerate the discovery of novel medicines.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Cheminformatics,Drug discovery,High-throughput screening}
}

@article{pandeyTransformationalRoleGPU2022a,
  title = {The Transformational Role of {{GPU}} Computing and Deep Learning in Drug Discovery},
  author = {Pandey, Mohit and Fernandez, Michael and Gentile, Francesco and Isayev, Olexandr and Tropsha, Alexander and Stern, Abraham C. and Cherkasov, Artem},
  year = {2022},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {3},
  pages = {211--221},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00463-x},
  urldate = {2023-02-15},
  abstract = {Deep learning has disrupted nearly every field of research, including those of direct importance to drug discovery, such as medicinal chemistry and pharmacology. This revolution has largely been attributed to the unprecedented advances in highly parallelizable graphics processing units (GPUs) and the development of GPU-enabled algorithms. In this Review, we present a comprehensive overview of historical trends and recent advances in GPU algorithms and discuss their immediate impact on the discovery of new drugs and drug targets. We also cover the state-of-the-art of deep learning architectures that have found practical applications in both early drug discovery and consequent hit-to-lead optimization stages, including the acceleration of molecular docking, the evaluation of off-target effects and the prediction of pharmacological properties. We conclude by discussing the impacts of GPU acceleration and deep learning models on the global democratization of the field of drug discovery that may lead to efficient exploration of the ever-expanding chemical universe to accelerate the discovery of novel medicines.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Cheminformatics,Drug discovery,High-throughput screening},
  file = {C:\Users\cleme\Zotero\storage\L4CRI87G\Pandey et al. - 2022 - The transformational role of GPU computing and dee.pdf}
}

@article{panwarFundusPhotography21st2016,
  title = {Fundus {{Photography}} in the 21st {{Century}}---{{A Review}} of {{Recent Technological Advances}} and {{Their Implications}} for {{Worldwide Healthcare}}},
  author = {Panwar, Nishtha and Huang, Philemon and Lee, Jiaying and Keane, Pearse A. and Chuan, Tjin Swee and Richhariya, Ashutosh and Teoh, Stephen and Lim, Tock Han and Agrawal, Rupesh},
  year = {2016},
  month = mar,
  journal = {Telemedicine Journal and e-Health},
  volume = {22},
  number = {3},
  pages = {198--208},
  issn = {1530-5627},
  doi = {10.1089/tmj.2015.0068},
  urldate = {2019-11-12},
  abstract = {Background: The introduction of fundus photography has impacted retinal imaging and retinal screening programs significantly. Literature Review: Fundus cameras play a vital role in addressing the cause of preventive blindness. More attention is being turned to developing countries, where infrastructure and access to healthcare are limited. One of the major limitations for tele-ophthalmology is restricted access to the office-based fundus camera. Results: Recent advances in access to telecommunications coupled with introduction of portable cameras and smartphone-based fundus imaging systems have resulted in an exponential surge in available technologies for portable fundus photography. Retinal cameras in the near future would have to cater to these needs by featuring a low-cost, portable design with automated controls and digitalized images with Web-based transfer. Conclusions: In this review, we aim to highlight the advances of fundus photography for retinal screening as well as discuss the advantages, disadvantages, and implications of the various technologies that are currently available.},
  pmcid = {PMC4790203},
  pmid = {26308281}
}

@article{panwarFundusPhotography21st2016a,
  title = {Fundus {{Photography}} in the 21st {{Century}}---{{A Review}} of {{Recent Technological Advances}} and {{Their Implications}} for {{Worldwide Healthcare}}},
  author = {Panwar, Nishtha and Huang, Philemon and Lee, Jiaying and Keane, Pearse A. and Chuan, Tjin Swee and Richhariya, Ashutosh and Teoh, Stephen and Lim, Tock Han and Agrawal, Rupesh},
  year = {2016},
  month = mar,
  journal = {Telemedicine Journal and e-Health},
  volume = {22},
  number = {3},
  pages = {198--208},
  issn = {1530-5627},
  doi = {10.1089/tmj.2015.0068},
  urldate = {2019-11-12},
  abstract = {Background: The introduction of fundus photography has impacted retinal imaging and retinal screening programs significantly. Literature Review: Fundus cameras play a vital role in addressing the cause of preventive blindness. More attention is being turned to developing countries, where infrastructure and access to healthcare are limited. One of the major limitations for tele-ophthalmology is restricted access to the office-based fundus camera. Results: Recent advances in access to telecommunications coupled with introduction of portable cameras and smartphone-based fundus imaging systems have resulted in an exponential surge in available technologies for portable fundus photography. Retinal cameras in the near future would have to cater to these needs by featuring a low-cost, portable design with automated controls and digitalized images with Web-based transfer. Conclusions: In this review, we aim to highlight the advances of fundus photography for retinal screening as well as discuss the advantages, disadvantages, and implications of the various technologies that are currently available.},
  pmcid = {PMC4790203},
  pmid = {26308281},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\FD6DKIPD\\panwar2016.pdf;C\:\\Users\\cleme\\Zotero\\storage\\YM2HE3ZU\\Panwar et al. - 2016 - Fundus Photography in the 21st Century—A Review of.pdf}
}

@misc{PapersCodeLayer,
  title = {Papers with {{Code}} - {{Layer Normalization}}},
  urldate = {2023-02-22},
  abstract = {Implemented in 29 code libraries.},
  langid = {english}
}

@misc{PapersCodeLayera,
  title = {Papers with {{Code}} - {{Layer Normalization Explained}}},
  urldate = {2023-02-22},
  abstract = {Unlike batch normalization, Layer Normalization directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for RNNs and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with Transformer models. We compute the layer normalization statistics over all the hidden units in the same layer as follows: \$\$ {\textbackslash}mu{\^\{}l\vphantom\{\} = {\textbackslash}frac\{1\}\{H\}{\textbackslash}sum{\^\{}H\vphantom\{\}\_\{i=1\}a\_\{i\}{\^\{}l\vphantom\{\} \$\$ \$\$ {\textbackslash}sigma{\^\{}l\vphantom\{\} = {\textbackslash}sqrt\{{\textbackslash}frac\{1\}\{H\}{\textbackslash}sum{\^\{}H\}\_\{i=1\}{\textbackslash}left(a\_\{i\}{\^\{}l\vphantom\{\}-{\textbackslash}mu{\^\{}l\vphantom\{\}{\textbackslash}right){\^\{}2\vphantom\{\}\vphantom\{\} \$\$ where \$H\$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms \${\textbackslash}mu\$ and \${\textbackslash}sigma\$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.},
  langid = {english}
}

@misc{PapersCodeLayerb,
  title = {Papers with {{Code}} - {{Layer Normalization Explained}}},
  urldate = {2023-02-22},
  abstract = {Unlike batch normalization, Layer Normalization directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for RNNs and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with Transformer models. We compute the layer normalization statistics over all the hidden units in the same layer as follows: \$\$ {\textbackslash}mu{\textasciicircum}\{l\} = {\textbackslash}frac\{1\}\{H\}{\textbackslash}sum{\textasciicircum}\{H\}\_\{i=1\}a\_\{i\}{\textasciicircum}\{l\} \$\$ \$\$ {\textbackslash}sigma{\textasciicircum}\{l\} = {\textbackslash}sqrt\{{\textbackslash}frac\{1\}\{H\}{\textbackslash}sum{\textasciicircum}\{H\}\_\{i=1\}{\textbackslash}left(a\_\{i\}{\textasciicircum}\{l\}-{\textbackslash}mu{\textasciicircum}\{l\}{\textbackslash}right){\textasciicircum}\{2\}\}  \$\$ where \$H\$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms \${\textbackslash}mu\$ and \${\textbackslash}sigma\$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.},
  howpublished = {https://paperswithcode.com/method/layer-normalization},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\G3P9JA3W\layer-normalization.html}
}

@misc{PapersCodeLayerc,
  title = {Papers with {{Code}} - {{Layer Normalization}}},
  urldate = {2023-02-22},
  abstract = {Implemented in 29 code libraries.},
  howpublished = {https://paperswithcode.com/paper/layer-normalization},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\AIJNX92X\layer-normalization.html}
}

@misc{PapersCodeSuperpixel,
  title = {Papers with {{Code}} - {{Superpixel Transformers}} for {{Efficient Semantic Segmentation}}},
  urldate = {2024-02-13},
  abstract = {No code available yet.},
  howpublished = {https://paperswithcode.com/paper/superpixel-transformers-for-efficient},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\BV3AQA7R\superpixel-transformers-for-efficient.html}
}

@article{parkOpticDiscHemorrhage2017,
  title = {Optic {{Disc Hemorrhage}} and {{Lamina Cribrosa Defects}} in {{Glaucoma Progression}}},
  author = {Park, Hae-Young Lopilly and Lee, Jiyoung and Jung, Younhea and Park, Chan Kee},
  year = {2017},
  month = jun,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {1--10},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-03828-0},
  urldate = {2019-11-15},
  abstract = {Both disc hemorrhages (DH) and focal lamina cribrosa (LC) defects are recently considered as a progression factor in glaucoma. However, the clinical relevance of the presence of LC findings at the site of the DH has not yet been determined. We conducted a prospective study enrolling a total of 98 glaucoma eyes with DH and 37 OAG eyes with focal LC defect without DH to determine whether visual field (VF) progression differs according to the findings of the LC that had been evaluated by enhanced depth imaging (EDI) of optical coherence tomography (OCT) and its relationship with DH. Only the presence of focal LC defects was significantly different between the progressing and stable patients (P\,{$<$}\,0.001). Baseline intraocular pressure (hazard ratio [HR], 1.076; P\,=\,0.098) and the presence of focal LC defects at the DH site (HR, 2.620; P\,=\,0.002) were found to be associated with VF progression. Glaucoma eyes with DH at the site of focal LC defects showed frequent and faster VF progression compared with DH not accompanied by LC alterations or LC alterations not accompanied by DH. Evaluating LC alterations in glaucoma eyes with DH may be important in predicting the progression of glaucoma.},
  copyright = {2017 The Author(s)},
  langid = {english}
}

@article{parkOpticDiscHemorrhage2017a,
  title = {Optic {{Disc Hemorrhage}} and {{Lamina Cribrosa Defects}} in {{Glaucoma Progression}}},
  author = {Park, Hae-Young Lopilly and Lee, Jiyoung and Jung, Younhea and Park, Chan Kee},
  year = {2017},
  month = jun,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {1--10},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-03828-0},
  urldate = {2019-11-15},
  abstract = {Both disc hemorrhages (DH) and focal lamina cribrosa (LC) defects are recently considered as a progression factor in glaucoma. However, the clinical relevance of the presence of LC findings at the site of the DH has not yet been determined. We conducted a prospective study enrolling a total of 98 glaucoma eyes with DH and 37 OAG eyes with focal LC defect without DH to determine whether visual field (VF) progression differs according to the findings of the LC that had been evaluated by enhanced depth imaging (EDI) of optical coherence tomography (OCT) and its relationship with DH. Only the presence of focal LC defects was significantly different between the progressing and stable patients (P\,{$<$}\,0.001). Baseline intraocular pressure (hazard ratio [HR], 1.076; P\,=\,0.098) and the presence of focal LC defects at the DH site (HR, 2.620; P\,=\,0.002) were found to be associated with VF progression. Glaucoma eyes with DH at the site of focal LC defects showed frequent and faster VF progression compared with DH not accompanied by LC alterations or LC alterations not accompanied by DH. Evaluating LC alterations in glaucoma eyes with DH may be important in predicting the progression of glaucoma.},
  copyright = {2017 The Author(s)},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\TFSWPKXV\\Park et al. - 2017 - Optic Disc Hemorrhage and Lamina Cribrosa Defects .pdf;C\:\\Users\\cleme\\Zotero\\storage\\LE4UEPYQ\\s41598-017-03828-0.html}
}

@article{parmarImageTransformer2018,
  title = {Image {{Transformer}}},
  author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, {\L}ukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  year = {2018},
  month = jun,
  journal = {arXiv:1802.05751 [cs]},
  eprint = {1802.05751},
  primaryclass = {cs},
  urldate = {2021-02-28},
  abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the selfattention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{parmarImageTransformer2018a,
  title = {Image {{Transformer}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Parmar, Niki J. and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  year = {2018},
  urldate = {2021-11-02}
}

@inproceedings{parmarImageTransformer2018b,
  title = {Image {{Transformer}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Parmar, Niki J. and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  year = {2018},
  urldate = {2021-11-02}
}

@article{parmarImageTransformer2018c,
  title = {Image {{Transformer}}},
  author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, {\L}ukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  year = {2018},
  month = jun,
  journal = {arXiv:1802.05751 [cs]},
  eprint = {1802.05751},
  primaryclass = {cs},
  urldate = {2021-02-28},
  abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the selfattention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\8KZM88PR\Parmar et al. - 2018 - Image Transformer.pdf}
}

@article{parvaizVisionTransformersMedical2023,
  title = {Vision {{Transformers}} in Medical Computer Vision---{{A}} Contemplative Retrospection},
  author = {Parvaiz, Arshi and Khalid, Muhammad Anwaar and Zafar, Rukhsana and Ameer, Huma and Ali, Muhammad and Fraz, Muhammad Moazam},
  year = {2023},
  month = jun,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {122},
  pages = {106126},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2023.106126},
  urldate = {2023-07-12},
  abstract = {Vision Transformers (ViTs), with the magnificent potential to unravel the information contained within images, have evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are immensely utilized by plenty of researchers to perform new as well as former experiments. Here, in this article, we investigate the intersection of vision transformers and medical images. We proffered an overview of various ViT based frameworks that are being used by different researchers to decipher the obstacles in medical computer vision. We surveyed the applications of Vision Transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion detection, captioning, report generation, and reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment process. Along with this, we also demystify several imaging modalities used in medical computer vision. Moreover, to get more insight and deeper understanding, the self-attention mechanism of transformers is also explained briefly. Conclusively, the ViT based solutions for each image analytics task are critically analyzed, open challenges are discussed and the pointers to possible solutions for future direction are deliberated. We hope this review article will open future research directions for medical computer vision researchers.},
  langid = {english},
  keywords = {Diagnostic image analysis,Literature survey,Medical computer vision,Medical image analysis,Self attention,Vision Transformers}
}

@article{parvaizVisionTransformersMedical2023a,
  title = {Vision {{Transformers}} in Medical Computer Vision---{{A}} Contemplative Retrospection},
  author = {Parvaiz, Arshi and Khalid, Muhammad Anwaar and Zafar, Rukhsana and Ameer, Huma and Ali, Muhammad and Fraz, Muhammad Moazam},
  year = {2023},
  month = jun,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {122},
  pages = {106126},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2023.106126},
  urldate = {2023-07-12},
  abstract = {Vision Transformers (ViTs), with the magnificent potential to unravel the information contained within images, have evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are immensely utilized by plenty of researchers to perform new as well as former experiments. Here, in this article, we investigate the intersection of vision transformers and medical images. We proffered an overview of various ViT based frameworks that are being used by different researchers to decipher the obstacles in medical computer vision. We surveyed the applications of Vision Transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion detection, captioning, report generation, and reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment process. Along with this, we also demystify several imaging modalities used in medical computer vision. Moreover, to get more insight and deeper understanding, the self-attention mechanism of transformers is also explained briefly. Conclusively, the ViT based solutions for each image analytics task are critically analyzed, open challenges are discussed and the pointers to possible solutions for future direction are deliberated. We hope this review article will open future research directions for medical computer vision researchers.},
  langid = {english},
  keywords = {Diagnostic image analysis,Literature survey,Medical computer vision,Medical image analysis,Self attention,Vision Transformers},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\GYWLCSNH\\Parvaiz et al. - 2023 - Vision Transformers in medical computer vision—A c.pdf;C\:\\Users\\cleme\\Zotero\\storage\\C3XVAGZP\\S095219762300310X.html}
}

@article{patelUltrawidefieldRetinalImaging2020,
  title = {Ultra-Widefield Retinal Imaging: An Update on Recent Advances},
  shorttitle = {Ultra-Widefield Retinal Imaging},
  author = {Patel, Samir N. and Shi, Angell and Wibbelsman, Turner D. and Klufas, Michael A.},
  year = {2020},
  month = jan,
  journal = {Therapeutic Advances in Ophthalmology},
  volume = {12},
  pages = {2515841419899495},
  publisher = {SAGE Publications Ltd STM},
  issn = {2515-8414},
  doi = {10.1177/2515841419899495},
  urldate = {2024-02-22},
  abstract = {The development of ultra-widefield retinal imaging has accelerated our understanding of common retinal diseases. As we continue to validate the diagnostic and prognostic significance of pathology in the retinal periphery, the ability to visualize and evaluate these features in an efficient and patient-friendly manner will become more important. Current interest in ultra-widefield imaging includes the development of potential biomarkers of disease progression and indicators of preclinical disease development. This article reviews the current ultra-widefield imaging systems and recent advances in their applications to clinical practice with a focus on diabetic retinopathy, retinal vein occlusion, uveitis, and pediatric retina.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\YDLVQ82X\Patel et al. - 2020 - Ultra-widefield retinal imaging an update on rece.pdf}
}

@misc{patricioExplainableDeepLearning2022,
  title = {Explainable {{Deep Learning Methods}} in {{Medical Imaging Diagnosis}}: {{A Survey}}},
  shorttitle = {Explainable {{Deep Learning Methods}} in {{Medical Imaging Diagnosis}}},
  author = {Patr{\'i}cio, Cristiano and Neves, Jo{\~a}o C. and Teixeira, Lu{\'i}s F.},
  year = {2022},
  month = jun,
  number = {arXiv:2205.04766},
  eprint = {2205.04766},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.04766},
  urldate = {2023-05-05},
  abstract = {The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box-ness of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation-based methods. Finally, the major challenges in applying XAI to medical imaging and the future research directions on the topic are also discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@misc{patricioExplainableDeepLearning2022a,
  title = {Explainable {{Deep Learning Methods}} in {{Medical Imaging Diagnosis}}: {{A Survey}}},
  shorttitle = {Explainable {{Deep Learning Methods}} in {{Medical Imaging Diagnosis}}},
  author = {Patr{\'i}cio, Cristiano and Neves, Jo{\~a}o C. and Teixeira, Lu{\'i}s F.},
  year = {2022},
  month = jun,
  number = {arXiv:2205.04766},
  eprint = {2205.04766},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.04766},
  urldate = {2023-05-05},
  abstract = {The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box-ness of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation-based methods. Finally, the major challenges in applying XAI to medical imaging and the future research directions on the topic are also discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\U7HDY55G\\Patrício et al. - 2022 - Explainable Deep Learning Methods in Medical Imagi.pdf;C\:\\Users\\cleme\\Zotero\\storage\\D87U2GQA\\2205.html}
}

@misc{PDFAgerelatedMacular,
  title = {(7) ({{PDF}}) {{Age-related Macular Degeneration Identification In Volumetric Optical Coherence Tomography Using Decomposition}} and {{Local Feature Extraction}}},
  urldate = {2019-11-22},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  langid = {english}
}

@misc{PDFAgerelatedMaculara,
  title = {(7) ({{PDF}}) {{Age-related Macular Degeneration Identification In Volumetric Optical Coherence Tomography Using Decomposition}} and {{Local Feature Extraction}}},
  journal = {ResearchGate},
  urldate = {2019-11-22},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  howpublished = {https://www.researchgate.net/publication/260814094\_Age-related\_Macular\_Degeneration\_Identification\_In\_Volumetric\_Optical\_Coherence\_Tomography\_Using\_Decomposition\_and\_Local\_Feature\_Extraction},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\53PTGPF6\260814094_Age-related_Macular_Degeneration_Identification_In_Volumetric_Optical_Coherence_Tomog.html}
}

@misc{PDFHierarchicalProbabilistic,
  title = {[{{PDF}}] {{A Hierarchical Probabilistic U-Net}} for {{Modeling Multi-Scale Ambiguities}} {\textbar} {{Semantic Scholar}}},
  urldate = {2023-09-06}
}

@misc{PDFHierarchicalProbabilistica,
  title = {[{{PDF}}] {{A Hierarchical Probabilistic U-Net}} for {{Modeling Multi-Scale Ambiguities}} {\textbar} {{Semantic Scholar}}},
  urldate = {2023-09-06},
  howpublished = {https://www.semanticscholar.org/paper/A-Hierarchical-Probabilistic-U-Net-for-Modeling-Kohl-Romera-Paredes/565763625ca9c070d95b40aa1351127127b2737e},
  file = {C:\Users\cleme\Zotero\storage\TJU82Q79\565763625ca9c070d95b40aa1351127127b2737e.html}
}

@misc{PDFRealtimeSurgical,
  title = {({{PDF}}) {{Real-time Surgical Instrument Detection}} in {{Robot-Assisted Surgery}} Using a {{Convolutional Neural Network Cascade}}},
  doi = {http://dx.doi.org/10.1049/htl.2019.0064},
  urldate = {2020-05-06},
  abstract = {PDF {\textbar} Surgical instrument detection in robot-assisted surgery videos is an import vision component for these systems. Most of the current deep learning... {\textbar} Find, read and cite all the research you need on ResearchGate},
  langid = {english}
}

@misc{PDFRealtimeSurgicala,
  title = {({{PDF}}) {{Real-time Surgical Instrument Detection}} in {{Robot-Assisted Surgery}} Using a {{Convolutional Neural Network Cascade}}},
  journal = {ResearchGate},
  doi = {http://dx.doi.org/10.1049/htl.2019.0064},
  urldate = {2020-05-06},
  abstract = {PDF {\textbar} Surgical instrument detection in robot-assisted surgery videos is an import vision component for these systems. Most of the current deep learning... {\textbar} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/336335279\_Real-time\_Surgical\_Instrument\_Detection\_in\_Robot-Assisted\_Surgery\_using\_a\_Convolutional\_Neural\_Network\_Cascade},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\7LUTJY2N\336335279_Real-time_Surgical_Instrument_Detection_in_Robot-Assisted_Surgery_using_a_Convolution.html}
}

@article{pearceReviewAdvancementsEvidence2020a,
  title = {A {{Review}} of {{Advancements}} and {{Evidence Gaps}} in {{Diabetic Retinopathy Screening Models}}},
  author = {Pearce, Elizabeth and Sivaprasad, Sobha},
  year = {2020},
  month = oct,
  journal = {Clinical Ophthalmology (Auckland, N.Z.)},
  volume = {14},
  pages = {3285--3296},
  issn = {1177-5467},
  doi = {10.2147/OPTH.S267521},
  urldate = {2021-04-12},
  abstract = {Diabetic retinopathy (DR) is a microvascular complication of diabetes with a prevalence of {\textasciitilde}35\%, and is one of the leading causes of visual impairment in people of working age in most developed countries. The earliest stage of DR, non-proliferative DR (NPDR), may progress to sight-threatening DR (STDR). Thus, early detection of DR and active regular screening of patients with diabetes are necessary for earlier intervention to prevent sight loss. While some countries offer systematic DR screening, most nations are reliant on opportunistic screening or do not offer any screening owing to limited healthcare resources and infrastructure. Currently, retinal imaging approaches for DR screening include those with and without mydriasis, imaging in single or multiple fields, and the use of conventional or ultra-wide-field imaging. Advances in telescreening and automated detection facilitate screening in previously hard-to-reach communities. Despite the heterogeneity in approaches to fit local needs, an evidence base must be created for each model to inform practice. In this review, we appraise different aspects of DR screening, including technological advances, identify evidence gaps, and propose several studies to improve DR screening globally, with a view to identifying patients with moderate-to-severe NPDR who would benefit if a convenient treatment option to delay progression to STDR became available.},
  pmcid = {PMC7569040},
  pmid = {33116380},
  file = {C:\Users\cleme\Zotero\storage\I9CXK3CT\Pearce et Sivaprasad - 2020 - A Review of Advancements and Evidence Gaps in Diab.pdf}
}

@misc{PearlCausalInference,
  title = {Pearl : {{Causal}} Inference in Statistics: {{An}} Overview},
  urldate = {2020-01-19}
}

@misc{PearlCausalInferencea,
  title = {Pearl : {{Causal}} Inference in Statistics: {{An}} Overview},
  urldate = {2020-01-19},
  howpublished = {https://projecteuclid.org/euclid.ssu/1255440554http://projecteuclid.org/euclid.ssu/1255440554},
  file = {C:\Users\cleme\Zotero\storage\V5I2BXU4\1255440554.html}
}

@article{pearlCausalInferenceStatistics2009,
  title = {Causal Inference in Statistics: {{An}} Overview},
  shorttitle = {Causal Inference in Statistics},
  author = {Pearl, Judea},
  year = {2009},
  journal = {Statistics Surveys},
  volume = {3},
  number = {0},
  pages = {96--146},
  issn = {1935-7516},
  doi = {10.1214/09-SS057},
  urldate = {2020-01-19},
  abstract = {This review presents empirical researchers with recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called ``causal effects'' or ``policy evaluation'') (2) queries about probabilities of counterfactuals, (including assessment of ``regret,'' ``attribution'' or ``causes of effects'') and (3) queries about direct and indirect effects (also known as ``mediation''). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
  langid = {english}
}

@article{pearlCausalInferenceStatistics2009a,
  title = {Causal Inference in Statistics: {{An}} Overview},
  shorttitle = {Causal Inference in Statistics},
  author = {Pearl, Judea},
  year = {2009},
  journal = {Statistics Surveys},
  volume = {3},
  number = {0},
  pages = {96--146},
  issn = {1935-7516},
  doi = {10.1214/09-SS057},
  urldate = {2020-01-19},
  abstract = {This review presents empirical researchers with recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called ``causal effects'' or ``policy evaluation'') (2) queries about probabilities of counterfactuals, (including assessment of ``regret,'' ``attribution'' or ``causes of effects'') and (3) queries about direct and indirect effects (also known as ``mediation''). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\VFJMB3YG\Pearl - 2009 - Causal inference in statistics An overview.pdf}
}

@article{pekalaDeepLearningBased2019,
  title = {Deep Learning Based Retinal {{OCT}} Segmentation},
  author = {Pekala, M. and Joshi, N. and Liu, T. Y. Alvin and Bressler, N. M. and DeBuc, D. Cabrera and Burlina, P.},
  year = {2019},
  month = nov,
  journal = {Computers in Biology and Medicine},
  volume = {114},
  pages = {103445},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2019.103445},
  urldate = {2022-07-08},
  abstract = {We look at the recent application of deep learning (DL) methods in automated fine-grained segmentation of spectral domain optical coherence tomography (OCT) images of the retina. We describe a new method combining fully convolutional networks (FCN) with Gaussian Processes for post processing. We report performance comparisons between the proposed approach, human clinicians, and other machine learning (ML) such as graph based approaches. The approach is demonstrated on an OCT dataset consisting of mild non-proliferative diabetic retinopathy from the University of Miami. The method is shown to have performance on par with humans, also compares favorably with the other ML methods, and appears to have as small or smaller mean unsigned error (equal to 1.06), versus errors ranging from 1.17 to 1.81 for other methods, and compared with human error of 1.10.},
  langid = {english},
  keywords = {Fully convolutional networks,Gaussian process regression,Neurodegenerative,OCT segmentation,Retinal and vascular diseases}
}

@article{pekalaDeepLearningBased2019a,
  title = {Deep Learning Based Retinal {{OCT}} Segmentation},
  author = {Pekala, M. and Joshi, N. and Liu, T. Y. Alvin and Bressler, N. M. and DeBuc, D. Cabrera and Burlina, P.},
  year = {2019},
  month = nov,
  journal = {Computers in Biology and Medicine},
  volume = {114},
  pages = {103445},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2019.103445},
  urldate = {2022-07-08},
  abstract = {We look at the recent application of deep learning (DL) methods in automated fine-grained segmentation of spectral domain optical coherence tomography (OCT) images of the retina. We describe a new method combining fully convolutional networks (FCN) with Gaussian Processes for post processing. We report performance comparisons between the proposed approach, human clinicians, and other machine learning (ML) such as graph based approaches. The approach is demonstrated on an OCT dataset consisting of mild non-proliferative diabetic retinopathy from the University of Miami. The method is shown to have performance on par with humans, also compares favorably with the other ML methods, and appears to have as small or smaller mean unsigned error (equal to 1.06), versus errors ranging from 1.17 to 1.81 for other methods, and compared with human error of 1.10.},
  langid = {english},
  keywords = {Fully convolutional networks,Gaussian process regression,Neurodegenerative,OCT segmentation,Retinal and vascular diseases},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\ZCCBJE8Z\\Pekala et al. - 2019 - Deep learning based retinal OCT segmentation.pdf;C\:\\Users\\cleme\\Zotero\\storage\\S9LXJ3T9\\S0010482519303221.html}
}

@inproceedings{perdomoOCTNETConvolutionalNetwork2018,
  title = {{{OCT-NET}}: {{A}} Convolutional Network for Automatic Classification of Normal and Diabetic Macular Edema Using Sd-Oct Volumes},
  shorttitle = {{{OCT-NET}}},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {Perdomo, Oscar and Ot{\'a}lora, Sebastian and Gonz{\'a}lez, Fabio A. and Meriaudeau, Fabrice and M{\"u}ller, Henning},
  year = {2018},
  month = apr,
  pages = {1423--1426},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2018.8363839},
  abstract = {Diabetic macular edema (DME) is one of the most common eye complication caused by diabetes mellitus, resulting in partial or total loss of vision. Optical Coherence Tomography (OCT) volumes have been widely used to diagnose different eye diseases, thanks to their sensitivity to represent small amounts of fluid, thickness between layers and swelling. However, the lack of tools for automatic image analysis for supporting disease diagnosis is still a problem. Convolutional neural networks (CNNs) have shown outstanding performance when applied to several medical images analysis tasks. This paper presents a model, OCT-NET, based on a CNN for the automatic classification of OCT volumes. The model was evaluated on a dataset of OCT volumes for DME diagnosis using a leave-one-out cross-validation strategy obtaining an accuracy, sensitivity, and specificity of 93.75\%.},
  keywords = {automatic classification,automatic image analysis,Biomedical imaging,biomedical optical imaging,convolutional neural network,convolutional neural networks,deep learning,Diabetes,diabetes mellitus,diabetic macular edema,Diabetic macular edema,disease diagnosis,diseases,DME diagnosis,eye,eye complication,eye disease diagnosis,eye diseases,feedforward neural nets,image classification,leave-one-out cross-validation strategy,Machine learning,medical image analysis tasks,medical image processing,normal macular edema,OCT,OCT-NET,Optical coherence tomography,Optical Coherence Tomography volumes,optical tomography,partial vision loss,Retina,Retinopathy,SD-OCT volumes,Sensitivity,total vision loss}
}

@inproceedings{perdomoOCTNETConvolutionalNetwork2018a,
  title = {{{OCT-NET}}: {{A}} Convolutional Network for Automatic Classification of Normal and Diabetic Macular Edema Using Sd-Oct Volumes},
  shorttitle = {{{OCT-NET}}},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {Perdomo, Oscar and Ot{\'a}lora, Sebastian and Gonz{\'a}lez, Fabio A. and Meriaudeau, Fabrice and M{\"u}ller, Henning},
  year = {2018},
  month = apr,
  pages = {1423--1426},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2018.8363839},
  abstract = {Diabetic macular edema (DME) is one of the most common eye complication caused by diabetes mellitus, resulting in partial or total loss of vision. Optical Coherence Tomography (OCT) volumes have been widely used to diagnose different eye diseases, thanks to their sensitivity to represent small amounts of fluid, thickness between layers and swelling. However, the lack of tools for automatic image analysis for supporting disease diagnosis is still a problem. Convolutional neural networks (CNNs) have shown outstanding performance when applied to several medical images analysis tasks. This paper presents a model, OCT-NET, based on a CNN for the automatic classification of OCT volumes. The model was evaluated on a dataset of OCT volumes for DME diagnosis using a leave-one-out cross-validation strategy obtaining an accuracy, sensitivity, and specificity of 93.75\%.},
  keywords = {Biomedical imaging,convolutional neural network,deep learning,Diabetes,Diabetic macular edema,eye disease diagnosis,Machine learning,OCT,Optical coherence tomography,Retina,Retinopathy,Sensitivity}
}

@inproceedings{perdomoOCTNETConvolutionalNetwork2018b,
  title = {{{OCT-NET}}: {{A}} Convolutional Network for Automatic Classification of Normal and Diabetic Macular Edema Using Sd-Oct Volumes},
  shorttitle = {{{OCT-NET}}},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {Perdomo, Oscar and Ot{\'a}lora, Sebastian and Gonz{\'a}lez, Fabio A. and Meriaudeau, Fabrice and M{\"u}ller, Henning},
  year = {2018},
  month = apr,
  pages = {1423--1426},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2018.8363839},
  abstract = {Diabetic macular edema (DME) is one of the most common eye complication caused by diabetes mellitus, resulting in partial or total loss of vision. Optical Coherence Tomography (OCT) volumes have been widely used to diagnose different eye diseases, thanks to their sensitivity to represent small amounts of fluid, thickness between layers and swelling. However, the lack of tools for automatic image analysis for supporting disease diagnosis is still a problem. Convolutional neural networks (CNNs) have shown outstanding performance when applied to several medical images analysis tasks. This paper presents a model, OCT-NET, based on a CNN for the automatic classification of OCT volumes. The model was evaluated on a dataset of OCT volumes for DME diagnosis using a leave-one-out cross-validation strategy obtaining an accuracy, sensitivity, and specificity of 93.75\%.},
  keywords = {automatic classification,automatic image analysis,Biomedical imaging,biomedical optical imaging,convolutional neural network,convolutional neural networks,deep learning,Diabetes,diabetes mellitus,diabetic macular edema,Diabetic macular edema,disease diagnosis,diseases,DME diagnosis,eye,eye complication,eye disease diagnosis,eye diseases,feedforward neural nets,image classification,leave-one-out cross-validation strategy,Machine learning,medical image analysis tasks,medical image processing,normal macular edema,OCT,OCT-NET,Optical coherence tomography,Optical Coherence Tomography volumes,optical tomography,partial vision loss,Retina,Retinopathy,SD-OCT volumes,Sensitivity,total vision loss},
  file = {C:\Users\cleme\Zotero\storage\Y6XGW2DH\8363839.html}
}

@inproceedings{perdomoOCTNETConvolutionalNetwork2018c,
  title = {{{OCT-NET}}: {{A}} Convolutional Network for Automatic Classification of Normal and Diabetic Macular Edema Using Sd-Oct Volumes},
  shorttitle = {{{OCT-NET}}},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {Perdomo, Oscar and Ot{\'a}lora, Sebastian and Gonz{\'a}lez, Fabio A. and Meriaudeau, Fabrice and M{\"u}ller, Henning},
  year = {2018},
  month = apr,
  pages = {1423--1426},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2018.8363839},
  abstract = {Diabetic macular edema (DME) is one of the most common eye complication caused by diabetes mellitus, resulting in partial or total loss of vision. Optical Coherence Tomography (OCT) volumes have been widely used to diagnose different eye diseases, thanks to their sensitivity to represent small amounts of fluid, thickness between layers and swelling. However, the lack of tools for automatic image analysis for supporting disease diagnosis is still a problem. Convolutional neural networks (CNNs) have shown outstanding performance when applied to several medical images analysis tasks. This paper presents a model, OCT-NET, based on a CNN for the automatic classification of OCT volumes. The model was evaluated on a dataset of OCT volumes for DME diagnosis using a leave-one-out cross-validation strategy obtaining an accuracy, sensitivity, and specificity of 93.75\%.},
  keywords = {Biomedical imaging,convolutional neural network,deep learning,Diabetes,Diabetic macular edema,eye disease diagnosis,Machine learning,OCT,Optical coherence tomography,Retina,Retinopathy,Sensitivity},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\AZAE5B6Z\\perdomo2018.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\MV968JCS\\Perdomo et al. - 2018 - OCT-NET A convolutional network for automatic cla.pdf;C\:\\Users\\cleme\\Zotero\\storage\\A9V5DZ2U\\8363839.html}
}

@inproceedings{peskineInterpretableDataDrivenScore2020,
  title = {An {{Interpretable Data-Driven Score}} for the {{Assessment}} of {{Fundus Images Quality}}},
  booktitle = {Image {{Analysis}} and {{Recognition}}},
  author = {Peskine, Youri and Boucher, Marie-Carole and Cheriet, Farida},
  editor = {Campilho, Aur{\'e}lio and Karray, Fakhri and Wang, Zhou},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {325--331},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-50516-5_28},
  abstract = {Fundus images are usually used for the diagnosis of ocular pathologies such as diabetic retinopathy. Image quality need however to be sufficient in order to enable grading of the severity of the condition. In this paper, we propose a new method to evaluate the quality of retinal images by computing a score for each image. Images are classified as gradable or ungradable based on this score. First, we use two different U-Net models to segment the macula and the vessels in the original image. We then extract a patch around the macula in the image containing the vessels. Finally, we compute a quality score based on the presence of small vessels in this patch. The score is interpretable as the method is heavily inspired by the way clinicians assess image quality, according to the Scottish Diabetic Retinopathy Grading Scheme. The performances are evaluated on a validation database labeled by a clinician. This method presented a sensitivity of 95\% and a specificity of 100\% on this database.},
  isbn = {978-3-030-50516-5},
  langid = {english},
  keywords = {Data-driven,Deep learning,Diabetic retinopathy,Image quality,Structure-based}
}

@inproceedings{peskineInterpretableDataDrivenScore2020a,
  title = {An {{Interpretable Data-Driven Score}} for the {{Assessment}} of {{Fundus Images Quality}}},
  booktitle = {Image {{Analysis}} and {{Recognition}}},
  author = {Peskine, Youri and Boucher, Marie-Carole and Cheriet, Farida},
  editor = {Campilho, Aur{\'e}lio and Karray, Fakhri and Wang, Zhou},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {325--331},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-50516-5_28},
  abstract = {Fundus images are usually used for the diagnosis of ocular pathologies such as diabetic retinopathy. Image quality need however to be sufficient in order to enable grading of the severity of the condition. In this paper, we propose a new method to evaluate the quality of retinal images by computing a score for each image. Images are classified as gradable or ungradable based on this score. First, we use two different U-Net models to segment the macula and the vessels in the original image. We then extract a patch around the macula in the image containing the vessels. Finally, we compute a quality score based on the presence of small vessels in this patch. The score is interpretable as the method is heavily inspired by the way clinicians assess image quality, according to the Scottish Diabetic Retinopathy Grading Scheme. The performances are evaluated on a validation database labeled by a clinician. This method presented a sensitivity of 95\% and a specificity of 100\% on this database.},
  isbn = {978-3-030-50516-5},
  langid = {english},
  keywords = {Data-driven,Deep learning,Diabetic retinopathy,Image quality,Structure-based}
}

@article{petchOpeningBlackBox2022,
  title = {Opening the {{Black Box}}: {{The Promise}} and {{Limitations}} of {{Explainable Machine Learning}} in {{Cardiology}}},
  shorttitle = {Opening the {{Black Box}}},
  author = {Petch, Jeremy and Di, Shuang and Nelson, Walter},
  year = {2022},
  month = feb,
  journal = {Canadian Journal of Cardiology},
  series = {Focus {{Issue}}: {{New Digital Technologies}} in {{Cardiology}}},
  volume = {38},
  number = {2},
  pages = {204--213},
  issn = {0828-282X},
  doi = {10.1016/j.cjca.2021.09.004},
  urldate = {2023-05-05},
  abstract = {Many clinicians remain wary of machine learning because of longstanding concerns about ``black box'' models. ``Black box'' is shorthand for models that are sufficiently complex that they are not straightforwardly interpretable to humans. Lack of interpretability in predictive models can undermine trust in those models, especially in health care, in which so many decisions are--- literally---life and death issues. There has been a recent explosion of research in the field of explainable machine learning aimed at addressing these concerns. The promise of explainable machine learning is considerable, but it is important for cardiologists who may encounter these techniques in clinical decision-support tools or novel research papers to have critical understanding of both their strengths and their limitations. This paper reviews key concepts and techniques in the field of explainable machine learning as they apply to cardiology. Key concepts reviewed include interpretability vs explainability and global vs local explanations. Techniques demonstrated include permutation importance, surrogate decision trees, local interpretable model-agnostic explanations, and partial dependence plots. We discuss several limitations with explainability techniques, focusing on the how the nature of explanations as approximations may omit important information about how black-box models work and why they make certain predictions. We conclude by proposing a rule of thumb about when it is appropriate to use black- box models with explanations rather than interpretable models. R{\'e}sum{\'e} De nombreux cliniciens restent m{\'e}fiants envers l'apprentissage automatique en raison de pr{\'e}occupations de longue date concernant les mod{\`e}les {\`a} << bo{\^i}te noire >>. Le terme << bo{\^i}te noire >> sert {\`a} d{\'e}signer des mod{\`e}les suffisamment complexes pour {\'e}chapper {\`a} une interpr{\'e}tation simple par un humain. Le manque d'interpr{\'e}tabilit{\'e} des mod{\`e}les pr{\'e}dictifs peut miner la confiance en ces mod{\`e}les, en particulier dans le domaine des soins de sant{\'e}, o{\`u} tant de d{\'e}cisions sont litt{\'e}ralement des questions de vie ou de mort. Il y a eu r{\'e}cemment une explosion de la recherche consacr{\'e}e {\`a} l'apprentissage automatique explicable visant {\`a} r{\'e}pondre {\`a} ces pr{\'e}occupations. L'apprentissage automatique explicable est tr{\`e}s prometteur, mais il importe que les cardiologues aient une compr{\'e}hension critique de ses forces et de ses limites puisqu'ils pourraient le retrouver dans des outils d'aide {\`a} la d{\'e}cision clinique ou des rapports de recherche de pointe. Cet article passe en revue les concepts et {\'e}l{\'e}ments techniques cl{\'e}s de l'apprentissage automatique explicable, tels qu'ils s'appliquent {\`a} la cardiologie. Les concepts cl{\'e}s que nous examinons comprennent l'interpr{\'e}tabilit{\'e} par rapport {\`a} l'explicabilit{\'e} et les explications globales par rapport aux explications locales. Au nombre des {\'e}l{\'e}ments techniques pr{\'e}sent{\'e}s figurent la mesure d'importance des variables {\`a} partir de permutations, les arbres de d{\'e}cision de substitution, l'algorithme LIME (local interpretable model-agnostic explanations) et les trac{\'e}s de d{\'e}pendance partielle. Nous abordons plusieurs limites des {\'e}l{\'e}ments techniques d'explicabilit{\'e}, en attirant l'attention sur le fait que la nature approximative des explications peut mener {\`a} l'omission d'informations importantes sur la fa{\c c}on dont les mod{\`e}les {\`a} bo{\^i}te noire fonctionnent et les fondements de certaines pr{\'e}visions obtenues {\`a} l'aide de ces mod{\`e}les. Pour conclure, nous proposons une m{\'e}thode empirique permettant de d{\'e}terminer quand il est appropri{\'e} d'utiliser des mod{\`e}les {\`a} bo{\^i}te noire assortis d'explications plut{\^o}t que des mod{\`e}les interpr{\'e}tables.},
  langid = {english}
}

@article{petchOpeningBlackBox2022a,
  title = {Opening the {{Black Box}}: {{The Promise}} and {{Limitations}} of {{Explainable Machine Learning}} in {{Cardiology}}},
  shorttitle = {Opening the {{Black Box}}},
  author = {Petch, Jeremy and Di, Shuang and Nelson, Walter},
  year = {2022},
  month = feb,
  journal = {Canadian Journal of Cardiology},
  series = {Focus {{Issue}}: {{New Digital Technologies}} in {{Cardiology}}},
  volume = {38},
  number = {2},
  pages = {204--213},
  issn = {0828-282X},
  doi = {10.1016/j.cjca.2021.09.004},
  urldate = {2023-05-05},
  abstract = {Many clinicians remain wary of machine learning because of longstanding concerns about ``black box'' models. ``Black box'' is shorthand for models that are sufficiently complex that they are not straightforwardly interpretable to humans. Lack of interpretability in predictive models can undermine trust in those models, especially in health care, in which so many decisions are--- literally---life and death issues. There has been a recent explosion of research in the field of explainable machine learning aimed at addressing these concerns. The promise of explainable machine learning is considerable, but it is important for cardiologists who may encounter these techniques in clinical decision-support tools or novel research papers to have critical understanding of both their strengths and their limitations. This paper reviews key concepts and techniques in the field of explainable machine learning as they apply to cardiology. Key concepts reviewed include interpretability vs explainability and global vs local explanations. Techniques demonstrated include permutation importance, surrogate decision trees, local interpretable model-agnostic explanations, and partial dependence plots. We discuss several limitations with explainability techniques, focusing on the how the nature of explanations as approximations may omit important information about how black-box models work and why they make certain predictions. We conclude by proposing a rule of thumb about when it is appropriate to use black- box models with explanations rather than interpretable models. R{\'e}sum{\'e} De nombreux cliniciens restent m{\'e}fiants envers l'apprentissage automatique en raison de pr{\'e}occupations de longue date concernant les mod{\`e}les {\`a} << bo{\^i}te noire >>. Le terme << bo{\^i}te noire >> sert {\`a} d{\'e}signer des mod{\`e}les suffisamment complexes pour {\'e}chapper {\`a} une interpr{\'e}tation simple par un humain. Le manque d'interpr{\'e}tabilit{\'e} des mod{\`e}les pr{\'e}dictifs peut miner la confiance en ces mod{\`e}les, en particulier dans le domaine des soins de sant{\'e}, o{\`u} tant de d{\'e}cisions sont litt{\'e}ralement des questions de vie ou de mort. Il y a eu r{\'e}cemment une explosion de la recherche consacr{\'e}e {\`a} l'apprentissage automatique explicable visant {\`a} r{\'e}pondre {\`a} ces pr{\'e}occupations. L'apprentissage automatique explicable est tr{\`e}s prometteur, mais il importe que les cardiologues aient une compr{\'e}hension critique de ses forces et de ses limites puisqu'ils pourraient le retrouver dans des outils d'aide {\`a} la d{\'e}cision clinique ou des rapports de recherche de pointe. Cet article passe en revue les concepts et {\'e}l{\'e}ments techniques cl{\'e}s de l'apprentissage automatique explicable, tels qu'ils s'appliquent {\`a} la cardiologie. Les concepts cl{\'e}s que nous examinons comprennent l'interpr{\'e}tabilit{\'e} par rapport {\`a} l'explicabilit{\'e} et les explications globales par rapport aux explications locales. Au nombre des {\'e}l{\'e}ments techniques pr{\'e}sent{\'e}s figurent la mesure d'importance des variables {\`a} partir de permutations, les arbres de d{\'e}cision de substitution, l'algorithme LIME (local interpretable model-agnostic explanations) et les trac{\'e}s de d{\'e}pendance partielle. Nous abordons plusieurs limites des {\'e}l{\'e}ments techniques d'explicabilit{\'e}, en attirant l'attention sur le fait que la nature approximative des explications peut mener {\`a} l'omission d'informations importantes sur la fa{\c c}on dont les mod{\`e}les {\`a} bo{\^i}te noire fonctionnent et les fondements de certaines pr{\'e}visions obtenues {\`a} l'aide de ces mod{\`e}les. Pour conclure, nous proposons une m{\'e}thode empirique permettant de d{\'e}terminer quand il est appropri{\'e} d'utiliser des mod{\`e}les {\`a} bo{\^i}te noire assortis d'explications plut{\^o}t que des mod{\`e}les interpr{\'e}tables.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\DX7SFCRY\Petch et al. - 2022 - Opening the Black Box The Promise and Limitations.pdf}
}

@misc{peymangholamiOpticalCoherenceTomography2019,
  title = {Optical {{Coherence Tomography Image Retinal Database}}},
  author = {Peyman Gholami, University of Waterloo and Vasudevan Lakshminarayanan, University of Waterloo},
  year = {2019},
  month = feb,
  doi = {10.3886/E108503V1},
  urldate = {2019-12-22},
  langid = {english}
}

@misc{peymangholamiOpticalCoherenceTomography2019a,
  title = {Optical {{Coherence Tomography Image Retinal Database}}},
  author = {Peyman Gholami, University of Waterloo and Vasudevan Lakshminarayanan, University of Waterloo},
  year = {2019},
  month = feb,
  doi = {10.3886/E108503V1},
  urldate = {2019-12-22},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\PRDL44FI\view.html}
}

@article{playoutFocusedAttentionTransformers2022,
  title = {Focused {{Attention}} in {{Transformers}} for Interpretable Classification of Retinal Images},
  author = {Playout, Cl{\'e}ment and Duval, Renaud and Boucher, Marie Carole and Cheriet, Farida},
  year = {2022},
  month = nov,
  journal = {Medical Image Analysis},
  volume = {82},
  pages = {102608},
  issn = {1361-8415},
  doi = {10.1016/j.media.2022.102608},
  urldate = {2022-11-15},
  abstract = {Vision Transformers have recently emerged as a competitive architecture in image classification. The tremendous popularity of this model and its variants comes from its high performance and its ability to produce interpretable predictions. However, both of these characteristics remain to be assessed in depth on retinal images. This study proposes a thorough performance evaluation of several Transformers compared to traditional Convolutional Neural Network (CNN) models for retinal disease classification. Special attention is given to multi-modality imaging (fundus and OCT) and generalization to external data. In addition, we propose a novel mechanism to generate interpretable predictions via attribution maps. Existing attribution methods from Transformer models have the disadvantage of producing low-resolution heatmaps. Our contribution, called Focused Attention, uses iterative conditional patch resampling to tackle this issue. By means of a survey involving four retinal specialists, we validated both the superior interpretability of Vision Transformers compared to the attribution maps produced from CNNs and the relevance of Focused Attention as a lesion detector.},
  langid = {english},
  keywords = {Attention,Classification,Fundus,Interpretability,OCT,Ophthalmology,Transformers}
}

@article{playoutFocusedAttentionTransformers2022a,
  title = {Focused {{Attention}} in {{Transformers}} for Interpretable Classification of Retinal Images},
  author = {Playout, Cl{\'e}ment and Duval, Renaud and Boucher, Marie Carole and Cheriet, Farida},
  year = {2022},
  month = nov,
  journal = {Medical Image Analysis},
  volume = {82},
  pages = {102608},
  issn = {1361-8415},
  doi = {10.1016/j.media.2022.102608},
  urldate = {2022-11-15},
  abstract = {Vision Transformers have recently emerged as a competitive architecture in image classification. The tremendous popularity of this model and its variants comes from its high performance and its ability to produce interpretable predictions. However, both of these characteristics remain to be assessed in depth on retinal images. This study proposes a thorough performance evaluation of several Transformers compared to traditional Convolutional Neural Network (CNN) models for retinal disease classification. Special attention is given to multi-modality imaging (fundus and OCT) and generalization to external data. In addition, we propose a novel mechanism to generate interpretable predictions via attribution maps. Existing attribution methods from Transformer models have the disadvantage of producing low-resolution heatmaps. Our contribution, called Focused Attention, uses iterative conditional patch resampling to tackle this issue. By means of a survey involving four retinal specialists, we validated both the superior interpretability of Vision Transformers compared to the attribution maps produced from CNNs and the relevance of Focused Attention as a lesion detector.},
  langid = {english},
  keywords = {Attention,Classification,Fundus,Interpretability,OCT,Ophthalmology,Transformers},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\3WBHISRC\\Playout et al. - 2022 - Focused Attention in Transformers for interpretabl.pdf;C\:\\Users\\cleme\\Zotero\\storage\\BSUGLQL6\\S1361841522002377.html}
}

@inproceedings{playoutMultitaskLearningArchitecture2018,
  title = {A {{Multitask Learning Architecture}} for {{Simultaneous Segmentation}} of {{Bright}} and {{Red Lesions}} in {{Fundus Images}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2018},
  author = {Playout, Cl{\'e}ment and Duval, Renaud and Cheriet, Farida},
  editor = {Frangi, Alejandro F. and Schnabel, Julia A. and Davatzikos, Christos and {Alberola-L{\'o}pez}, Carlos and Fichtinger, Gabor},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {101--108},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-00934-2_12},
  abstract = {Recent CNN architectures have established state-of-the-art results in a large range of medical imaging applications. We propose an extension to the U-Net architecture relying on multi-task learning: while keeping a single encoding module, multiple decoding modules are used for concurrent segmentation tasks. We propose improvements of the encoding module based on the latest CNN developments: residual connections at every scale, mixed pooling for spatial compression and large kernels for convolutions at the lowest scale. We also use dense connections within the different scales based on multi-size pooling regions. We use this new architecture to jointly detect and segment red and bright retinal lesions which are essential biomarkers of diabetic retinopathy. Each of the two categories is handled by a specialized decoding module. Segmentation outputs are refined with conditional random fields (CRF) as RNN and the network is trained end-to-end with an effective Kappa-based function loss. Preliminary results on a public dataset in the segmentation task on red (resp. bright) lesions shows a sensitivity of 66,9\% (resp. 75,3\%) and a specificity of 99,8\% (resp. 99,9\%).},
  isbn = {978-3-030-00934-2},
  langid = {english},
  keywords = {Conditional Random Fields (CRFs),Decoder Module,Fundus Images,Mixed Pool,Multitask Learning}
}

@article{playoutNovelWeaklySupervised2019a,
  title = {A {{Novel Weakly Supervised Multitask Architecture}} for {{Retinal Lesions Segmentation}} on {{Fundus Images}}},
  author = {Playout, Clement and Duval, Renaud and Cheriet, Farida},
  year = {2019},
  month = oct,
  journal = {IEEE transactions on medical imaging},
  volume = {38},
  number = {10},
  pages = {2434--2444},
  issn = {1558-254X},
  doi = {10.1109/TMI.2019.2906319},
  abstract = {Obtaining the complete segmentation map of retinal lesions is the first step toward an automated diagnosis tool for retinopathy that is interpretable in its decision-making. However, the limited availability of ground truth lesion detection maps at a pixel level restricts the ability of deep segmentation neural networks to generalize over large databases. In this paper, we propose a novel approach for training a convolutional multi-task architecture with supervised learning and reinforcing it with weakly supervised learning. The architecture is simultaneously trained for three tasks: segmentation of red lesions and of bright lesions, those two tasks done concurrently with lesion detection. In addition, we propose and discuss the advantages of a new preprocessing method that guarantees the color consistency between the raw image and its enhanced version. Our complete system produces segmentations of both red and bright lesions. The method is validated at the pixel level and per-image using four databases and a cross-validation strategy. When evaluated on the task of screening for the presence or absence of lesions on the Messidor image set, the proposed method achieves an area under the ROC curve of 0.839, comparable with the state-of-the-art.},
  langid = {english},
  pmid = {30908197},
  keywords = {Computer-aided diagnostic,Computer-Assisted,Databases,Databases Factual,Diagnostic Techniques,Diagnostic Techniques Ophthalmological,Diseases,Factual,Feature extraction,fundus imaging,Fundus Oculi,Humans,Image Interpretation,Image Interpretation Computer-Assisted,Image segmentation,Lesions,lesions segmentations,Ophthalmological,retina,Retina,Retinal Diseases,ROC Curve,screening,Supervised Machine Learning,Task analysis,Training},
  file = {C:\Users\cleme\Zotero\storage\XWZ4TMZT\8672120.html}
}

@mastersthesis{playoutSystemeApprentissageMultitache2018,
  title = {{Syst{\`e}me d'apprentissage multit{\^a}che d{\'e}di{\'e} {\`a} la segmentation des l{\'e}sions sombres et claires de la r{\'e}tine dans les images de fond d'oeil}},
  author = {Playout, Cl{\'e}ment},
  year = {2018},
  month = aug,
  urldate = {2019-12-24},
  abstract = {Le travail de recherche men{\'e} dans le cadre de cette ma{\^i}trise porte sur l'exploitation de l'imagerie de la r{\'e}tine {\`a} des fins de diagnostic automatique. Il se concentre sur l'image de fond d'oeil, qui donne acc{\`e}s {\`a} une repr{\'e}sentation en deux dimensions et en couleur de la surface de la r{\'e}tine. Ces images peuvent pr{\'e}senter des sympt{\^o}mes de maladie, sous forme de l{\'e}sions ou de d{\'e}formations des structures anatomiques de la r{\'e}tine. L'objet de cette ma{\^i}trise est de proposer une m{\'e}thodologie de segmentation simultan{\'e}e de ces l{\'e}sions dans l'image de fond d'oeil, regroup{\'e}es en deux cat{\'e}gories : claires ou sombres. R{\'e}aliser cette double segmentation de fa{\c c}on simultan{\'e}e est in{\'e}dit : la vaste majorit{\'e} des travaux pr{\'e}c{\'e}dents se concentrant sur un seul type de l{\'e}sions. Or, du fait des contraintes de temps et de la difficult{\'e} que cela repr{\'e}sente dans un environnement clinique, il est impossible pour un clinicien de tester la multitude d'algorithmes existants. D'autant plus que lorsqu'un patient se pr{\'e}sente pour un examen, le clinicien n'a aucune connaissance a priori sur le type de pathologie et par cons{\'e}quent sur le type d'algorithme {\`a} utiliser. Pour envisager une utilisation clinique, il est donc important de r{\'e}fl{\'e}chir {\`a} une solution polyvalente, rapide et ais{\'e}ment d{\'e}ployable. Parall{\`e}lement, l'apprentissage profond a d{\'e}montr{\'e} sa capacit{\'e} {\`a} s'adapter {\`a} de nombreux probl{\`e}mes de visions par ordinateur et {\`a} g{\'e}n{\'e}raliser ses performances sur des donn{\'e}es vari{\'e}es malgr{\'e} des ensembles d'entra{\^i}nement parfois restreints. Pour cela, de nouvelles strat{\'e}gies sont r{\'e}guli{\`e}rement propos{\'e}es, ambitionnant d'extraire toujours mieux les informations issues de la base d'entra{\^i}nement. En cons{\'e}quence, nous nous sommes fix{\'e}s pour objectif de d{\'e}velopper une architecture de r{\'e}seaux de neurones capable de rechercher toutes les l{\'e}sions dans une image de fond d'oeil. Pour r{\'e}pondre {\`a} cet objectif, notre m{\'e}thodologie s'appuie sur une nouvelle architecture de r{\'e}seaux de neurones convolutifs reposant sur une structure multit{\^a}che entra{\^i}n{\'e}e selon une approche hybride faisant appel {\`a} de l'apprentissage supervis{\'e} et faiblement supervis{\'e}. L'architecture se compose d'un encodeur partag{\'e} par deux d{\'e}codeurs sp{\'e}cialis{\'e}s chacun dans un type de l{\'e}sions. Ainsi, les m{\^e}mes caract{\'e}ristiques sont extraites par l'encodeur pour les deux d{\'e}codeurs. Dans un premier temps, le r{\'e}seau est entra{\^i}n{\'e} avec des r{\'e}gions d'images et la v{\'e}rit{\'e} terrain correspondante indiquant les l{\'e}sions (apprentissage supervis{\'e}). Dans un second temps, seul l'encodeur est r{\'e}-entra{\^i}n{\'e} avec des images compl{\`e}tes avec une v{\'e}rit{\'e} terrain compos{\'e} d'un simple scalaire indiquant si l'image pr{\'e}sente des pathologies ou non, sans pr{\'e}ciser leur position et leur type (apprentissage faiblement supervis{\'e}).----------ABSTRACT: This work focuses on automatic diagnosis on fundus images, which are a bidimensional representation of the inner structure of the eye. The aim of this master's thesis is to discuss a solution for an automatic segmentation of the lesions that can be observed in the retina. The proposed methodology regroups those lesions in two categories: red and bright. Obtaining a simultaneous double segmentation is a novel approach; most of the previous works focus on the detection of a single type of lesions. However, due to time constraints and the tedeous nature of this work, clinicians usually can not test all the existing methods. Moreover, from a screening perspective, the clinician has no clue a priori on the nature of the pathology he deals with and thus on which algorithm to start with. Therefore, the proposed algorithm requires to be versatile, fast and easily deployable. Conforted by the recent progresses obtained with machine learning methods (and especially deep learning), we decide to develop a novel convolutional neural network able to segment both types of lesions on fundus images. To reach this goal, our methodology relies on a new multitask architecture, trained on a hybrid method combining weak and normal supervised training. The architecture relies on hard parameter sharing: two decoders (one per type of lesion) share a single encoder. Therefore, the encoder is trained on deriving an abstrast representation of the input image. Those extracted features permit a discrimination between both bright and red lesions. In other words, the encoder is trained on detecting pathological tissues from normal ones. The training is done in two steps. During the first one, the whole architecture is trained with patches, with a groundtruth at a pixel level, which is the typical way of training a segmentation network. The second step consists in weak supervision. Only the encoder is trained with full images and its task is to predict the status of the given image (pathological or healthy), without specifying anything concerning the potential lesions in it (neither location nor type). In this case, the groundtruth is a simple boolean number. This second step allows the network to see a larger number of images: indeed, this type of groundtruth is considerably easier to acquire and already available in large public databases. This step relies on the hypothesis that it is possible to use an annotation at an image level (globally) to enhance the performance at a pixel level (locally). This is an intuitive idea, as the pathological status is directly correlated with the presence of lesions.},
  langid = {french},
  school = {{\'E}cole Polytechnique de Montr{\'e}al}
}

@mastersthesis{playoutSystemeApprentissageMultitache2018a,
  title = {{Syst{\`e}me d'apprentissage multit{\^a}che d{\'e}di{\'e} {\`a} la segmentation des l{\'e}sions sombres et claires de la r{\'e}tine dans les images de fond d'oeil}},
  author = {Playout, Cl{\'e}ment},
  year = {2018},
  month = aug,
  urldate = {2019-12-24},
  abstract = {Le travail de recherche men{\'e} dans le cadre de cette ma{\^i}trise porte sur l'exploitation de l'imagerie de la r{\'e}tine {\`a} des fins de diagnostic automatique. Il se concentre sur l'image de fond d'oeil, qui donne acc{\`e}s {\`a} une repr{\'e}sentation en deux dimensions et en couleur de la surface de la r{\'e}tine. Ces images peuvent pr{\'e}senter des sympt{\^o}mes de maladie, sous forme de l{\'e}sions ou de d{\'e}formations des structures anatomiques de la r{\'e}tine. L'objet de cette ma{\^i}trise est de proposer une m{\'e}thodologie de segmentation simultan{\'e}e de ces l{\'e}sions dans l'image de fond d'oeil, regroup{\'e}es en deux cat{\'e}gories : claires ou sombres. R{\'e}aliser cette double segmentation de fa{\c c}on simultan{\'e}e est in{\'e}dit : la vaste majorit{\'e} des travaux pr{\'e}c{\'e}dents se concentrant sur un seul type de l{\'e}sions. Or, du fait des contraintes de temps et de la difficult{\'e} que cela repr{\'e}sente dans un environnement clinique, il est impossible pour un clinicien de tester la multitude d'algorithmes existants. D'autant plus que lorsqu'un patient se pr{\'e}sente pour un examen, le clinicien n'a aucune connaissance a priori sur le type de pathologie et par cons{\'e}quent sur le type d'algorithme {\`a} utiliser. Pour envisager une utilisation clinique, il est donc important de r{\'e}fl{\'e}chir {\`a} une solution polyvalente, rapide et ais{\'e}ment d{\'e}ployable. Parall{\`e}lement, l'apprentissage profond a d{\'e}montr{\'e} sa capacit{\'e} {\`a} s'adapter {\`a} de nombreux probl{\`e}mes de visions par ordinateur et {\`a} g{\'e}n{\'e}raliser ses performances sur des donn{\'e}es vari{\'e}es malgr{\'e} des ensembles d'entra{\^i}nement parfois restreints. Pour cela, de nouvelles strat{\'e}gies sont r{\'e}guli{\`e}rement propos{\'e}es, ambitionnant d'extraire toujours mieux les informations issues de la base d'entra{\^i}nement. En cons{\'e}quence, nous nous sommes fix{\'e}s pour objectif de d{\'e}velopper une architecture de r{\'e}seaux de neurones capable de rechercher toutes les l{\'e}sions dans une image de fond d'oeil. Pour r{\'e}pondre {\`a} cet objectif, notre m{\'e}thodologie s'appuie sur une nouvelle architecture de r{\'e}seaux de neurones convolutifs reposant sur une structure multit{\^a}che entra{\^i}n{\'e}e selon une approche hybride faisant appel {\`a} de l'apprentissage supervis{\'e} et faiblement supervis{\'e}. L'architecture se compose d'un encodeur partag{\'e} par deux d{\'e}codeurs sp{\'e}cialis{\'e}s chacun dans un type de l{\'e}sions. Ainsi, les m{\^e}mes caract{\'e}ristiques sont extraites par l'encodeur pour les deux d{\'e}codeurs. Dans un premier temps, le r{\'e}seau est entra{\^i}n{\'e} avec des r{\'e}gions d'images et la v{\'e}rit{\'e} terrain correspondante indiquant les l{\'e}sions (apprentissage supervis{\'e}). Dans un second temps, seul l'encodeur est r{\'e}-entra{\^i}n{\'e} avec des images compl{\`e}tes avec une v{\'e}rit{\'e} terrain compos{\'e} d'un simple scalaire indiquant si l'image pr{\'e}sente des pathologies ou non, sans pr{\'e}ciser leur position et leur type (apprentissage faiblement supervis{\'e}).----------ABSTRACT: This work focuses on automatic diagnosis on fundus images, which are a bidimensional representation of the inner structure of the eye. The aim of this master's thesis is to discuss a solution for an automatic segmentation of the lesions that can be observed in the retina. The proposed methodology regroups those lesions in two categories: red and bright. Obtaining a simultaneous double segmentation is a novel approach; most of the previous works focus on the detection of a single type of lesions. However, due to time constraints and the tedeous nature of this work, clinicians usually can not test all the existing methods. Moreover, from a screening perspective, the clinician has no clue a priori on the nature of the pathology he deals with and thus on which algorithm to start with. Therefore, the proposed algorithm requires to be versatile, fast and easily deployable. Conforted by the recent progresses obtained with machine learning methods (and especially deep learning), we decide to develop a novel convolutional neural network able to segment both types of lesions on fundus images. To reach this goal, our methodology relies on a new multitask architecture, trained on a hybrid method combining weak and normal supervised training. The architecture relies on hard parameter sharing: two decoders (one per type of lesion) share a single encoder. Therefore, the encoder is trained on deriving an abstrast representation of the input image. Those extracted features permit a discrimination between both bright and red lesions. In other words, the encoder is trained on detecting pathological tissues from normal ones. The training is done in two steps. During the first one, the whole architecture is trained with patches, with a groundtruth at a pixel level, which is the typical way of training a segmentation network. The second step consists in weak supervision. Only the encoder is trained with full images and its task is to predict the status of the given image (pathological or healthy), without specifying anything concerning the potential lesions in it (neither location nor type). In this case, the groundtruth is a simple boolean number. This second step allows the network to see a larger number of images: indeed, this type of groundtruth is considerably easier to acquire and already available in large public databases. This step relies on the hypothesis that it is possible to use an annotation at an image level (globally) to enhance the performance at a pixel level (locally). This is an intuitive idea, as the pathological status is directly correlated with the presence of lesions.},
  langid = {french},
  school = {{\'E}cole Polytechnique de Montr{\'e}al},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\SQWUE75H\\Playout - 2018 - Système d'apprentissage multitâche dédié à la segm.pdf;C\:\\Users\\cleme\\Zotero\\storage\\IIYD4A4R\\3257.html}
}

@article{pluimMutualinformationbasedRegistrationMedical2003,
  title = {Mutual-Information-Based Registration of Medical Images: A Survey},
  shorttitle = {Mutual-Information-Based Registration of Medical Images},
  author = {Pluim, J. P. W. and Maintz, J. B. A. and Viergever, M. A.},
  year = {2003},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {22},
  number = {8},
  pages = {986--1004},
  issn = {0278-0062},
  doi = {10.1109/TMI.2003.815867},
  abstract = {An overview is presented of the medical image processing literature on mutual-information-based registration. The aim of the survey is threefold: an introduction for those new to the field, an overview for those working in the field, and a reference for those searching for literature on a specific application. Methods are classified according to the different aspects of mutual-information-based registration. The main division is in aspects of the methodology and of the application. The part on methodology describes choices made on facets such as preprocessing of images, gray value interpolation, optimization, adaptations to the mutual information measure, and different types of geometrical transformations. The part on applications is a reference of the literature available on different modalities, on interpatient registration and on different anatomical objects. Comparison studies including mutual information are also considered. The paper starts with a description of entropy and mutual information and it closes with a discussion on past achievements and some future challenges.},
  keywords = {Algorithms,anatomical objects,Anatomy,Automated,Biomedical equipment,Biomedical image processing,Biomedical imaging,Computer-Assisted,Cross-Sectional,entropy,Entropy,future challenges,Humans,Image Enhancement,Image Interpretation,image matching,image registration,Image registration,images preprocessing,Imaging,interpatient registration,interpolation,Interpolation,medical diagnostic imaging,medical image processing,medical image processing literature,Medical services,Mutual information,mutual-information-based registration,optimisation,Optimization methods,past achievements,Pattern Recognition,reviews,Springs,Subtraction Technique,Three-Dimensional}
}

@article{pluimMutualinformationbasedRegistrationMedical2003a,
  title = {Mutual-Information-Based Registration of Medical Images: A Survey},
  shorttitle = {Mutual-Information-Based Registration of Medical Images},
  author = {Pluim, J. P. W. and Maintz, J. B. A. and Viergever, M. A.},
  year = {2003},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {22},
  number = {8},
  pages = {986--1004},
  issn = {0278-0062},
  doi = {10.1109/TMI.2003.815867},
  abstract = {An overview is presented of the medical image processing literature on mutual-information-based registration. The aim of the survey is threefold: an introduction for those new to the field, an overview for those working in the field, and a reference for those searching for literature on a specific application. Methods are classified according to the different aspects of mutual-information-based registration. The main division is in aspects of the methodology and of the application. The part on methodology describes choices made on facets such as preprocessing of images, gray value interpolation, optimization, adaptations to the mutual information measure, and different types of geometrical transformations. The part on applications is a reference of the literature available on different modalities, on interpatient registration and on different anatomical objects. Comparison studies including mutual information are also considered. The paper starts with a description of entropy and mutual information and it closes with a discussion on past achievements and some future challenges.},
  keywords = {Algorithms,anatomical objects,Anatomy Cross-Sectional,Biomedical equipment,Biomedical image processing,Biomedical imaging,entropy,Entropy,future challenges,Humans,Image Enhancement,Image Interpretation Computer-Assisted,image matching,image registration,Image registration,images preprocessing,Imaging Three-Dimensional,interpatient registration,interpolation,Interpolation,medical diagnostic imaging,medical image processing,medical image processing literature,Medical services,Mutual information,mutual-information-based registration,optimisation,Optimization methods,past achievements,Pattern Recognition Automated,reviews,Springs,Subtraction Technique},
  file = {C:\Users\cleme\Zotero\storage\GEEQDEPZ\1216223.html}
}

@inproceedings{pmlr-v139-touvron21a,
  title = {Training Data-Efficient Image Transformers \&amp; Distillation through Attention},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021-07-18/2021-07-24},
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {10347--10357},
  publisher = {PMLR},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2\% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.}
}

@inproceedings{pmlr-v139-touvron21a,
  title = {Training Data-Efficient Image Transformers \&amp; Distillation through Attention},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {10347--10357},
  publisher = {PMLR},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2\% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.}
}

@inproceedings{pmlr-v75-golowich18a,
  title = {Size-Independent Sample Complexity of Neural Networks},
  booktitle = {Proceedings of the 31st Conference on Learning Theory},
  author = {Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  editor = {Bubeck, S{\'e}bastien and Perchet, Vianney and Rigollet, Philippe},
  year = {2018-07-06/2018-07-09},
  series = {Proceedings of Machine Learning Research},
  volume = {75},
  pages = {297--299},
  publisher = {PMLR},
  abstract = {We study the sample complexity of learning neural networks, by providing new bounds on their Rademacher complexity assuming norm constraints on the parameter matrix of each layer. Compared to previous work, these complexity bounds have improved dependence on the network depth, and under some additional assumptions, are fully independent of the network size (both depth and width). These results are derived using some novel techniques, which may be of independent interest.}
}

@inproceedings{pmlr-v75-golowich18a,
  title = {Size-Independent Sample Complexity of Neural Networks},
  booktitle = {Proceedings of the 31st Conference on Learning Theory},
  author = {Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  editor = {Bubeck, S{\'e}bastien and Perchet, Vianney and Rigollet, Philippe},
  year = {2018},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {75},
  pages = {297--299},
  publisher = {PMLR},
  abstract = {We study the sample complexity of learning neural networks, by providing new bounds on their Rademacher complexity assuming norm constraints on the parameter matrix of each layer. Compared to previous work, these complexity bounds have improved dependence on the network depth, and under some additional assumptions, are fully independent of the network size (both depth and width). These results are derived using some novel techniques, which may be of independent interest.}
}

@inproceedings{popeExplainabilityMethodsGraph2019,
  title = {Explainability {{Methods}} for {{Graph Convolutional Neural Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pope, Phillip E. and Kolouri, Soheil and Rostami, Mohammad and Martin, Charles E. and Hoffmann, Heiko},
  year = {2019},
  month = jun,
  pages = {10764--10773},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.01103},
  urldate = {2023-10-03},
  abstract = {With the growing use of graph convolutional neural networks (GCNNs) comes the need for explainability. In this paper, we introduce explainability methods for GCNNs. We develop the graph analogues of three prominent explainability methods for convolutional neural networks: contrastive gradient-based (CG) saliency maps, Class Activation Mapping (CAM), and Excitation Backpropagation (EB) and their variants, gradient-weighted CAM (Grad-CAM) and contrastive EB (c-EB). We show a proof-of-concept of these methods on classification problems in two application domains: visual scene graphs and molecular graphs. To compare the methods, we identify three desirable properties of explanations: (1) their importance to classification, as measured by the impact of occlusions, (2) their contrastivity with respect to different classes, and (3) their sparseness on a graph. We call the corresponding quantitative metrics fidelity, contrastivity, and sparsity and evaluate them for each method. Lastly, we analyze the salient subgraphs obtained from explanations and report frequently occurring patterns.},
  isbn = {978-1-72813-293-8},
  langid = {english}
}

@inproceedings{popeExplainabilityMethodsGraph2019a,
  title = {Explainability {{Methods}} for {{Graph Convolutional Neural Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pope, Phillip E. and Kolouri, Soheil and Rostami, Mohammad and Martin, Charles E. and Hoffmann, Heiko},
  year = {2019},
  month = jun,
  pages = {10764--10773},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.01103},
  urldate = {2023-10-03},
  abstract = {With the growing use of graph convolutional neural networks (GCNNs) comes the need for explainability. In this paper, we introduce explainability methods for GCNNs. We develop the graph analogues of three prominent explainability methods for convolutional neural networks: contrastive gradient-based (CG) saliency maps, Class Activation Mapping (CAM), and Excitation Backpropagation (EB) and their variants, gradient-weighted CAM (Grad-CAM) and contrastive EB (c-EB). We show a proof-of-concept of these methods on classification problems in two application domains: visual scene graphs and molecular graphs. To compare the methods, we identify three desirable properties of explanations: (1) their importance to classification, as measured by the impact of occlusions, (2) their contrastivity with respect to different classes, and (3) their sparseness on a graph. We call the corresponding quantitative metrics fidelity, contrastivity, and sparsity and evaluate them for each method. Lastly, we analyze the salient subgraphs obtained from explanations and report frequently occurring patterns.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\MB8SN84T\Pope et al. - 2019 - Explainability Methods for Graph Convolutional Neu.pdf}
}

@article{porwalIDRiDDiabeticRetinopathy2020,
  title = {{{IDRiD}}: {{Diabetic Retinopathy}} -- {{Segmentation}} and {{Grading Challenge}}},
  shorttitle = {{{IDRiD}}},
  author = {Porwal, Prasanna and Pachade, Samiksha and Kokare, Manesh and Deshmukh, Girish and Son, Jaemin and Bae, Woong and Liu, Lihong and Wang, Jianzong and Liu, Xinhui and Gao, Liangxin and Wu, TianBo and Xiao, Jing and Wang, Fengyan and Yin, Baocai and Wang, Yunzhi and Danala, Gopichandh and He, Linsheng and Choi, Yoon Ho and Lee, Yeong Chan and Jung, Sang-Hyuk and Li, Zhongyu and Sui, Xiaodan and Wu, Junyan and Li, Xiaolong and Zhou, Ting and Toth, Janos and Baran, Agnes and Kori, Avinash and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Lyu, Xingzheng and Cheng, Li and Chu, Qinhao and Li, Pengcheng and Ji, Xin and Zhang, Sanyuan and Shen, Yaxin and Dai, Ling and Saha, Oindrila and Sathish, Rachana and Melo, T{\^a}nia and Ara{\'u}jo, Teresa and Harangi, Balazs and Sheng, Bin and Fang, Ruogu and Sheet, Debdoot and Hajdu, Andras and Zheng, Yuanjie and Mendon{\c c}a, Ana Maria and Zhang, Shaoting and Campilho, Aur{\'e}lio and Zheng, Bin and Shen, Dinggang and Giancardo, Luca and Quellec, Gwenol{\'e} and M{\'e}riaudeau, Fabrice},
  year = {2020},
  month = jan,
  journal = {Medical Image Analysis},
  volume = {59},
  pages = {101561},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.101561},
  urldate = {2020-02-18},
  abstract = {Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on ``Diabetic Retinopathy -- Segmentation and Grading'' was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular.},
  langid = {english},
  keywords = {Challenge,Deep learning,Diabetic Retinopathy,Retinal image analysis}
}

@article{porwalIDRiDDiabeticRetinopathy2020a,
  title = {{{IDRiD}}: {{Diabetic Retinopathy}} -- {{Segmentation}} and {{Grading Challenge}}},
  shorttitle = {{{IDRiD}}},
  author = {Porwal, Prasanna and Pachade, Samiksha and Kokare, Manesh and Deshmukh, Girish and Son, Jaemin and Bae, Woong and Liu, Lihong and Wang, Jianzong and Liu, Xinhui and Gao, Liangxin and Wu, TianBo and Xiao, Jing and Wang, Fengyan and Yin, Baocai and Wang, Yunzhi and Danala, Gopichandh and He, Linsheng and Choi, Yoon Ho and Lee, Yeong Chan and Jung, Sang-Hyuk and Li, Zhongyu and Sui, Xiaodan and Wu, Junyan and Li, Xiaolong and Zhou, Ting and Toth, Janos and Baran, Agnes and Kori, Avinash and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Lyu, Xingzheng and Cheng, Li and Chu, Qinhao and Li, Pengcheng and Ji, Xin and Zhang, Sanyuan and Shen, Yaxin and Dai, Ling and Saha, Oindrila and Sathish, Rachana and Melo, T{\^a}nia and Ara{\'u}jo, Teresa and Harangi, Balazs and Sheng, Bin and Fang, Ruogu and Sheet, Debdoot and Hajdu, Andras and Zheng, Yuanjie and Mendon{\c c}a, Ana Maria and Zhang, Shaoting and Campilho, Aur{\'e}lio and Zheng, Bin and Shen, Dinggang and Giancardo, Luca and Quellec, Gwenol{\'e} and M{\'e}riaudeau, Fabrice},
  year = {2020},
  month = jan,
  journal = {Medical Image Analysis},
  volume = {59},
  pages = {101561},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.101561},
  urldate = {2020-02-18},
  abstract = {Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on ``Diabetic Retinopathy -- Segmentation and Grading'' was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular.},
  langid = {english},
  keywords = {Challenge,Deep learning,Diabetic Retinopathy,Retinal image analysis},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\CBEK5CKT\\Porwal et al. - 2020 - IDRiD Diabetic Retinopathy – Segmentation and Gra.pdf;C\:\\Users\\cleme\\Zotero\\storage\\YSZR8GLC\\porwal2019.pdf;C\:\\Users\\cleme\\Zotero\\storage\\CB3EB28P\\S1361841519301033.html}
}

@misc{porwalIndianDiabeticRetinopathy2018,
  title = {Indian {{Diabetic Retinopathy Image Dataset}} ({{IDRiD}})},
  author = {Porwal, Prasanna},
  year = {2018},
  month = apr,
  urldate = {2019-12-26},
  abstract = {Access the dataset for images of typical diabetic retinopathy lesions and also normal retinal structures annotated at a pixel level, focused on an Indian population. This dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image.},
  langid = {english}
}

@misc{porwalIndianDiabeticRetinopathy2018a,
  title = {Indian {{Diabetic Retinopathy Image Dataset}} ({{IDRiD}})},
  author = {Porwal, Prasanna},
  year = {2018},
  month = apr,
  publisher = {IEEE},
  urldate = {2022-10-23},
  abstract = {Access the dataset for images of typical diabetic retinopathy lesions and also normal retinal structures annotated at a pixel level, focused on an Indian population. This dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image.},
  langid = {english}
}

@misc{porwalIndianDiabeticRetinopathy2018b,
  title = {Indian {{Diabetic Retinopathy Image Dataset}} ({{IDRiD}})},
  author = {Porwal, Prasanna},
  year = {2018},
  month = apr,
  urldate = {2019-12-26},
  abstract = {Access the dataset for images of typical diabetic retinopathy lesions and also normal retinal structures annotated at a pixel level, focused on an Indian population. This dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\Z8P63HPH\indian-diabetic-retinopathy-image-dataset-idrid.html}
}

@misc{porwalIndianDiabeticRetinopathy2018c,
  title = {Indian {{Diabetic Retinopathy Image Dataset}} ({{IDRiD}})},
  author = {Porwal, Prasanna},
  year = {2018},
  month = apr,
  publisher = {IEEE},
  urldate = {2022-10-23},
  abstract = {Access the dataset for images of typical diabetic retinopathy lesions and also normal retinal structures annotated at a pixel level, focused on an Indian population. This dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\YQU2Z4X3\indian-diabetic-retinopathy-image-dataset-idrid.html}
}

@article{pradhanTropicalCycloneIntensity2018,
  title = {Tropical {{Cyclone Intensity Estimation Using}} a {{Deep Convolutional Neural Network}}},
  author = {Pradhan, R. and Aygun, R. S. and Maskey, M. and Ramachandran, R. and Cecil, D. J.},
  year = {2018},
  month = feb,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {2},
  pages = {692--702},
  issn = {1057-7149},
  doi = {10.1109/TIP.2017.2766358},
  abstract = {Tropical cyclone intensity estimation is a challenging task as it required domain knowledge while extracting features, significant pre-processing, various sets of parameters obtained from satellites, and human intervention for analysis. The inconsistency of results, significant pre-processing of data, complexity of the problem domain, and problems on generalizability are some of the issues related to intensity estimation. In this study, we design a deep convolutional neural network architecture for categorizing hurricanes based on intensity using graphics processing unit. Our model has achieved better accuracy and lower root-mean-square error by just using satellite images than 'state-of-the-art' techniques. Visualizations of learned features at various layers and their deconvolutions are also presented for understanding the learning process.},
  keywords = {Computer architecture,convolutional neural networks,deep convolutional neural network architecture,Deep learning,domain knowledge,Estimation,feature extraction,Feature extraction,feedforward neural nets,geophysical image processing,graphics processing unit,Hurricanes,image processing,learning (artificial intelligence),learning process,mean square error methods,Neural networks,root-mean-square error,satellite images,storms,tropical cyclone category and intensity estimation,tropical cyclone intensity estimation,Tropical cyclones}
}

@article{pradhanTropicalCycloneIntensity2018a,
  title = {Tropical {{Cyclone Intensity Estimation Using}} a {{Deep Convolutional Neural Network}}},
  author = {Pradhan, R. and Aygun, R. S. and Maskey, M. and Ramachandran, R. and Cecil, D. J.},
  year = {2018},
  month = feb,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {2},
  pages = {692--702},
  issn = {1057-7149},
  doi = {10.1109/TIP.2017.2766358},
  abstract = {Tropical cyclone intensity estimation is a challenging task as it required domain knowledge while extracting features, significant pre-processing, various sets of parameters obtained from satellites, and human intervention for analysis. The inconsistency of results, significant pre-processing of data, complexity of the problem domain, and problems on generalizability are some of the issues related to intensity estimation. In this study, we design a deep convolutional neural network architecture for categorizing hurricanes based on intensity using graphics processing unit. Our model has achieved better accuracy and lower root-mean-square error by just using satellite images than 'state-of-the-art' techniques. Visualizations of learned features at various layers and their deconvolutions are also presented for understanding the learning process.},
  keywords = {Computer architecture,convolutional neural networks,deep convolutional neural network architecture,Deep learning,domain knowledge,Estimation,feature extraction,Feature extraction,feedforward neural nets,geophysical image processing,graphics processing unit,Hurricanes,image processing,learning (artificial intelligence),learning process,mean square error methods,Neural networks,root-mean-square error,satellite images,storms,tropical cyclone category and intensity estimation,tropical cyclone intensity estimation,Tropical cyclones},
  file = {C:\Users\cleme\Zotero\storage\NBZP6YAW\8082557.html}
}

@article{prattConvolutionalNeuralNetworks2016,
  title = {Convolutional {{Neural Networks}} for {{Diabetic Retinopathy}}},
  author = {Pratt, Harry and Coenen, Frans and Broadbent, Deborah M. and Harding, Simon P. and Zheng, Yalin},
  year = {2016},
  month = jan,
  journal = {Procedia Computer Science},
  series = {20th {{Conference}} on {{Medical Image Understanding}} and {{Analysis}} ({{MIUA}} 2016)},
  volume = {90},
  pages = {200--205},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2016.07.014},
  urldate = {2019-11-20},
  abstract = {The diagnosis of diabetic retinopathy (DR) through colour fundus images requires experienced clinicians to identify the presence and significance of many small features which, along with a complex grading system, makes this a difficult and time consuming task. In this paper, we propose a CNN approach to diagnosing DR from digital fundus images and accurately classifying its severity. We develop a network with CNN architecture and data augmentation which can identify the intricate features involved in the classification task such as micro-aneurysms, exudate and haemorrhages on the retina and consequently provide a diagnosis automatically and without user input. We train this network using a high-end graphics processor unit (GPU) on the publicly available Kaggle dataset and demonstrate impressive results, particularly for a high-level classification task. On the data set of 80,000 images used our proposed CNN achieves a sensitivity of 95\% and an accuracy of 75\% on 5,000 validation images.},
  langid = {english},
  keywords = {Convolutional Neural Networks,Deep Learning,Diabetes,Diabetic Retinopathy,Image Classification}
}

@article{prattConvolutionalNeuralNetworks2016a,
  title = {Convolutional {{Neural Networks}} for {{Diabetic Retinopathy}}},
  author = {Pratt, Harry and Coenen, Frans and Broadbent, Deborah M. and Harding, Simon P. and Zheng, Yalin},
  year = {2016},
  month = jan,
  journal = {Procedia Computer Science},
  series = {20th {{Conference}} on {{Medical Image Understanding}} and {{Analysis}} ({{MIUA}} 2016)},
  volume = {90},
  pages = {200--205},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2016.07.014},
  urldate = {2019-11-20},
  abstract = {The diagnosis of diabetic retinopathy (DR) through colour fundus images requires experienced clinicians to identify the presence and significance of many small features which, along with a complex grading system, makes this a difficult and time consuming task. In this paper, we propose a CNN approach to diagnosing DR from digital fundus images and accurately classifying its severity. We develop a network with CNN architecture and data augmentation which can identify the intricate features involved in the classification task such as micro-aneurysms, exudate and haemorrhages on the retina and consequently provide a diagnosis automatically and without user input. We train this network using a high-end graphics processor unit (GPU) on the publicly available Kaggle dataset and demonstrate impressive results, particularly for a high-level classification task. On the data set of 80,000 images used our proposed CNN achieves a sensitivity of 95\% and an accuracy of 75\% on 5,000 validation images.},
  langid = {english},
  keywords = {Convolutional Neural Networks,Deep Learning,Diabetes,Diabetic Retinopathy,Image Classification},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\4W54L4AF\\pratt2016.pdf;C\:\\Users\\cleme\\Zotero\\storage\\A33HM5FS\\Pratt et al. - 2016 - Convolutional Neural Networks for Diabetic Retinop.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6S39V56R\\S1877050916311929.html}
}

@misc{PredictingConversionWet,
  title = {Predicting Conversion to Wet Age-Related Macular Degeneration Using Deep Learning {\textbar} {{Nature Medicine}}},
  urldate = {2020-05-25}
}

@misc{PredictingConversionWeta,
  title = {Predicting Conversion to Wet Age-Related Macular Degeneration Using Deep Learning {\textbar} {{Nature Medicine}}},
  urldate = {2020-05-25},
  howpublished = {https://www.nature.com/articles/s41591-020-0867-7},
  file = {C:\Users\cleme\Zotero\storage\ZGU5X6A3\s41591-020-0867-7.html}
}

@misc{pressAIDiabeticRetinopathy,
  title = {{{AI For Diabetic Retinopathy Screening Gets U}}.{{S}}. {{FDA Approval}}},
  author = {Press, Gil},
  urldate = {2023-06-26},
  abstract = {The AEYE Health AI model performs better than human experts, diagnosing diabetes from otherwise healthy retinas.},
  chapter = {Enterprise \& Cloud},
  langid = {english}
}

@incollection{purvesRetinotopicRepresentationVisual2001,
  title = {The {{Retinotopic Representation}} of the {{Visual Field}}},
  booktitle = {Neuroscience. 2nd Edition},
  author = {Purves, Dale and Augustine, George J. and Fitzpatrick, David and Katz, Lawrence C. and LaMantia, Anthony-Samuel and McNamara, James O. and Williams, S. Mark},
  year = {2001},
  publisher = {Sinauer Associates},
  urldate = {2023-09-23},
  abstract = {The spatial relationships among the ganglion cells in the retina are maintained in their central targets as orderly representations or ``maps'' of visual space. Importantly, information from the left half of the visual world is represented in the right half of the brain, and vice versa.},
  langid = {english}
}

@incollection{purvesRetinotopicRepresentationVisual2001a,
  title = {The {{Retinotopic Representation}} of the {{Visual Field}}},
  booktitle = {Neuroscience. 2nd Edition},
  author = {Purves, Dale and Augustine, George J. and Fitzpatrick, David and Katz, Lawrence C. and LaMantia, Anthony-Samuel and McNamara, James O. and Williams, S. Mark},
  year = {2001},
  publisher = {Sinauer Associates},
  urldate = {2023-09-23},
  abstract = {The spatial relationships among the ganglion cells in the retina are maintained in their central targets as orderly representations or ``maps'' of visual space. Importantly, information from the left half of the visual world is represented in the right half of the brain, and vice versa.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\7KD4A786\NBK10944.html}
}

@inproceedings{qian2022pointnext,
  title = {{{PointNeXt}}: {{Revisiting PointNet}}++ with Improved Training and Scaling Strategies},
  booktitle = {Advances in Neural Information Processing Systems ({{NeurIPS}})},
  author = {Qian, Guocheng and Li, Yuchen and Peng, Houwen and Mai, Jinjie and Hammoud, Hasan and Elhoseiny, Mohamed and Ghanem, Bernard},
  year = {2022}
}

@inproceedings{qian2022pointnext,
  title = {{{PointNeXt}}: {{Revisiting PointNet}}++ with Improved Training and Scaling Strategies},
  booktitle = {Advances in Neural Information Processing Systems ({{NeurIPS}})},
  author = {Qian, Guocheng and Li, Yuchen and Peng, Houwen and Mai, Jinjie and Hammoud, Hasan and Elhoseiny, Mohamed and Ghanem, Bernard},
  year = {2022}
}

@article{qiPointNetDeepHierarchical,
  title = {{{PointNet}}++: {{Deep Hierarchical Feature Learning}} on {{Point Sets}} in a {{Metric Space}}},
  author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  langid = {english}
}

@inproceedings{qiPointNetDeepHierarchical2017,
  title = {{{PointNet}}++: {{Deep}} Hierarchical Feature Learning on Point Sets in a Metric Space},
  shorttitle = {{{PointNet}}++},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  year = {2017},
  month = dec,
  series = {{{NIPS}}'17},
  pages = {5105--5114},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-04-17},
  abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  isbn = {978-1-5108-6096-4}
}

@inproceedings{qiPointNetDeepHierarchical2017a,
  title = {{{PointNet}}++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
  shorttitle = {{{PointNet}}++},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  year = {2017},
  month = dec,
  series = {{{NIPS}}'17},
  pages = {5105--5114},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-04-17},
  abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  isbn = {978-1-5108-6096-4},
  file = {C:\Users\cleme\Zotero\storage\ET2ZDEBL\Qi et al. - 2017 - PointNet++ deep hierarchical feature learning on .pdf}
}

@article{qiPointNetDeepHierarchicala,
  title = {{{PointNet}}++: {{Deep Hierarchical Feature Learning}} on {{Point Sets}} in a {{Metric Space}}},
  author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  langid = {english}
}

@article{qiPointNetDeepHierarchicalb,
  title = {{{PointNet}}++: {{Deep Hierarchical Feature Learning}} on {{Point Sets}} in a {{Metric Space}}},
  author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\GR4GZL7F\Qi et al. - PointNet++ Deep Hierarchical Feature Learning on .pdf}
}

@article{qiPointNetDeepHierarchicalc,
  title = {{{PointNet}}++: {{Deep Hierarchical Feature Learning}} on {{Point Sets}} in a {{Metric Space}}},
  author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\7MJRDKS4\Qi et al. - PointNet++ Deep Hierarchical Feature Learning on .pdf}
}

@incollection{qiu2021modal,
  title = {Modal Uncertainty Estimation for Medical Imaging Based Diagnosis},
  booktitle = {Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, and Perinatal Imaging, Placental and Preterm Image Analysis},
  author = {Qiu, Di and Lui, Lok Ming},
  year = {2021},
  pages = {3--13},
  publisher = {Springer}
}

@article{quellec2021explain,
  title = {{{ExplAIn}}: {{Explanatory}} Artificial Intelligence for Diabetic Retinopathy Diagnosis},
  author = {Quellec, Gwenol{\'e} and Al Hajj, Hassan and Lamard, Mathieu and Conze, Pierre-Henri and Massin, Pascale and Cochener, B{\'e}atrice},
  year = {2021},
  journal = {Medical Image Analysis},
  pages = {102118},
  publisher = {Elsevier}
}

@article{quellec2021explain,
  title = {{{ExplAIn}}: {{Explanatory}} Artificial Intelligence for Diabetic Retinopathy Diagnosis},
  author = {Quellec, Gwenol{\'e} and Al Hajj, Hassan and Lamard, Mathieu and Conze, Pierre-Henri and Massin, Pascale and Cochener, B{\'e}atrice},
  year = {2021},
  journal = {Medical Image Analysis},
  pages = {102118},
  publisher = {Elsevier}
}

@article{quellecAutomaticDetectionRare2020,
  title = {Automatic Detection of Rare Pathologies in Fundus Photographs Using Few-Shot Learning},
  author = {Quellec, Gwenol{\'e} and Lamard, Mathieu and Conze, Pierre-Henri and Massin, Pascale and Cochener, B{\'e}atrice},
  year = {2020},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {61},
  pages = {101660},
  issn = {1361-8415},
  doi = {10.1016/j.media.2020.101660},
  urldate = {2021-04-12},
  abstract = {In the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy (DR) screening networks. Through deep learning, these datasets were used to train automatic detectors for DR and a few other frequent pathologies, with the goal to automate screening. One challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy. The reason is that standard deep learning requires too many examples of these conditions. However, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category. This paper presents a new few-shot learning framework that extends convolutional neural networks (CNNs), trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection. It is based on the observation that CNNs often perceive photographs containing the same anomalies as similar, even though these CNNs were trained to detect unrelated conditions. This observation was based on the t-SNE visualization tool, which we decided to incorporate in our probabilistic model. Experiments on a dataset of 164,660 screening examinations from the OPHDIAT screening network show that 37 conditions, out of 41, can be detected with an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938). In particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and Siamese networks, another few-shot learning solution. We expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy screening,Few-shot learning,Rare conditions}
}

@article{quellecAutomaticDetectionRare2020a,
  title = {Automatic Detection of Rare Pathologies in Fundus Photographs Using Few-Shot Learning},
  author = {Quellec, Gwenol{\'e} and Lamard, Mathieu and Conze, Pierre-Henri and Massin, Pascale and Cochener, B{\'e}atrice},
  year = {2020},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {61},
  pages = {101660},
  issn = {1361-8415},
  doi = {10.1016/j.media.2020.101660},
  urldate = {2021-04-12},
  abstract = {In the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy (DR) screening networks. Through deep learning, these datasets were used to train automatic detectors for DR and a few other frequent pathologies, with the goal to automate screening. One challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy. The reason is that standard deep learning requires too many examples of these conditions. However, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category. This paper presents a new few-shot learning framework that extends convolutional neural networks (CNNs), trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection. It is based on the observation that CNNs often perceive photographs containing the same anomalies as similar, even though these CNNs were trained to detect unrelated conditions. This observation was based on the t-SNE visualization tool, which we decided to incorporate in our probabilistic model. Experiments on a dataset of 164,660 screening examinations from the OPHDIAT screening network show that 37 conditions, out of 41, can be detected with an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938). In particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and Siamese networks, another few-shot learning solution. We expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy screening,Few-shot learning,Rare conditions},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\WC2GQAYQ\\Quellec et al. - 2020 - Automatic detection of rare pathologies in fundus .pdf;C\:\\Users\\cleme\\Zotero\\storage\\T8NKBRSA\\S1361841520300256.html}
}

@article{quellecDeepImageMining2017,
  title = {Deep Image Mining for Diabetic Retinopathy Screening},
  author = {Quellec, Gwenol{\'e} and Charri{\`e}re, Katia and Boudi, Yassine and Cochener, B{\'e}atrice and Lamard, Mathieu},
  year = {2017},
  month = jul,
  journal = {Medical Image Analysis},
  volume = {39},
  pages = {178--193},
  issn = {1361-8415},
  doi = {10.1016/j.media.2017.04.012},
  urldate = {2019-09-23},
  abstract = {Deep learning is quickly becoming the leading methodology for medical image analysis. Given a large medical archive, where each image is associated with a diagnosis, efficient pathology detectors or classifiers can be trained with virtually no expert knowledge about the target pathologies. However, deep learning algorithms, including the popular ConvNets, are black boxes: little is known about the local patterns analyzed by ConvNets to make a decision at the image level. A solution is proposed in this paper to create heatmaps showing which pixels in images play a role in the image-level predictions. In other words, a ConvNet trained for image-level classification can be used to detect lesions as well. A generalization of the backpropagation method is proposed in order to train ConvNets that produce high-quality heatmaps. The proposed solution is applied to diabetic retinopathy (DR) screening in a dataset of almost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy competition and a private dataset of almost 110,000 photographs (e-ophtha). For the task of detecting referable DR, very good detection performance was achieved: Az=0.954 in Kaggle's dataset and Az=0.949 in e-ophtha. Performance was also evaluated at the image level and at the lesion level in the DiaretDB1 dataset, where four types of lesions are manually segmented: microaneurysms, hemorrhages, exudates and cotton-wool spots. For the task of detecting images containing these four lesion types, the proposed detector, which was trained to detect referable DR, outperforms recent algorithms trained to detect those lesions specifically, with pixel-level supervision. At the lesion level, the proposed detector outperforms heatmap generation algorithms for ConvNets. This detector is part of the Messidor{\textregistered} system for mobile eye pathology screening. Because it does not rely on expert knowledge or manual segmentation for detecting relevant patterns, the proposed solution is a promising image mining tool, which has the potential to discover new biomarkers in images.},
  keywords = {Deep learning,Diabetic retinopathy screening,Image mining,Lesion detection}
}

@article{quellecDeepImageMining2017a,
  title = {Deep Image Mining for Diabetic Retinopathy Screening},
  author = {Quellec, Gwenol{\'e} and Charri{\`e}re, Katia and Boudi, Yassine and Cochener, B{\'e}atrice and Lamard, Mathieu},
  year = {2017},
  month = jul,
  journal = {Medical Image Analysis},
  volume = {39},
  pages = {178--193},
  issn = {1361-8415},
  doi = {10.1016/j.media.2017.04.012},
  urldate = {2021-04-13},
  abstract = {Deep learning is quickly becoming the leading methodology for medical image analysis. Given a large medical archive, where each image is associated with a diagnosis, efficient pathology detectors or classifiers can be trained with virtually no expert knowledge about the target pathologies. However, deep learning algorithms, including the popular ConvNets, are black boxes: little is known about the local patterns analyzed by ConvNets to make a decision at the image level. A solution is proposed in this paper to create heatmaps showing which pixels in images play a role in the image-level predictions. In other words, a ConvNet trained for image-level classification can be used to detect lesions as well. A generalization of the backpropagation method is proposed in order to train ConvNets that produce high-quality heatmaps. The proposed solution is applied to diabetic retinopathy (DR) screening in a dataset of almost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy competition and a private dataset of almost 110,000 photographs (e-ophtha). For the task of detecting referable DR, very good detection performance was achieved: Az=0.954 in Kaggle's dataset and Az=0.949 in e-ophtha. Performance was also evaluated at the image level and at the lesion level in the DiaretDB1 dataset, where four types of lesions are manually segmented: microaneurysms, hemorrhages, exudates and cotton-wool spots. For the task of detecting images containing these four lesion types, the proposed detector, which was trained to detect referable DR, outperforms recent algorithms trained to detect those lesions specifically, with pixel-level supervision. At the lesion level, the proposed detector outperforms heatmap generation algorithms for ConvNets. This detector is part of the Messidor{\textregistered} system for mobile eye pathology screening. Because it does not rely on expert knowledge or manual segmentation for detecting relevant patterns, the proposed solution is a promising image mining tool, which has the potential to discover new biomarkers in images.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy screening,Image mining,Lesion detection}
}

@article{quellecDeepImageMining2017b,
  title = {Deep Image Mining for Diabetic Retinopathy Screening},
  author = {Quellec, Gwenol{\'e} and Charri{\`e}re, Katia and Boudi, Yassine and Cochener, B{\'e}atrice and Lamard, Mathieu},
  year = {2017},
  month = jul,
  journal = {Medical Image Analysis},
  volume = {39},
  pages = {178--193},
  issn = {1361-8415},
  doi = {10.1016/j.media.2017.04.012},
  urldate = {2019-09-23},
  abstract = {Deep learning is quickly becoming the leading methodology for medical image analysis. Given a large medical archive, where each image is associated with a diagnosis, efficient pathology detectors or classifiers can be trained with virtually no expert knowledge about the target pathologies. However, deep learning algorithms, including the popular ConvNets, are black boxes: little is known about the local patterns analyzed by ConvNets to make a decision at the image level. A solution is proposed in this paper to create heatmaps showing which pixels in images play a role in the image-level predictions. In other words, a ConvNet trained for image-level classification can be used to detect lesions as well. A generalization of the backpropagation method is proposed in order to train ConvNets that produce high-quality heatmaps. The proposed solution is applied to diabetic retinopathy (DR) screening in a dataset of almost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy competition and a private dataset of almost 110,000 photographs (e-ophtha). For the task of detecting referable DR, very good detection performance was achieved: Az=0.954 in Kaggle's dataset and Az=0.949 in e-ophtha. Performance was also evaluated at the image level and at the lesion level in the DiaretDB1 dataset, where four types of lesions are manually segmented: microaneurysms, hemorrhages, exudates and cotton-wool spots. For the task of detecting images containing these four lesion types, the proposed detector, which was trained to detect referable DR, outperforms recent algorithms trained to detect those lesions specifically, with pixel-level supervision. At the lesion level, the proposed detector outperforms heatmap generation algorithms for ConvNets. This detector is part of the Messidor{\textregistered} system for mobile eye pathology screening. Because it does not rely on expert knowledge or manual segmentation for detecting relevant patterns, the proposed solution is a promising image mining tool, which has the potential to discover new biomarkers in images.},
  keywords = {Deep learning,Diabetic retinopathy screening,Image mining,Lesion detection},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\EDY3AE3J\\Quellec et al. - 2017 - Deep image mining for diabetic retinopathy screeni.pdf;C\:\\Users\\cleme\\Zotero\\storage\\EDSZ3SU2\\S136184151730066X.html}
}

@article{quellecDeepImageMining2017c,
  title = {Deep Image Mining for Diabetic Retinopathy Screening},
  author = {Quellec, Gwenol{\'e} and Charri{\`e}re, Katia and Boudi, Yassine and Cochener, B{\'e}atrice and Lamard, Mathieu},
  year = {2017},
  month = jul,
  journal = {Medical Image Analysis},
  volume = {39},
  pages = {178--193},
  issn = {1361-8415},
  doi = {10.1016/j.media.2017.04.012},
  urldate = {2021-04-13},
  abstract = {Deep learning is quickly becoming the leading methodology for medical image analysis. Given a large medical archive, where each image is associated with a diagnosis, efficient pathology detectors or classifiers can be trained with virtually no expert knowledge about the target pathologies. However, deep learning algorithms, including the popular ConvNets, are black boxes: little is known about the local patterns analyzed by ConvNets to make a decision at the image level. A solution is proposed in this paper to create heatmaps showing which pixels in images play a role in the image-level predictions. In other words, a ConvNet trained for image-level classification can be used to detect lesions as well. A generalization of the backpropagation method is proposed in order to train ConvNets that produce high-quality heatmaps. The proposed solution is applied to diabetic retinopathy (DR) screening in a dataset of almost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy competition and a private dataset of almost 110,000 photographs (e-ophtha). For the task of detecting referable DR, very good detection performance was achieved: Az=0.954 in Kaggle's dataset and Az=0.949 in e-ophtha. Performance was also evaluated at the image level and at the lesion level in the DiaretDB1 dataset, where four types of lesions are manually segmented: microaneurysms, hemorrhages, exudates and cotton-wool spots. For the task of detecting images containing these four lesion types, the proposed detector, which was trained to detect referable DR, outperforms recent algorithms trained to detect those lesions specifically, with pixel-level supervision. At the lesion level, the proposed detector outperforms heatmap generation algorithms for ConvNets. This detector is part of the Messidor{\textregistered} system for mobile eye pathology screening. Because it does not rely on expert knowledge or manual segmentation for detecting relevant patterns, the proposed solution is a promising image mining tool, which has the potential to discover new biomarkers in images.},
  langid = {english},
  keywords = {Deep learning,Diabetic retinopathy screening,Image mining,Lesion detection},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\XBC47VWN\\Quellec et al. - 2017 - Deep image mining for diabetic retinopathy screeni.pdf;C\:\\Users\\cleme\\Zotero\\storage\\R3MNE6XE\\S136184151730066X.html}
}

@article{quellecExplAInExplanatoryArtificial,
  title = {{{ExplAIn}}: {{Explanatory Artificial Intelligence}} for {{Diabetic Retinopathy Diagnosis}}},
  author = {Quellec, Gwenole and Hajj, Hassan Al and Lamard, Mathieu and Conze, Pierre-Henri and Massin, Pascale and Cochener, Beatrice},
  pages = {15},
  abstract = {In recent years, Artificial Intelligence (AI) has proven its relevance for medical decision support. However, the ``black-box'' nature of successful AI algorithms still holds back their wide-spread deployment. In this paper, we describe an eXplanatory Artificial Intelligence (XAI) that reaches the same level of performance as black-box AI, for the task of classifying Diabetic Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm, called ExplAIn, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations. The novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box AI algorithms: the concepts of lesions and lesion categories emerge by themselves. For improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image. The advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences. ExplAIn is evaluated at the image level and at the pixel level on various CFP image datasets. We expect this new framework, which jointly offers high classification performance and explainability, to facilitate AI deployment.},
  langid = {english}
}

@article{quellecExplAInExplanatoryArtificiala,
  title = {{{ExplAIn}}: {{Explanatory Artificial Intelligence}} for {{Diabetic Retinopathy Diagnosis}}},
  author = {Quellec, Gwenole and Hajj, Hassan Al and Lamard, Mathieu and Conze, Pierre-Henri and Massin, Pascale and Cochener, Beatrice},
  pages = {15},
  abstract = {In recent years, Artificial Intelligence (AI) has proven its relevance for medical decision support. However, the ``black-box'' nature of successful AI algorithms still holds back their wide-spread deployment. In this paper, we describe an eXplanatory Artificial Intelligence (XAI) that reaches the same level of performance as black-box AI, for the task of classifying Diabetic Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm, called ExplAIn, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations. The novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box AI algorithms: the concepts of lesions and lesion categories emerge by themselves. For improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image. The advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences. ExplAIn is evaluated at the image level and at the pixel level on various CFP image datasets. We expect this new framework, which jointly offers high classification performance and explainability, to facilitate AI deployment.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\KA7P75NW\Quellec et al. - ExplAIn Explanatory Artiﬁcial Intelligence for Di.pdf}
}

@article{rahimyDeepLearningApplications2018,
  title = {Deep Learning Applications in Ophthalmology},
  author = {Rahimy, Ehsan},
  year = {2018},
  month = may,
  journal = {Current Opinion in Ophthalmology},
  volume = {29},
  number = {3},
  pages = {254},
  issn = {1040-8738},
  doi = {10.1097/ICU.0000000000000470},
  urldate = {2019-11-20},
  abstract = {Purpose of review To describe the emerging applications of deep learning in ophthalmology. Recent findings Recent studies have shown that various deep learning models are capable of detecting and diagnosing various diseases afflicting the posterior segment of the eye with high accuracy. Most of the initial studies have centered around detection of referable diabetic retinopathy, age-related macular degeneration, and glaucoma. Summary Deep learning has shown promising results in automated image analysis of fundus photographs and optical coherence tomography images. Additional testing and research is required to clinically validate this technology.},
  langid = {american}
}

@article{rahimyDeepLearningApplications2018a,
  title = {Deep Learning Applications in Ophthalmology},
  author = {Rahimy, Ehsan},
  year = {2018},
  month = may,
  journal = {Current Opinion in Ophthalmology},
  volume = {29},
  number = {3},
  pages = {254},
  issn = {1040-8738},
  doi = {10.1097/ICU.0000000000000470},
  urldate = {2019-11-20},
  abstract = {Purpose of review~To describe the emerging applications of deep learning in ophthalmology.         Recent findings~Recent studies have shown that various deep learning models are capable of detecting and diagnosing various diseases afflicting the posterior segment of the eye with high accuracy. Most of the initial studies have centered around detection of referable diabetic retinopathy, age-related macular degeneration, and glaucoma.         Summary~Deep learning has shown promising results in automated image analysis of fundus photographs and optical coherence tomography images. Additional testing and research is required to clinically validate this technology.},
  langid = {american},
  file = {C:\Users\cleme\Zotero\storage\7A2EEB8V\Deep_learning_applications_in_ophthalmology.11.html}
}

@article{ramachandramDeepMultimodalLearning2017,
  title = {Deep {{Multimodal Learning}}: {{A Survey}} on {{Recent Advances}} and {{Trends}}},
  shorttitle = {Deep {{Multimodal Learning}}},
  author = {Ramachandram, Dhanesh and Taylor, Graham W.},
  year = {2017},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {96--108},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2738401},
  abstract = {The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures. We highlight two areas of research-regularization strategies and methods that learn or optimize multimodal fusion structures-as exciting areas for future work.},
  keywords = {Computer architecture,Emotion recognition,Face recognition,Machine learning,Sensors,Training data}
}

@article{ramachandramDeepMultimodalLearning2017a,
  title = {Deep {{Multimodal Learning}}: {{A Survey}} on {{Recent Advances}} and {{Trends}}},
  shorttitle = {Deep {{Multimodal Learning}}},
  author = {Ramachandram, Dhanesh and Taylor, Graham W.},
  year = {2017},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {96--108},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2738401},
  abstract = {The success of deep learning has been a catalyst to solving increasingly complex machine-learning problems, which often involve multiple data modalities. We review recent advances in deep multimodal learning and highlight the state-of the art, as well as gaps and challenges in this active research field. We first classify deep multimodal learning architectures and then discuss methods to fuse learned multimodal representations in deep-learning architectures. We highlight two areas of research-regularization strategies and methods that learn or optimize multimodal fusion structures-as exciting areas for future work.},
  keywords = {Computer architecture,Emotion recognition,Face recognition,Machine learning,Sensors,Training data},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\YSU33T3Z\\Ramachandram et Taylor - 2017 - Deep Multimodal Learning A Survey on Recent Advan.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6GDHQLEE\\8103116.html}
}

@misc{ramanHowAccurateDiagnosis2014,
  title = {How Accurate Is the Diagnosis of Diabetic Retinopathy on Telescreening? {{The Indian}} Scenario},
  shorttitle = {How Accurate Is the Diagnosis of Diabetic Retinopathy on Telescreening?},
  author = {Raman, Rajiv and Bhojwani, Deepak and Sharma, Tarun},
  year = {2014},
  month = oct,
  volume = {14},
  number = {4},
  doi = {10.22605/RRH2809},
  urldate = {2022-10-20},
  abstract = {Published article 2809 Rural and Remote Health},
  langid = {english}
}

@misc{ramanHowAccurateDiagnosis2014a,
  title = {How Accurate Is the Diagnosis of Diabetic Retinopathy on Telescreening? {{The Indian}} Scenario},
  shorttitle = {How Accurate Is the Diagnosis of Diabetic Retinopathy on Telescreening?},
  author = {Raman, Rajiv and Bhojwani, Deepak and Sharma, Tarun},
  year = {2014},
  month = oct,
  volume = {14},
  number = {4},
  doi = {10.22605/RRH2809},
  urldate = {2022-10-20},
  abstract = {Published article 2809 Rural and Remote Health},
  howpublished = {https://www.rrh.org.au/journal/article/2809/},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HWIKA89I\\Raman et al. - 2014 - How accurate is the diagnosis of diabetic retinopa.pdf;C\:\\Users\\cleme\\Zotero\\storage\\I23R35GL\\2809.html}
}

@article{randiveReviewComputeraidedRecent2019,
  title = {A Review on Computer-Aided Recent Developments for Automatic Detection of Diabetic Retinopathy},
  author = {Randive, Santosh Nagnath and Senapati, Ranjan K. and Rahulkar, Amol D.},
  year = {2019},
  month = feb,
  journal = {Journal of Medical Engineering \& Technology},
  volume = {43},
  number = {2},
  pages = {87--99},
  issn = {1464-522X},
  doi = {10.1080/03091902.2019.1576790},
  abstract = {Diabetic retinopathy is a serious microvascular disorder that might result in loss of vision and blindness. It seriously damages the retinal blood vessels and reduces the light-sensitive inner layer of the eye. Due to the manual inspection of retinal fundus images on diabetic retinopathy to detect the morphological abnormalities in Microaneurysms (MAs), Exudates (EXs), Haemorrhages (HMs), and Inter retinal microvascular abnormalities (IRMA) is very difficult and time consuming process. In order to avoid this, the regular follow-up screening process, and early automatic Diabetic Retinopathy detection are necessary. This paper discusses various methods of analysing automatic retinopathy detection and classification of different grading based on the severity levels. In addition, retinal blood vessel detection techniques are also discussed for the ultimate detection and diagnostic procedure of proliferative diabetic retinopathy. Furthermore, the paper elaborately discussed the systematic review accessed by authors on various publicly available databases collected from different medical sources. In the survey, meta-analysis of several methods for diabetic feature extraction, segmentation and various types of classifiers have been used to evaluate the system performance metrics for the diagnosis of DR. This survey will be helpful for the technical persons and researchers who want to focus on enhancing the diagnosis of a system that would be more powerful in real life.},
  langid = {english},
  pmid = {31198073},
  keywords = {Algorithms,Animals,classification,Computer-Assisted,Diabetic retinopathy,Diabetic Retinopathy,exudate,haemorrhage,Hemorrhage,Humans,Image Interpretation,Image Interpretation Computer-Assisted,machine learning,Machine Learning,microaneurysm,Microaneurysm}
}

@inproceedings{ranjan2020asap,
  title = {Asap: {{Adaptive}} Structure Aware Pooling for Learning Hierarchical Graph Representations},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Ranjan, Ekagra and Sanyal, Soumya and Talukdar, Partha},
  year = {2020},
  volume = {34},
  pages = {5470--5477}
}

@inproceedings{ranjan2020asap,
  title = {Asap: {{Adaptive}} Structure Aware Pooling for Learning Hierarchical Graph Representations},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Ranjan, Ekagra and Sanyal, Soumya and Talukdar, Partha},
  year = {2020},
  volume = {34},
  pages = {5470--5477}
}

@article{rappaportAdvancesChallengesNational2009,
  title = {Advances and {{Challenges}} at the {{National Hurricane Center}}},
  author = {Rappaport, Edward N. and Franklin, James L. and Avila, Lixion A. and Baig, Stephen R. and Beven, John L. and Blake, Eric S. and Burr, Christopher A. and Jiing, Jiann-Gwo and Juckins, Christopher A. and Knabb, Richard D. and Landsea, Christopher W. and Mainelli, Michelle and Mayfield, Max and McAdie, Colin J. and Pasch, Richard J. and Sisko, Christopher and Stewart, Stacy R. and Tribble, Ahsha N.},
  year = {2009},
  month = apr,
  journal = {Weather and Forecasting},
  volume = {24},
  number = {2},
  pages = {395--419},
  issn = {0882-8156},
  doi = {10.1175/2008WAF2222128.1},
  urldate = {2019-06-10},
  abstract = {The National Hurricane Center issues analyses, forecasts, and warnings over large parts of the North Atlantic and Pacific Oceans, and in support of many nearby countries. Advances in observational capabilities, operational numerical weather prediction, and forecaster tools and support systems over the past 15--20 yr have enabled the center to make more accurate forecasts, extend forecast lead times, and provide new products and services. Important limitations, however, persist. This paper discusses the current workings and state of the nation's hurricane warning program, and highlights recent improvements and the enabling science and technology. It concludes with a look ahead at opportunities to address challenges.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9LRGMM6V\\Rappaport et al. - 2009 - Advances and Challenges at the National Hurricane .pdf;C\:\\Users\\cleme\\Zotero\\storage\\EHPTTDMS\\2008WAF2222128.html}
}

@incollection{rasExplanationMethodsDeep2018,
  title = {Explanation {{Methods}} in {{Deep Learning}}: {{Users}}, {{Values}}, {{Concerns}} and {{Challenges}}},
  shorttitle = {Explanation {{Methods}} in {{Deep Learning}}},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  author = {Ras, Gabri{\"e}lle and {van Gerven}, Marcel and Haselager, Pim},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar{\'o}, Xavier and G{\"u}{\c c}l{\"u}t{\"u}rk, Ya{\u g}mur and G{\"u}{\c c}l{\"u}, Umut and {van Gerven}, Marcel},
  year = {2018},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  pages = {19--36},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-98131-4_2},
  urldate = {2023-05-04},
  abstract = {Issues regarding explainable AI involve four components: users, laws and regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods/interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.},
  isbn = {978-3-319-98131-4},
  langid = {english},
  keywords = {Artificial intelligence,Deep neural networks,Explainable AI,Explanation methods,Interpretability}
}

@incollection{rasExplanationMethodsDeep2018a,
  title = {Explanation {{Methods}} in {{Deep Learning}}: {{Users}}, {{Values}}, {{Concerns}} and {{Challenges}}},
  shorttitle = {Explanation {{Methods}} in {{Deep Learning}}},
  booktitle = {Explainable and {{Interpretable Models}} in {{Computer Vision}} and {{Machine Learning}}},
  author = {Ras, Gabri{\"e}lle and {van Gerven}, Marcel and Haselager, Pim},
  editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar{\'o}, Xavier and G{\"u}{\c c}l{\"u}t{\"u}rk, Ya{\u g}mur and G{\"u}{\c c}l{\"u}, Umut and {van Gerven}, Marcel},
  year = {2018},
  series = {The {{Springer Series}} on {{Challenges}} in {{Machine Learning}}},
  pages = {19--36},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-98131-4_2},
  urldate = {2023-05-04},
  abstract = {Issues regarding explainable AI involve four components: users, laws and regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods/interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.},
  isbn = {978-3-319-98131-4},
  langid = {english},
  keywords = {Artificial intelligence,Deep neural networks,Explainable AI,Explanation methods,Interpretability},
  file = {C:\Users\cleme\Zotero\storage\B3UI87BD\Ras et al. - 2018 - Explanation Methods in Deep Learning Users, Value.pdf}
}

@article{rasheedExplainableTrustworthyEthical2022,
  title = {Explainable, Trustworthy, and Ethical Machine Learning for Healthcare: {{A}} Survey},
  shorttitle = {Explainable, Trustworthy, and Ethical Machine Learning for Healthcare},
  author = {Rasheed, Khansa and Qayyum, Adnan and Ghaly, Mohammed and {Al-Fuqaha}, Ala and Razi, Adeel and Qadir, Junaid},
  year = {2022},
  month = oct,
  journal = {Computers in Biology and Medicine},
  volume = {149},
  pages = {106043},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2022.106043},
  urldate = {2023-05-05},
  abstract = {With the advent of machine learning (ML) and deep learning (DL) empowered applications for critical applications like healthcare, the questions about liability, trust, and interpretability of their outputs are raising. The black-box nature of various DL models is a roadblock to clinical utilization. Therefore, to gain the trust of clinicians and patients, we need to provide explanations about the decisions of models. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provided a comprehensive review of explainable and interpretable ML techniques for various healthcare applications. Along with highlighting security, safety, and robustness challenges that hinder the trustworthiness of ML, we also discussed the ethical issues arising because of the use of ML/DL for healthcare. We also describe how explainable and trustworthy ML can resolve all these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.},
  langid = {english},
  keywords = {Explainable machine learning,Healthcare,Interpretable machine learning,Trustworthiness}
}

@article{rasheedExplainableTrustworthyEthical2022a,
  title = {Explainable, Trustworthy, and Ethical Machine Learning for Healthcare: {{A}} Survey},
  shorttitle = {Explainable, Trustworthy, and Ethical Machine Learning for Healthcare},
  author = {Rasheed, Khansa and Qayyum, Adnan and Ghaly, Mohammed and {Al-Fuqaha}, Ala and Razi, Adeel and Qadir, Junaid},
  year = {2022},
  month = oct,
  journal = {Computers in Biology and Medicine},
  volume = {149},
  pages = {106043},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2022.106043},
  urldate = {2023-05-05},
  abstract = {With the advent of machine learning (ML) and deep learning (DL) empowered applications for critical applications like healthcare, the questions about liability, trust, and interpretability of their outputs are raising. The black-box nature of various DL models is a roadblock to clinical utilization. Therefore, to gain the trust of clinicians and patients, we need to provide explanations about the decisions of models. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provided a comprehensive review of explainable and interpretable ML techniques for various healthcare applications. Along with highlighting security, safety, and robustness challenges that hinder the trustworthiness of ML, we also discussed the ethical issues arising because of the use of ML/DL for healthcare. We also describe how explainable and trustworthy ML can resolve all these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.},
  langid = {english},
  keywords = {Explainable machine learning,Healthcare,Interpretable machine learning,Trustworthiness},
  file = {C:\Users\cleme\Zotero\storage\LED7ALYF\Rasheed et al. - 2022 - Explainable, trustworthy, and ethical machine lear.pdf}
}

@article{rastaComparativeStudyPreprocessing2015a,
  title = {A {{Comparative Study}} on {{Preprocessing Techniques}} in {{Diabetic Retinopathy Retinal Images}}: {{Illumination Correction}} and {{Contrast Enhancement}}},
  shorttitle = {A {{Comparative Study}} on {{Preprocessing Techniques}} in {{Diabetic Retinopathy Retinal Images}}},
  author = {Rasta, Seyed Hossein and Partovi, Mahsa Eisazadeh and Seyedarabi, Hadi and Javadzadeh, Alireza},
  year = {2015},
  journal = {Journal of Medical Signals and Sensors},
  volume = {5},
  number = {1},
  pages = {40--48},
  issn = {2228-7477},
  urldate = {2019-10-09},
  abstract = {To investigate the effect of preprocessing techniques including contrast enhancement and illumination correction on retinal image quality, a comparative study was carried out. We studied and implemented a few illumination correction and contrast enhancement techniques on color retinal images to find out the best technique for optimum image enhancement. To compare and choose the best illumination correction technique we analyzed the corrected red and green components of color retinal images statistically and visually. The two contrast enhancement techniques were analyzed using a vessel segmentation algorithm by calculating the sensitivity and specificity. The statistical evaluation of the illumination correction techniques were carried out by calculating the coefficients of variation. The dividing method using the median filter to estimate background illumination showed the lowest Coefficients of variations in the red component. The quotient and homomorphic filtering methods after the dividing method presented good results based on their low Coefficients of variations. The contrast limited adaptive histogram equalization increased the sensitivity of the vessel segmentation algorithm up to 5\% in the same amount of accuracy. The contrast limited adaptive histogram equalization technique has a higher sensitivity than the polynomial transformation operator as a contrast enhancement technique for vessel segmentation. Three techniques including the dividing method using the median filter to estimate background, quotient based and homomorphic filtering were found as the effective illumination correction techniques based on a statistical evaluation. Applying the local contrast enhancement technique, such as CLAHE, for fundus images presented good potentials in enhancing the vasculature segmentation.},
  pmcid = {PMC4335144},
  pmid = {25709940}
}

@article{reinkeUnderstandingMetricrelatedPitfalls2023,
  title = {Understanding Metric-Related Pitfalls in Image Analysis Validation},
  author = {REINKE, {\relax ANNIKA} and TIZABI, MINU D. and BAUMGARTNER, {\relax MICHAEL} and EISENMANN, {\relax MATTHIAS} and {HECKMANN-N{\"O}TZEL}, {\relax DOREEN} and KAVUR, A. EMRE and R{\"A}DSCH, {\relax TIM} and SUDRE, CAROLE H. and ACION, {\relax LAURA} and ANTONELLI, {\relax MICHELA} and ARBEL, {\relax TAL} and BAKAS, {\relax SPYRIDON} and BENIS, {\relax ARRIEL} and BLASCHKO, {\relax MATTHEW} and B{\"U}TTNER, {\relax FLORIAN} and CARDOSO, M. JORGE and CHEPLYGINA, {\relax VERONIKA} and CHEN, {\relax JIANXU} and CHRISTODOULOU, {\relax EVANGELIA} and CIMINI, BETH A. and COLLINS, GARY S. and FARAHANI, {\relax KEYVAN} and FERRER, {\relax LUCIANA} and GALDRAN, {\relax ADRIAN} and VAN GINNEKEN, {\relax BRAM} and GLOCKER, {\relax BEN} and GODAU, {\relax PATRICK} and HAASE, {\relax ROBERT} and HASHIMOTO, DANIEL A. and HOFFMAN, MICHAEL M. and HUISMAN, {\relax MEREL} and ISENSEE, {\relax FABIAN} and JANNIN, {\relax PIERRE} and KAHN, CHARLES E. and KAINMUELLER, {\relax DAGMAR} and KAINZ, {\relax BERNHARD} and KARARGYRIS, {\relax ALEXANDROS} and KARTHIKESALINGAM, {\relax ALAN} and KENNGOTT, {\relax HANNES} and KLEESIEK, {\relax JENS} and KOFLER, {\relax FLORIAN} and KOOI, {\relax THIJS} and {KOPP-SCHNEIDER}, {\relax ANNETTE} and KOZUBEK, {\relax MICHAL} and KRESHUK, {\relax ANNA} and KURC, {\relax TAHSIN} and LANDMAN, BENNETT A. and LITJENS, {\relax GEERT} and MADANI, {\relax AMIN} and {MAIER-HEIN}, {\relax KLAUS} and MARTEL, ANNE L. and MATTSON, {\relax PETER} and MEIJERING, {\relax ERIK} and MENZE, {\relax BJOERN} and MOONS, KAREL G.M. and M{\"U}LLER, {\relax HENNING} and NICHYPORUK, {\relax BRENNAN} and NICKEL, {\relax FELIX} and PETERSEN, {\relax JENS} and RAFELSKI, SUSANNE M. and RAJPOOT, {\relax NASIR} and REYES, {\relax MAURICIO} and RIEGLER, MICHAEL A. and RIEKE, {\relax NICOLA} and {SAEZ-RODRIGUEZ}, {\relax JULIO} and S{\'A}NCHEZ, CLARA I. and SHETTY, {\relax SHRAVYA} and VAN SMEDEN, {\relax MAARTEN} and SUMMERS, RONALD M. and TAHA, ABDEL A. and TIULPIN, {\relax ALEKSEI} and TSAFTARIS, SOTIRIOS A. and VAN CALSTER, {\relax BEN} and VAROQUAUX, {\relax GA{\"E}L} and WIESENFARTH, {\relax MANUEL} and YANIV, ZIV R. and J{\"A}GER, PAUL F. and {MAIER-HEIN}, {\relax LENA}},
  year = {2023},
  month = feb,
  journal = {ArXiv},
  pages = {arXiv:2302.01790v2},
  issn = {2331-8422},
  urldate = {2023-04-24},
  abstract = {Validation metrics are key for the reliable tracking of scientific progress and for bridging the current chasm between artificial intelligence (AI) research and its translation into practice. However, increasing evidence shows that particularly in image analysis, metrics are often chosen inadequately in relation to the underlying research problem. This could be attributed to a lack of accessibility of metric-related knowledge: While taking into account the individual strengths, weaknesses, and limitations of validation metrics is a critical prerequisite to making educated choices, the relevant knowledge is currently scattered and poorly accessible to individual researchers. Based on a multi-stage Delphi process conducted by a multidisciplinary expert consortium as well as extensive community feedback, the present work provides the first reliable and comprehensive common point of access to information on pitfalls related to validation metrics in image analysis. Focusing on biomedical image analysis but with the potential of transfer to other fields, the addressed pitfalls generalize across application domains and are categorized according to a newly created, domain-agnostic taxonomy. To facilitate comprehension, illustrations and specific examples accompany each pitfall. As a structured body of information accessible to researchers of all levels of expertise, this work enhances global comprehension of a key topic in image analysis validation.},
  pmcid = {PMC10029046},
  pmid = {36945687}
}

@misc{reinkeUnderstandingMetricrelatedPitfalls2023a,
  title = {Understanding Metric-Related Pitfalls in Image Analysis Validation.},
  author = {Reinke, Annika and Tizabi, Minu D. and Baumgartner, Michael and Eisenmann, Matthias and {Heckmann-N{\"o}tzel}, Doreen and Kavur, A. Emre and R{\"a}dsch, Tim and Sudre, Carole H. and Acion, Laura and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Benis, Arriel and Blaschko, Matthew and B{\"u}ttner, Florian and Cardoso, M. Jorge and Cheplygina, Veronika and Chen, Jianxu and Christodoulou, Evangelia and Cimini, Beth A. and Collins, Gary S. and Farahani, Keyvan and Ferrer, Luciana and Galdran, Adrian and {van Ginneken}, Bram and Glocker, Ben and Godau, Patrick and Haase, Robert and Hashimoto, Daniel A. and Hoffman, Michael M. and Huisman, Merel and Isensee, Fabian and Jannin, Pierre and Kahn, Charles E. and Kainmueller, Dagmar and Kainz, Bernhard and Karargyris, Alexandros and Karthikesalingam, Alan and Kenngott, Hannes and Kleesiek, Jens and Kofler, Florian and Kooi, Thijs and {Kopp-Schneider}, Annette and Kozubek, Michal and Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A. and Litjens, Geert and Madani, Amin and {Maier-Hein}, Klaus and Martel, Anne L. and Mattson, Peter and Meijering, Erik and Menze, Bjoern and Moons, Karel G. M. and M{\"u}ller, Henning and Nichyporuk, Brennan and Nickel, Felix and Petersen, Jens and Rafelski, Susanne M. and Rajpoot, Nasir and Reyes, Mauricio and Riegler, Michael A. and Rieke, Nicola and {Saez-Rodriguez}, Julio and S{\'a}nchez, Clara I. and Shetty, Shravya and {van Smeden}, Maarten and Summers, Ronald M. and Taha, Abdel A. and Tiulpin, Aleksei and Tsaftaris, Sotirios A. and Calster, Ben Van and Varoquaux, Ga{\"e}l and Wiesenfarth, Manuel and Yaniv, Ziv R. and J{\"a}ger, Paul F. and {Maier-Hein}, Lena},
  year = {2023},
  month = feb,
  address = {United States},
  issn = {2331-8422},
  abstract = {Validation metrics are key for the reliable tracking of scientific progress and for bridging the current chasm between artificial intelligence (AI) research and its translation into practice. However, increasing evidence shows that particularly in image analysis, metrics are often chosen inadequately in relation to the underlying research problem. This could be attributed to a lack of accessibility of metric-related knowledge: While taking into account the individual strengths, weaknesses, and limitations of validation metrics is a critical prerequisite to making educated choices, the relevant knowledge is currently scattered and poorly accessible to individual researchers. Based on a multi-stage Delphi process conducted by a multidisciplinary expert consortium as well as extensive community feedback, the present work provides the first reliable and comprehensive common point of access to information on pitfalls related to validation metrics in image analysis. Focusing on biomedical image analysis but with the potential of transfer to other fields, the addressed pitfalls generalize across application domains and are categorized according to a newly created, domain-agnostic taxonomy. To facilitate comprehension, illustrations and specific examples accompany each pitfall. As a structured body of information accessible to researchers of all levels of expertise, this work enhances global comprehension of a key topic in image analysis validation.},
  langid = {english},
  pmcid = {PMC10029046},
  pmid = {36945687}
}

@misc{reinkeUnderstandingMetricrelatedPitfalls2023b,
  title = {Understanding Metric-Related Pitfalls in Image Analysis Validation.},
  author = {Reinke, Annika and Tizabi, Minu D. and Baumgartner, Michael and Eisenmann, Matthias and {Heckmann-N{\"o}tzel}, Doreen and Kavur, A. Emre and R{\"a}dsch, Tim and Sudre, Carole H. and Acion, Laura and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Benis, Arriel and Blaschko, Matthew and B{\"u}ttner, Florian and Cardoso, M. Jorge and Cheplygina, Veronika and Chen, Jianxu and Christodoulou, Evangelia and Cimini, Beth A. and Collins, Gary S. and Farahani, Keyvan and Ferrer, Luciana and Galdran, Adrian and {van Ginneken}, Bram and Glocker, Ben and Godau, Patrick and Haase, Robert and Hashimoto, Daniel A. and Hoffman, Michael M. and Huisman, Merel and Isensee, Fabian and Jannin, Pierre and Kahn, Charles E. and Kainmueller, Dagmar and Kainz, Bernhard and Karargyris, Alexandros and Karthikesalingam, Alan and Kenngott, Hannes and Kleesiek, Jens and Kofler, Florian and Kooi, Thijs and {Kopp-Schneider}, Annette and Kozubek, Michal and Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A. and Litjens, Geert and Madani, Amin and {Maier-Hein}, Klaus and Martel, Anne L. and Mattson, Peter and Meijering, Erik and Menze, Bjoern and Moons, Karel G. M. and M{\"u}ller, Henning and Nichyporuk, Brennan and Nickel, Felix and Petersen, Jens and Rafelski, Susanne M. and Rajpoot, Nasir and Reyes, Mauricio and Riegler, Michael A. and Rieke, Nicola and {Saez-Rodriguez}, Julio and S{\'a}nchez, Clara I. and Shetty, Shravya and {van Smeden}, Maarten and Summers, Ronald M. and Taha, Abdel A. and Tiulpin, Aleksei and Tsaftaris, Sotirios A. and Calster, Ben Van and Varoquaux, Ga{\"e}l and Wiesenfarth, Manuel and Yaniv, Ziv R. and J{\"a}ger, Paul F. and {Maier-Hein}, Lena},
  year = {2023},
  month = feb,
  journal = {ArXiv},
  address = {United States},
  issn = {2331-8422},
  abstract = {Validation metrics are key for the reliable tracking of scientific progress and for bridging the current chasm between artificial intelligence (AI) research and  its translation into practice. However, increasing evidence shows that  particularly in image analysis, metrics are often chosen inadequately in relation  to the underlying research problem. This could be attributed to a lack of  accessibility of metric-related knowledge: While taking into account the  individual strengths, weaknesses, and limitations of validation metrics is a  critical prerequisite to making educated choices, the relevant knowledge is  currently scattered and poorly accessible to individual researchers. Based on a  multi-stage Delphi process conducted by a multidisciplinary expert consortium as  well as extensive community feedback, the present work provides the first  reliable and comprehensive common point of access to information on pitfalls  related to validation metrics in image analysis. Focusing on biomedical image  analysis but with the potential of transfer to other fields, the addressed  pitfalls generalize across application domains and are categorized according to a  newly created, domain-agnostic taxonomy. To facilitate comprehension,  illustrations and specific examples accompany each pitfall. As a structured body  of information accessible to researchers of all levels of expertise, this work  enhances global comprehension of a key topic in image analysis validation.},
  langid = {english},
  pmcid = {PMC10029046},
  pmid = {36945687}
}

@article{reinkeUnderstandingMetricrelatedPitfalls2023c,
  title = {Understanding Metric-Related Pitfalls in Image Analysis Validation},
  author = {REINKE, {\relax ANNIKA} and TIZABI, MINU D. and BAUMGARTNER, {\relax MICHAEL} and EISENMANN, {\relax MATTHIAS} and {HECKMANN-N{\"O}TZEL}, {\relax DOREEN} and KAVUR, A. EMRE and R{\"A}DSCH, {\relax TIM} and SUDRE, CAROLE H. and ACION, {\relax LAURA} and ANTONELLI, {\relax MICHELA} and ARBEL, {\relax TAL} and BAKAS, {\relax SPYRIDON} and BENIS, {\relax ARRIEL} and BLASCHKO, {\relax MATTHEW} and B{\"U}TTNER, {\relax FLORIAN} and CARDOSO, M. JORGE and CHEPLYGINA, {\relax VERONIKA} and CHEN, {\relax JIANXU} and CHRISTODOULOU, {\relax EVANGELIA} and CIMINI, BETH A. and COLLINS, GARY S. and FARAHANI, {\relax KEYVAN} and FERRER, {\relax LUCIANA} and GALDRAN, {\relax ADRIAN} and VAN GINNEKEN, {\relax BRAM} and GLOCKER, {\relax BEN} and GODAU, {\relax PATRICK} and HAASE, {\relax ROBERT} and HASHIMOTO, DANIEL A. and HOFFMAN, MICHAEL M. and HUISMAN, {\relax MEREL} and ISENSEE, {\relax FABIAN} and JANNIN, {\relax PIERRE} and KAHN, CHARLES E. and KAINMUELLER, {\relax DAGMAR} and KAINZ, {\relax BERNHARD} and KARARGYRIS, {\relax ALEXANDROS} and KARTHIKESALINGAM, {\relax ALAN} and KENNGOTT, {\relax HANNES} and KLEESIEK, {\relax JENS} and KOFLER, {\relax FLORIAN} and KOOI, {\relax THIJS} and {KOPP-SCHNEIDER}, {\relax ANNETTE} and KOZUBEK, {\relax MICHAL} and KRESHUK, {\relax ANNA} and KURC, {\relax TAHSIN} and LANDMAN, BENNETT A. and LITJENS, {\relax GEERT} and MADANI, {\relax AMIN} and {MAIER-HEIN}, {\relax KLAUS} and MARTEL, ANNE L. and MATTSON, {\relax PETER} and MEIJERING, {\relax ERIK} and MENZE, {\relax BJOERN} and MOONS, KAREL G.M. and M{\"U}LLER, {\relax HENNING} and NICHYPORUK, {\relax BRENNAN} and NICKEL, {\relax FELIX} and PETERSEN, {\relax JENS} and RAFELSKI, SUSANNE M. and RAJPOOT, {\relax NASIR} and REYES, {\relax MAURICIO} and RIEGLER, MICHAEL A. and RIEKE, {\relax NICOLA} and {SAEZ-RODRIGUEZ}, {\relax JULIO} and S{\'A}NCHEZ, CLARA I. and SHETTY, {\relax SHRAVYA} and VAN SMEDEN, {\relax MAARTEN} and SUMMERS, RONALD M. and TAHA, ABDEL A. and TIULPIN, {\relax ALEKSEI} and TSAFTARIS, SOTIRIOS A. and VAN CALSTER, {\relax BEN} and VAROQUAUX, {\relax GA{\"E}L} and WIESENFARTH, {\relax MANUEL} and YANIV, ZIV R. and J{\"A}GER, PAUL F. and {MAIER-HEIN}, {\relax LENA}},
  year = {2023},
  month = feb,
  journal = {ArXiv},
  pages = {arXiv:2302.01790v2},
  issn = {2331-8422},
  urldate = {2023-04-24},
  abstract = {Validation metrics are key for the reliable tracking of scientific progress and for bridging the current chasm between artificial intelligence (AI) research and its translation into practice. However, increasing evidence shows that particularly in image analysis, metrics are often chosen inadequately in relation to the underlying research problem. This could be attributed to a lack of accessibility of metric-related knowledge: While taking into account the individual strengths, weaknesses, and limitations of validation metrics is a critical prerequisite to making educated choices, the relevant knowledge is currently scattered and poorly accessible to individual researchers. Based on a multi-stage Delphi process conducted by a multidisciplinary expert consortium as well as extensive community feedback, the present work provides the first reliable and comprehensive common point of access to information on pitfalls related to validation metrics in image analysis. Focusing on biomedical image analysis but with the potential of transfer to other fields, the addressed pitfalls generalize across application domains and are categorized according to a newly created, domain-agnostic taxonomy. To facilitate comprehension, illustrations and specific examples accompany each pitfall. As a structured body of information accessible to researchers of all levels of expertise, this work enhances global comprehension of a key topic in image analysis validation.},
  pmcid = {PMC10029046},
  pmid = {36945687},
  file = {C:\Users\cleme\Zotero\storage\Z4H4SYR9\REINKE et al. - 2023 - Understanding metric-related pitfalls in image ana.pdf}
}

@article{ritchieTropicalCycloneIntensity2012,
  title = {Tropical {{Cyclone Intensity Estimation}} in the {{North Atlantic Basin Using}} an {{Improved Deviation Angle Variance Technique}}},
  author = {Ritchie, Elizabeth A. and {Valliere-Kelley}, Genevieve and Pi{\~n}eros, Miguel F. and Tyo, J. Scott},
  year = {2012},
  month = jun,
  journal = {Weather and Forecasting},
  volume = {27},
  number = {5},
  pages = {1264--1277},
  issn = {0882-8156},
  doi = {10.1175/WAF-D-11-00156.1},
  urldate = {2019-02-18},
  abstract = {This paper describes results from an improvement to the objective deviation angle variance technique to estimate the intensity of tropical cyclones from satellite infrared imagery in the North Atlantic basin. The technique quantifies the level of organization of the infrared cloud signature of a tropical cyclone as an indirect measurement of its maximum wind speed. The major change described here is to use the National Hurricane Center's best-track database to constrain the technique. Results are shown for the 2004--10 North Atlantic hurricane seasons and include an overall root-mean-square intensity error of 12.9 kt (6.6 m s-1, where 1 kt = 0.514 m s-1) and annual root-mean-square intensity errors ranging from 10.3 to 14.1 kt. A direct comparison between the previous version and the one reported here shows root-mean-square intensity error improvements in all years with a best improvement in 2009 from 17.9 to 10.6 kt and an overall improvement from 14.8 to 12.9 kt. In addition, samples from the 7-yr period are binned based on level of intensity and on the strength of environmental vertical wind shear as extracted from Statistical Hurricane Intensity Prediction Scheme (SHIPS) data. Preliminary results suggest that the deviation angle variance technique performs best at the weakest intensity categories of tropical storm through hurricane category 3, representing 90\% of the samples, and then degrades in performance for hurricane categories 4 and 5. For environmental vertical wind shear, there is far less spread in the results with the technique performing better with increasing vertical wind shear.},
  keywords = {deviation angle variance,intensity estimation,low relevance}
}

@article{ritchieTropicalCycloneIntensity2012a,
  title = {Tropical {{Cyclone Intensity Estimation}} in the {{North Atlantic Basin Using}} an {{Improved Deviation Angle Variance Technique}}},
  author = {Ritchie, Elizabeth A. and {Valliere-Kelley}, Genevieve and Pi{\~n}eros, Miguel F. and Tyo, J. Scott},
  year = {2012},
  month = jun,
  journal = {Weather and Forecasting},
  volume = {27},
  number = {5},
  pages = {1264--1277},
  issn = {0882-8156},
  doi = {10.1175/WAF-D-11-00156.1},
  urldate = {2019-02-18},
  abstract = {This paper describes results from an improvement to the objective deviation angle variance technique to estimate the intensity of tropical cyclones from satellite infrared imagery in the North Atlantic basin. The technique quantifies the level of organization of the infrared cloud signature of a tropical cyclone as an indirect measurement of its maximum wind speed. The major change described here is to use the National Hurricane Center's best-track database to constrain the technique. Results are shown for the 2004--10 North Atlantic hurricane seasons and include an overall root-mean-square intensity error of 12.9 kt (6.6 m s-1, where 1 kt = 0.514 m s-1) and annual root-mean-square intensity errors ranging from 10.3 to 14.1 kt. A direct comparison between the previous version and the one reported here shows root-mean-square intensity error improvements in all years with a best improvement in 2009 from 17.9 to 10.6 kt and an overall improvement from 14.8 to 12.9 kt. In addition, samples from the 7-yr period are binned based on level of intensity and on the strength of environmental vertical wind shear as extracted from Statistical Hurricane Intensity Prediction Scheme (SHIPS) data. Preliminary results suggest that the deviation angle variance technique performs best at the weakest intensity categories of tropical storm through hurricane category 3, representing 90\% of the samples, and then degrades in performance for hurricane categories 4 and 5. For environmental vertical wind shear, there is far less spread in the results with the technique performing better with increasing vertical wind shear.},
  keywords = {deviation angle variance,intensity estimation,low relevance},
  file = {C:\Users\cleme\Zotero\storage\V8S54CLB\Ritchie et al. - 2012 - Tropical Cyclone Intensity Estimation in the North.pdf}
}

@article{ritterRegistrationStereoTemporal1999,
  title = {Registration of Stereo and Temporal Images of the Retina},
  author = {Ritter, N. and Owens, R. and Cooper, J. and Eikelboom, R.H. and Van Saarloos, P.P.},
  year = {1999},
  month = may,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {18},
  number = {5},
  pages = {404--418},
  issn = {02780062},
  doi = {10.1109/42.774168},
  urldate = {2019-08-16},
  abstract = {The registration of retinal images is required to facilitate the study of the optic nerve head and the retina. The method we propose combines the use of mutual information as the similarity measure and simulated annealing as the search technique. It is robust toward large transformations between the images and significant changes in light intensity. By using a pyramid sampling approach combined with simulated reannealing we find that registration can be achieved to predetermined precision, subject to choice of interpolation and the constraint of time. The algorithm was tested on 49 pairs of stereo images and 48 pairs of temporal images with success.},
  langid = {english}
}

@article{ritterRegistrationStereoTemporal1999a,
  title = {Registration of Stereo and Temporal Images of the Retina},
  author = {Ritter, N. and Owens, R. and Cooper, J. and Eikelboom, R.H. and Van Saarloos, P.P.},
  year = {1999},
  month = may,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {18},
  number = {5},
  pages = {404--418},
  issn = {02780062},
  doi = {10.1109/42.774168},
  urldate = {2019-08-16},
  abstract = {The registration of retinal images is required to facilitate the study of the optic nerve head and the retina. The method we propose combines the use of mutual information as the similarity measure and simulated annealing as the search technique. It is robust toward large transformations between the images and significant changes in light intensity. By using a pyramid sampling approach combined with simulated reannealing we find that registration can be achieved to predetermined precision, subject to choice of interpolation and the constraint of time. The algorithm was tested on 49 pairs of stereo images and 48 pairs of temporal images with success.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\67X92IX6\Ritter et al. - 1999 - Registration of stereo and temporal images of the .pdf}
}

@misc{RobustRigidRegistration,
  title = {Robust Rigid Registration of Retinal Angiograms through Optimization - {{ScienceDirect}}},
  urldate = {2019-12-10}
}

@misc{RobustRigidRegistrationa,
  title = {Robust Rigid Registration of Retinal Angiograms through Optimization - {{ScienceDirect}}},
  urldate = {2019-12-10},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S0895611106000760},
  file = {C:\Users\cleme\Zotero\storage\MLMXNPAX\S0895611106000760.html}
}

@article{romPredictingFutureDevelopment2022,
  title = {Predicting the Future Development of Diabetic Retinopathy Using a Deep Learning Algorithm for the Analysis of Non-Invasive Retinal Imaging},
  author = {Rom, Yovel and Aviv, Rachelle and Ianchulev, Tsontcho and {Dvey-Aharon}, Zack},
  year = {2022},
  month = dec,
  journal = {BMJ Open Ophthalmology},
  volume = {7},
  number = {1},
  pages = {e001140},
  publisher = {BMJ Specialist Journals},
  issn = {2397-3269},
  doi = {10.1136/bmjophth-2022-001140},
  urldate = {2023-10-05},
  abstract = {Aims Diabetic retinopathy (DR) is the most common cause of vision loss in the working age. This research aimed to develop an artificial intelligence (AI) machine learning model which can predict the development of referable DR from fundus imagery of otherwise healthy eyes. Methods Our researchers trained a machine learning algorithm on the EyePACS data set, consisting of 156 363 fundus images. Referrable DR was defined as any level above mild on the International Clinical Diabetic Retinopathy scale. Results The algorithm achieved 0.81 area under receiver operating curve (AUC) when averaging scores from multiple images on the task of predicting development of referrable DR, and 0.76 AUC when using a single image. Conclusion Our results suggest that risk of DR may be predicted from fundus photography alone. Prediction of personalised risk of DR may become key in treatment and contribute to patient compliance across the board, particularly when supported by further prospective research.},
  chapter = {Retina},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2022. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
  langid = {english},
  keywords = {Diagnostic tests/Investigation,Macula,Retina}
}

@article{romPredictingFutureDevelopment2022a,
  title = {Predicting the Future Development of Diabetic Retinopathy Using a Deep Learning Algorithm for the Analysis of Non-Invasive Retinal Imaging},
  author = {Rom, Yovel and Aviv, Rachelle and Ianchulev, Tsontcho and {Dvey-Aharon}, Zack},
  year = {2022},
  month = dec,
  journal = {BMJ Open Ophthalmology},
  volume = {7},
  number = {1},
  pages = {e001140},
  publisher = {BMJ Specialist Journals},
  issn = {2397-3269},
  doi = {10.1136/bmjophth-2022-001140},
  urldate = {2023-10-05},
  abstract = {Aims Diabetic retinopathy (DR) is the most common cause of vision loss in the working age. This research aimed to develop an artificial intelligence (AI) machine learning model which can predict the development of referable DR from fundus imagery of otherwise healthy eyes. Methods Our researchers trained a machine learning algorithm on the EyePACS data set, consisting of 156 363 fundus images. Referrable DR was defined as any level above mild on the International Clinical Diabetic Retinopathy scale. Results The algorithm achieved 0.81 area under receiver operating curve (AUC) when averaging scores from multiple images on the task of predicting development of referrable DR, and 0.76 AUC when using a single image. Conclusion Our results suggest that risk of DR may be predicted from fundus photography alone. Prediction of personalised risk of DR may become key in treatment and contribute to patient compliance across the board, particularly when supported by further prospective research.},
  chapter = {Retina},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2022. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See:~http://creativecommons.org/licenses/by-nc/4.0/.},
  langid = {english},
  keywords = {Diagnostic tests/Investigation,Macula,Retina},
  file = {C:\Users\cleme\Zotero\storage\JIC73PQ7\Rom et al. - 2022 - Predicting the future development of diabetic reti.pdf}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015a,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015b,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015c,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\4GDNUYPA\\ronneberger2015.pdf;C\:\\Users\\cleme\\Zotero\\storage\\FQSXTLF5\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;C\:\\Users\\cleme\\Zotero\\storage\\H7IQUWN2\\ronneberger2015.pdf}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015d,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  file = {C:\Users\cleme\Zotero\storage\GRKYFQTM\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015e,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  isbn = {978-3-319-24574-4},
  langid = {english},
  keywords = {Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5TIPBXCL\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;C\:\\Users\\cleme\\Zotero\\storage\\TCHNCCJN\\ronneberger2015.pdf.pdf}
}

@misc{ronyProximalSplittingAdversarial2023,
  title = {Proximal {{Splitting Adversarial Attacks}} for {{Semantic Segmentation}}},
  author = {Rony, J{\'e}r{\^o}me and Pesquet, Jean-Christophe and Ayed, Ismail Ben},
  year = {2023},
  month = mar,
  number = {arXiv:2206.07179},
  eprint = {2206.07179},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.07179},
  urldate = {2024-06-10},
  abstract = {Classification has been the focal point of research on adversarial attacks, but only a few works investigate methods suited to denser prediction tasks, such as semantic segmentation. The methods proposed in these works do not accurately solve the adversarial segmentation problem and, therefore, overestimate the size of the perturbations required to fool models. Here, we propose a white-box attack for these models based on a proximal splitting to produce adversarial perturbations with much smaller \${\textbackslash}ell\_{\textbackslash}infty\$ norms. Our attack can handle large numbers of constraints within a nonconvex minimization framework via an Augmented Lagrangian approach, coupled with adaptive constraint scaling and masking strategies. We demonstrate that our attack significantly outperforms previously proposed ones, as well as classification attacks that we adapted for segmentation, providing a first comprehensive benchmark for this dense task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\4R24KGP7\Rony et al. - 2023 - Proximal Splitting Adversarial Attacks for Semanti.pdf}
}

@misc{ROSERetinalOCTAngiography,
  title = {{{ROSE}}: {{A Retinal OCT-Angiography Vessel Segmentation Dataset}} and {{New Model}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2023-10-05}
}

@misc{ROSERetinalOCTAngiographya,
  title = {{{ROSE}}: {{A Retinal OCT-Angiography Vessel Segmentation Dataset}} and {{New Model}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2023-10-05},
  howpublished = {https://ieeexplore.ieee.org/document/9284503},
  file = {C:\Users\cleme\Zotero\storage\YNLFKGAF\9284503.html}
}

@article{rossIncrementalLearningRobust2008,
  title = {Incremental {{Learning}} for {{Robust Visual Tracking}}},
  author = {Ross, David A. and Lim, Jongwoo and Lin, Ruei-Sung and Yang, Ming-Hsuan},
  year = {2008},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {77},
  number = {1-3},
  pages = {125--141},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-007-0075-7},
  urldate = {2019-06-13},
  abstract = {Visual tracking, in essence, deals with non-stationary image streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail in the presence of significant variation of the object's appearance or surrounding illumination. One reason for such failures is that many algorithms employ fixed appearance models of the target. Such models are trained using only appearance data available before tracking begins, which in practice limits the range of appearances that are modelled, and ignores the large volume of information (such as shape changes or specific lighting conditions) that becomes available during tracking. In this paper, we present a tracking method that incrementally learns a low-dimensional subspace representation, efficiently adapting online to changes in the appearance of the target. The model update, based on incremental algorithms for principal component analysis, includes two important features: a method for correctly updating the sample mean, and a forgetting factor to ensure less modelling power is expended fitting older observations. Both of these features contribute measurably to improving overall tracking performance. Numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large changes in pose, scale, and illumination.},
  langid = {english}
}

@article{rossIncrementalLearningRobust2008a,
  title = {Incremental {{Learning}} for {{Robust Visual Tracking}}},
  author = {Ross, David A. and Lim, Jongwoo and Lin, Ruei-Sung and Yang, Ming-Hsuan},
  year = {2008},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {77},
  number = {1-3},
  pages = {125--141},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-007-0075-7},
  urldate = {2019-06-13},
  abstract = {Visual tracking, in essence, deals with non-stationary image streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail in the presence of significant variation of the object's appearance or surrounding illumination. One reason for such failures is that many algorithms employ fixed appearance models of the target. Such models are trained using only appearance data available before tracking begins, which in practice limits the range of appearances that are modelled, and ignores the large volume of information (such as shape changes or specific lighting conditions) that becomes available during tracking. In this paper, we present a tracking method that incrementally learns a low-dimensional subspace representation, efficiently adapting online to changes in the appearance of the target. The model update, based on incremental algorithms for principal component analysis, includes two important features: a method for correctly updating the sample mean, and a forgetting factor to ensure less modelling power is expended fitting older observations. Both of these features contribute measurably to improving overall tracking performance. Numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large changes in pose, scale, and illumination.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\PRYRSWY9\Ross et al. - 2008 - Incremental Learning for Robust Visual Tracking.pdf}
}

@book{rostamizadehFoundationsMachineLearning2018,
  title = {Foundations of {{Machine Learning}}, {{Second Edition}}},
  author = {Rostamizadeh, Mohri, Talwalkar},
  year = {2018},
  series = {{{MIT Press}}},
  edition = {2},
  urldate = {2023-05-04},
  isbn = {978-0-262-36412-6}
}

@book{rostamizadehFoundationsMachineLearning2018a,
  title = {Foundations of {{Machine Learning}}, {{Second Edition}}},
  author = {Rostamizadeh, Mohri, Talwalkar},
  year = {2018},
  series = {{{MIT Press}}},
  edition = {2},
  urldate = {2023-05-04},
  isbn = {978-0-262-36412-6},
  file = {C:\Users\cleme\Zotero\storage\MSTMHD7B\30.html}
}

@article{rothausSeparationRetinalVascular2009,
  title = {Separation of the Retinal Vascular Graph in Arteries and Veins Based upon Structural Knowledge},
  author = {Rothaus, Kai and Jiang, Xiaoyi and Rhiem, Paul},
  year = {2009},
  month = jun,
  journal = {Image and Vision Computing},
  series = {7th {{IAPR-TC15 Workshop}} on {{Graph-based Representations}} ({{GbR}} 2007)},
  volume = {27},
  number = {7},
  pages = {864--875},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2008.02.013},
  urldate = {2019-12-06},
  abstract = {The vascular structure of the retina consists of two kinds of vessels: arteries and veins. Together these vessels form the vascular graph. In this paper, we present an approach to separate arteries and veins based on a pre-segmentation and a few hand-labelled vessel segments. We use a rule-based method to propagate the vessel labels through the vascular graph. The anatomical characteristics of the vessels on the retina are modelled as a dual constraint graph. We embed this task as double-layered constrained search problem steered by a heuristical AC-3 algorithm to overcome the NP-hard computational complexity. Results are presented on vascular graphs generated from manual as well as on automatical segmentation.},
  langid = {english},
  keywords = {Artery,Constraint propagation,Constraint satisfaction problem,Retinal vascular graph,Vein}
}

@article{rothausSeparationRetinalVascular2009a,
  title = {Separation of the Retinal Vascular Graph in Arteries and Veins Based upon Structural Knowledge},
  author = {Rothaus, Kai and Jiang, Xiaoyi and Rhiem, Paul},
  year = {2009},
  month = jun,
  journal = {Image and Vision Computing},
  series = {7th {{IAPR-TC15 Workshop}} on {{Graph-based Representations}} ({{GbR}} 2007)},
  volume = {27},
  number = {7},
  pages = {864--875},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2008.02.013},
  urldate = {2019-12-06},
  abstract = {The vascular structure of the retina consists of two kinds of vessels: arteries and veins. Together these vessels form the vascular graph. In this paper, we present an approach to separate arteries and veins based on a pre-segmentation and a few hand-labelled vessel segments. We use a rule-based method to propagate the vessel labels through the vascular graph. The anatomical characteristics of the vessels on the retina are modelled as a dual constraint graph. We embed this task as double-layered constrained search problem steered by a heuristical AC-3 algorithm to overcome the NP-hard computational complexity. Results are presented on vascular graphs generated from manual as well as on automatical segmentation.},
  langid = {english},
  keywords = {Artery,Constraint propagation,Constraint satisfaction problem,Retinal vascular graph,Vein},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\U7JYE2B6\\Rothaus et al. - 2009 - Separation of the retinal vascular graph in arteri.pdf;C\:\\Users\\cleme\\Zotero\\storage\\VPZIBD8U\\S0262885608000590.html}
}

@article{royReLayNetRetinalLayer2017,
  title = {{{ReLayNet}}: {{Retinal}} Layer and Fluid Segmentation of Macular Optical Coherence Tomography Using Fully Convolutional Networks},
  shorttitle = {{{ReLayNet}}},
  author = {Roy, Abhijit Guha and Conjeti, Sailesh and Karri, Sri Phani Krishna and Sheet, Debdoot and Katouzian, Amin and Wachinger, Christian and Navab, Nassir},
  year = {2017},
  month = jul,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {8},
  pages = {3627--3642},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.003627},
  urldate = {2019-12-14},
  abstract = {Optical coherence tomography (OCT) is used for non-invasive diagnosis of diabetic macular edema assessing the retinal layers. In this paper, we propose a new fully convolutional deep architecture, termed ReLayNet, for end-to-end segmentation of retinal layers and fluid masses in eye OCT scans. ReLayNet uses a contracting path of convolutional blocks (encoders) to learn a hierarchy of contextual features, followed by an expansive path of convolutional blocks (decoders) for semantic segmentation. ReLayNet is trained to optimize a joint loss function comprising of weighted logistic regression and Dice overlap loss. The framework is validated on a publicly available benchmark dataset with comparisons against five state-of-the-art segmentation methods including two deep learning based approaches to substantiate its effectiveness.},
  pmcid = {PMC5560830},
  pmid = {28856040}
}

@article{royReLayNetRetinalLayer2017a,
  title = {{{ReLayNet}}: Retinal Layer and Fluid Segmentation of Macular Optical Coherence Tomography Using Fully Convolutional Networks},
  shorttitle = {{{ReLayNet}}},
  author = {Roy, Abhijit Guha and Conjeti, Sailesh and Karri, Sri Phani Krishna and Sheet, Debdoot and Katouzian, Amin and Wachinger, Christian and Navab, Nassir},
  year = {2017},
  month = jul,
  journal = {Biomedical Optics Express},
  volume = {8},
  number = {8},
  pages = {3627--3642},
  issn = {2156-7085},
  doi = {10.1364/BOE.8.003627},
  urldate = {2019-12-14},
  abstract = {Optical coherence tomography (OCT) is used for non-invasive diagnosis of diabetic macular edema assessing the retinal layers. In this paper, we propose a new fully convolutional deep architecture, termed ReLayNet, for end-to-end segmentation of retinal layers and fluid masses in eye OCT scans. ReLayNet uses a contracting path of convolutional blocks (encoders) to learn a hierarchy of contextual features, followed by an expansive path of convolutional blocks (decoders) for semantic segmentation. ReLayNet is trained to optimize a joint loss function comprising of weighted logistic regression and Dice overlap loss. The framework is validated on a publicly available benchmark dataset with comparisons against five state-of-the-art segmentation methods including two deep learning based approaches to substantiate its effectiveness.},
  pmcid = {PMC5560830},
  pmid = {28856040},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\6A97V88X\\roy2017.pdf;C\:\\Users\\cleme\\Zotero\\storage\\DKKLB5KJ\\Roy et al. - 2017 - ReLayNet retinal layer and fluid segmentation of .pdf}
}

@article{roySecondEyeInvolvement1990,
  title = {Second Eye Involvement in Age-Related Macular Degeneration: A Four-Year Prospective Study},
  shorttitle = {Second Eye Involvement in Age-Related Macular Degeneration},
  author = {Roy, M. and {Kaiser-Kupfer}, M.},
  year = {1990},
  journal = {Eye (London, England)},
  volume = {4 ( Pt 6)},
  pages = {813--818},
  issn = {0950-222X},
  doi = {10.1038/eye.1990.128},
  abstract = {Age-related macular degeneration (AMD) usually affects both eyes over time. Among patients with advanced AMD in one eye estimates of the risk to the second eye have been diversely reported. Therefore we examined, over a four year period, the rate of second eye involvement in 41 patients with either exudative or advanced atrophic AMD in one eye, and early macular changes in the second eye with best corrected vision of 20/30 or better. The cumulative risk of developing either exudative AMD or atrophic AMD and 20/80 or less vision in the second eye was 23\% at four years. The cumulative risk of losing 10 or more letters on the ETDRS visual acuity chart in the second eye was 35\% at four years. These results are discussed in relation to previously reported rates of second eye involvement in AMD.},
  langid = {english},
  pmid = {2101112},
  keywords = {80 and over,Age Factors,Aged,Eyeglasses,Follow-Up Studies,Humans,Macular Degeneration,Middle Aged,Prospective Studies,Retina,Risk,Visual Acuity}
}

@article{roySecondEyeInvolvement1990a,
  title = {Second Eye Involvement in Age-Related Macular Degeneration: A Four-Year Prospective Study},
  shorttitle = {Second Eye Involvement in Age-Related Macular Degeneration},
  author = {Roy, M. and {Kaiser-Kupfer}, M.},
  year = {1990},
  journal = {Eye (London, England)},
  volume = {4 ( Pt 6)},
  pages = {813--818},
  issn = {0950-222X},
  doi = {10.1038/eye.1990.128},
  abstract = {Age-related macular degeneration (AMD) usually affects both eyes over time. Among patients with advanced AMD in one eye estimates of the risk to the second eye have been diversely reported. Therefore we examined, over a four year period, the rate of second eye involvement in 41 patients with either exudative or advanced atrophic AMD in one eye, and early macular changes in the second eye with best corrected vision of 20/30 or better. The cumulative risk of developing either exudative AMD or atrophic AMD and 20/80 or less vision in the second eye was 23\% at four years. The cumulative risk of losing 10 or more letters on the ETDRS visual acuity chart in the second eye was 35\% at four years. These results are discussed in relation to previously reported rates of second eye involvement in AMD.},
  langid = {english},
  pmid = {2101112},
  keywords = {Age Factors,Aged,Aged 80 and over,Eyeglasses,Follow-Up Studies,Humans,Macular Degeneration,Middle Aged,Prospective Studies,Retina,Risk,Visual Acuity},
  file = {C:\Users\cleme\Zotero\storage\Q6XYGJI2\Roy et Kaiser-Kupfer - 1990 - Second eye involvement in age-related macular dege.pdf}
}

@article{rundoUSENetIncorporatingSqueezeandExcitation2019,
  title = {{{USE-Net}}: {{Incorporating Squeeze-and-Excitation}} Blocks into {{U-Net}} for Prostate Zonal Segmentation of Multi-Institutional {{MRI}} Datasets},
  shorttitle = {{{USE-Net}}},
  author = {Rundo, Leonardo and Han, Changhee and Nagano, Yudai and Zhang, Jin and Hataya, Ryuichiro and Militello, Carmelo and Tangherloni, Andrea and Nobile, Marco S. and Ferretti, Claudio and Besozzi, Daniela and Gilardi, Maria Carla and Vitabile, Salvatore and Mauri, Giancarlo and Nakayama, Hideki and Cazzaniga, Paolo},
  year = {2019},
  month = nov,
  journal = {Neurocomputing},
  volume = {365},
  pages = {31--43},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.07.006},
  urldate = {2023-05-18},
  abstract = {Prostate cancer is the most common malignant tumors in men but prostate Magnetic Resonance Imaging (MRI) analysis remains challenging. Besides whole prostate gland segmentation, the capability to differentiate between the blurry boundary of the Central Gland (CG) and Peripheral Zone (PZ) can lead to differential diagnosis, since the frequency and severity of tumors differ in these regions. To tackle the prostate zonal segmentation task, we propose a novel Convolutional Neural Network (CNN), called USE-Net, which incorporates Squeeze-and-Excitation (SE) blocks into U-Net, i.e., one of the most effective CNNs in biomedical image segmentation. Especially, the SE blocks are added after every Encoder (Enc USE-Net) or Encoder-Decoder block (Enc-Dec USE-Net). This study evaluates the generalization ability of CNN-based architectures on three T2-weighted MRI datasets, each one consisting of a different number of patients and heterogeneous image characteristics, collected by different institutions. The following mixed scheme is used for training/testing: (i) training on either each individual dataset or multiple prostate MRI datasets and (ii) testing on all three datasets with all possible training/testing combinations. USE-Net is compared against three state-of-the-art CNN-based architectures (i.e., U-Net, pix2pix, and Mixed-Scale Dense Network), along with a semi-automatic continuous max-flow model. The results show that training on the union of the datasets generally outperforms training on each dataset separately, allowing for both intra-/cross-dataset generalization. Enc USE-Net shows good overall generalization under any training condition, while Enc-Dec USE-Net remarkably outperforms the other methods when trained on all datasets. These findings reveal that the SE blocks' adaptive feature recalibration provides excellent cross-dataset generalization when testing is performed on samples of the datasets used during training. Therefore, we should consider multi-dataset training and SE blocks together as mutually indispensable methods to draw out each other's full potential. In conclusion, adaptive mechanisms (e.g., feature recalibration) may be a valuable solution in medical imaging applications involving multi-institutional settings.},
  langid = {english},
  keywords = {Anatomical MRI,Convolutional neural networks,Cross-dataset generalization,Prostate cancer,Prostate zonal segmentation,USE-Net}
}

@article{rundoUSENetIncorporatingSqueezeandExcitation2019a,
  title = {{{USE-Net}}: {{Incorporating Squeeze-and-Excitation}} Blocks into {{U-Net}} for Prostate Zonal Segmentation of Multi-Institutional {{MRI}} Datasets},
  shorttitle = {{{USE-Net}}},
  author = {Rundo, Leonardo and Han, Changhee and Nagano, Yudai and Zhang, Jin and Hataya, Ryuichiro and Militello, Carmelo and Tangherloni, Andrea and Nobile, Marco S. and Ferretti, Claudio and Besozzi, Daniela and Gilardi, Maria Carla and Vitabile, Salvatore and Mauri, Giancarlo and Nakayama, Hideki and Cazzaniga, Paolo},
  year = {2019},
  month = nov,
  journal = {Neurocomputing},
  volume = {365},
  pages = {31--43},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.07.006},
  urldate = {2023-05-18},
  abstract = {Prostate cancer is the most common malignant tumors in men but prostate Magnetic Resonance Imaging (MRI) analysis remains challenging. Besides whole prostate gland segmentation, the capability to differentiate between the blurry boundary of the Central Gland (CG) and Peripheral Zone (PZ) can lead to differential diagnosis, since the frequency and severity of tumors differ in these regions. To tackle the prostate zonal segmentation task, we propose a novel Convolutional Neural Network (CNN), called USE-Net, which incorporates Squeeze-and-Excitation (SE) blocks into U-Net, i.e., one of the most effective CNNs in biomedical image segmentation. Especially, the SE blocks are added after every Encoder (Enc USE-Net) or Encoder-Decoder block (Enc-Dec USE-Net). This study evaluates the generalization ability of CNN-based architectures on three T2-weighted MRI datasets, each one consisting of a different number of patients and heterogeneous image characteristics, collected by different institutions. The following mixed scheme is used for training/testing: (i) training on either each individual dataset or multiple prostate MRI datasets and (ii) testing on all three datasets with all possible training/testing combinations. USE-Net is compared against three state-of-the-art CNN-based architectures (i.e., U-Net, pix2pix, and Mixed-Scale Dense Network), along with a semi-automatic continuous max-flow model. The results show that training on the union of the datasets generally outperforms training on each dataset separately, allowing for both intra-/cross-dataset generalization. Enc USE-Net shows good overall generalization under any training condition, while Enc-Dec USE-Net remarkably outperforms the other methods when trained on all datasets. These findings reveal that the SE blocks' adaptive feature recalibration provides excellent cross-dataset generalization when testing is performed on samples of the datasets used during training. Therefore, we should consider multi-dataset training and SE blocks together as mutually indispensable methods to draw out each other's full potential. In conclusion, adaptive mechanisms (e.g., feature recalibration) may be a valuable solution in medical imaging applications involving multi-institutional settings.},
  langid = {english},
  keywords = {Anatomical MRI,Convolutional neural networks,Cross-dataset generalization,Prostate cancer,Prostate zonal segmentation,USE-Net},
  file = {C:\Users\cleme\Zotero\storage\EDMEAQV3\Rundo et al. - 2019 - USE-Net Incorporating Squeeze-and-Excitation bloc.pdf}
}

@misc{rw2019timm,
  title = {{{PyTorch}} Image Models},
  author = {Wightman, Ross},
  year = {2019},
  publisher = {GitHub},
  doi = {10.5281/zenodo.4414861}
}

@misc{rw2019timm,
  title = {{{PyTorch}} Image Models},
  author = {Wightman, Ross},
  year = {2019},
  publisher = {GitHub},
  doi = {10.5281/zenodo.4414861}
}

@article{saalfeldTopologicallyConsistentLine1999,
  title = {Topologically {{Consistent Line Simplification}} with the {{Douglas-Peucker Algorithm}}},
  author = {Saalfeld, Alan},
  year = {1999},
  month = jan,
  journal = {Cartography and Geographic Information Science},
  volume = {26},
  number = {1},
  pages = {7--18},
  issn = {1523-0406},
  doi = {10.1559/152304099782424901},
  urldate = {2019-12-11},
  abstract = {We examine key properties of the Douglas-Peucker polyline simplification algorithm which are shared with many similar "vertex sub-sampling" algorithms. We examine how the Douglas-Peucker algorithm and similar algorithms can fail to maintain consistent or correct topological relations among features. We then prove that a simple test added to the stopping condition of Douglas-Peucker-like algorithms can guarantee that the resulting simplified polyline is topologically consistent with itself and with all of its neighboring features, and is correctly situated topologically with respect to all other features. We describe how a dynamically updated convex hull data structure may be used to efficiently detect and remove potential topological conflicts of the polyline with itself and with other features in that polyline's neighborhood.},
  keywords = {APPROXIMATION,CONVEX HULL,GENERALIZATION,SUB-SAMPLING ALGORITHMS}
}

@article{saalfeldTopologicallyConsistentLine1999a,
  title = {Topologically {{Consistent Line Simplification}} with the {{Douglas-Peucker Algorithm}}},
  author = {Saalfeld, Alan},
  year = {1999},
  month = jan,
  journal = {Cartography and Geographic Information Science},
  volume = {26},
  number = {1},
  pages = {7--18},
  issn = {1523-0406},
  doi = {10.1559/152304099782424901},
  urldate = {2019-12-11},
  abstract = {We examine key properties of the Douglas-Peucker polyline simplification algorithm which are shared with many similar "vertex sub-sampling" algorithms. We examine how the Douglas-Peucker algorithm and similar algorithms can fail to maintain consistent or correct topological relations among features. We then prove that a simple test added to the stopping condition of Douglas-Peucker-like algorithms can guarantee that the resulting simplified polyline is topologically consistent with itself and with all of its neighboring features, and is correctly situated topologically with respect to all other features. We describe how a dynamically updated convex hull data structure may be used to efficiently detect and remove potential topological conflicts of the polyline with itself and with other features in that polyline's neighborhood.},
  keywords = {APPROXIMATION,CONVEX HULL,GENERALIZATION,SUB-SAMPLING ALGORITHMS},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\NKL2C5UX\\Saalfeld - 1999 - Topologically Consistent Line Simplification with .pdf;C\:\\Users\\cleme\\Zotero\\storage\\S8ZAV85J\\saalfeld1999.pdf;C\:\\Users\\cleme\\Zotero\\storage\\MA5RXQ3Q\\152304099782424901.html}
}

@article{sahlstenDeepLearningFundus2019,
  title = {Deep {{Learning Fundus Image Analysis}} for {{Diabetic Retinopathy}} and {{Macular Edema Grading}}},
  author = {Sahlsten, Jaakko and Jaskari, Joel and Kivinen, Jyri and Turunen, Lauri and Jaanio, Esa and Hietala, Kustaa and Kaski, Kimmo},
  year = {2019},
  month = jul,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {1--11},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-47181-w},
  urldate = {2019-12-09},
  abstract = {Diabetes is a globally prevalent disease that can cause visible microvascular complications such as diabetic retinopathy and macular edema in the human eye retina, the images of which are today used for manual disease screening and diagnosis. This labor-intensive task could greatly benefit from automatic detection using deep learning technique. Here we present a deep learning system that identifies referable diabetic retinopathy comparably or better than presented in the previous studies, although we use only a small fraction of images ({$<$}1/4) in training but are aided with higher image resolutions. We also provide novel results for five different screening and clinical grading systems for diabetic retinopathy and macular edema classification, including state-of-the-art results for accurately classifying images according to clinical five-grade diabetic retinopathy and for the first time for the four-grade diabetic macular edema scales. These results suggest, that a deep learning system could increase the cost-effectiveness of screening and diagnosis, while attaining higher than recommended performance, and that the system could be applied in clinical examinations requiring finer grading.},
  copyright = {2019 The Author(s)},
  langid = {english}
}

@article{sahlstenDeepLearningFundus2019a,
  title = {Deep {{Learning Fundus Image Analysis}} for {{Diabetic Retinopathy}} and {{Macular Edema Grading}}},
  author = {Sahlsten, Jaakko and Jaskari, Joel and Kivinen, Jyri and Turunen, Lauri and Jaanio, Esa and Hietala, Kustaa and Kaski, Kimmo},
  year = {2019},
  month = jul,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {10750},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-47181-w},
  urldate = {2021-10-04},
  abstract = {Diabetes is a globally prevalent disease that can cause visible microvascular complications such as diabetic retinopathy and macular edema in the human eye retina, the images of which are today used for manual disease screening and diagnosis. This labor-intensive task could greatly benefit from automatic detection using deep learning technique. Here we present a deep learning system that identifies referable diabetic retinopathy comparably or better than presented in the previous studies, although we use only a small fraction of images ({$<$}1/4) in training but are aided with higher image resolutions. We also provide novel results for five different screening and clinical grading systems for diabetic retinopathy and macular edema classification, including state-of-the-art results for accurately classifying images according to clinical five-grade diabetic retinopathy and for the first time for the four-grade diabetic macular edema scales. These results suggest, that a deep learning system could increase the cost-effectiveness of screening and diagnosis, while attaining higher than recommended performance, and that the system could be applied in clinical examinations requiring finer grading.},
  copyright = {2019 The Author(s)},
  langid = {english}
}

@article{sahlstenDeepLearningFundus2019b,
  title = {Deep {{Learning Fundus Image Analysis}} for {{Diabetic Retinopathy}} and {{Macular Edema Grading}}},
  author = {Sahlsten, Jaakko and Jaskari, Joel and Kivinen, Jyri and Turunen, Lauri and Jaanio, Esa and Hietala, Kustaa and Kaski, Kimmo},
  year = {2019},
  month = jul,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {1--11},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-47181-w},
  urldate = {2019-12-09},
  abstract = {Diabetes is a globally prevalent disease that can cause visible microvascular complications such as diabetic retinopathy and macular edema in the human eye retina, the images of which are today used for manual disease screening and diagnosis. This labor-intensive task could greatly benefit from automatic detection using deep learning technique. Here we present a deep learning system that identifies referable diabetic retinopathy comparably or better than presented in the previous studies, although we use only a small fraction of images ({$<$}1/4) in training but are aided with higher image resolutions. We also provide novel results for five different screening and clinical grading systems for diabetic retinopathy and macular edema classification, including state-of-the-art results for accurately classifying images according to clinical five-grade diabetic retinopathy and for the first time for the four-grade diabetic macular edema scales. These results suggest, that a deep learning system could increase the cost-effectiveness of screening and diagnosis, while attaining higher than recommended performance, and that the system could be applied in clinical examinations requiring finer grading.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\P4RSKCV4\\Sahlsten et al. - 2019 - Deep Learning Fundus Image Analysis for Diabetic R.pdf;C\:\\Users\\cleme\\Zotero\\storage\\TPC5GUM5\\sahlsten2019.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6A5Z9JF6\\s41598-019-47181-w.html}
}

@article{sahlstenDeepLearningFundus2019c,
  title = {Deep {{Learning Fundus Image Analysis}} for {{Diabetic Retinopathy}} and {{Macular Edema Grading}}},
  author = {Sahlsten, Jaakko and Jaskari, Joel and Kivinen, Jyri and Turunen, Lauri and Jaanio, Esa and Hietala, Kustaa and Kaski, Kimmo},
  year = {2019},
  month = jul,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {10750},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-47181-w},
  urldate = {2021-10-04},
  abstract = {Diabetes is a globally prevalent disease that can cause visible microvascular complications such as diabetic retinopathy and macular edema in the human eye retina, the images of which are today used for manual disease screening and diagnosis. This labor-intensive task could greatly benefit from automatic detection using deep learning technique. Here we present a deep learning system that identifies referable diabetic retinopathy comparably or better than presented in the previous studies, although we use only a small fraction of images ({$<$}1/4) in training but are aided with higher image resolutions. We also provide novel results for five different screening and clinical grading systems for diabetic retinopathy and macular edema classification, including state-of-the-art results for accurately classifying images according to clinical five-grade diabetic retinopathy and for the first time for the four-grade diabetic macular edema scales. These results suggest, that a deep learning system could increase the cost-effectiveness of screening and diagnosis, while attaining higher than recommended performance, and that the system could be applied in clinical examinations requiring finer grading.},
  copyright = {2019 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cc\_license\_type: cc\_by\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Machine learning;Medical imaging;Retinal diseases\\
Subject\_term\_id: machine-learning;medical-imaging;retinal-diseases},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\TM8Q7JLK\\Sahlsten et al. - 2019 - Deep Learning Fundus Image Analysis for Diabetic R.pdf;C\:\\Users\\cleme\\Zotero\\storage\\5CYX97WY\\s41598-019-47181-w.html}
}

@article{salahuddinTransparencyDeepNeural2022,
  title = {Transparency of Deep Neural Networks for Medical Image Analysis: {{A}} Review of Interpretability Methods},
  shorttitle = {Transparency of Deep Neural Networks for Medical Image Analysis},
  author = {Salahuddin, Zohaib and Woodruff, Henry C. and Chatterjee, Avishek and Lambin, Philippe},
  year = {2022},
  month = jan,
  journal = {Computers in Biology and Medicine},
  volume = {140},
  pages = {105111},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.105111},
  urldate = {2023-05-04},
  abstract = {Artificial Intelligence (AI) has emerged as a useful aid in numerous clinical applications for diagnosis and treatment decisions. Deep neural networks have shown the same or better performance than clinicians in many tasks owing to the rapid increase in the available data and computational power. In order to conform to the principles of trustworthy AI, it is essential that the AI system be transparent, robust, fair, and ensure accountability. Current deep neural solutions are referred to as black-boxes due to a lack of understanding of the specifics concerning the decision-making process. Therefore, there is a need to ensure the interpretability of deep neural networks before they can be incorporated into the routine clinical workflow. In this narrative review, we utilized systematic keyword searches and domain expertise to identify nine different types of interpretability methods that have been used for understanding deep learning models for medical image analysis applications based on the type of generated explanations and technical similarities. Furthermore, we report the progress made towards evaluating the explanations produced by various interpretability methods. Finally, we discuss limitations, provide guidelines for using interpretability methods and future directions concerning the interpretability of deep neural networks for medical imaging analysis.},
  langid = {english},
  keywords = {Deep neural networks,Explainability,Explainable artificial intelligence,Interpretability,Medical imaging}
}

@article{salahuddinTransparencyDeepNeural2022a,
  title = {Transparency of Deep Neural Networks for Medical Image Analysis: {{A}} Review of Interpretability Methods},
  shorttitle = {Transparency of Deep Neural Networks for Medical Image Analysis},
  author = {Salahuddin, Zohaib and Woodruff, Henry C. and Chatterjee, Avishek and Lambin, Philippe},
  year = {2022},
  month = jan,
  journal = {Computers in Biology and Medicine},
  volume = {140},
  pages = {105111},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.105111},
  urldate = {2023-05-04},
  abstract = {Artificial Intelligence (AI) has emerged as a useful aid in numerous clinical applications for diagnosis and treatment decisions. Deep neural networks have shown the same or better performance than clinicians in many tasks owing to the rapid increase in the available data and computational power. In order to conform to the principles of trustworthy AI, it is essential that the AI system be transparent, robust, fair, and ensure accountability. Current deep neural solutions are referred to as black-boxes due to a lack of understanding of the specifics concerning the decision-making process. Therefore, there is a need to ensure the interpretability of deep neural networks before they can be incorporated into the routine clinical workflow. In this narrative review, we utilized systematic keyword searches and domain expertise to identify nine different types of interpretability methods that have been used for understanding deep learning models for medical image analysis applications based on the type of generated explanations and technical similarities. Furthermore, we report the progress made towards evaluating the explanations produced by various interpretability methods. Finally, we discuss limitations, provide guidelines for using interpretability methods and future directions concerning the interpretability of deep neural networks for medical imaging analysis.},
  langid = {english},
  keywords = {Deep neural networks,Explainability,Explainable artificial intelligence,Interpretability,Medical imaging},
  file = {C:\Users\cleme\Zotero\storage\IALVFEU4\Salahuddin et al. - 2022 - Transparency of deep neural networks for medical i.pdf}
}

@inproceedings{sandlerMobileNetV2InvertedResiduals2018,
  title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
  shorttitle = {{{MobileNetV2}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  year = {2018},
  pages = {4510--4520},
  urldate = {2019-06-13}
}

@inproceedings{sandlerMobileNetV2InvertedResiduals2018a,
  title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
  shorttitle = {{{MobileNetV2}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  year = {2018},
  pages = {4510--4520},
  urldate = {2019-06-13},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\J6KR2FNW\\Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf;C\:\\Users\\cleme\\Zotero\\storage\\9VERKZX9\\Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html}
}

@article{sankarSCHEIEVisualField2017,
  title = {The {{SCHEIE Visual Field Grading System}}},
  author = {Sankar, Prithvi S. and O'Keefe, Laura and Choi, Daniel and Salowe, Rebecca and {Miller-Ellis}, Eydie and Lehman, Amanda and Addis, Victoria and Ramakrishnan, Meera and Natesh, Vikas and Whitehead, Gideon and Khachatryan, Naira and O'Brien, Joan},
  year = {2017},
  month = jun,
  journal = {Journal of clinical \& experimental ophthalmology},
  volume = {8},
  number = {3},
  issn = {2155-9570},
  doi = {10.4172/2155-9570.1000651},
  urldate = {2019-12-27},
  abstract = {Objective No method of grading visual field (VF) defects has been widely accepted throughout the glaucoma community. The SCHEIE (Systematic Classification of Humphrey visual fields-Easy Interpretation and Evaluation) grading system for glaucomatous visual fields was created to convey qualitative and quantitative information regarding visual field defects in an objective, reproducible, and easily applicable manner for research purposes. Methods The SCHEIE grading system is composed of a qualitative and quantitative score. The qualitative score consists of designation in one or more of the following categories: normal, central scotoma, paracentral scotoma, paracentral crescent, temporal quadrant, nasal quadrant, peripheral arcuate defect, expansive arcuate, or altitudinal defect. The quantitative component incorporates the Humphrey visual field index (VFI), location of visual defects for superior and inferior hemifields, and blind spot involvement. Accuracy and speed at grading using the qualitative and quantitative components was calculated for non-physician graders. Results Graders had a median accuracy of 96.67\% for their qualitative scores and a median accuracy of 98.75\% for their quantitative scores. Graders took a mean of 56 seconds per visual field to assign a qualitative score and 20 seconds per visual field to assign a quantitative score. Conclusion The SCHEIE grading system is a reproducible tool that combines qualitative and quantitative measurements to grade glaucomatous visual field defects. The system aims to standardize clinical staging and to make specific visual field defects more easily identifiable. Specific patterns of visual field loss may also be associated with genetic variants in future genetic analysis.},
  pmcid = {PMC5602567},
  pmid = {28932621}
}

@article{sankarSCHEIEVisualField2017a,
  title = {The {{SCHEIE Visual Field Grading System}}},
  author = {Sankar, Prithvi S. and O'Keefe, Laura and Choi, Daniel and Salowe, Rebecca and {Miller-Ellis}, Eydie and Lehman, Amanda and Addis, Victoria and Ramakrishnan, Meera and Natesh, Vikas and Whitehead, Gideon and Khachatryan, Naira and O'Brien, Joan},
  year = {2017},
  month = jun,
  journal = {Journal of clinical \& experimental ophthalmology},
  volume = {8},
  number = {3},
  issn = {2155-9570},
  doi = {10.4172/2155-9570.1000651},
  urldate = {2019-12-27},
  abstract = {Objective No method of grading visual field (VF) defects has been widely accepted throughout the glaucoma community. The SCHEIE (Systematic Classification of Humphrey visual fields-Easy Interpretation and Evaluation) grading system for glaucomatous visual fields was created to convey qualitative and quantitative information regarding visual field defects in an objective, reproducible, and easily applicable manner for research purposes. Methods The SCHEIE grading system is composed of a qualitative and quantitative score. The qualitative score consists of designation in one or more of the following categories: normal, central scotoma, paracentral scotoma, paracentral crescent, temporal quadrant, nasal quadrant, peripheral arcuate defect, expansive arcuate, or altitudinal defect. The quantitative component incorporates the Humphrey visual field index (VFI), location of visual defects for superior and inferior hemifields, and blind spot involvement. Accuracy and speed at grading using the qualitative and quantitative components was calculated for non-physician graders. Results Graders had a median accuracy of 96.67\% for their qualitative scores and a median accuracy of 98.75\% for their quantitative scores. Graders took a mean of 56 seconds per visual field to assign a qualitative score and 20 seconds per visual field to assign a quantitative score. Conclusion The SCHEIE grading system is a reproducible tool that combines qualitative and quantitative measurements to grade glaucomatous visual field defects. The system aims to standardize clinical staging and to make specific visual field defects more easily identifiable. Specific patterns of visual field loss may also be associated with genetic variants in future genetic analysis.},
  pmcid = {PMC5602567},
  pmid = {28932621},
  file = {C:\Users\cleme\Zotero\storage\46NQPZXZ\Sankar et al. - 2017 - The SCHEIE Visual Field Grading System.pdf}
}

@article{sarrafOcularImagingCanadian2019,
  title = {Ocular Imaging in the {{Canadian Journal}} of {{Ophthalmology}}},
  author = {Sarraf, David},
  year = {2019},
  month = aug,
  journal = {Canadian Journal of Ophthalmology},
  volume = {54},
  number = {4},
  pages = {410},
  issn = {0008-4182, 1715-3360},
  doi = {10.1016/j.jcjo.2019.06.002},
  urldate = {2019-12-24},
  abstract = {The development of novel therapeutics for the treatment of blinding retinal diseases has been one of the most remarkable achievements in the field of medicine. Our unique era of therapeutic innovation however, has been matched by the astonishing advancements in the field of ocular imaging. The advent of optical coherence tomography (OCT), specifically, has had a major impact in the field of ophthalmology from the front to the back of the eye. Segmentation of the different layers of the cornea is a major innovative capability of OCT technology and OCT of the optic nerve can provide significant information regarding the health of the nerve fiber layer and ganglion cell layer in glaucoma patients.},
  langid = {english},
  pmid = {31358134}
}

@article{sarrafOcularImagingCanadian2019a,
  title = {Ocular Imaging in the {{Canadian Journal}} of {{Ophthalmology}}},
  author = {Sarraf, David},
  year = {2019},
  month = aug,
  journal = {Canadian Journal of Ophthalmology},
  volume = {54},
  number = {4},
  pages = {410},
  issn = {0008-4182, 1715-3360},
  doi = {10.1016/j.jcjo.2019.06.002},
  urldate = {2019-12-24},
  abstract = {The development of novel therapeutics for the treatment of blinding retinal diseases has been one of the most remarkable achievements in the field of medicine. Our unique era of therapeutic innovation however, has been matched by the astonishing advancements in the field of ocular imaging. The advent of optical coherence tomography (OCT), specifically, has had a major impact in the field of ophthalmology from the front to the back of the eye. Segmentation of the different layers of the cornea is a major innovative capability of OCT technology and OCT of the optic nerve can provide significant information regarding the health of the nerve fiber layer and ganglion cell layer in glaucoma patients.},
  langid = {english},
  pmid = {31358134},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\FKDGLXZU\\sarraf2019.pdf;C\:\\Users\\cleme\\Zotero\\storage\\DEJC3FZ3\\fulltext.html}
}

@article{sasongkoRetinalVascularTortuosity2011,
  title = {Retinal Vascular Tortuosity in Persons with Diabetes and Diabetic Retinopathy},
  author = {Sasongko, M. B. and Wong, T. Y. and Nguyen, T. T. and Cheung, C. Y. and Shaw, J. E. and Wang, J. J.},
  year = {2011},
  month = sep,
  journal = {Diabetologia},
  volume = {54},
  number = {9},
  pages = {2409--2416},
  issn = {1432-0428},
  doi = {10.1007/s00125-011-2200-y},
  abstract = {AIM/HYPOTHESIS: The aim of this hypothesis was to examine the association of retinal vessel tortuosity with diabetes and diabetic retinopathy (DR). METHODS: A clinic-based study of 327 participants (224 with diabetes and 103 non-diabetic controls) aged {$\geq$} 18 years. DR was graded from fundus photographs according to the modified Airlie House Classification system and categorised into mild non-proliferative DR (NPDR), moderate NPDR and vision-threatening DR (VTDR). Retinal vessel tortuosity was measured from disc-centred retinal photographs. Measurements were taken, using a semi-automated computer program by a single grader, of arterioles and venules within 0.5 to 2 disc diameters away from the optic disc. RESULTS: There were 114 (44\%) participants with DR. In the multivariate analysis, retinal arteriolar and venular tortuosity were increased in participants with diabetes without DR (mean difference 12.4 {\texttimes} 10(-5) and 13.3 {\texttimes} 10(-5), respectively; both p {$<$} 0.05) and in those with DR (mean difference 15.4 {\texttimes} 10(-5) and 15.0 {\texttimes} 10(-5), respectively; both p {$<$} 0.01) compared with non-diabetic participants. Among participants with diabetes, increased arteriolar tortuosity was significantly associated with mild NPDR (OR 1.53, 95\% CI 1.03-2.05, per SD increase in arteriolar tortuosity) and moderate NPDR (OR 1.67, 95\% CI 1.10-2.55) but not VTDR (OR 0.91, 95\% CI 0.54-1.54). No association with DR was found for venular tortuosity. CONCLUSIONS/INTERPRETATION: Persons with diabetes had more tortuous retinal vasculature than persons without diabetes. In persons with diabetes, increased arteriolar tortuosity was associated with mild and moderate stages of DR. This suggests that retinal vascular tortuosity might be an early indicator of microvascular damage in diabetes; thus, further investigation is indicated.},
  langid = {english},
  pmid = {21625945},
  keywords = {Adolescent,Adult,Aged,Arterioles,Case-Control Studies,Computer-Assisted,Diabetes Mellitus,Diabetic Retinopathy,Female,Humans,Image Processing,Male,Middle Aged,Retinal Vessels,Risk Factors,Venules,Young Adult}
}

@article{sasongkoRetinalVascularTortuosity2011a,
  title = {Retinal Vascular Tortuosity in Persons with Diabetes and Diabetic Retinopathy},
  author = {Sasongko, M. B. and Wong, T. Y. and Nguyen, T. T. and Cheung, C. Y. and Shaw, J. E. and Wang, J. J.},
  year = {2011},
  month = sep,
  journal = {Diabetologia},
  volume = {54},
  number = {9},
  pages = {2409--2416},
  issn = {1432-0428},
  doi = {10.1007/s00125-011-2200-y},
  abstract = {AIM/HYPOTHESIS: The aim of this hypothesis was to examine the association of retinal vessel tortuosity with diabetes and diabetic retinopathy (DR). METHODS: A clinic-based study of 327 participants (224 with diabetes and 103 non-diabetic controls) aged {$\geq$} 18 years. DR was graded from fundus photographs according to the modified Airlie House Classification system and categorised into mild non-proliferative DR (NPDR), moderate NPDR and vision-threatening DR (VTDR). Retinal vessel tortuosity was measured from disc-centred retinal photographs. Measurements were taken, using a semi-automated computer program by a single grader, of arterioles and venules within 0.5 to 2 disc diameters away from the optic disc. RESULTS: There were 114 (44\%) participants with DR. In the multivariate analysis, retinal arteriolar and venular tortuosity were increased in participants with diabetes without DR (mean difference 12.4 {\texttimes} 10(-5) and 13.3 {\texttimes} 10(-5), respectively; both p {$<$} 0.05) and in those with DR (mean difference 15.4 {\texttimes} 10(-5) and 15.0 {\texttimes} 10(-5), respectively; both p {$<$} 0.01) compared with non-diabetic participants. Among participants with diabetes, increased arteriolar tortuosity was significantly associated with mild NPDR (OR 1.53, 95\% CI 1.03-2.05, per SD increase in arteriolar tortuosity) and moderate NPDR (OR 1.67, 95\% CI 1.10-2.55) but not VTDR (OR 0.91, 95\% CI 0.54-1.54). No association with DR was found for venular tortuosity. CONCLUSIONS/INTERPRETATION: Persons with diabetes had more tortuous retinal vasculature than persons without diabetes. In persons with diabetes, increased arteriolar tortuosity was associated with mild and moderate stages of DR. This suggests that retinal vascular tortuosity might be an early indicator of microvascular damage in diabetes; thus, further investigation is indicated.},
  langid = {english},
  pmid = {21625945},
  keywords = {Adolescent,Adult,Aged,Arterioles,Case-Control Studies,Diabetes Mellitus,Diabetic Retinopathy,Female,Humans,Image Processing Computer-Assisted,Male,Middle Aged,Retinal Vessels,Risk Factors,Venules,Young Adult},
  file = {C:\Users\cleme\Zotero\storage\4PG44F3B\Sasongko et al. - 2011 - Retinal vascular tortuosity in persons with diabet.pdf}
}

@article{sbAutomaticDetectionDiabetic2012,
  title = {Automatic {{Detection}} of {{Diabetic Retinopathy}} in {{Non-dilated RGB Retinal Fundus Images}}},
  author = {Sb, SujithKumar and Singh, Vipula},
  year = {2012},
  month = jun,
  journal = {International Journal of Computer Applications},
  volume = {47},
  number = {19},
  pages = {26--32},
  issn = {09758887},
  doi = {10.5120/7297-0511},
  urldate = {2019-11-19},
  abstract = {In this paper, a method for automatic detection of microaneurysms in digital eye fundus image is described. To develop an automated diabetic retinopathy screening system, a detection of dark lesions in digital fundus photographs is needed. Microaneurysms are the first clinical sign of diabetic retinopathy and they appear small red dots on retinal fundus images. The number of microaneurysms is used to indicate the severity of the disease. Early microaneurysm detection can help reduce the incidence of blindness. Here, we have discussed a method for the automatic detection of Diabetic Retinopathy (ADDR) in color fundus images. Different preprocessing, feature extraction and classification algorithms are used. The performance of the automated system is assessed based on Sensitivity and Specificity. The Sensitivity and Specificity of this approach are 94.44 \% and 87.5 \%, respectively.},
  langid = {english}
}

@article{sbAutomaticDetectionDiabetic2012a,
  title = {Automatic {{Detection}} of {{Diabetic Retinopathy}} in {{Non-dilated RGB Retinal Fundus Images}}},
  author = {Sb, SujithKumar and Singh, Vipula},
  year = {2012},
  month = jun,
  journal = {International Journal of Computer Applications},
  volume = {47},
  number = {19},
  pages = {26--32},
  issn = {09758887},
  doi = {10.5120/7297-0511},
  urldate = {2019-11-19},
  abstract = {In this paper, a method for automatic detection of microaneurysms in digital eye fundus image is described. To develop an automated diabetic retinopathy screening system, a detection of dark lesions in digital fundus photographs is needed. Microaneurysms are the first clinical sign of diabetic retinopathy and they appear small red dots on retinal fundus images. The number of microaneurysms is used to indicate the severity of the disease. Early microaneurysm detection can help reduce the incidence of blindness. Here, we have discussed a method for the automatic detection of Diabetic Retinopathy (ADDR) in color fundus images. Different preprocessing, feature extraction and classification algorithms are used. The performance of the automated system is assessed based on Sensitivity and Specificity. The Sensitivity and Specificity of this approach are 94.44 \% and 87.5 \%, respectively.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\DCGES2Z4\Sb et Singh - 2012 - Automatic Detection of Diabetic Retinopathy in Non.pdf}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, F. and Gori, M. and {Ah Chung Tsoi} and Hagenbuchner, M. and Monfardini, G.},
  year = {2009},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1045-9227, 1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  urldate = {2019-11-28},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  langid = {english}
}

@article{scarselliGraphNeuralNetwork2009a,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, F. and Gori, M. and {Ah Chung Tsoi} and Hagenbuchner, M. and Monfardini, G.},
  year = {2009},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1045-9227, 1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  urldate = {2019-11-28},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\CX2G49NY\Scarselli et al. - 2009 - The Graph Neural Network Model.pdf}
}

@article{schleglFAnoGANFastUnsupervised2019,
  title = {F-{{AnoGAN}}: {{Fast}} Unsupervised Anomaly Detection with Generative Adversarial Networks},
  shorttitle = {F-{{AnoGAN}}},
  author = {Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M. and Langs, Georg and {Schmidt-Erfurth}, Ursula},
  year = {2019},
  month = may,
  journal = {Medical Image Analysis},
  volume = {54},
  pages = {30--44},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.01.010},
  urldate = {2019-07-23},
  abstract = {Obtaining expert labels in clinical imaging is difficult since exhaustive annotation is time-consuming. Furthermore, not all possibly relevant markers may be known and sufficiently well described a priori to even guide annotation. While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of findings, we can detect and exploit, is limited to the annotated lesions. Here, we present fast AnoGAN (f-AnoGAN), a generative adversarial network (GAN) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates. We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the GAN's latent space. The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model -- comprising a discriminator feature residual error and an image reconstruction error. In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f-AnoGAN outperforms alternative approaches and yields high anomaly detection accuracy. In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal OCT images. The f-AnoGAN code is available at https://github.com/tSchlegl/f-AnoGAN.},
  keywords = {Anomaly detection,Optical coherence tomography,Unsupervised learning,Wasserstein generative adversarial network}
}

@article{schleglFAnoGANFastUnsupervised2019a,
  title = {F-{{AnoGAN}}: {{Fast}} Unsupervised Anomaly Detection with Generative Adversarial Networks},
  shorttitle = {F-{{AnoGAN}}},
  author = {Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M. and Langs, Georg and {Schmidt-Erfurth}, Ursula},
  year = {2019},
  month = may,
  journal = {Medical Image Analysis},
  volume = {54},
  pages = {30--44},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.01.010},
  urldate = {2022-07-09},
  abstract = {Obtaining expert labels in clinical imaging is difficult since exhaustive annotation is time-consuming. Furthermore, not all possibly relevant markers may be known and sufficiently well described a priori to even guide annotation. While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of findings, we can detect and exploit, is limited to the annotated lesions. Here, we present fast AnoGAN (f-AnoGAN), a generative adversarial network (GAN) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates. We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the GAN's latent space. The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model -- comprising a discriminator feature residual error and an image reconstruction error. In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f-AnoGAN outperforms alternative approaches and yields high anomaly detection accuracy. In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal OCT images. The f-AnoGAN code is available at https://github.com/tSchlegl/f-AnoGAN.},
  langid = {english},
  keywords = {Anomaly detection,Optical coherence tomography,Unsupervised learning,Wasserstein generative adversarial network}
}

@article{schleglFAnoGANFastUnsupervised2019b,
  title = {F-{{AnoGAN}}: {{Fast}} Unsupervised Anomaly Detection with Generative Adversarial Networks},
  shorttitle = {F-{{AnoGAN}}},
  author = {Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M. and Langs, Georg and {Schmidt-Erfurth}, Ursula},
  year = {2019},
  month = may,
  journal = {Medical Image Analysis},
  volume = {54},
  pages = {30--44},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.01.010},
  urldate = {2019-07-23},
  abstract = {Obtaining expert labels in clinical imaging is difficult since exhaustive annotation is time-consuming. Furthermore, not all possibly relevant markers may be known and sufficiently well described a priori to even guide annotation. While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of findings, we can detect and exploit, is limited to the annotated lesions. Here, we present fast AnoGAN (f-AnoGAN), a generative adversarial network (GAN) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates. We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the GAN's latent space. The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model -- comprising a discriminator feature residual error and an image reconstruction error. In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f-AnoGAN outperforms alternative approaches and yields high anomaly detection accuracy. In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal OCT images. The f-AnoGAN code is available at https://github.com/tSchlegl/f-AnoGAN.},
  keywords = {Anomaly detection,Optical coherence tomography,Unsupervised learning,Wasserstein generative adversarial network},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\IMHPZ7JX\\Schlegl et al. - 2019 - f-AnoGAN Fast unsupervised anomaly detection with.pdf;C\:\\Users\\cleme\\Zotero\\storage\\CBM44FIN\\S1361841518302640.html}
}

@article{schleglFAnoGANFastUnsupervised2019c,
  title = {F-{{AnoGAN}}: {{Fast}} Unsupervised Anomaly Detection with Generative Adversarial Networks},
  shorttitle = {F-{{AnoGAN}}},
  author = {Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M. and Langs, Georg and {Schmidt-Erfurth}, Ursula},
  year = {2019},
  month = may,
  journal = {Medical Image Analysis},
  volume = {54},
  pages = {30--44},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.01.010},
  urldate = {2022-07-09},
  abstract = {Obtaining expert labels in clinical imaging is difficult since exhaustive annotation is time-consuming. Furthermore, not all possibly relevant markers may be known and sufficiently well described a priori to even guide annotation. While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of findings, we can detect and exploit, is limited to the annotated lesions. Here, we present fast AnoGAN (f-AnoGAN), a generative adversarial network (GAN) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates. We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the GAN's latent space. The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model -- comprising a discriminator feature residual error and an image reconstruction error. In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f-AnoGAN outperforms alternative approaches and yields high anomaly detection accuracy. In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal OCT images. The f-AnoGAN code is available at https://github.com/tSchlegl/f-AnoGAN.},
  langid = {english},
  keywords = {Anomaly detection,Optical coherence tomography,Unsupervised learning,Wasserstein generative adversarial network},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\F7ZPKT37\\schlegl2019.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\HRJWSMLF\\S1361841518302640.html}
}

@article{schleglFullyAutomatedDetection2018,
  title = {Fully {{Automated Detection}} and {{Quantification}} of {{Macular Fluid}} in {{OCT Using Deep Learning}}},
  author = {Schlegl, Thomas and Waldstein, Sebastian M. and Bogunovic, Hrvoje and Endstra{\ss}er, Franz and Sadeghipour, Amir and Philip, Ana-Maria and Podkowinski, Dominika and Gerendas, Bianca S. and Langs, Georg and {Schmidt-Erfurth}, Ursula},
  year = {2018},
  month = apr,
  journal = {Ophthalmology},
  volume = {125},
  number = {4},
  pages = {549--558},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2017.10.031},
  urldate = {2022-07-08},
  abstract = {Purpose Development and validation of a fully automated method to detect and quantify macular fluid in conventional OCT images. Design Development of a diagnostic modality. Participants The clinical dataset for fluid detection consisted of 1200 OCT volumes of patients with neovascular age-related macular degeneration (AMD, n = 400), diabetic macular edema (DME, n = 400), or retinal vein occlusion (RVO, n = 400) acquired with Zeiss Cirrus (Carl Zeiss Meditec, Dublin, CA) (n = 600) or Heidelberg Spectralis (Heidelberg Engineering, Heidelberg, Germany) (n = 600) OCT devices. Methods A method based on deep learning to automatically detect and quantify intraretinal cystoid fluid (IRC) and subretinal fluid (SRF) was developed. The performance of the algorithm in accurately identifying fluid localization and extent was evaluated against a manual consensus reading of 2 masked reading center graders. Main Outcome Measures Performance of a fully automated method to accurately detect, differentiate, and quantify intraretinal and SRF using area under the receiver operating characteristics curves, precision, and recall. Results The newly designed, fully automated diagnostic method based on deep learning achieved optimal accuracy for the detection and quantification of IRC for all 3 macular pathologies with a mean accuracy (AUC) of 0.94 (range, 0.91--0.97), a mean precision of 0.91, and a mean recall of 0.84. The detection and measurement of SRF were also highly accurate with an AUC of 0.92 (range, 0.86--0.98), a mean precision of 0.61, and a mean recall of 0.81, with superior performance in neovascular AMD and RVO compared with DME, which was represented rarely in the population studied. High linear correlation was confirmed between automated and manual fluid localization and quantification, yielding an average Pearson's correlation coefficient of 0.90 for IRC and of 0.96 for SRF. Conclusions Deep learning in retinal image analysis achieves excellent accuracy for the differential detection of retinal fluid types across the most prevalent exudative macular diseases and OCT devices. Furthermore, quantification of fluid achieves a high level of concordance with manual expert assessment. Fully automated analysis of retinal OCT images from clinical routine provides a promising horizon in improving accuracy and reliability of retinal diagnosis for research and clinical practice in ophthalmology.},
  langid = {english}
}

@article{schleglFullyAutomatedDetection2018a,
  title = {Fully {{Automated Detection}} and {{Quantification}} of {{Macular Fluid}} in {{OCT Using Deep Learning}}},
  author = {Schlegl, Thomas and Waldstein, Sebastian M. and Bogunovic, Hrvoje and Endstra{\ss}er, Franz and Sadeghipour, Amir and Philip, Ana-Maria and Podkowinski, Dominika and Gerendas, Bianca S. and Langs, Georg and {Schmidt-Erfurth}, Ursula},
  year = {2018},
  month = apr,
  journal = {Ophthalmology},
  volume = {125},
  number = {4},
  pages = {549--558},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2017.10.031},
  urldate = {2022-07-08},
  abstract = {Purpose Development and validation of a fully automated method to detect and quantify macular fluid in conventional OCT images. Design Development of a diagnostic modality. Participants The clinical dataset for fluid detection consisted of 1200 OCT volumes of patients with neovascular age-related macular degeneration (AMD, n~= 400), diabetic macular edema (DME, n~= 400), or retinal vein occlusion (RVO, n~= 400) acquired with Zeiss Cirrus (Carl Zeiss Meditec, Dublin, CA) (n~= 600) or Heidelberg Spectralis (Heidelberg Engineering, Heidelberg, Germany) (n~= 600) OCT devices. Methods A method based on deep learning to automatically detect and quantify intraretinal cystoid fluid (IRC) and subretinal fluid (SRF) was developed. The performance of the algorithm in accurately identifying fluid localization and extent was evaluated against a manual consensus reading of 2 masked reading center graders. Main Outcome Measures Performance of a fully automated method to accurately detect, differentiate, and quantify intraretinal and SRF using area under the receiver operating characteristics curves, precision, and recall. Results The newly designed, fully automated diagnostic method based on deep learning achieved optimal accuracy for the detection and quantification of IRC for all 3 macular pathologies with a mean accuracy (AUC) of 0.94 (range, 0.91--0.97), a mean precision of 0.91, and a mean recall of 0.84. The detection and measurement of SRF were also highly accurate with an AUC of 0.92 (range, 0.86--0.98), a mean precision of 0.61, and a mean recall of 0.81, with superior performance in neovascular AMD and RVO compared with DME, which was represented rarely in the population studied. High linear correlation was confirmed between automated and manual fluid localization and quantification, yielding an average Pearson's correlation coefficient of 0.90 for IRC and of 0.96 for SRF. Conclusions Deep learning in retinal image analysis achieves excellent accuracy for the differential detection of retinal fluid types across the most prevalent exudative macular diseases and OCT devices. Furthermore, quantification of fluid achieves a high level of concordance with manual expert assessment. Fully automated analysis of retinal OCT images from clinical routine provides a promising horizon in improving accuracy and reliability of retinal diagnosis for research and clinical practice in ophthalmology.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\8RIIKYGR\\Schlegl et al. - 2018 - Fully Automated Detection and Quantification of Ma.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RVJQ99ED\\S0161642017314240.html}
}

@inproceedings{schlichtkrull2021interpreting,
  title = {Interpreting Graph Neural Networks for \{\vphantom\}{{NLP}}\vphantom\{\} with Differentiable Edge Masking},
  booktitle = {International Conference on Learning Representations},
  author = {Schlichtkrull, Michael Sejr and Cao, Nicola De and Titov, Ivan},
  year = {2021}
}

@inproceedings{schlichtkrull2021interpreting,
  title = {Interpreting Graph Neural Networks for \{\vphantom\}{{NLP}}\vphantom\{\} with Differentiable Edge Masking},
  booktitle = {International Conference on Learning Representations},
  author = {Schlichtkrull, Michael Sejr and Cao, Nicola De and Titov, Ivan},
  year = {2021}
}

@article{schmidt-erfurthPredictionIndividualDisease2018,
  title = {Prediction of {{Individual Disease Conversion}} in {{Early AMD Using Artificial Intelligence}}},
  author = {{Schmidt-Erfurth}, Ursula and Waldstein, Sebastian M. and Klimscha, Sophie and Sadeghipour, Amir and Hu, Xiaofeng and Gerendas, Bianca S. and Osborne, Aaron and Bogunovi{\'c}, Hrvoje},
  year = {2018},
  month = jul,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {59},
  number = {8},
  pages = {3199--3208},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-24106},
  urldate = {2023-06-13},
  abstract = {While millions of individuals show early age-related macular degeneration (AMD) signs, yet have excellent vision, the risk of progression to advanced AMD with legal blindness is highly variable. We suggest means of artificial intelligence to individually predict AMD progression. In eyes with intermediate AMD, progression to the neovascular type with choroidal neovascularization (CNV) or the dry type with geographic atrophy (GA) was diagnosed based on standardized monthly optical coherence tomography (OCT) images by independent graders. We obtained automated volumetric segmentation of outer neurosensory layers and retinal pigment epithelium, drusen, and hyperreflective foci by spectral domain-OCT image analysis. Using imaging, demographic, and genetic input features, we developed and validated a machine learning--based predictive model assessing the risk of conversion to advanced AMD. Of a total of 495 eyes, 159 eyes (32\%) had converted to advanced AMD within 2 years, 114 eyes progressed to CNV, and 45 to GA. Our predictive model differentiated converting versus nonconverting eyes with a performance of 0.68 and 0.80 for CNV and GA, respectively. The most critical quantitative features for progression were outer retinal thickness, hyperreflective foci, and drusen area. The features for conversion showed pathognomonic patterns that were distinctly different for the neovascular and the atrophic pathways. Predictive hallmarks for CNV were mostly drusen-centric, while GA markers were associated with neurosensory retina and age. Artificial intelligence with automated analysis of imaging biomarkers allows personalized prediction of AMD progression. Moreover, pathways of progression may be specific in respect to the neovascular/atrophic type.}
}

@article{schmidt-erfurthPredictionIndividualDisease2018a,
  title = {Prediction of {{Individual Disease Conversion}} in {{Early AMD Using Artificial Intelligence}}},
  author = {{Schmidt-Erfurth}, Ursula and Waldstein, Sebastian M. and Klimscha, Sophie and Sadeghipour, Amir and Hu, Xiaofeng and Gerendas, Bianca S. and Osborne, Aaron and Bogunovi{\'c}, Hrvoje},
  year = {2018},
  month = jul,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {59},
  number = {8},
  pages = {3199--3208},
  issn = {1552-5783},
  doi = {10.1167/iovs.18-24106},
  urldate = {2023-06-13},
  abstract = {While millions of individuals show early age-related macular degeneration (AMD) signs, yet have excellent vision, the risk of progression to advanced AMD with legal blindness is highly variable. We suggest means of artificial intelligence to individually predict AMD progression.    In eyes with intermediate AMD, progression to the neovascular type with choroidal neovascularization (CNV) or the dry type with geographic atrophy (GA) was diagnosed based on standardized monthly optical coherence tomography (OCT) images by independent graders. We obtained automated volumetric segmentation of outer neurosensory layers and retinal pigment epithelium, drusen, and hyperreflective foci by spectral domain-OCT image analysis. Using imaging, demographic, and genetic input features, we developed and validated a machine learning--based predictive model assessing the risk of conversion to advanced AMD.    Of a total of 495 eyes, 159 eyes (32\%) had converted to advanced AMD within 2 years, 114 eyes progressed to CNV, and 45 to GA. Our predictive model differentiated converting versus nonconverting eyes with a performance of 0.68 and 0.80 for CNV and GA, respectively. The most critical quantitative features for progression were outer retinal thickness, hyperreflective foci, and drusen area. The features for conversion showed pathognomonic patterns that were distinctly different for the neovascular and the atrophic pathways. Predictive hallmarks for CNV were mostly drusen-centric, while GA markers were associated with neurosensory retina and age.    Artificial intelligence with automated analysis of imaging biomarkers allows personalized prediction of AMD progression. Moreover, pathways of progression may be specific in respect to the neovascular/atrophic type.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\DDLEVJTC\\Schmidt-Erfurth et al. - 2018 - Prediction of Individual Disease Conversion in Ear.pdf;C\:\\Users\\cleme\\Zotero\\storage\\CTHS5R8I\\article.html}
}

@article{schmittOpticalCoherenceTomography1999,
  title = {Optical Coherence Tomography ({{OCT}}): A Review},
  shorttitle = {Optical Coherence Tomography ({{OCT}})},
  author = {Schmitt, J.M.},
  year = {1999},
  month = jul,
  journal = {IEEE Journal of Selected Topics in Quantum Electronics},
  volume = {5},
  number = {4},
  pages = {1205--1215},
  issn = {1558-4542},
  doi = {10.1109/2944.796348},
  abstract = {This paper reviews the state of the art of optical coherence tomography (OCT), an interferometric imaging technique that provides cross-sectional views of the subsurface microstructure of biological tissue. Following a discussion of the basic theory of OCT, an overview of the issues involved in the design of the main components of OCT systems is presented. The review concludes by introducing new imaging modes being developed to extract additional diagnostic information.},
  keywords = {Adaptive optics,Biological tissues,Biomedical imaging,Biomedical optical imaging,Medical diagnostic imaging,Optical imaging,Optical interferometry,Optical microscopy,Optical scattering,Tomography}
}

@article{schmittOpticalCoherenceTomography1999a,
  title = {Optical Coherence Tomography ({{OCT}}): A Review},
  shorttitle = {Optical Coherence Tomography ({{OCT}})},
  author = {Schmitt, J.M.},
  year = {1999},
  month = jul,
  journal = {IEEE Journal of Selected Topics in Quantum Electronics},
  volume = {5},
  number = {4},
  pages = {1205--1215},
  issn = {1558-4542},
  doi = {10.1109/2944.796348},
  abstract = {This paper reviews the state of the art of optical coherence tomography (OCT), an interferometric imaging technique that provides cross-sectional views of the subsurface microstructure of biological tissue. Following a discussion of the basic theory of OCT, an overview of the issues involved in the design of the main components of OCT systems is presented. The review concludes by introducing new imaging modes being developed to extract additional diagnostic information.},
  keywords = {Adaptive optics,Biological tissues,Biomedical imaging,Biomedical optical imaging,Medical diagnostic imaging,Optical imaging,Optical interferometry,Optical microscopy,Optical scattering,Tomography}
}

@article{schmittOpticalCoherenceTomography1999b,
  title = {Optical Coherence Tomography ({{OCT}}): A Review},
  shorttitle = {Optical Coherence Tomography ({{OCT}})},
  author = {Schmitt, J.M.},
  year = {1999},
  month = jul,
  journal = {IEEE Journal of Selected Topics in Quantum Electronics},
  volume = {5},
  number = {4},
  pages = {1205--1215},
  issn = {1558-4542},
  doi = {10.1109/2944.796348},
  abstract = {This paper reviews the state of the art of optical coherence tomography (OCT), an interferometric imaging technique that provides cross-sectional views of the subsurface microstructure of biological tissue. Following a discussion of the basic theory of OCT, an overview of the issues involved in the design of the main components of OCT systems is presented. The review concludes by introducing new imaging modes being developed to extract additional diagnostic information.},
  keywords = {Adaptive optics,Biological tissues,Biomedical imaging,Biomedical optical imaging,Medical diagnostic imaging,Optical imaging,Optical interferometry,Optical microscopy,Optical scattering,Tomography},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\YTH5CZCB\\Schmitt - 1999 - Optical coherence tomography (OCT) a review.pdf;C\:\\Users\\cleme\\Zotero\\storage\\NVDXKVWB\\796348.html}
}

@article{schmittOpticalCoherenceTomography1999c,
  title = {Optical Coherence Tomography ({{OCT}}): A Review},
  shorttitle = {Optical Coherence Tomography ({{OCT}})},
  author = {Schmitt, J.M.},
  year = {1999},
  month = jul,
  journal = {IEEE Journal of Selected Topics in Quantum Electronics},
  volume = {5},
  number = {4},
  pages = {1205--1215},
  issn = {1558-4542},
  doi = {10.1109/2944.796348},
  abstract = {This paper reviews the state of the art of optical coherence tomography (OCT), an interferometric imaging technique that provides cross-sectional views of the subsurface microstructure of biological tissue. Following a discussion of the basic theory of OCT, an overview of the issues involved in the design of the main components of OCT systems is presented. The review concludes by introducing new imaging modes being developed to extract additional diagnostic information.},
  keywords = {Adaptive optics,Biological tissues,Biomedical imaging,Biomedical optical imaging,Medical diagnostic imaging,Optical imaging,Optical interferometry,Optical microscopy,Optical scattering,Tomography},
  file = {C:\Users\cleme\Zotero\storage\DP6FT4HP\796348.html}
}

@article{schollInterIntraobserverVariability2003,
  title = {Inter- and Intra-Observer Variability in Grading Lesions of Age-Related Maculopathy and Macular Degeneration},
  author = {Scholl, Hendrik P. N. and Peto, Tunde and Dandekar, Samantha and Bunce, Catey and Xing, Wen and Jenkins, Sharon and Bird, Alan C.},
  year = {2003},
  month = jan,
  journal = {Graefe's Archive for Clinical and Experimental Ophthalmology},
  volume = {241},
  number = {1},
  pages = {39--47},
  issn = {1435-702X},
  doi = {10.1007/s00417-002-0602-8},
  urldate = {2020-02-18},
  abstract = {To introduce a revised version of the grading system established by the International ARM Epidemiological Study Group for identifying and quantifying abnormalities of age-related maculopathy (ARM) and age-related degeneration (AMD) and to investigate its reliability, specifically the inter- and intra-observer variability.},
  langid = {english}
}

@article{schollInterIntraobserverVariability2003a,
  title = {Inter- and Intra-Observer Variability in Grading Lesions of Age-Related Maculopathy and Macular Degeneration},
  author = {Scholl, Hendrik P. N. and Peto, Tunde and Dandekar, Samantha and Bunce, Catey and Xing, Wen and Jenkins, Sharon and Bird, Alan C.},
  year = {2003},
  month = jan,
  journal = {Graefe's Archive for Clinical and Experimental Ophthalmology},
  volume = {241},
  number = {1},
  pages = {39--47},
  issn = {1435-702X},
  doi = {10.1007/s00417-002-0602-8},
  urldate = {2020-02-18},
  abstract = {To introduce a revised version of the grading system established by the International ARM Epidemiological Study Group for identifying and quantifying abnormalities of age-related maculopathy (ARM) and age-related degeneration (AMD) and to investigate its reliability, specifically the inter- and intra-observer variability.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\BBFJ7KCJ\\scholl2003.pdf;C\:\\Users\\cleme\\Zotero\\storage\\HSQDXVA3\\Scholl et al. - 2003 - Inter- and intra-observer variability in grading l.pdf}
}

@article{schwartzObjectiveEvaluationProliferative2020,
  title = {Objective {{Evaluation}} of {{Proliferative Diabetic Retinopathy Using OCT}}},
  author = {Schwartz, Roy and Khalid, Hagar and Sivaprasad, Sobha and Nicholson, Luke and Anikina, Evgenia and Sullivan, Paul and Patel, Praveen J. and Balaskas, Konstantinos and Keane, Pearse A.},
  year = {2020},
  month = feb,
  journal = {Ophthalmology Retina},
  volume = {4},
  number = {2},
  pages = {164--174},
  issn = {2468-6530},
  doi = {10.1016/j.oret.2019.09.004},
  urldate = {2022-06-29},
  abstract = {Purpose To present the routine use of OCT and OCT angiography (OCTA) for the objective diagnosis and monitoring of proliferative diabetic retinopathy (PDR). Design Retrospective, observational case series. Participants Patients with diabetic retinopathy imaged using a standardized PDR protocol. Methods Patients routinely imaged with a standardized PDR protocol between March 2017 and January 2019 were included. This included a 12{\texttimes}9-mm structural OCT volume centered on the macula and a 6{\texttimes}6-mm OCTA scan centered on the optic nerve head obtained using a Topcon swept-source system (DRI OCT-1 Triton, Topcon, Tokyo, Japan). Ultra-widefield fluorescein angiography (FA) was also performed when clinically indicated. The ground truth for each case was determined by merging the findings from biomicroscopy and imaging modalities to generate the maximum level of detection for each finding. Main Outcome Measures Detection rates of new-onset, regression, and reactivation of neovascularization of the disc (NVD) and neovascularization elsewhere (NVE) using different modalities (biomicroscopy/color photography, structural OCT, B-scan OCTA, en face OCTA). Detection of progression of tractional retinal detachment (TRD). Results A total of 383 eyes of 204 patients were evaluated. After excluding patients without PDR or with insufficient image quality, 47 eyes of 35 patients were included. For the detection of new-onset NVD and NVE, structural OCT had the highest detection rate (100\%) of all modalities. However, for the detection of regression or reactivation of neovascularization (NV), B-scan OCTA had the highest detection rate (100\%). Structural OCT detected regression only in 45.5\% of cases, resulting in a low detection rate of reactivation (12.5\%). Among 10 eyes with TRD, OCT detected fovea-threatening TRD during follow-up in 7 eyes, resulting in vitrectomy. Conclusions This study demonstrates the utility of novel multimodal imaging in the daily management of patients with PDR. Posterior pole structural OCT had the best detection rate for NV, and B-scan OCTA showed the most potential for objective monitoring of disease after treatment.},
  langid = {english}
}

@article{schwartzObjectiveEvaluationProliferative2020a,
  title = {Objective {{Evaluation}} of {{Proliferative Diabetic Retinopathy Using OCT}}},
  author = {Schwartz, Roy and Khalid, Hagar and Sivaprasad, Sobha and Nicholson, Luke and Anikina, Evgenia and Sullivan, Paul and Patel, Praveen J. and Balaskas, Konstantinos and Keane, Pearse A.},
  year = {2020},
  month = feb,
  journal = {Ophthalmology Retina},
  volume = {4},
  number = {2},
  pages = {164--174},
  issn = {2468-6530},
  doi = {10.1016/j.oret.2019.09.004},
  urldate = {2022-06-29},
  abstract = {Purpose To present the routine use of OCT and OCT angiography (OCTA) for the objective diagnosis and monitoring of proliferative diabetic retinopathy (PDR). Design Retrospective, observational case series. Participants Patients with diabetic retinopathy imaged using a standardized PDR protocol. Methods Patients routinely imaged with a standardized PDR protocol between March 2017 and January 2019 were included. This included a 12{\texttimes}9-mm structural OCT volume centered on the macula and a 6{\texttimes}6-mm OCTA scan centered on the optic nerve head obtained using a Topcon swept-source system (DRI OCT-1 Triton, Topcon, Tokyo, Japan). Ultra-widefield fluorescein angiography (FA) was also performed when clinically indicated. The ground truth for each case was determined by merging the findings from biomicroscopy and imaging modalities to generate the maximum level of detection for each finding. Main Outcome Measures Detection rates of new-onset, regression, and reactivation of neovascularization of the disc (NVD) and neovascularization elsewhere (NVE) using different modalities (biomicroscopy/color photography, structural OCT, B-scan OCTA, en face OCTA). Detection of progression of tractional retinal detachment (TRD). Results A total of 383 eyes of 204 patients were evaluated. After excluding patients without PDR or with insufficient image quality, 47 eyes of 35 patients were included. For the detection of new-onset NVD and NVE, structural OCT had the highest detection rate (100\%) of all modalities. However, for the detection of regression or reactivation of neovascularization (NV), B-scan OCTA had the highest detection rate (100\%). Structural OCT detected regression only in 45.5\% of cases, resulting in a low detection rate of reactivation (12.5\%). Among 10 eyes with TRD, OCT detected fovea-threatening TRD during follow-up in 7 eyes, resulting in vitrectomy. Conclusions This study demonstrates the utility of novel multimodal imaging in the daily management of patients with PDR. Posterior pole structural OCT had the best detection rate for NV, and B-scan OCTA showed the most potential for objective monitoring of disease after treatment.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\VXUQGCCS\Schwartz et al. - 2020 - Objective Evaluation of Proliferative Diabetic Ret.pdf}
}

@inproceedings{schwarzenbergLayerwiseRelevanceVisualization2019,
  title = {Layerwise {{Relevance Visualization}} in {{Convolutional Text Graph Classifiers}}},
  booktitle = {Proceedings of the {{Thirteenth Workshop}} on {{Graph-Based Methods}} for {{Natural Language Processing}} ({{TextGraphs-13}})},
  author = {Schwarzenberg, Robert and H{\"u}bner, Marc and Harbecke, David and Alt, Christoph and Hennig, Leonhard},
  year = {2019},
  month = nov,
  pages = {58--62},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong},
  doi = {10.18653/v1/D19-5308},
  urldate = {2023-10-03},
  abstract = {Representations in the hidden layers of Deep Neural Networks (DNN) are often hard to interpret since it is difficult to project them into an interpretable domain. Graph Convolutional Networks (GCN) allow this projection, but existing explainability methods do not exploit this fact, i.e. do not focus their explanations on intermediate states. In this work, we present a novel method that traces and visualizes features that contribute to a classification decision in the visible and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in the input graph structure. We experimentally demonstrate that it yields meaningful layerwise explanations for a GCN sentence classifier.}
}

@inproceedings{schwarzenbergLayerwiseRelevanceVisualization2019a,
  title = {Layerwise {{Relevance Visualization}} in {{Convolutional Text Graph Classifiers}}},
  booktitle = {Proceedings of the {{Thirteenth Workshop}} on {{Graph-Based Methods}} for {{Natural Language Processing}} ({{TextGraphs-13}})},
  author = {Schwarzenberg, Robert and H{\"u}bner, Marc and Harbecke, David and Alt, Christoph and Hennig, Leonhard},
  year = {2019},
  month = nov,
  pages = {58--62},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong},
  doi = {10.18653/v1/D19-5308},
  urldate = {2023-10-03},
  abstract = {Representations in the hidden layers of Deep Neural Networks (DNN) are often hard to interpret since it is difficult to project them into an interpretable domain. Graph Convolutional Networks (GCN) allow this projection, but existing explainability methods do not exploit this fact, i.e. do not focus their explanations on intermediate states. In this work, we present a novel method that traces and visualizes features that contribute to a classification decision in the visible and hidden layers of a GCN. Our method exposes hidden cross-layer dynamics in the input graph structure. We experimentally demonstrate that it yields meaningful layerwise explanations for a GCN sentence classifier.},
  file = {C:\Users\cleme\Zotero\storage\S6SUHCMH\Schwarzenberg et al. - 2019 - Layerwise Relevance Visualization in Convolutional.pdf}
}

@inproceedings{sechidisStratificationMultilabelData2011,
  title = {On the {{Stratification}} of {{Multi-label Data}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {145--158},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23808-6_10},
  abstract = {Stratified sampling is a sampling method that takes into account the existence of disjoint groups within a population and produces samples where the proportion of these groups is maintained. In single-label classification tasks, groups are differentiated based on the value of the target variable. In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratified sampling could/should be performed. This paper investigates stratification in the multi-label data context. It considers two stratification methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria. The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.},
  isbn = {978-3-642-23808-6},
  langid = {english},
  keywords = {Average Precision,Average Rank,Binary Relevance,Label Distribution,Mean Average Precision}
}

@inproceedings{sechidisStratificationMultilabelData2011a,
  title = {On the {{Stratification}} of {{Multi-label Data}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {145--158},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23808-6_10},
  abstract = {Stratified sampling is a sampling method that takes into account the existence of disjoint groups within a population and produces samples where the proportion of these groups is maintained. In single-label classification tasks, groups are differentiated based on the value of the target variable. In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratified sampling could/should be performed. This paper investigates stratification in the multi-label data context. It considers two stratification methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria. The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.},
  isbn = {978-3-642-23808-6},
  langid = {english},
  keywords = {Average Precision,Average Rank,Binary Relevance,Label Distribution,Mean Average Precision},
  file = {C:\Users\cleme\Zotero\storage\9WEBPIIW\Sechidis et al. - 2011 - On the Stratification of Multi-label Data.pdf}
}

@inproceedings{seferbekovFeaturePyramidNetwork2018,
  title = {Feature {{Pyramid Network}} for {{Multi-class Land Segmentation}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Seferbekov, Selim and Iglovikov, Vladimir and Buslaev, Alexander and Shvets, Alexey},
  year = {2018},
  month = jun,
  pages = {272--2723},
  publisher = {IEEE},
  address = {Salt Lake City, UT, USA},
  doi = {10.1109/CVPRW.2018.00051},
  urldate = {2023-03-20},
  abstract = {Semantic segmentation is in-demand in satellite imagery processing. Because of the complex environment, automatic categorization and segmentation of land cover is a challenging problem. Solving it can help to overcome many obstacles in urban planning, environmental engineering or natural landscape monitoring. In this paper, we propose an approach for automatic multi-class land segmentation based on a fully convolutional neural network of feature pyramid network (FPN) family. This network is consisted of pre-trained on ImageNet Resnet50 encoder and neatly developed decoder. Based on validation results, leaderboard score and our own experience this network shows reliable results for the DEEPGLOBE - CVPR 2018 land cover classification sub-challenge. Moreover, this network moderately uses memory that allows using GTX 1080 or 1080 TI video cards to perform whole training and makes pretty fast predictions.},
  isbn = {978-1-5386-6100-0},
  langid = {english}
}

@inproceedings{seferbekovFeaturePyramidNetwork2018a,
  title = {Feature {{Pyramid Network}} for {{Multi-class Land Segmentation}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Seferbekov, Selim and Iglovikov, Vladimir and Buslaev, Alexander and Shvets, Alexey},
  year = {2018},
  month = jun,
  pages = {272--2723},
  publisher = {IEEE},
  address = {Salt Lake City, UT, USA},
  doi = {10.1109/CVPRW.2018.00051},
  urldate = {2023-03-20},
  abstract = {Semantic segmentation is in-demand in satellite imagery processing. Because of the complex environment, automatic categorization and segmentation of land cover is a challenging problem. Solving it can help to overcome many obstacles in urban planning, environmental engineering or natural landscape monitoring. In this paper, we propose an approach for automatic multi-class land segmentation based on a fully convolutional neural network of feature pyramid network (FPN) family. This network is consisted of pre-trained on ImageNet Resnet50 encoder and neatly developed decoder. Based on validation results, leaderboard score and our own experience this network shows reliable results for the DEEPGLOBE - CVPR 2018 land cover classification sub-challenge. Moreover, this network moderately uses memory that allows using GTX 1080 or 1080 TI video cards to perform whole training and makes pretty fast predictions.},
  isbn = {978-1-5386-6100-0},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\D4K46UN8\Seferbekov et al. - 2018 - Feature Pyramid Network for Multi-class Land Segme.pdf}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  month = oct,
  pages = {618--626},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.74},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  keywords = {Cats,CNN,CNN model-families,coarse localization map,Computer architecture,convolution,Convolutional Neural Network,data visualisation,deep networks,Dogs,fine-grained visualizations,Grad- CAM,Grad-CAM explanations,gradient methods,gradient-based localization,Gradient-weighted Class Activation Mapping,Guided Grad-CAM,high-resolution class-discriminative visualization,image captioning,image classification,image classification models,image representation,inference mechanisms,Knowledge discovery,learning (artificial intelligence),neural nets,nonattention based models,object detection,object recognition,reinforcement learning,visual explanations,visual question answering models,Visualization}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017a,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  month = oct,
  pages = {618--626},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.74},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  keywords = {Cats,Computer architecture,Dogs,Knowledge discovery,Visualization}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017b,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  month = oct,
  pages = {618--626},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.74},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  keywords = {Cats,Computer architecture,Dogs,Knowledge discovery,Visualization}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017c,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  month = oct,
  pages = {618--626},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.74},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  keywords = {Cats,CNN,CNN model-families,coarse localization map,Computer architecture,convolution,Convolutional Neural Network,data visualisation,deep networks,Dogs,fine-grained visualizations,Grad- CAM,Grad-CAM explanations,gradient methods,gradient-based localization,Gradient-weighted Class Activation Mapping,Guided Grad-CAM,high-resolution class-discriminative visualization,image captioning,image classification,image classification models,image representation,inference mechanisms,Knowledge discovery,learning (artificial intelligence),neural nets,nonattention based models,object detection,object recognition,reinforcement learning,visual explanations,visual question answering models,Visualization},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\Z949QGC4\\Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf;C\:\\Users\\cleme\\Zotero\\storage\\33J74R57\\selvaraju2017.html;C\:\\Users\\cleme\\Zotero\\storage\\57HF7CVH\\8237336.html}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017d,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  month = oct,
  pages = {618--626},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.74},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  keywords = {Cats,Computer architecture,Dogs,Knowledge discovery,Visualization},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\M4PSWTZU\\Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf;C\:\\Users\\cleme\\Zotero\\storage\\U585HY46\\8237336.html}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017e,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  month = oct,
  pages = {618--626},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.74},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  keywords = {Cats,Computer architecture,Dogs,Knowledge discovery,Visualization},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\GKAAQQBP\\Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf;C\:\\Users\\cleme\\Zotero\\storage\\JW45TWSG\\8237336.html}
}

@article{senguptaOphthalmicDiagnosisUsing2020,
  title = {Ophthalmic Diagnosis Using Deep Learning with Fundus Images -- {{A}} Critical Review},
  author = {Sengupta, Sourya and Singh, Amitojdeep and Leopold, Henry A. and Gulati, Tanmay and Lakshminarayanan, Vasudevan},
  year = {2020},
  month = jan,
  journal = {Artificial Intelligence in Medicine},
  volume = {102},
  pages = {101758},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2019.101758},
  urldate = {2019-12-28},
  abstract = {An overview of the applications of deep learning for ophthalmic diagnosis using retinal fundus images is presented. We describe various retinal image datasets that can be used for deep learning purposes. Applications of deep learning for segmentation of optic disk, optic cup, blood vessels as well as detection of lesions are reviewed. Recent deep learning models for classification of diseases such as age-related macular degeneration, glaucoma, and diabetic retinopathy are also discussed. Important critical insights and future research directions are given.},
  langid = {english},
  keywords = {Classification,Deep learning,Fundus image datasets,Fundus photos,Image segmentation,Ophthalmology,Retina}
}

@article{senguptaOphthalmicDiagnosisUsing2020a,
  title = {Ophthalmic Diagnosis Using Deep Learning with Fundus Images -- {{A}} Critical Review},
  author = {Sengupta, Sourya and Singh, Amitojdeep and Leopold, Henry A. and Gulati, Tanmay and Lakshminarayanan, Vasudevan},
  year = {2020},
  month = jan,
  journal = {Artificial Intelligence in Medicine},
  volume = {102},
  pages = {101758},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2019.101758},
  urldate = {2021-10-02},
  abstract = {An overview of the applications of deep learning for ophthalmic diagnosis using retinal fundus images is presented. We describe various retinal image datasets that can be used for deep learning purposes. Applications of deep learning for segmentation of optic disk, optic cup, blood vessels as well as detection of lesions are reviewed. Recent deep learning models for classification of diseases such as age-related macular degeneration, glaucoma, and diabetic retinopathy are also discussed. Important critical insights and future research directions are given.},
  langid = {english},
  keywords = {Classification,Deep learning,Fundus image datasets,Fundus photos,Image segmentation,Ophthalmology,Retina}
}

@article{senguptaOphthalmicDiagnosisUsing2020b,
  title = {Ophthalmic Diagnosis Using Deep Learning with Fundus Images -- {{A}} Critical Review},
  author = {Sengupta, Sourya and Singh, Amitojdeep and Leopold, Henry A. and Gulati, Tanmay and Lakshminarayanan, Vasudevan},
  year = {2020},
  month = jan,
  journal = {Artificial Intelligence in Medicine},
  volume = {102},
  pages = {101758},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2019.101758},
  urldate = {2019-12-28},
  abstract = {An overview of the applications of deep learning for ophthalmic diagnosis using retinal fundus images is presented. We describe various retinal image datasets that can be used for deep learning purposes. Applications of deep learning for segmentation of optic disk, optic cup, blood vessels as well as detection of lesions are reviewed. Recent deep learning models for classification of diseases such as age-related macular degeneration, glaucoma, and diabetic retinopathy are also discussed. Important critical insights and future research directions are given.},
  langid = {english},
  keywords = {Classification,Deep learning,Fundus image datasets,Fundus photos,Image segmentation,Ophthalmology,Retina},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\BHJ3EJJF\\Sengupta et al. - 2020 - Ophthalmic diagnosis using deep learning with fund.pdf;C\:\\Users\\cleme\\Zotero\\storage\\KLYK47QN\\S0933365719305858.html}
}

@article{senguptaOphthalmicDiagnosisUsing2020c,
  title = {Ophthalmic Diagnosis Using Deep Learning with Fundus Images -- {{A}} Critical Review},
  author = {Sengupta, Sourya and Singh, Amitojdeep and Leopold, Henry A. and Gulati, Tanmay and Lakshminarayanan, Vasudevan},
  year = {2020},
  month = jan,
  journal = {Artificial Intelligence in Medicine},
  volume = {102},
  pages = {101758},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2019.101758},
  urldate = {2021-10-02},
  abstract = {An overview of the applications of deep learning for ophthalmic diagnosis using retinal fundus images is presented. We describe various retinal image datasets that can be used for deep learning purposes. Applications of deep learning for segmentation of optic disk, optic cup, blood vessels as well as detection of lesions are reviewed. Recent deep learning models for classification of diseases such as age-related macular degeneration, glaucoma, and diabetic retinopathy are also discussed. Important critical insights and future research directions are given.},
  langid = {english},
  keywords = {Classification,Deep learning,Fundus image datasets,Fundus photos,Image segmentation,Ophthalmology,Retina},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\7R594R2I\\Sengupta et al. - 2020 - Ophthalmic diagnosis using deep learning with fund.pdf;C\:\\Users\\cleme\\Zotero\\storage\\4ANHH9BS\\S0933365719305858.html}
}

@article{seoudRedLesionDetection2016,
  title = {Red {{Lesion Detection Using Dynamic Shape Features}} for {{Diabetic Retinopathy Screening}}},
  author = {Seoud, Lama and Hurtut, Thomas and Chelbi, Jihed and Cheriet, Farida and Langlois, J. M. Pierre},
  year = {2016},
  month = apr,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {35},
  number = {4},
  pages = {1116--1126},
  issn = {1558-254X},
  doi = {10.1109/TMI.2015.2509785},
  abstract = {The development of an automatic telemedicine system for computer-aided screening and grading of diabetic retinopathy depends on reliable detection of retinal lesions in fundus images. In this paper, a novel method for automatic detection of both microaneurysms and hemorrhages in color fundus images is described and validated. The main contribution is a new set of shape features, called Dynamic Shape Features, that do not require precise segmentation of the regions to be classified. These features represent the evolution of the shape during image flooding and allow to discriminate between lesions and vessel segments. The method is validated per-lesion and per-image using six databases, four of which are publicly available. It proves to be robust with respect to variability in image resolution, quality and acquisition system. On the Retinopathy Online Challenge's database, the method achieves a FROC score of 0.420 which ranks it fourth. On the Messidor database, when detecting images with diabetic retinopathy, the proposed method achieves an area under the ROC curve of 0.899, comparable to the score of human experts, and it outperforms state-of-the-art approaches.},
  keywords = {acquisition system,Algorithms,automatic telemedicine system,biomedical optical imaging,color fundus imaging,Colored noise,Computer aided diagnostic,computer-aided screening,Computer-Assisted,Databases,Diabetes,diabetic retinopathy,Diabetic Retinopathy,diabetic retinopathy grading,diabetic retinopathy screening,Diagnostic Techniques,diseases,dynamic shape features,eye,Factual,Feature extraction,FROC score,fundus IMAGING,hemorrhages,Humans,image classification,Image color analysis,image flooding,Image Interpretation,image resolution,image segmentation,lesion detection,Lesions,medical image processing,Messidor data-base,microaneurysms,Ophthalmological,red lesion detection,reliable detection,retina,Retina,retinal lesions,retinopathy online challenge database,ROC curve,ROC Curve,screening,Shape,state-of-the-art approaches,vessel segmentation,vision defects}
}

@article{seoudRedLesionDetection2016a,
  title = {Red {{Lesion Detection Using Dynamic Shape Features}} for {{Diabetic Retinopathy Screening}}},
  author = {Seoud, Lama and Hurtut, Thomas and Chelbi, Jihed and Cheriet, Farida and Langlois, J. M. Pierre},
  year = {2016},
  month = apr,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {35},
  number = {4},
  pages = {1116--1126},
  issn = {1558-254X},
  doi = {10.1109/TMI.2015.2509785},
  abstract = {The development of an automatic telemedicine system for computer-aided screening and grading of diabetic retinopathy depends on reliable detection of retinal lesions in fundus images. In this paper, a novel method for automatic detection of both microaneurysms and hemorrhages in color fundus images is described and validated. The main contribution is a new set of shape features, called Dynamic Shape Features, that do not require precise segmentation of the regions to be classified. These features represent the evolution of the shape during image flooding and allow to discriminate between lesions and vessel segments. The method is validated per-lesion and per-image using six databases, four of which are publicly available. It proves to be robust with respect to variability in image resolution, quality and acquisition system. On the Retinopathy Online Challenge's database, the method achieves a FROC score of 0.420 which ranks it fourth. On the Messidor database, when detecting images with diabetic retinopathy, the proposed method achieves an area under the ROC curve of 0.899, comparable to the score of human experts, and it outperforms state-of-the-art approaches.},
  keywords = {acquisition system,Algorithms,automatic telemedicine system,biomedical optical imaging,color fundus imaging,Colored noise,Computer aided diagnostic,computer-aided screening,Databases Factual,Diabetes,diabetic retinopathy,Diabetic Retinopathy,diabetic retinopathy grading,diabetic retinopathy screening,Diagnostic Techniques Ophthalmological,diseases,dynamic shape features,eye,Feature extraction,FROC score,fundus IMAGING,hemorrhages,Humans,image classification,Image color analysis,image flooding,Image Interpretation Computer-Assisted,image resolution,image segmentation,lesion detection,Lesions,medical image processing,Messidor data-base,microaneurysms,red lesion detection,reliable detection,retina,Retina,retinal lesions,retinopathy online challenge database,ROC curve,ROC Curve,screening,Shape,state-of-the-art approaches,vessel segmentation,vision defects},
  file = {C:\Users\cleme\Zotero\storage\SYT23H2X\7360182.html}
}

@article{shahMakingMachineLearning2019,
  title = {Making {{Machine Learning Models Clinically Useful}}},
  author = {Shah, Nigam H. and Milstein, Arnold and Bagley, Steven C., PhD},
  year = {2019},
  month = oct,
  journal = {JAMA : the journal of the American Medical Association},
  volume = {322},
  number = {14},
  pages = {1351--1352},
  issn = {0098-7484},
  doi = {10.1001/jama.2019.10306},
  urldate = {2023-04-23},
  abstract = {Recent advances in supervised machine learning have improved diagnostic accuracy and prediction of treatment outcomes, in some cases surpassing the performance of clinicians. In supervised machine learning, a mathematical function is constructed via automated analysis of training data, which consists of input features (such as retinal images) and output labels (such as the grade of macular edema). With large training data sets and minimal human guidance, a computer learns to generalize from the information contained in the training data. The result is a mathematical function, a model, that can be used to map a new record to the corresponding diagnosis, such as an image to grade macular edema. Although machine learning--based models for classification or for predicting a future health state are being developed for diverse clinical applications, evidence is lacking that deployment of these models has improved care and patient outcomes.}
}

@article{shahMakingMachineLearning2019a,
  title = {Making {{Machine Learning Models Clinically Useful}}},
  author = {Shah, Nigam H. and Milstein, Arnold and Bagley, Steven C., PhD},
  year = {2019},
  month = oct,
  journal = {JAMA},
  volume = {322},
  number = {14},
  pages = {1351--1352},
  issn = {0098-7484},
  doi = {10.1001/jama.2019.10306},
  urldate = {2023-04-23},
  abstract = {Recent advances in supervised machine learning have improved diagnostic accuracy and prediction of treatment outcomes, in some cases surpassing the performance of clinicians. In supervised machine learning, a mathematical function is constructed via automated analysis of training data, which consists of input features (such as retinal images) and output labels (such as the grade of macular edema). With large training data sets and minimal human guidance, a computer learns to generalize from the information contained in the training data. The result is a mathematical function, a model, that can be used to map a new record to the corresponding diagnosis, such as an image to grade macular edema. Although machine learning--based models for classification or for predicting a future health state are being developed for diverse clinical applications, evidence is lacking that deployment of these models has improved care and patient outcomes.},
  file = {C:\Users\cleme\Zotero\storage\2DCU9NEB\2748179.html}
}

@article{sharafeldeenPreciseHigherorderReflectivity2021,
  title = {Precise Higher-Order Reflectivity and Morphology Models for Early Diagnosis of Diabetic Retinopathy Using {{OCT}} Images},
  author = {Sharafeldeen, A. and Elsharkawy, M. and Khalifa, F. and Soliman, A. and Ghazal, M. and AlHalabi, M. and Yaghi, M. and Alrahmawy, M. and Elmougy, S. and Sandhu, H. S. and {El-Baz}, A.},
  year = {2021},
  month = feb,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {4730},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-83735-7},
  urldate = {2022-06-29},
  abstract = {This study proposes a novel computer assisted diagnostic (CAD) system for early diagnosis of diabetic retinopathy (DR) using optical coherence tomography (OCT) B-scans. The CAD system is based on fusing novel OCT markers that describe both the morphology/anatomy and the reflectivity of retinal layers to improve DR diagnosis. This system separates retinal layers automatically using a segmentation approach based on an adaptive appearance and their prior shape information. High-order morphological and novel reflectivity markers are extracted from individual segmented layers. Namely, the morphological markers are layer thickness and tortuosity while the reflectivity markers are the 1st-order reflectivity of the layer in addition to local and global high-order reflectivity based on Markov-Gibbs random field (MGRF) and gray-level co-occurrence matrix (GLCM), respectively. The extracted image-derived markers are represented using cumulative distribution function (CDF) descriptors. The constructed CDFs are then described using their statistical measures, i.e., the 10th through 90th percentiles with a 10\% increment. For individual layer classification, each extracted descriptor of a given layer is fed to a support vector machine (SVM) classifier with a linear kernel. The results of the four classifiers are then fused using a backpropagation neural network (BNN) to diagnose each retinal layer. For global subject diagnosis, classification outputs (probabilities) of the twelve layers are fused using another BNN to make the final diagnosis of the B-scan. This system is validated and tested on 130 patients, with two scans for both eyes (i.e. 260 OCT images), with a balanced number of normal and DR subjects using different validation metrics: 2-folds, 4-folds, 10-folds, and leave-one-subject-out (LOSO) cross-validation approaches. The performance of the proposed system was evaluated using sensitivity, specificity, F1-score, and accuracy metrics. The system's performance after the fusion of these different markers showed better performance compared with individual markers and other machine learning fusion methods. Namely, it achieved \$\$96.15{\textbackslash}\%\$\$, \$\$99.23{\textbackslash}\%\$\$, \$\$97.66{\textbackslash}\%\$\$, and \$\$97.69{\textbackslash}\%\$\$, respectively, using the LOSO cross-validation technique. The reported results, based on the integration of morphology and reflectivity markers and by using state-of-the-art machine learning classifications, demonstrate the ability of the proposed system to diagnose the DR early.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Biomedical engineering,Engineering}
}

@article{sharafeldeenPreciseHigherorderReflectivity2021a,
  title = {Precise Higher-Order Reflectivity and Morphology Models for Early Diagnosis of Diabetic Retinopathy Using {{OCT}} Images},
  author = {Sharafeldeen, A. and Elsharkawy, M. and Khalifa, F. and Soliman, A. and Ghazal, M. and AlHalabi, M. and Yaghi, M. and Alrahmawy, M. and Elmougy, S. and Sandhu, H. S. and {El-Baz}, A.},
  year = {2021},
  month = feb,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {4730},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-83735-7},
  urldate = {2022-06-29},
  abstract = {This study proposes a novel computer assisted diagnostic (CAD) system for early diagnosis of diabetic retinopathy (DR) using optical coherence tomography (OCT) B-scans. The CAD system is based on fusing novel OCT markers that describe both the morphology/anatomy and the reflectivity of retinal layers to improve DR diagnosis. This system separates retinal layers automatically using a segmentation approach based on an adaptive appearance and their prior shape information. High-order morphological and novel reflectivity markers are extracted from individual segmented layers. Namely, the morphological markers are layer thickness and tortuosity while the reflectivity markers are the 1st-order reflectivity of the layer in addition to local and global high-order reflectivity based on Markov-Gibbs random field (MGRF) and gray-level co-occurrence matrix (GLCM), respectively. The extracted image-derived markers are represented using cumulative distribution function (CDF) descriptors. The constructed CDFs are then described using their statistical measures, i.e., the 10th through 90th percentiles with a 10\% increment. For individual layer classification, each extracted descriptor of a given layer is fed to a support vector machine (SVM) classifier with a linear kernel. The results of the four classifiers are then fused using a backpropagation neural network (BNN) to diagnose each retinal layer. For global subject diagnosis, classification outputs (probabilities) of the twelve layers are fused using another BNN to make the final diagnosis of the B-scan. This system is validated and tested on 130 patients, with two scans for both eyes (i.e. 260 OCT images), with a balanced number of normal and DR subjects using different validation metrics: 2-folds, 4-folds, 10-folds, and leave-one-subject-out (LOSO) cross-validation approaches. The performance of the proposed system was evaluated using sensitivity, specificity, F1-score, and accuracy metrics. The system's performance after the fusion of these different markers showed better performance compared with individual markers and other machine learning fusion methods. Namely, it achieved \$\$96.15{\textbackslash}\%\$\$, \$\$99.23{\textbackslash}\%\$\$, \$\$97.66{\textbackslash}\%\$\$, and \$\$97.69{\textbackslash}\%\$\$, respectively, using the LOSO cross-validation technique. The reported results, based on the integration of morphology and reflectivity markers and by using state-of-the-art machine learning classifications, demonstrate the ability of the proposed system to diagnose the DR early.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Biomedical engineering,Engineering},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\U4GE8QC5\\Sharafeldeen et al. - 2021 - Precise higher-order reflectivity and morphology m.pdf;C\:\\Users\\cleme\\Zotero\\storage\\IERA8FH3\\s41598-021-83735-7.html}
}

@article{sharmaPrevalencePeripheralRetinal2023,
  title = {Prevalence of Peripheral Retinal Findings in Retinal Patients Using Ultra-Widefield Pseudocolor Fundus Imaging},
  author = {Sharma, Paripoorna and Shareef, Ihab and Kalaw, Fritz Gerald P. and Kako, Rasha Nabil and Lin, Andrew and Alex, Varsha and Nudleman, Eric and Walker, Evan H. and Borooah, Shyamanga},
  year = {2023},
  month = nov,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {20515},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-47761-x},
  urldate = {2024-02-22},
  abstract = {Ultra-widefield retinal imaging is increasingly used in ophthalmology and optometry practices to image patients identifying peripheral abnormalities. However, the clinical relevance of these peripheral retinal abnormalities is unclear. This cross-sectional study aims to firstly validate a new grading system, secondly, assess the prevalence of peripheral retinal abnormalities in retinal patients, and finally understand how peripheral findings may associate with retinal disease. Ultra-widefield pseudocolor fundus images were taken from the eyes of clinic patients. Demographic data and clinical diagnosis for each patient was noted. The grading system was validated using masked retinal specialists. Logistic regression identified associations between retinal disease and peripheral retinal findings. Using the grading system, inter-observer agreement was 76.1\% with Cohen's Kappa coefficient 0.542 (p\,{$<$}\,0.0001) and the test--retest agreement was 95.1\% with Kappa 0.677(p\,{$<$}\,0.0001). 971 images were included, with 625 eyes (64.4\%) having peripheral abnormalities. Peripheral drusen was the most common abnormality (n\,=\,221, 22.76\%) and correlated with age-related macular degeneration (p\,{$<$}\,0.001). Novel correlations were also identified between diabetic retinopathy and retinal pigmentation as well as pigmentary degeneration. This study provides a validated system for identifying peripheral abnormalities and adds to literature highlighting peripheral retinal associations with retinal disease which would benefit from further study.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Diagnostic markers,Eye diseases},
  file = {C:\Users\cleme\Zotero\storage\7W88LRAH\Sharma et al. - 2023 - Prevalence of peripheral retinal findings in retin.pdf}
}

@article{shelley_krippendorff_1984,
  title = {Content Analysis: {{An}} Introduction to Its Methodology.},
  author = {Shelley, Mack and Krippendorff, Klaus},
  year = {1984},
  journal = {Journal of the American Statistical Association},
  volume = {79},
  number = {385},
  pages = {240},
  doi = {10.2307/2288384}
}

@article{shelley_krippendorff_1984,
  title = {Content Analysis: {{An}} Introduction to Its Methodology.},
  author = {Shelley, Mack and Krippendorff, Klaus},
  year = {1984},
  journal = {Journal of the American Statistical Association},
  volume = {79},
  number = {385},
  pages = {240},
  doi = {10.2307/2288384}
}

@article{shenDomaininvariantInterpretableFundus2020,
  title = {Domain-Invariant Interpretable Fundus Image Quality Assessment},
  author = {Shen, Yaxin and Sheng, Bin and Fang, Ruogu and Li, Huating and Dai, Ling and Stolte, Skylar and Qin, Jing and Jia, Weiping and Shen, Dinggang},
  year = {2020},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {61},
  pages = {101654},
  issn = {1361-8415},
  doi = {10.1016/j.media.2020.101654},
  urldate = {2024-06-10},
  abstract = {Objective and quantitative assessment of fundus image quality is essential for the diagnosis of retinal diseases. The major factors in fundus image quality assessment are image artifact, clarity, and field definition. Unfortunately, most of existing quality assessment methods focus on the quality of overall image, without interpretable quality feedback for real-time adjustment. Furthermore, these models are often sensitive to the specific imaging devices, and cannot generalize well under different imaging conditions. This paper presents a new multi-task domain adaptation framework to automatically assess fundus image quality. The proposed framework provides interpretable quality assessment with both quantitative scores and quality visualization for potential real-time image recapture with proper adjustment. In particular, the present approach can detect optic disc and fovea structures as landmarks, to assist the assessment through coarse-to-fine feature encoding. The framework also exploit semi-tied adversarial discriminative domain adaptation to make the model generalizable across different data sources. Experimental results demonstrated that the proposed algorithm outperforms different state-of-the-art approaches and achieves an area under the ROC curve of 0.9455 for the overall quality classification.},
  keywords = {Domain adaptation,Fundus image quality assessment,Interpretability,Multi-task learning}
}

@misc{shlapentokh-rothmanRegionBasedRepresentationsRevisited2024,
  title = {Region-{{Based Representations Revisited}}},
  author = {{Shlapentokh-Rothman}, Michal and Blume, Ansel and Xiao, Yao and Wu, Yuqun and T V, Sethuraman and Tao, Heyi and Lee, Jae Yong and Torres, Wilfredo and Wang, Yu-Xiong and Hoiem, Derek},
  year = {2024},
  month = feb,
  number = {arXiv:2402.02352},
  eprint = {2402.02352},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {We investigate whether region-based representations are effective for recognition. Regions were once a mainstay in recognition approaches, but pixel and patch-based features are now used almost exclusively. We show that recent class-agnostic segmenters like SAM can be effectively combined with strong self-supervised representations, like those from DINOv2, and used for a wide variety of tasks, including semantic segmentation, object-based image retrieval, and multi-image analysis. Once the masks and features are extracted, these representations, even with linear decoders, enable competitive performance, making them well suited to applications that require custom queries. The representations' compactness also makes them well-suited to video analysis and other problems requiring inference across many images.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\QTI7XFI4\Shlapentokh-Rothman et al. - 2024 - Region-Based Representations Revisited.pdf}
}

@misc{shlapentokh-rothmanRegionBasedRepresentationsRevisited2024a,
  title = {Region-{{Based Representations Revisited}}},
  author = {{Shlapentokh-Rothman}, Michal and Blume, Ansel and Xiao, Yao and Wu, Yuqun and T V, Sethuraman and Tao, Heyi and Lee, Jae Yong and Torres, Wilfredo and Wang, Yu-Xiong and Hoiem, Derek},
  year = {2024},
  month = feb,
  number = {arXiv:2402.02352},
  eprint = {2402.02352},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.02352},
  urldate = {2024-02-13},
  abstract = {We investigate whether region-based representations are effective for recognition. Regions were once a mainstay in recognition approaches, but pixel and patch-based features are now used almost exclusively. We show that recent class-agnostic segmenters like SAM can be effectively combined with strong unsupervised representations like DINOv2 and used for a wide variety of tasks, including semantic segmentation, object-based image retrieval, and multi-image analysis. Once the masks and features are extracted, these representations, even with linear decoders, enable competitive performance, making them well suited to applications that require custom queries. The compactness of the representation also makes it well-suited to video analysis and other problems requiring inference across many images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\HIMS2FVN\\Shlapentokh-Rothman et al. - 2024 - Region-Based Representations Revisited.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6GQC9ZF6\\2402.html}
}

@inproceedings{silva-rodriguezExploringTransferabilityFoundation2024,
  title = {Exploring the~{{Transferability}} of~a~{{Foundation Model}} for~{{Fundus Images}}: {{Application}} to~{{Hypertensive Retinopathy}}},
  shorttitle = {Exploring the~{{Transferability}} of~a~{{Foundation Model}} for~{{Fundus Images}}},
  booktitle = {Advances in {{Computer Graphics}}},
  author = {{Silva-Rodriguez}, Julio and Chelbi, Jihed and Kabir, Waziha and Chakor, Hadi and Dolz, Jose and Ayed, Ismail Ben and Kobbi, Riadh},
  editor = {Sheng, Bin and Bi, Lei and Kim, Jinman and {Magnenat-Thalmann}, Nadia and Thalmann, Daniel},
  year = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {427--437},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-50075-6_33},
  abstract = {Using deep learning models pre-trained on Imagenet is the traditional solution for medical image classification to deal with data scarcity. Nevertheless, relevant literature supports that this strategy may offer limited gains due to the high dissimilarity between domains. Currently, the paradigm of adapting domain-specialized foundation models is proving to be a promising alternative. However, how to perform such knowledge transfer, and the benefits and limitations it presents, are under study. The CGI-HRDC challenge for Hypertensive Retinopathy diagnosis on fundus images introduces an appealing opportunity to evaluate the transferability of a recently released vision-language foundation model of the retina, FLAIR~[42]. In this work, we explore the potential of using FLAIR features as starting point for fundus image classification, and we compare its performance with regard to Imagenet initialization on two popular transfer learning methods: Linear Probing (LP) and Fine-Tuning (FP). Our empirical observations suggest that, in any case, the use of the traditional strategy provides performance gains. In contrast, direct transferability from FLAIR model allows gains of \$\${\textbackslash}sim \$\${$\sim$}2.5\%. When fine-tuning the whole network, the performance gap increases up to \$\${\textbackslash}sim \$\${$\sim$}4\%. In this case, we show that avoiding feature deterioration via LP initialization of the classifier allows the best re-use of the rich pre-trained features. Although direct transferability using LP still offers limited performance, we believe that foundation models such as FLAIR will drive the evolution of deep-learning-based fundus image analysis.},
  isbn = {978-3-031-50075-6},
  langid = {english},
  keywords = {Foundation Models,Hypertensive Retinopathy,Transfer Learning}
}

@inproceedings{Simonyan14a,
  title = {Deep inside Convolutional Networks: {{Visualising}} Image Classification Models and Saliency Maps},
  booktitle = {Workshop at International Conference on Learning Representations},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014}
}

@inproceedings{Simonyan14a,
  title = {Deep inside Convolutional Networks: {{Visualising}} Image Classification Models and Saliency Maps},
  booktitle = {Workshop at International Conference on Learning Representations},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014}
}

@article{simonyanVeryDeepConvolutional2015,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author = {Simonyan, K. and Zisserman, A.},
  year = {2015},
  journal = {3rd International Conference on Learning Representations (ICLR 2015)},
  publisher = {{Computational and Biological Learning Society}},
  urldate = {2023-02-15},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  langid = {english}
}

@inproceedings{simonyanVeryDeepConvolutional2015a,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}}, {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2015},
  urldate = {2023-02-15}
}

@inproceedings{simonyanVeryDeepConvolutional2015b,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}}, {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2015},
  urldate = {2023-02-15}
}

@article{simonyanVeryDeepConvolutional2015c,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author = {Simonyan, K. and Zisserman, A.},
  year = {2015},
  journal = {3rd International Conference on Learning Representations (ICLR 2015)},
  publisher = {{Computational and Biological Learning Society}},
  urldate = {2023-02-15},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\VPLI3GHF\Simonyan et Zisserman - 2015 - Very deep convolutional networks for large-scale i.pdf}
}

@article{singhExplainableDeepLearning2020,
  title = {Explainable {{Deep Learning Models}} in {{Medical Image Analysis}}},
  author = {Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan},
  year = {2020},
  month = jun,
  journal = {Journal of Imaging},
  volume = {6},
  number = {6},
  pages = {52},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-433X},
  doi = {10.3390/jimaging6060052},
  urldate = {2023-05-04},
  abstract = {Deep learning methods have been very effective for a variety of medical diagnostic tasks and have even outperformed human experts on some of those. However, the black-box nature of the algorithms has restricted their clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.},
  langid = {english},
  keywords = {deep learning,diagnosis,explainability,explainable AI,medical imaging,XAI}
}

@article{singhExplainableDeepLearning2020a,
  title = {Explainable {{Deep Learning Models}} in {{Medical Image Analysis}}},
  author = {Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan},
  year = {2020},
  month = jun,
  journal = {Journal of Imaging},
  volume = {6},
  number = {6},
  pages = {52},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-433X},
  doi = {10.3390/jimaging6060052},
  urldate = {2023-05-04},
  abstract = {Deep learning methods have been very effective for a variety of medical diagnostic tasks and have even outperformed human experts on some of those. However, the black-box nature of the algorithms has restricted their clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,diagnosis,explainability,explainable AI,medical imaging,XAI},
  file = {C:\Users\cleme\Zotero\storage\BM33AJI8\Singh et al. - 2020 - Explainable Deep Learning Models in Medical Image .pdf}
}

@inproceedings{singhWhatOptimalAttribution2020,
  title = {What Is the {{Optimal Attribution Method}} for {{Explainable Ophthalmic Disease Classification}}?},
  booktitle = {Ophthalmic {{Medical Image Analysis}}},
  author = {Singh, Amitojdeep and Sengupta, Sourya and J., Jothi Balaji and Mohammed, Abdul Rasheed and Faruq, Ibrahim and Jayakumar, Varadharajan and Zelek, John and Lakshminarayanan, Vasudevan},
  editor = {Fu, Huazhu and Garvin, Mona K. and MacGillivray, Tom and Xu, Yanwu and Zheng, Yalin},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {21--31},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-63419-3_3},
  abstract = {Deep learning methods for ophthalmic diagnosis have shown success for tasks like segmentation and classification but their implementation in the clinical setting is limited by the black-box nature of the algorithms. Very few studies have explored the explainability of deep learning in this domain. Attribution methods explain the decisions by assigning a relevance score to each input feature. Here, we present a comparative analysis of multiple attribution methods to explain the decisions of a convolutional neural network (CNN) in retinal disease classification from OCT images. This is the first such study to perform both quantitative and qualitative analyses. The former was performed using robustness, runtime, and sensitivity while the latter was done by a panel of eye care clinicians who rated the methods based on their correlation with diagnostic features. The study emphasizes the need for developing explainable models that address the end-user requirements, hence increasing the clinical acceptance of deep learning.},
  isbn = {978-3-030-63419-3},
  langid = {english},
  keywords = {Attributions,Clinical ophthalmology,Deep learning,Explainability,Image classification,OCT,Retina,XAI}
}

@inproceedings{singhWhatOptimalAttribution2020a,
  title = {What Is the {{Optimal Attribution Method}} for {{Explainable Ophthalmic Disease Classification}}?},
  booktitle = {Ophthalmic {{Medical Image Analysis}}},
  author = {Singh, Amitojdeep and Sengupta, Sourya and J, Jothi Balaji and Mohammed, Abdul Rasheed and Faruq, Ibrahim and Jayakumar, Varadharajan and Zelek, John and Lakshminarayanan, Vasudevan},
  year = {2020},
  month = oct,
  pages = {21--31},
  publisher = {Springer, Cham},
  doi = {10.1007/978-3-030-63419-3_3},
  urldate = {2021-04-13},
  abstract = {Deep learning methods for ophthalmic diagnosis have shown success for tasks like segmentation and classification but their implementation in the clinical setting is limited by the black-box nature...},
  langid = {english}
}

@inproceedings{singhWhatOptimalAttribution2020b,
  title = {What Is the {{Optimal Attribution Method}} for {{Explainable Ophthalmic Disease Classification}}?},
  booktitle = {Ophthalmic {{Medical Image Analysis}}},
  author = {Singh, Amitojdeep and Sengupta, Sourya and J, Jothi Balaji and Mohammed, Abdul Rasheed and Faruq, Ibrahim and Jayakumar, Varadharajan and Zelek, John and Lakshminarayanan, Vasudevan},
  year = {2020},
  month = oct,
  pages = {21--31},
  publisher = {Springer, Cham},
  doi = {10.1007/978-3-030-63419-3_3},
  urldate = {2021-04-13},
  abstract = {Deep learning methods for ophthalmic diagnosis have shown success for tasks like segmentation and classification but their implementation in the clinical setting is limited by the black-box nature...},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\ML5WEEBQ\\Singh et al. - 2020 - What is the Optimal Attribution Method for Explain.pdf;C\:\\Users\\cleme\\Zotero\\storage\\REMZWSFG\\10.html}
}

@inproceedings{singhWhatOptimalAttribution2020c,
  title = {What Is the {{Optimal Attribution Method}} for {{Explainable Ophthalmic Disease Classification}}?},
  booktitle = {Ophthalmic {{Medical Image Analysis}}},
  author = {Singh, Amitojdeep and Sengupta, Sourya and J., Jothi Balaji and Mohammed, Abdul Rasheed and Faruq, Ibrahim and Jayakumar, Varadharajan and Zelek, John and Lakshminarayanan, Vasudevan},
  editor = {Fu, Huazhu and Garvin, Mona K. and MacGillivray, Tom and Xu, Yanwu and Zheng, Yalin},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {21--31},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-63419-3_3},
  abstract = {Deep learning methods for ophthalmic diagnosis have shown success for tasks like segmentation and classification but their implementation in the clinical setting is limited by the black-box nature of the algorithms. Very few studies have explored the explainability of deep learning in this domain. Attribution methods explain the decisions by assigning a relevance score to each input feature. Here, we present a comparative analysis of multiple attribution methods to explain the decisions of a convolutional neural network (CNN) in retinal disease classification from OCT images. This is the first such study to perform both quantitative and qualitative analyses. The former was performed using robustness, runtime, and sensitivity while the latter was done by a panel of eye care clinicians who rated the methods based on their correlation with diagnostic features. The study emphasizes the need for developing explainable models that address the end-user requirements, hence increasing the clinical acceptance of deep learning.},
  isbn = {978-3-030-63419-3},
  langid = {english},
  keywords = {Attributions,Clinical ophthalmology,Deep learning,Explainability,Image classification,OCT,Retina,XAI}
}

@article{sinthanayothinAutomatedDetectionDiabetic2002,
  title = {Automated Detection of Diabetic Retinopathy on Digital Fundus Images},
  author = {Sinthanayothin, C. and Boyce, J. F. and Williamson, T. H. and Cook, H. L. and Mensah, E. and Lal, S. and Usher, D.},
  year = {2002},
  month = feb,
  journal = {Diabetic Medicine},
  volume = {19},
  number = {2},
  pages = {105--112},
  issn = {0742-3071},
  doi = {10.1046/j.1464-5491.2002.00613.x},
  urldate = {2019-11-19},
  abstract = {Abstract Aims The aim was to develop an automated screening system to analyse digital colour retinal images for important features of non-proliferative diabetic retinopathy (NPDR). Methods High performance pre-processing of the colour images was performed. Previously described automated image analysis systems were used to detect major landmarks of the retinal image (optic disc, blood vessels and fovea). Recursive region growing segmentation algorithms combined with the use of a new technique, termed a ?Moat Operator?, were used to automatically detect features of NPDR. These features included haemorrhages and microaneurysms (HMA), which were treated as one group, and hard exudates as another group. Sensitivity and specificity data were calculated by comparison with an experienced fundoscopist. Results The algorithm for exudate recognition was applied to 30 retinal images of which 21 contained exudates and nine were without pathology. The sensitivity and specificity for exudate detection were 88.5\% and 99.7\%, respectively, when compared with the ophthalmologist. HMA were present in 14 retinal images. The algorithm achieved a sensitivity of 77.5\% and specificity of 88.7\% for detection of HMA. Conclusions Fully automated computer algorithms were able to detect hard exudates and HMA. This paper presents encouraging results in automatic identification of important features of NPDR. Diabet. Med. 19, 105?112 (2002)},
  keywords = {diabetic diagnosis,image analysis,image recognition,neural network,retinopathy}
}

@article{sinthanayothinAutomatedDetectionDiabetic2002a,
  title = {Automated Detection of Diabetic Retinopathy on Digital Fundus Images},
  author = {Sinthanayothin, C. and Boyce, J. F. and Williamson, T. H. and Cook, H. L. and Mensah, E. and Lal, S. and Usher, D.},
  year = {2002},
  month = feb,
  journal = {Diabetic Medicine},
  volume = {19},
  number = {2},
  pages = {105--112},
  issn = {0742-3071},
  doi = {10.1046/j.1464-5491.2002.00613.x},
  urldate = {2019-11-19},
  abstract = {Abstract Aims The aim was to develop an automated screening system to analyse digital colour retinal images for important features of non-proliferative diabetic retinopathy (NPDR). Methods High performance pre-processing of the colour images was performed. Previously described automated image analysis systems were used to detect major landmarks of the retinal image (optic disc, blood vessels and fovea). Recursive region growing segmentation algorithms combined with the use of a new technique, termed a ?Moat Operator?, were used to automatically detect features of NPDR. These features included haemorrhages and microaneurysms (HMA), which were treated as one group, and hard exudates as another group. Sensitivity and specificity data were calculated by comparison with an experienced fundoscopist. Results The algorithm for exudate recognition was applied to 30 retinal images of which 21 contained exudates and nine were without pathology. The sensitivity and specificity for exudate detection were 88.5\% and 99.7\%, respectively, when compared with the ophthalmologist. HMA were present in 14 retinal images. The algorithm achieved a sensitivity of 77.5\% and specificity of 88.7\% for detection of HMA. Conclusions Fully automated computer algorithms were able to detect hard exudates and HMA. This paper presents encouraging results in automatic identification of important features of NPDR. Diabet. Med. 19, 105?112 (2002)},
  keywords = {diabetic diagnosis,image analysis,image recognition,neural network,retinopathy},
  file = {C:\Users\cleme\Zotero\storage\LYVEXUQD\j.1464-5491.2002.00613.html}
}

@article{sisodiaDiabeticRetinalFundus2017,
  title = {Diabetic {{Retinal Fundus Images}}: {{Preprocessing}} and {{Feature Extraction}} for {{Early Detection}} of {{Diabetic Retinopathy}}},
  shorttitle = {Diabetic {{Retinal Fundus Images}}},
  author = {Sisodia, Dilip Singh and Nair, Shruti and Khobragade, Pooja},
  year = {2017},
  month = jun,
  journal = {Biomedical and Pharmacology Journal},
  volume = {10},
  number = {2},
  pages = {615--626},
  urldate = {2019-12-05},
  abstract = {Introduction In the recent years, there has been a dramatic increase in the number of diabetic patients suffering from diabetic retinopathy (DR). DR is one of the most chronic diseases which make the key cause of vision loss in middle-aged people in the developed world [1]. DR emerges as small ch},
  langid = {american}
}

@article{sisodiaDiabeticRetinalFundus2017a,
  title = {Diabetic {{Retinal Fundus Images}}: {{Preprocessing}} and {{Feature Extraction}} for {{Early Detection}} of {{Diabetic Retinopathy}}},
  shorttitle = {Diabetic {{Retinal Fundus Images}}},
  author = {Sisodia, Dilip Singh and Nair, Shruti and Khobragade, Pooja},
  year = {2017},
  month = jun,
  journal = {Biomedical and Pharmacology Journal},
  volume = {10},
  number = {2},
  pages = {615--626},
  urldate = {2019-12-05},
  abstract = {Introduction In the recent years, there has been a dramatic increase in the number of diabetic patients suffering from diabetic retinopathy (DR). DR is one of the most chronic diseases which make the key cause of vision loss in middle-aged people in the developed world [1]. DR emerges as small ch},
  langid = {american},
  file = {C:\Users\cleme\Zotero\storage\F6RTP6PG\diabetic-retinal-fundus-images-preprocessing-and-feature-extraction-for-early-detection-of-diab.html}
}

@inproceedings{sivaswamyComprehensiveRetinalImage2015,
  title = {A {{Comprehensive Retinal Image Dataset}} for the {{Assessment}} of {{Glaucoma}} from the {{Optic Nerve Head Analysis}}},
  author = {Sivaswamy, Jayanthi and Chakravarty, Arunava and Joshi, Gopal Datt and Syed, Tabish A.},
  year = {2015},
  abstract = {Optic nerve head (ONH) segmentation problem is of interest for automated glaucoma assessment. Although various segmentation methods have been proposed in the recent past, it is difficult to evaluate and compare the performance of individual methods due to a lack of a benchmark dataset. The assessment involves segmentation of optic disk and cup region within the ONH. In this paper, we present a comprehensive dataset of retinal images of both normal and glaucomatous eyes with manual segmentations from multiple human experts. The dataset also provides expert opinion on an image representing a normal or glaucomatous eye and on the presence of notching in an image. Several state of the art methods are assessed against this dataset using cup to disc diameter ratio (CDR), area and boundary-based evaluation measures. These are presented to aid benchmarking of new methods. A supervised, notch detection method based on the segmentation results is also proposed and its assessment results are included for benchmarking.},
  keywords = {Benchmark (computing),Bolo (tank),Comstock-Needham system,Comstock\textendashNeedham system,Database,Diabetes Mellitus,Diabetic Retinopathy,Ephedra sinica,Eye,Glaucoma,Ground truth,Image analysis,Matched filter,Medical image computing,Nerve Tissue,Optic Disk,Optic Nerve (GCHQ),Protocols documentation,Retina,Retinal Diseases,Silo (dataset),Supervised learning,Upload,Wafer-level packaging,Wikipedia,WL 276}
}

@inproceedings{sivaswamyDrishtiGSRetinalImage2014,
  title = {Drishti-{{GS}}: {{Retinal}} Image Dataset for Optic Nerve Head({{ONH}}) Segmentation},
  shorttitle = {Drishti-{{GS}}},
  booktitle = {2014 {{IEEE}} 11th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  author = {Sivaswamy, Jayanthi and Krishnadas, S. R. and Datt Joshi, Gopal and Jain, Madhulika and Syed Tabish, A. Ujjwaft},
  year = {2014},
  month = apr,
  pages = {53--56},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2014.6867807},
  abstract = {Optic nerve head (ONH) segmentation problem has been of interest for automated glaucoma assessment. Although various segmentation methods have been proposed in the recent past, it is difficult to evaluate and compare the performance of individual methods due to a lack of a benchmark dataset. The problem of segmentation involves segmentation of optic disk and cup region within ONH region. Available datasets do not incorporate challenges present in this problem. In this data paper, we present a comprehensive dataset of retinal images which include both normal and glaucomatous eyes and manual segmentations from multiple human experts. Both area and boundary-based evaluation measures are presented to evaluate a method on various aspects relevant to the problem of glaucoma assessment.},
  keywords = {Cup,cup region segmentation,Dataset,eye,Glaucoma,glaucoma assessment,glaucomatous eye,Image color analysis,image segmentation,Image segmentation,Manuals,medical disorders,medical image processing,Optic Disk,optic disk segmentation,Optic Nerve Head,optic nerve head segmentation,Optical imaging,Retina,retinal image dataset,Shape}
}

@inproceedings{sivaswamyDrishtiGSRetinalImage2014a,
  title = {Drishti-{{GS}}: {{Retinal}} Image Dataset for Optic Nerve Head({{ONH}}) Segmentation},
  shorttitle = {Drishti-{{GS}}},
  booktitle = {2014 {{IEEE}} 11th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  author = {Sivaswamy, Jayanthi and Krishnadas, S. R. and Datt Joshi, Gopal and Jain, Madhulika and Syed Tabish, A. Ujjwaft},
  year = {2014},
  month = apr,
  pages = {53--56},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2014.6867807},
  abstract = {Optic nerve head (ONH) segmentation problem has been of interest for automated glaucoma assessment. Although various segmentation methods have been proposed in the recent past, it is difficult to evaluate and compare the performance of individual methods due to a lack of a benchmark dataset. The problem of segmentation involves segmentation of optic disk and cup region within ONH region. Available datasets do not incorporate challenges present in this problem. In this data paper, we present a comprehensive dataset of retinal images which include both normal and glaucomatous eyes and manual segmentations from multiple human experts. Both area and boundary-based evaluation measures are presented to evaluate a method on various aspects relevant to the problem of glaucoma assessment.},
  keywords = {Cup,cup region segmentation,Dataset,eye,Glaucoma,glaucoma assessment,glaucomatous eye,Image color analysis,image segmentation,Image segmentation,Manuals,medical disorders,medical image processing,Optic Disk,optic disk segmentation,Optic Nerve Head,optic nerve head segmentation,Optical imaging,Retina,retinal image dataset,Shape},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\MXWP8I9Q\\Sivaswamy et al. - 2014 - Drishti-GS Retinal image dataset for optic nerve .pdf;C\:\\Users\\cleme\\Zotero\\storage\\SNVK5NWY\\6867807.html}
}

@inproceedings{skokanRegistrationMultimodalImages2002,
  title = {Registration of Multimodal Images of Retina},
  booktitle = {Proceedings of the {{Second Joint}} 24th {{Annual Conference}} and the {{Annual Fall Meeting}} of the {{Biomedical Engineering Society}}] [{{Engineering}} in {{Medicine}} and {{Biology}}},
  author = {Skokan, M. and Skoupy, A. and Jan, J.},
  year = {2002},
  month = oct,
  volume = {2},
  pages = {1094-1096 vol.2},
  issn = {1094-687X},
  doi = {10.1109/IEMBS.2002.1106294},
  abstract = {Registration of retinal images provided by different modalities is required to facilitate diagnosis of the optic nerve head and retina. For reliable automatic segmentation of the optic disk, it seems essential to join the image data produced by the Heidelberg Retina Tomograph (HRT) and the standard colour photograph. The proposed method is based on registering both (very different) images using mutual information as the coincidence measure.},
  keywords = {3D data block,affine transform,Biomedical engineering,biomedical optical imaging,Biomedical optical imaging,coincidence measure,eye,focus planes,Geometry,glaucoma progression measurement,greyscale image slices,Head,Heidelberg retina tomograph,Image converters,image registration,image segmentation,Image segmentation,laser applications in medicine,medical image processing,monochrome red laser light,multimodal images,mutual information,Mutual information,optic disk,optic nerve head,Optical distortion,Optical imaging,photographic applications,reliable automatic segmentation,Retina,retinal confocal laser scanning system,standard colour photograph,voxel intensities sum}
}

@inproceedings{skokanRegistrationMultimodalImages2002a,
  title = {Registration of Multimodal Images of Retina},
  booktitle = {Proceedings of the {{Second Joint}} 24th {{Annual Conference}} and the {{Annual Fall Meeting}} of the {{Biomedical Engineering Society}}] [{{Engineering}} in {{Medicine}} and {{Biology}}},
  author = {Skokan, M. and Skoupy, A. and Jan, J.},
  year = {2002},
  month = oct,
  volume = {2},
  pages = {1094-1096 vol.2},
  issn = {1094-687X},
  doi = {10.1109/IEMBS.2002.1106294},
  abstract = {Registration of retinal images provided by different modalities is required to facilitate diagnosis of the optic nerve head and retina. For reliable automatic segmentation of the optic disk, it seems essential to join the image data produced by the Heidelberg Retina Tomograph (HRT) and the standard colour photograph. The proposed method is based on registering both (very different) images using mutual information as the coincidence measure.},
  keywords = {3D data block,affine transform,Biomedical engineering,biomedical optical imaging,Biomedical optical imaging,coincidence measure,eye,focus planes,Geometry,glaucoma progression measurement,greyscale image slices,Head,Heidelberg retina tomograph,Image converters,image registration,image segmentation,Image segmentation,laser applications in medicine,medical image processing,monochrome red laser light,multimodal images,mutual information,Mutual information,optic disk,optic nerve head,Optical distortion,Optical imaging,photographic applications,reliable automatic segmentation,Retina,retinal confocal laser scanning system,standard colour photograph,voxel intensities sum},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\3KIC8Y8K\\Skokan et al. - 2002 - Registration of multimodal images of retina.pdf;C\:\\Users\\cleme\\Zotero\\storage\\NS3Z66BF\\registration-of-multimodal-images-of-retina.pdf;C\:\\Users\\cleme\\Zotero\\storage\\YUEFBJC9\\1106294.html}
}

@article{smilkovSmoothgradRemovingNoise2017,
  title = {Smoothgrad: {{Removing}} Noise by Adding Noise},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = {2017},
  journal = {Workshop on Visualization for Deep Learning, ICML},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.}
}

@article{smilkovSmoothgradRemovingNoise2017a,
  title = {Smoothgrad: Removing Noise by Adding Noise},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = {2017},
  journal = {Workshop on Visualization for Deep Learning, ICML},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.}
}

@article{songLearningNoisyLabels2022,
  title = {Learning {{From Noisy Labels With Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Learning {{From Noisy Labels With Deep Neural Networks}}},
  author = {Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--19},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3152527},
  abstract = {Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into five groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.},
  keywords = {Classification,Data models,deep learning,Deep learning,label noise,Noise measurement,noisy label,robust deep learning,robust optimization,survey.,Task analysis,Taxonomy,Training,Training data}
}

@article{songLearningNoisyLabels2022a,
  title = {Learning {{From Noisy Labels With Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Learning {{From Noisy Labels With Deep Neural Networks}}},
  author = {Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--19},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3152527},
  abstract = {Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into five groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.},
  keywords = {Classification,Data models,deep learning,Deep learning,label noise,Noise measurement,noisy label,robust deep learning,robust optimization,survey.,Task analysis,Taxonomy,Training,Training data},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9BM9ERMC\\Song et al. - 2022 - Learning From Noisy Labels With Deep Neural Networ.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ZWB5JHUK\\stamp.html}
}

@article{sotirasDeformableMedicalImage2013,
  title = {Deformable {{Medical Image Registration}}: {{A Survey}}},
  shorttitle = {Deformable {{Medical Image Registration}}},
  author = {Sotiras, A. and Davatzikos, C. and Paragios, N.},
  year = {2013},
  month = jul,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {32},
  number = {7},
  pages = {1153--1190},
  issn = {0278-0062},
  doi = {10.1109/TMI.2013.2265603},
  abstract = {Deformable image registration is a fundamental task in medical image processing. Among its most important applications, one may cite: 1) multi-modality fusion, where information acquired by different imaging devices or protocols is fused to facilitate diagnosis and treatment planning; 2) longitudinal studies, where temporal structural or anatomical changes are investigated; and 3) population modeling and statistical atlases used to study normal anatomical variability. In this paper, we attempt to give an overview of deformable registration methods, putting emphasis on the most recent advances in the domain. Additional emphasis has been given to techniques applied to medical images. In order to study image registration methods in depth, their main components are identified and studied independently. The most recent techniques are presented in a systematic fashion. The contribution of this paper is to provide an extensive account of registration techniques in a systematic manner.},
  keywords = {Algorithms,Bibliographical review,Biomedical imaging,Computational modeling,Computer-Assisted,deformable medical image registration,Deformable models,deformable registration,diagnosis,Diagnostic Imaging,Humans,image fusion,Image Processing,image registration,Image registration,imaging devices,information acquisition,Linear programming,Mathematical model,medical image analysis,medical image processing,multimodality fusion,normal anatomical variability,patient treatment,population modeling,protocols,statistical analysis,statistical atlases,temporal anatomical changes,temporal structural changes,Topology,treatment planning}
}

@article{sotirasDeformableMedicalImage2013a,
  title = {Deformable {{Medical Image Registration}}: {{A Survey}}},
  shorttitle = {Deformable {{Medical Image Registration}}},
  author = {Sotiras, A. and Davatzikos, C. and Paragios, N.},
  year = {2013},
  month = jul,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {32},
  number = {7},
  pages = {1153--1190},
  issn = {0278-0062},
  doi = {10.1109/TMI.2013.2265603},
  abstract = {Deformable image registration is a fundamental task in medical image processing. Among its most important applications, one may cite: 1) multi-modality fusion, where information acquired by different imaging devices or protocols is fused to facilitate diagnosis and treatment planning; 2) longitudinal studies, where temporal structural or anatomical changes are investigated; and 3) population modeling and statistical atlases used to study normal anatomical variability. In this paper, we attempt to give an overview of deformable registration methods, putting emphasis on the most recent advances in the domain. Additional emphasis has been given to techniques applied to medical images. In order to study image registration methods in depth, their main components are identified and studied independently. The most recent techniques are presented in a systematic fashion. The contribution of this paper is to provide an extensive account of registration techniques in a systematic manner.},
  keywords = {Algorithms,Bibliographical review,Biomedical imaging,Computational modeling,deformable medical image registration,Deformable models,deformable registration,diagnosis,Diagnostic Imaging,Humans,image fusion,Image Processing Computer-Assisted,image registration,Image registration,imaging devices,information acquisition,Linear programming,Mathematical model,medical image analysis,medical image processing,multimodality fusion,normal anatomical variability,patient treatment,population modeling,protocols,statistical analysis,statistical atlases,temporal anatomical changes,temporal structural changes,Topology,treatment planning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\Q6MYWG2N\\Sotiras et al. - 2013 - Deformable Medical Image Registration A Survey.pdf;C\:\\Users\\cleme\\Zotero\\storage\\NU5CMANN\\6522524.html}
}

@article{spaideConsensusNomenclatureReporting2020,
  title = {Consensus {{Nomenclature}} for {{Reporting Neovascular Age-Related Macular Degeneration Data}}: {{Consensus}} on {{Neovascular Age-Related Macular Degeneration Nomenclature Study Group}}},
  shorttitle = {Consensus {{Nomenclature}} for {{Reporting Neovascular Age-Related Macular Degeneration Data}}},
  author = {Spaide, Richard F. and Jaffe, Glenn J. and Sarraf, David and Freund, K. Bailey and Sadda, Srinivas R. and Staurenghi, Giovanni and Waheed, Nadia K. and Chakravarthy, Usha and Rosenfeld, Philip J. and Holz, Frank G. and Souied, Eric H. and Cohen, Salomon Y. and Querques, Giuseppe and {Ohno-Matsui}, Kyoko and Boyer, David and Gaudric, Alain and Blodi, Barbara and Baumal, Caroline R. and Li, Xiaoxin and Coscas, Gabriel J. and Brucker, Alexander and Singerman, Lawrence and Luthert, Phil and {Schmitz-Valckenberg}, Steffen and {Schmidt-Erfurth}, Ursula and Grossniklaus, Hans E. and Wilson, David J. and Guymer, Robyn and Yannuzzi, Lawrence A. and Chew, Emily Y. and Csaky, Karl and Mon{\'e}s, Jordi M. and Pauleikhoff, Daniel and Tadayoni, Ramin and Fujimoto, James},
  year = {2020},
  month = may,
  journal = {Ophthalmology},
  volume = {127},
  number = {5},
  pages = {616--636},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2019.11.004},
  urldate = {2023-06-13},
  abstract = {Purpose To establish a process to evaluate and standardize a state-of-the-art nomenclature for reporting neovascular age-related macular degeneration (AMD) data. Design Consensus meeting. Participants An international panel of retina specialists, imaging and image reading center experts, and ocular pathologists. Methods During several meetings organized under the auspices of the Macula Society, an international study group discussed and codified a set nomenclature framework for classifying the subtypes of neovascular AMD and associated lesion components. Main Outcome Measures A consensus classification of neovascular AMD. Results The study group created a standardized working definition of AMD. The components of neovascular AMD were defined and subclassified. Disease consequences of macular neovascularization were delineated. Conclusions The framework of a consensus nomenclature system, a definition of AMD, and a delineation of the subtypes of neovascular AMD were developed. Establishing a uniform set of definitions will facilitate comparison of diverse patient groups and different studies. The framework presented is modified and updated readily, processes that are anticipated to occur on a periodic basis. The study group suggests that the consensus standards outlined in this article be used in future reported studies of neovascular AMD and clinical practice.},
  langid = {english}
}

@article{spaideConsensusNomenclatureReporting2020a,
  title = {Consensus {{Nomenclature}} for {{Reporting Neovascular Age-Related Macular Degeneration Data}}: {{Consensus}} on {{Neovascular Age-Related Macular Degeneration Nomenclature Study Group}}},
  shorttitle = {Consensus {{Nomenclature}} for {{Reporting Neovascular Age-Related Macular Degeneration Data}}},
  author = {Spaide, Richard F. and Jaffe, Glenn J. and Sarraf, David and Freund, K. Bailey and Sadda, Srinivas R. and Staurenghi, Giovanni and Waheed, Nadia K. and Chakravarthy, Usha and Rosenfeld, Philip J. and Holz, Frank G. and Souied, Eric H. and Cohen, Salomon Y. and Querques, Giuseppe and {Ohno-Matsui}, Kyoko and Boyer, David and Gaudric, Alain and Blodi, Barbara and Baumal, Caroline R. and Li, Xiaoxin and Coscas, Gabriel J. and Brucker, Alexander and Singerman, Lawrence and Luthert, Phil and {Schmitz-Valckenberg}, Steffen and {Schmidt-Erfurth}, Ursula and Grossniklaus, Hans E. and Wilson, David J. and Guymer, Robyn and Yannuzzi, Lawrence A. and Chew, Emily Y. and Csaky, Karl and Mon{\'e}s, Jordi M. and Pauleikhoff, Daniel and Tadayoni, Ramin and Fujimoto, James},
  year = {2020},
  month = may,
  journal = {Ophthalmology},
  volume = {127},
  number = {5},
  pages = {616--636},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2019.11.004},
  urldate = {2023-06-13},
  abstract = {Purpose To establish a process to evaluate and standardize a state-of-the-art nomenclature for reporting neovascular age-related macular degeneration (AMD) data. Design Consensus meeting. Participants An international panel of retina specialists, imaging and image reading center experts, and ocular pathologists. Methods During several meetings organized under the auspices of the Macula Society, an international study group discussed and codified a set nomenclature framework for classifying the subtypes of neovascular AMD and associated lesion components. Main Outcome Measures A consensus classification of neovascular AMD. Results The study group created a standardized working definition of AMD. The components of neovascular AMD were defined and subclassified. Disease consequences of macular neovascularization were delineated. Conclusions The framework of a consensus nomenclature system, a definition of AMD, and a delineation of the subtypes of neovascular AMD were developed. Establishing a uniform set of definitions will facilitate comparison of diverse patient groups and different studies. The framework presented is modified and updated readily, processes that are anticipated to occur on a periodic basis. The study group suggests that the consensus standards outlined in this article be used in future reported studies of neovascular AMD and clinical practice.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\8LZ9E5P8\\Spaide et al. - 2020 - Consensus Nomenclature for Reporting Neovascular A.pdf;C\:\\Users\\cleme\\Zotero\\storage\\L3DSPTZW\\S0161642019322432.html}
}

@inproceedings{speithReviewTaxonomiesExplainable2022,
  title = {A {{Review}} of {{Taxonomies}} of {{Explainable Artificial Intelligence}} ({{XAI}}) {{Methods}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Speith, Timo},
  year = {2022},
  month = jun,
  series = {{{FAccT}} '22},
  pages = {2239--2250},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3531146.3534639},
  urldate = {2023-05-04},
  abstract = {The recent surge in publications related to explainable artificial intelligence (XAI) has led to an almost insurmountable wall if one wants to get started or stay up to date with XAI. For this reason, articles and reviews that present taxonomies of XAI methods seem to be a welcomed way to get an overview of the field. Building on this idea, there is currently a trend of producing such taxonomies, leading to several competing approaches to construct them. In this paper, we will review recent approaches to constructing taxonomies of XAI methods and discuss general challenges concerning them as well as their individual advantages and limitations. Our review is intended to help scholars be aware of challenges current taxonomies face. As we will argue, when charting the field of XAI, it may not be sufficient to rely on one of the approaches we found. To amend this problem, we will propose and discuss three possible solutions: a new taxonomy that incorporates the reviewed ones, a database of XAI methods, and a decision tree to help choose fitting methods.},
  isbn = {978-1-4503-9352-2},
  keywords = {explainability,explainable artificial intelligence,interpretability,review,taxonomy,transparency,XAI},
  file = {C:\Users\cleme\Zotero\storage\PU4XP9WM\Speith - 2022 - A Review of Taxonomies of Explainable Artificial I.pdf}
}

@misc{spilsburyDonIgnoreDropout2019,
  title = {Don't Ignore {{Dropout}} in {{Fully Convolutional Networks}}},
  author = {Spilsbury, Thomas and Camps, Paavo},
  year = {2019},
  month = aug,
  number = {arXiv:1908.09162},
  eprint = {1908.09162},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-02-10},
  abstract = {Data for image segmentation models can be costly to obtain due to the precision required by human annotators. We run a series of experiments showing the effect of different kinds of Dropout training on the DeepLabv3+ image segmentation model when trained using a small dataset. We find that when appropriate forms of Dropout are applied in the right place in the model architecture that non-insignificant improvement in Mean Intersection over Union (mIoU) score can be observed. In our best case, we find that applying Dropout scheduling in conjunction with SpatialDropout improves baseline mIoU from 0.49 to 0.59. This result shows that even where a model architecture makes extensive use of Batch Normalization, Dropout can still be an effective way of improving performance in low data situations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{spilsburyDonIgnoreDropout2019a,
  title = {Don't Ignore {{Dropout}} in {{Fully Convolutional Networks}}},
  author = {Spilsbury, Thomas and Camps, Paavo},
  year = {2019},
  month = aug,
  number = {arXiv:1908.09162},
  eprint = {1908.09162},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-02-10},
  abstract = {Data for image segmentation models can be costly to obtain due to the precision required by human annotators. We run a series of experiments showing the effect of different kinds of Dropout training on the DeepLabv3+ image segmentation model when trained using a small dataset. We find that when appropriate forms of Dropout are applied in the right place in the model architecture that non-insignificant improvement in Mean Intersection over Union (mIoU) score can be observed. In our best case, we find that applying Dropout scheduling in conjunction with SpatialDropout improves baseline mIoU from 0.49 to 0.59. This result shows that even where a model architecture makes extensive use of Batch Normalization, Dropout can still be an effective way of improving performance in low data situations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\M9WULXKV\Spilsbury et Camps - 2019 - Don't ignore Dropout in Fully Convolutional Networ.pdf}
}

@article{srinivasanFullyAutomatedDetection2014,
  title = {Fully Automated Detection of Diabetic Macular Edema and Dry Age-Related Macular Degeneration from Optical Coherence Tomography Images},
  author = {Srinivasan, Pratul P. and Kim, Leo A. and Mettu, Priyatham S. and Cousins, Scott W. and Comer, Grant M. and Izatt, Joseph A. and Farsiu, Sina},
  year = {2014},
  month = oct,
  journal = {Biomedical Optics Express},
  volume = {5},
  number = {10},
  pages = {3568--3577},
  issn = {2156-7085},
  doi = {10.1364/BOE.5.003568},
  urldate = {2019-11-21},
  abstract = {We present a novel fully automated algorithm for the detection of retinal diseases via optical coherence tomography (OCT) imaging. Our algorithm utilizes multiscale histograms of oriented gradient descriptors as feature vectors of a support vector machine based classifier. The spectral domain OCT data sets used for cross-validation consisted of volumetric scans acquired from 45 subjects: 15 normal subjects, 15 patients with dry age-related macular degeneration (AMD), and 15 patients with diabetic macular edema (DME). Our classifier correctly identified 100\% of cases with AMD, 100\% cases with DME, and 86.67\% cases of normal subjects. This algorithm is a potentially impactful tool for the remote diagnosis of ophthalmic diseases.},
  copyright = {\&\#169; 2014 Optical Society of America},
  langid = {english},
  keywords = {Image processing,Nonlinear spatial filtering,Optical coherence tomography,Speckle noise,Speckle patterns,Spectral domain optical coherence tomography}
}

@article{srinivasanFullyAutomatedDetection2014a,
  title = {Fully Automated Detection of Diabetic Macular Edema and Dry Age-Related Macular Degeneration from Optical Coherence Tomography Images},
  author = {Srinivasan, Pratul P. and Kim, Leo A. and Mettu, Priyatham S. and Cousins, Scott W. and Comer, Grant M. and Izatt, Joseph A. and Farsiu, Sina},
  year = {2014},
  month = oct,
  journal = {Biomedical Optics Express},
  volume = {5},
  number = {10},
  pages = {3568--3577},
  issn = {2156-7085},
  doi = {10.1364/BOE.5.003568},
  urldate = {2019-11-21},
  abstract = {We present a novel fully automated algorithm for the detection of retinal diseases via optical coherence tomography (OCT) imaging. Our algorithm utilizes multiscale histograms of oriented gradient descriptors as feature vectors of a support vector machine based classifier. The spectral domain OCT data sets used for cross-validation consisted of volumetric scans acquired from 45 subjects: 15 normal subjects, 15 patients with dry age-related macular degeneration (AMD), and 15 patients with diabetic macular edema (DME). Our classifier correctly identified 100\% of cases with AMD, 100\% cases with DME, and 86.67\% cases of normal subjects. This algorithm is a potentially impactful tool for the remote diagnosis of ophthalmic diseases.},
  copyright = {\&\#169; 2014 Optical Society of America},
  langid = {english},
  keywords = {Image processing,Nonlinear spatial filtering,Optical coherence tomography,Speckle noise,Speckle patterns,Spectral domain optical coherence tomography},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\S9GSBIW4\\Srinivasan et al. - 2014 - Fully automated detection of diabetic macular edem.pdf;C\:\\Users\\cleme\\Zotero\\storage\\Q4V6LGU2\\abstract.html}
}

@misc{srivastavaHighwayNetworks2015,
  title = {Highway {{Networks}}},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  year = {2015},
  month = nov,
  number = {arXiv:1505.00387},
  eprint = {1505.00387},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.00387},
  urldate = {2023-02-22},
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
  archiveprefix = {arXiv},
  keywords = {68T01,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,G.1.6,I.2.6}
}

@misc{srivastavaHighwayNetworks2015a,
  title = {Highway {{Networks}}},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  year = {2015},
  month = nov,
  number = {arXiv:1505.00387},
  eprint = {1505.00387},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.00387},
  urldate = {2023-02-22},
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
  archiveprefix = {arXiv},
  keywords = {68T01,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,G.1.6,I.2.6},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\CJFEWACS\\Srivastava et al. - 2015 - Highway Networks.pdf;C\:\\Users\\cleme\\Zotero\\storage\\M5YBUV9X\\1505.html}
}

@article{staalRidgebasedVesselSegmentation2004,
  title = {Ridge-Based Vessel Segmentation in Color Images of the Retina},
  author = {Staal, J. and Abramoff, M.D. and Niemeijer, M. and Viergever, M.A. and {van Ginneken}, B.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {23},
  number = {4},
  pages = {501--509},
  issn = {1558-254X},
  doi = {10.1109/TMI.2004.825627},
  abstract = {A method is presented for automated segmentation of vessels in two-dimensional color images of the retina. This method can be used in computer analyses of retinal images, e.g., in automated screening for diabetic retinopathy. The system is based on extraction of image ridges, which coincide approximately with vessel centerlines. The ridges are used to compose primitives in the form of line elements. With the line elements an image is partitioned into patches by assigning each image pixel to the closest line element. Every line element constitutes a local coordinate frame for its corresponding patch. For every pixel, feature vectors are computed that make use of properties of the patches and the line elements. The feature vectors are classified using a kNN-classifier and sequential forward feature selection. The algorithm was tested on a database consisting of 40 manually labeled images. The method achieves an area under the receiver operating characteristic curve of 0.952. The method is compared with two recently published rule-based methods of Hoover et al. and Jiang et al. . The results show that our method is significantly better than the two rule-based methods (p{$<$}0.01). The accuracy of our method is 0.944 versus 0.947 for a second observer.},
  keywords = {Algorithms,Automated,automated segmentation,classifier,Cluster Analysis,Color,Computer-Assisted,Databases,Diabetes,diabetic retinopathy,Diabetic Retinopathy,Expert Systems,eye,Factual,feature extraction,Fluorescein Angiography,Humans,Image analysis,Image databases,Image Interpretation,image ridge extraction,image segmentation,Image segmentation,medical image processing,Ophthalmoscopy,Pattern Recognition,Pixel,Reproducibility of Results,retina,Retina,Retinal Vessels,Retinopathy,ridge-based vessel segmentation,Sensitivity and Specificity,sequential forward feature selection,Spatial databases,Testing,two-dimensional color images}
}

@article{staalRidgebasedVesselSegmentation2004a,
  title = {Ridge-Based Vessel Segmentation in Color Images of the Retina},
  author = {Staal, J. and Abramoff, M.D. and Niemeijer, M. and Viergever, M.A. and {van Ginneken}, B.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {23},
  number = {4},
  pages = {501--509},
  issn = {1558-254X},
  doi = {10.1109/TMI.2004.825627},
  abstract = {A method is presented for automated segmentation of vessels in two-dimensional color images of the retina. This method can be used in computer analyses of retinal images, e.g., in automated screening for diabetic retinopathy. The system is based on extraction of image ridges, which coincide approximately with vessel centerlines. The ridges are used to compose primitives in the form of line elements. With the line elements an image is partitioned into patches by assigning each image pixel to the closest line element. Every line element constitutes a local coordinate frame for its corresponding patch. For every pixel, feature vectors are computed that make use of properties of the patches and the line elements. The feature vectors are classified using a kNN-classifier and sequential forward feature selection. The algorithm was tested on a database consisting of 40 manually labeled images. The method achieves an area under the receiver operating characteristic curve of 0.952. The method is compared with two recently published rule-based methods of Hoover et al. and Jiang et al. . The results show that our method is significantly better than the two rule-based methods (p{$<$}0.01). The accuracy of our method is 0.944 versus 0.947 for a second observer.},
  keywords = {Algorithms,automated segmentation,classifier,Cluster Analysis,Color,Databases Factual,Diabetes,diabetic retinopathy,Diabetic Retinopathy,Expert Systems,eye,feature extraction,Fluorescein Angiography,Humans,Image analysis,Image databases,Image Interpretation Computer-Assisted,image ridge extraction,image segmentation,Image segmentation,medical image processing,Ophthalmoscopy,Pattern Recognition Automated,Pixel,Reproducibility of Results,retina,Retina,Retinal Vessels,Retinopathy,ridge-based vessel segmentation,Sensitivity and Specificity,sequential forward feature selection,Spatial databases,Testing,two-dimensional color images},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\3WIRGLX8\\1282003.html;C\:\\Users\\cleme\\Zotero\\storage\\L52NDCIN\\staal2004.html}
}

@article{strudelSegmenterTransformerSemantic2021,
  title = {Segmenter: {{Transformer}} for {{Semantic Segmentation}}},
  shorttitle = {Segmenter},
  author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  year = {2021},
  month = may,
  journal = {arXiv:2105.05633 [cs]},
  eprint = {2105.05633},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on the challenging ADE20K dataset and performs on-par on Pascal Context and Cityscapes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{strudelSegmenterTransformerSemantic2021a,
  title = {Segmenter: {{Transformer}} for {{Semantic Segmentation}}},
  shorttitle = {Segmenter},
  author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  year = {2021},
  month = may,
  journal = {arXiv:2105.05633 [cs]},
  eprint = {2105.05633},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on the challenging ADE20K dataset and performs on-par on Pascal Context and Cityscapes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{strudelSegmenterTransformerSemantic2021b,
  title = {Segmenter: {{Transformer}} for {{Semantic Segmentation}}},
  shorttitle = {Segmenter},
  author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  year = {2021},
  month = may,
  journal = {arXiv:2105.05633 [cs]},
  eprint = {2105.05633},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on the challenging ADE20K dataset and performs on-par on Pascal Context and Cityscapes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\4LS66HZJ\Strudel et al. - 2021 - Segmenter Transformer for Semantic Segmentation.pdf}
}

@article{strudelSegmenterTransformerSemantic2021c,
  title = {Segmenter: {{Transformer}} for {{Semantic Segmentation}}},
  shorttitle = {Segmenter},
  author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  year = {2021},
  month = may,
  journal = {arXiv:2105.05633 [cs]},
  eprint = {2105.05633},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on the challenging ADE20K dataset and performs on-par on Pascal Context and Cityscapes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\2DMLTCJJ\Strudel et al. - 2021 - Segmenter Transformer for Semantic Segmentation.pdf}
}

@article{stylesIntroducingAutomatedDiabetic2019,
  title = {Introducing Automated Diabetic Retinopathy Systems: {{It}}'s Not Just about Sensitivity and Specificity},
  shorttitle = {Introducing Automated Diabetic Retinopathy Systems},
  author = {Styles, Caroline Jane},
  year = {2019},
  month = sep,
  journal = {Eye (London, England)},
  volume = {33},
  number = {9},
  pages = {1357--1358},
  publisher = {Nature Publishing Group},
  issn = {1476-5454},
  doi = {10.1038/s41433-019-0535-7},
  urldate = {2021-04-12},
  copyright = {2019 The Royal College of Ophthalmologists},
  langid = {english}
}

@article{stylesIntroducingAutomatedDiabetic2019a,
  title = {Introducing Automated Diabetic Retinopathy Systems: {{It}}'s Not Just about Sensitivity and Specificity},
  shorttitle = {Introducing Automated Diabetic Retinopathy Systems},
  author = {Styles, Caroline Jane},
  year = {2019},
  month = sep,
  journal = {Eye (London, England)},
  volume = {33},
  number = {9},
  pages = {1357--1358},
  publisher = {Nature Publishing Group},
  issn = {1476-5454},
  doi = {10.1038/s41433-019-0535-7},
  urldate = {2021-11-29},
  copyright = {2019 The Royal College of Ophthalmologists},
  langid = {english},
  keywords = {Health care,Medical imaging}
}

@article{stylesIntroducingAutomatedDiabetic2019b,
  title = {Introducing Automated Diabetic Retinopathy Systems: It's Not Just about Sensitivity and Specificity},
  shorttitle = {Introducing Automated Diabetic Retinopathy Systems},
  author = {Styles, Caroline Jane},
  year = {2019},
  month = sep,
  journal = {Eye},
  volume = {33},
  number = {9},
  pages = {1357--1358},
  publisher = {Nature Publishing Group},
  issn = {1476-5454},
  doi = {10.1038/s41433-019-0535-7},
  urldate = {2021-11-29},
  copyright = {2019 The Royal College of Ophthalmologists},
  langid = {english},
  keywords = {Health care,Medical imaging},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Editorial\\
Subject\_term: Health care;Medical imaging\\
Subject\_term\_id: health-care;medical-imaging},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\KWESSVZF\\Styles - 2019 - Introducing automated diabetic retinopathy systems.pdf;C\:\\Users\\cleme\\Zotero\\storage\\SS76NV4Y\\s41433-019-0535-7.html}
}

@article{stylesIntroducingAutomatedDiabetic2019c,
  title = {Introducing Automated Diabetic Retinopathy Systems: It's Not Just about Sensitivity and Specificity},
  shorttitle = {Introducing Automated Diabetic Retinopathy Systems},
  author = {Styles, Caroline Jane},
  year = {2019},
  month = sep,
  journal = {Eye},
  volume = {33},
  number = {9},
  pages = {1357--1358},
  publisher = {Nature Publishing Group},
  issn = {1476-5454},
  doi = {10.1038/s41433-019-0535-7},
  urldate = {2021-04-12},
  copyright = {2019 The Royal College of Ophthalmologists},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\TGLT93VE\\Styles - 2019 - Introducing automated diabetic retinopathy systems.pdf;C\:\\Users\\cleme\\Zotero\\storage\\F7UQMPGL\\s41433-019-0535-7.html}
}

@article{sudreGeneralisedDiceOverlap2017,
  title = {Generalised {{Dice Overlap}} as a {{Deep Learning Loss Function}} for {{Highly Unbalanced Segmentations}}},
  author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, Sebastien and Jorge Cardoso, M.},
  year = {2017},
  journal = {Deep learning in medical image analysis and multimodal learning for clinical decision support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, held in conjunction with MICCAI 2017 Quebec City, QC,...},
  volume = {2017},
  pages = {240--248},
  doi = {10.1007/978-3-319-67558-9_28},
  abstract = {Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.},
  langid = {english},
  pmcid = {PMC7610921},
  pmid = {34104926}
}

@article{sudreGeneralisedDiceOverlap2017a,
  title = {Generalised {{Dice Overlap}} as a {{Deep Learning Loss Function}} for {{Highly Unbalanced Segmentations}}},
  author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, Sebastien and Jorge Cardoso, M.},
  year = {2017},
  journal = {Deep learning in medical image analysis and multimodal learning for clinical decision support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, held in conjunction with MICCAI 2017 Quebec City, QC,...},
  volume = {2017},
  pages = {240--248},
  doi = {10.1007/978-3-319-67558-9_28},
  abstract = {Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.},
  langid = {english},
  pmcid = {PMC7610921},
  pmid = {34104926},
  file = {C:\Users\cleme\Zotero\storage\4ZISD9AW\Sudre et al. - 2017 - Generalised Dice Overlap as a Deep Learning Loss F.pdf}
}

@article{sundararajanAxiomaticAttributionDeep,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  pages = {10},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  langid = {english}
}

@inproceedings{sundararajanAxiomaticAttributionDeep2017,
  title = {Axiomatic Attribution for Deep Networks},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  month = aug,
  series = {{{ICML}}'17},
  pages = {3319--3328},
  publisher = {JMLR.org},
  address = {Sydney, NSW, Australia},
  urldate = {2023-06-06},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms--- Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.}
}

@inproceedings{sundararajanAxiomaticAttributionDeep2017a,
  title = {Axiomatic Attribution for Deep Networks},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  month = aug,
  series = {{{ICML}}'17},
  pages = {3319--3328},
  publisher = {JMLR.org},
  address = {Sydney, NSW, Australia},
  urldate = {2023-06-06},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms--- Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  file = {C:\Users\cleme\Zotero\storage\TMGSYEBG\Sundararajan et al. - 2017 - Axiomatic attribution for deep networks.pdf}
}

@article{sundararajanAxiomaticAttributionDeepa,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  pages = {10},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\KZXGXA6Y\Sundararajan et al. - Axiomatic Attribution for Deep Networks.pdf}
}

@inproceedings{sunLesionAwareTransformersDiabetic2021,
  title = {Lesion-{{Aware Transformers}} for {{Diabetic Retinopathy Grading}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sun, Rui and Li, Yihao and Zhang, Tianzhu and Mao, Zhendong and Wu, Feng and Zhang, Yongdong},
  year = {2021},
  month = jun,
  pages = {10933--10942},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.01079},
  abstract = {Diabetic retinopathy (DR) is the leading cause of permanent blindness in the working-age population. And automatic DR diagnosis can assist ophthalmologists to design tailored treatments for patients, including DR grading and lesion discovery. However, most of existing methods treat DR grading and lesion discovery as two independent tasks, which require lesion annotations as a learning guidance and limits the actual deployment. To alleviate this problem, we propose a novel lesion-aware transformer (LAT) for DR grading and lesion discovery jointly in a unified deep model via an encoder-decoder structure including a pixel relation based encoder and a lesion filter based decoder. The proposed LAT enjoys several merits. First, to the best of our knowledge, this is the first work to formulate lesion discovery as a weakly supervised lesion localization problem via a transformer decoder. Second, to learn lesion filters well with only image-level labels, we design two effective mechanisms including lesion region importance and lesion region diversity for identifying diverse lesion regions. Extensive experimental results on three challenging benchmarks including Messidor-1, Messidor-2 and EyePACS demonstrate that the proposed LAT performs favorably against state-of-the-art DR grading and lesion discovery methods.},
  keywords = {Benchmark testing,Decoding,Diabetes,Pattern recognition,Retinopathy,Sociology,Transformers}
}

@inproceedings{sunLesionAwareTransformersDiabetic2021a,
  title = {Lesion-{{Aware Transformers}} for {{Diabetic Retinopathy Grading}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sun, Rui and Li, Yihao and Zhang, Tianzhu and Mao, Zhendong and Wu, Feng and Zhang, Yongdong},
  year = {2021},
  month = jun,
  pages = {10933--10942},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.01079},
  abstract = {Diabetic retinopathy (DR) is the leading cause of permanent blindness in the working-age population. And automatic DR diagnosis can assist ophthalmologists to design tailored treatments for patients, including DR grading and lesion discovery. However, most of existing methods treat DR grading and lesion discovery as two independent tasks, which require lesion annotations as a learning guidance and limits the actual deployment. To alleviate this problem, we propose a novel lesion-aware transformer (LAT) for DR grading and lesion discovery jointly in a unified deep model via an encoder-decoder structure including a pixel relation based encoder and a lesion filter based decoder. The proposed LAT enjoys several merits. First, to the best of our knowledge, this is the first work to formulate lesion discovery as a weakly supervised lesion localization problem via a transformer decoder. Second, to learn lesion filters well with only image-level labels, we design two effective mechanisms including lesion region importance and lesion region diversity for identifying diverse lesion regions. Extensive experimental results on three challenging benchmarks including Messidor-1, Messidor-2 and EyePACS demonstrate that the proposed LAT performs favorably against state-of-the-art DR grading and lesion discovery methods.},
  keywords = {Benchmark testing,Decoding,Diabetes,Pattern recognition,Retinopathy,Sociology,Transformers}
}

@inproceedings{sunLesionAwareTransformersDiabetic2021b,
  title = {Lesion-{{Aware Transformers}} for {{Diabetic Retinopathy Grading}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sun, Rui and Li, Yihao and Zhang, Tianzhu and Mao, Zhendong and Wu, Feng and Zhang, Yongdong},
  year = {2021},
  month = jun,
  pages = {10933--10942},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.01079},
  abstract = {Diabetic retinopathy (DR) is the leading cause of permanent blindness in the working-age population. And automatic DR diagnosis can assist ophthalmologists to design tailored treatments for patients, including DR grading and lesion discovery. However, most of existing methods treat DR grading and lesion discovery as two independent tasks, which require lesion annotations as a learning guidance and limits the actual deployment. To alleviate this problem, we propose a novel lesion-aware transformer (LAT) for DR grading and lesion discovery jointly in a unified deep model via an encoder-decoder structure including a pixel relation based encoder and a lesion filter based decoder. The proposed LAT enjoys several merits. First, to the best of our knowledge, this is the first work to formulate lesion discovery as a weakly supervised lesion localization problem via a transformer decoder. Second, to learn lesion filters well with only image-level labels, we design two effective mechanisms including lesion region importance and lesion region diversity for identifying diverse lesion regions. Extensive experimental results on three challenging benchmarks including Messidor-1, Messidor-2 and EyePACS demonstrate that the proposed LAT performs favorably against state-of-the-art DR grading and lesion discovery methods.},
  keywords = {Benchmark testing,Decoding,Diabetes,Pattern recognition,Retinopathy,Sociology,Transformers}
}

@inproceedings{sunLesionAwareTransformersDiabetic2021c,
  title = {Lesion-{{Aware Transformers}} for {{Diabetic Retinopathy Grading}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sun, Rui and Li, Yihao and Zhang, Tianzhu and Mao, Zhendong and Wu, Feng and Zhang, Yongdong},
  year = {2021},
  month = jun,
  pages = {10933--10942},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.01079},
  abstract = {Diabetic retinopathy (DR) is the leading cause of permanent blindness in the working-age population. And automatic DR diagnosis can assist ophthalmologists to design tailored treatments for patients, including DR grading and lesion discovery. However, most of existing methods treat DR grading and lesion discovery as two independent tasks, which require lesion annotations as a learning guidance and limits the actual deployment. To alleviate this problem, we propose a novel lesion-aware transformer (LAT) for DR grading and lesion discovery jointly in a unified deep model via an encoder-decoder structure including a pixel relation based encoder and a lesion filter based decoder. The proposed LAT enjoys several merits. First, to the best of our knowledge, this is the first work to formulate lesion discovery as a weakly supervised lesion localization problem via a transformer decoder. Second, to learn lesion filters well with only image-level labels, we design two effective mechanisms including lesion region importance and lesion region diversity for identifying diverse lesion regions. Extensive experimental results on three challenging benchmarks including Messidor-1, Messidor-2 and EyePACS demonstrate that the proposed LAT performs favorably against state-of-the-art DR grading and lesion discovery methods.},
  keywords = {Benchmark testing,Decoding,Diabetes,Pattern recognition,Retinopathy,Sociology,Transformers},
  file = {C:\Users\cleme\Zotero\storage\3TX6LEG3\9578017.html}
}

@inproceedings{sunLesionAwareTransformersDiabetic2021d,
  title = {Lesion-{{Aware Transformers}} for {{Diabetic Retinopathy Grading}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sun, Rui and Li, Yihao and Zhang, Tianzhu and Mao, Zhendong and Wu, Feng and Zhang, Yongdong},
  year = {2021},
  month = jun,
  pages = {10933--10942},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.01079},
  abstract = {Diabetic retinopathy (DR) is the leading cause of permanent blindness in the working-age population. And automatic DR diagnosis can assist ophthalmologists to design tailored treatments for patients, including DR grading and lesion discovery. However, most of existing methods treat DR grading and lesion discovery as two independent tasks, which require lesion annotations as a learning guidance and limits the actual deployment. To alleviate this problem, we propose a novel lesion-aware transformer (LAT) for DR grading and lesion discovery jointly in a unified deep model via an encoder-decoder structure including a pixel relation based encoder and a lesion filter based decoder. The proposed LAT enjoys several merits. First, to the best of our knowledge, this is the first work to formulate lesion discovery as a weakly supervised lesion localization problem via a transformer decoder. Second, to learn lesion filters well with only image-level labels, we design two effective mechanisms including lesion region importance and lesion region diversity for identifying diverse lesion regions. Extensive experimental results on three challenging benchmarks including Messidor-1, Messidor-2 and EyePACS demonstrate that the proposed LAT performs favorably against state-of-the-art DR grading and lesion discovery methods.},
  keywords = {Benchmark testing,Decoding,Diabetes,Pattern recognition,Retinopathy,Sociology,Transformers},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\FBVMQUAH\\Sun et al. - 2021 - Lesion-Aware Transformers for Diabetic Retinopathy.pdf;C\:\\Users\\cleme\\Zotero\\storage\\JYGZ9IDN\\9578017.html}
}

@inproceedings{sunLesionAwareTransformersDiabetic2021e,
  title = {Lesion-{{Aware Transformers}} for {{Diabetic Retinopathy Grading}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sun, Rui and Li, Yihao and Zhang, Tianzhu and Mao, Zhendong and Wu, Feng and Zhang, Yongdong},
  year = {2021},
  month = jun,
  pages = {10933--10942},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.01079},
  abstract = {Diabetic retinopathy (DR) is the leading cause of permanent blindness in the working-age population. And automatic DR diagnosis can assist ophthalmologists to design tailored treatments for patients, including DR grading and lesion discovery. However, most of existing methods treat DR grading and lesion discovery as two independent tasks, which require lesion annotations as a learning guidance and limits the actual deployment. To alleviate this problem, we propose a novel lesion-aware transformer (LAT) for DR grading and lesion discovery jointly in a unified deep model via an encoder-decoder structure including a pixel relation based encoder and a lesion filter based decoder. The proposed LAT enjoys several merits. First, to the best of our knowledge, this is the first work to formulate lesion discovery as a weakly supervised lesion localization problem via a transformer decoder. Second, to learn lesion filters well with only image-level labels, we design two effective mechanisms including lesion region importance and lesion region diversity for identifying diverse lesion regions. Extensive experimental results on three challenging benchmarks including Messidor-1, Messidor-2 and EyePACS demonstrate that the proposed LAT performs favorably against state-of-the-art DR grading and lesion discovery methods.},
  keywords = {Benchmark testing,Decoding,Diabetes,Pattern recognition,Retinopathy,Sociology,Transformers}
}

@article{sunOCTAngiographyMetrics2019,
  title = {{{OCT Angiography Metrics Predict Progression}} of {{Diabetic Retinopathy}} and {{Development}} of {{Diabetic Macular Edema}}: {{A Prospective Study}}},
  shorttitle = {{{OCT Angiography Metrics Predict Progression}} of {{Diabetic Retinopathy}} and {{Development}} of {{Diabetic Macular Edema}}},
  author = {Sun, Zihan and Tang, Fangyao and Wong, Raymond and Lok, Jerry and Szeto, Simon K. H. and Chan, Jason C. K. and Chan, Carmen K. M. and Tham, Clement C. and Ng, Danny S. and Cheung, Carol Y.},
  year = {2019},
  month = dec,
  journal = {Ophthalmology},
  volume = {126},
  number = {12},
  pages = {1675--1684},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2019.06.016},
  urldate = {2022-06-29},
  abstract = {Purpose To prospectively determine the relationship of OCT angiography (OCTA) metrics to diabetic retinopathy (DR) progression and development of diabetic macular edema (DME). Design Prospective, observational study. Participants A total of 205 eyes from 129 patients with diabetes mellitus followed up for at least 2 years. Methods All participants underwent OCTA with a swept-source OCT device (DRI-OCT Triton, Topcon, Inc, Tokyo, Japan). Individual OCTA images of superficial capillary plexus (SCP) and deep capillary plexus (DCP) were generated by IMAGEnet6 (Basic License 10). After a quality check, automated measurements of foveal avascular zone (FAZ) area, FAZ circularity, vessel density (VD), and fractal dimension (FD) of both SCP and DCP were then obtained. Main Outcome Measures Progression of DR and development of DME. Results Over a median follow-up of 27.14 months (interquartile range, 24.16--30.41 months), 28 of the 205 eyes (13.66\%) developed DR progression. Of the 194 eyes without DME at baseline, 17 (8.76\%) developed DME. Larger FAZ area (hazard ratio [HR], 1.829 per SD increase; 95\% confidence interval [CI], 1.332--2.512), lower VD (HR, 1.908 per SD decrease; 95\% CI, 1.303--2.793), and lower FD (HR, 4.464 per SD decrease; 95\% CI, 1.337--14.903) of DCP were significantly associated with DR progression after adjusting for established risk factors (DR severity, glycated hemoglobin, duration of diabetes, age, and mean arterial blood pressure at baseline). Lower VD of SCP (HR, 1.789 per SD decrease; 95\% CI, 1.027--4.512) was associated with DME development. Compared with the model with established risk factors alone, the addition of OCTA metrics improved the predictive discrimination of DR progression (FAZ area of DCP, C-statistics 0.723 vs. 0.677, P {$<$}{\textasciitilde}0.001; VD of DCP, C-statistics 0.727 vs. 0.677, P = 0.001; FD of DCP, C-statistics 0.738 vs. 0.677, P {$<$} 0.001) and DME development (VD of SCP, C-statistics 0.904 vs. 0.875, P = 0.036). Conclusions The FAZ area, VD, and FD of DCP predict DR progression, whereas VD of SCP predicts DME development. Our findings provide evidence to support that OCTA metrics improve the evaluation of risk of DR progression and DME development beyond traditional risk factors.},
  langid = {english}
}

@article{sunOCTAngiographyMetrics2019a,
  title = {{{OCT Angiography Metrics Predict Progression}} of {{Diabetic Retinopathy}} and {{Development}} of {{Diabetic Macular Edema}}: {{A Prospective Study}}},
  shorttitle = {{{OCT Angiography Metrics Predict Progression}} of {{Diabetic Retinopathy}} and {{Development}} of {{Diabetic Macular Edema}}},
  author = {Sun, Zihan and Tang, Fangyao and Wong, Raymond and Lok, Jerry and Szeto, Simon K. H. and Chan, Jason C. K. and Chan, Carmen K. M. and Tham, Clement C. and Ng, Danny S. and Cheung, Carol Y.},
  year = {2019},
  month = dec,
  journal = {Ophthalmology},
  volume = {126},
  number = {12},
  pages = {1675--1684},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2019.06.016},
  urldate = {2022-06-29},
  abstract = {Purpose To prospectively determine the relationship of OCT angiography (OCTA) metrics to diabetic retinopathy (DR) progression and development of diabetic macular edema (DME). Design Prospective, observational study. Participants A total of 205 eyes from 129 patients with diabetes mellitus followed up for at least 2 years. Methods All participants underwent OCTA with a swept-source OCT device (DRI-OCT Triton, Topcon, Inc, Tokyo, Japan). Individual OCTA images of superficial capillary plexus (SCP) and deep capillary plexus (DCP) were generated by IMAGEnet6 (Basic License 10). After a quality check, automated measurements of foveal avascular zone (FAZ) area, FAZ circularity, vessel density (VD), and fractal dimension (FD) of both SCP and DCP were then obtained. Main Outcome Measures Progression of DR and development of DME. Results Over a median follow-up of 27.14 months (interquartile range, 24.16--30.41 months), 28 of the 205 eyes (13.66\%) developed DR progression. Of the 194 eyes without DME at baseline, 17 (8.76\%) developed DME. Larger FAZ area (hazard ratio [HR], 1.829 per SD increase; 95\% confidence interval [CI], 1.332--2.512), lower VD (HR, 1.908 per SD decrease; 95\% CI, 1.303--2.793), and lower FD (HR, 4.464 per SD decrease; 95\% CI, 1.337--14.903) of DCP were significantly associated with DR progression after adjusting for established risk factors (DR severity, glycated hemoglobin, duration of diabetes, age, and mean arterial blood pressure at baseline). Lower VD of SCP (HR, 1.789 per SD decrease; 95\% CI, 1.027--4.512) was associated with DME development. Compared with the model with established risk factors alone, the addition of OCTA metrics improved the predictive discrimination of DR progression (FAZ area of DCP, C-statistics 0.723 vs. 0.677, P~{$<~$}0.001; VD of DCP, C-statistics 0.727 vs. 0.677, P~= 0.001; FD of DCP, C-statistics 0.738 vs. 0.677, P {$<$} 0.001) and DME development (VD of SCP, C-statistics 0.904 vs. 0.875, P~= 0.036). Conclusions The FAZ area, VD, and FD of DCP predict DR progression, whereas VD of SCP predicts DME development. Our findings provide evidence to support that OCTA metrics improve the evaluation of risk of DR progression and DME development beyond traditional risk factors.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\M5SETFI9\\Sun et al. - 2019 - OCT Angiography Metrics Predict Progression of Dia.pdf;C\:\\Users\\cleme\\Zotero\\storage\\T2RFQNNN\\S0161642019306086.html}
}

@article{sunUpdatingStagingSystem2021,
  title = {Updating the {{Staging System}} for {{Diabetic Retinal Disease}}},
  author = {Sun, Jennifer K. and Aiello, Lloyd Paul and Abr{\`a}moff, Michael D. and Antonetti, David A and Dutta, Sanjoy and Pragnell, Marlon and Levine, S. Robert and Gardner, Thomas W},
  year = {2021},
  month = apr,
  journal = {Ophthalmology},
  volume = {128},
  number = {4},
  pages = {490--493},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2020.10.008},
  urldate = {2023-08-25},
  pmcid = {PMC8378594},
  pmid = {33218709}
}

@article{sunUpdatingStagingSystem2021a,
  title = {Updating the {{Staging System}} for {{Diabetic Retinal Disease}}},
  author = {Sun, Jennifer K. and Aiello, Lloyd Paul and Abr{\`a}moff, Michael D. and Antonetti, David A and Dutta, Sanjoy and Pragnell, Marlon and Levine, S. Robert and Gardner, Thomas W},
  year = {2021},
  month = apr,
  journal = {Ophthalmology},
  volume = {128},
  number = {4},
  pages = {490--493},
  issn = {0161-6420},
  doi = {10.1016/j.ophtha.2020.10.008},
  urldate = {2023-08-25},
  pmcid = {PMC8378594},
  pmid = {33218709},
  file = {C:\Users\cleme\Zotero\storage\Q9XNC77G\Sun et al. - 2021 - Updating the Staging System for Diabetic Retinal D.pdf}
}

@article{suriyasekeranAlgorithmsDiagnosisDiabetic2021,
  title = {Algorithms for {{Diagnosis}} of {{Diabetic Retinopathy}} and {{Diabetic Macula Edema- A Review}}},
  author = {Suriyasekeran, Karkuzhali and Santhanamahalingam, Senthilkumar and Duraisamy, Manimegalai},
  year = {2021},
  journal = {Advances in Experimental Medicine and Biology},
  volume = {1307},
  pages = {357--373},
  issn = {0065-2598},
  doi = {10.1007/5584_2020_499},
  abstract = {Human eye is one of the important organs in human body, with iris, pupil, sclera, cornea, lens, retina and optic nerve. Many important eye diseases as well as systemic diseases manifest themselves in the retina. The most widespread causes of blindness in the industrialized world are glaucoma, Age Related Macular Degeneration (ARMD), Diabetic Retinopathy (DR) and Diabetic Macula Edema (DME). The development of a retinal image analysis system is a demanding research topic for early detection, progression analysis and diagnosis of eye diseases. Early diagnosis and treatment of retinal diseases are essential to prevent vision loss. The huge and growing number of retinal disease affected patients, cost of current hospital-based detection methods (by eye care specialists) and scarcity in the number of ophthalmologists are the barriers to achieve the recommended screening compliance in the patient who is at the risk of retinal diseases. Developing an automated system which uses pattern recognition, computer vision and machine learning to diagnose retinal diseases is a potential solution to this problem. Damage to the tiny blood vessels in the retina in the posterior part of the eye due to diabetes is named as DR. Diabetes is a disease which occurs when the pancreas does not secrete enough insulin or the body does not utilize it properly. This disease slowly affects the circulatory system including that of the retina. As diabetes intensifies, the vision of a patient may start deteriorating and leading to DR. The retinal landmarks like OD and blood vessels, white lesions and red lesions are segmented to develop automated screening system for DR. DME is an advanced symptom of DR that can lead to irreversible vision loss. DME is a general term defined as retinal thickening or exudates present within 2 disk diameter of the fovea center; it can either focal or diffuse DME in distribution. In this paper, review the algorithms used in diagnosis of DR and DME.},
  langid = {english},
  pmid = {32166636},
  keywords = {Algorithms,Blood vessels,Classification,Diabetes Mellitus,Diabetic edema,Diabetic retinopathy,Diabetic Retinopathy,Edema,Exudates,Hemorrhages,Humans,Macula,Macular Edema,Microanaurysms,Optic disc,Retina,Segmentation}
}

@article{suriyasekeranAlgorithmsDiagnosisDiabetic2021a,
  title = {Algorithms for {{Diagnosis}} of {{Diabetic Retinopathy}} and {{Diabetic Macula Edema- A Review}}},
  author = {Suriyasekeran, Karkuzhali and Santhanamahalingam, Senthilkumar and Duraisamy, Manimegalai},
  year = {2021},
  journal = {Advances in Experimental Medicine and Biology},
  volume = {1307},
  pages = {357--373},
  issn = {0065-2598},
  doi = {10.1007/5584_2020_499},
  abstract = {Human eye is one of the important organs in human body, with iris, pupil, sclera, cornea, lens, retina and optic nerve. Many important eye diseases as well as systemic diseases manifest themselves in the retina. The most widespread causes of blindness in the industrialized world are glaucoma, Age Related Macular Degeneration (ARMD), Diabetic Retinopathy (DR) and Diabetic Macula Edema (DME). The development of a retinal image analysis system is a demanding research topic for early detection, progression analysis and diagnosis of eye diseases. Early diagnosis and treatment of retinal diseases are essential to prevent vision loss. The huge and growing number of retinal disease affected patients, cost of current hospital-based detection methods (by eye care specialists) and scarcity in the number of ophthalmologists are the barriers to achieve the recommended screening compliance in the patient who is at the risk of retinal diseases. Developing an automated system which uses pattern recognition, computer vision and machine learning to diagnose retinal diseases is a potential solution to this problem. Damage to the tiny blood vessels in the retina in the posterior part of the eye due to diabetes is named as DR. Diabetes is a disease which occurs when the pancreas does not secrete enough insulin or the body does not utilize it properly. This disease slowly affects the circulatory system including that of the retina. As diabetes intensifies, the vision of a patient may start deteriorating and leading to DR. The retinal landmarks like OD and blood vessels, white lesions and red lesions are segmented to develop automated screening system for DR. DME is an advanced symptom of DR that can lead to irreversible vision loss. DME is a general term defined as retinal thickening or exudates present within 2 disk diameter of the fovea center; it can either focal or diffuse DME in distribution. In this paper, review the algorithms used in diagnosis of DR and DME.},
  langid = {english},
  pmid = {32166636},
  keywords = {Algorithms,Blood vessels,Classification,Diabetes Mellitus,Diabetic edema,Diabetic retinopathy,Diabetic Retinopathy,Edema,Exudates,Hemorrhages,Humans,Macula,Macular Edema,Microanaurysms,Optic disc,Retina,Segmentation}
}

@article{susanvanGlobalBurdenDiabetes2010,
  title = {The Global Burden of Diabetes and Its Complications: {{An}} Emerging Pandemic},
  shorttitle = {The Global Burden of Diabetes and Its Complications},
  author = {{Susan van}, Dieren and Beulens, Joline W.J. and {Yvonne T. van der}, Schouw and Grobbee, Diederick E. and Nealb, Bruce},
  year = {2010},
  month = may,
  journal = {European Journal of Cardiovascular Prevention \& Rehabilitation},
  volume = {17},
  number = {1\_suppl},
  pages = {s3-s8},
  issn = {1741-8267},
  doi = {10.1097/01.hjr.0000368191.86614.5a},
  urldate = {2019-11-19},
  abstract = {The number of patients with type 2 diabetes is increasing rapidly in both developed and developing countries around the world. The emerging pandemic is driven by the combined effects of population ageing, rising levels of obesity and inactivity, and greater longevity among patients with diabetes that is attributable to improved management. The vascular complications of type 2 diabetes account for the majority of the social and economic burden among patients and society more broadly. This review summarizes the burden of type 2 diabetes, impaired glucose tolerance, and their vascular complications. It is projected that by 2025 there will be 380 million people with type 2 diabetes and 418 million people with impaired glucose tolerance. Diabetes is a major global cause of premature mortality that is widely underestimated, because only a minority of persons with diabetes dies from a cause uniquely related to the condition. Approximately one half of patients with type 2 diabetes die prematurely of a cardiovascular cause and approximately 10\% die of renal failure. Global excess mortality attributable to diabetes in adults was estimated to be 3.8 million deaths. Eur J Cardiovasc Prev Rehabil 17 (Suppl 1):S3-S8 {\copyright} 2010 The European Society of Cardiology},
  langid = {english}
}

@article{susanvanGlobalBurdenDiabetes2010a,
  title = {The Global Burden of Diabetes and Its Complications: An Emerging Pandemic},
  shorttitle = {The Global Burden of Diabetes and Its Complications},
  author = {{Susan van}, Dieren and Beulens, Joline W.J. and {Yvonne T. van der}, Schouw and Grobbee, Diederick E. and Nealb, Bruce},
  year = {2010},
  month = may,
  journal = {European Journal of Cardiovascular Prevention \& Rehabilitation},
  volume = {17},
  number = {1\_suppl},
  pages = {s3-s8},
  issn = {1741-8267},
  doi = {10.1097/01.hjr.0000368191.86614.5a},
  urldate = {2019-11-19},
  abstract = {The number of patients with type 2 diabetes is increasing rapidly in both developed and developing countries around the world. The emerging pandemic is driven by the combined effects of population ageing, rising levels of obesity and inactivity, and greater longevity among patients with diabetes that is attributable to improved management. The vascular complications of type 2 diabetes account for the majority of the social and economic burden among patients and society more broadly. This review summarizes the burden of type 2 diabetes, impaired glucose tolerance, and their vascular complications. It is projected that by 2025 there will be 380 million people with type 2 diabetes and 418 million people with impaired glucose tolerance. Diabetes is a major global cause of premature mortality that is widely underestimated, because only a minority of persons with diabetes dies from a cause uniquely related to the condition. Approximately one half of patients with type 2 diabetes die prematurely of a cardiovascular cause and approximately 10\% die of renal failure. Global excess mortality attributable to diabetes in adults was estimated to be 3.8 million deaths. Eur J Cardiovasc Prev Rehabil 17 (Suppl 1):S3-S8 {\copyright} 2010 The European Society of Cardiology},
  langid = {english}
}

@inproceedings{swiebocka-wiekDetectionRetinaLesions2017,
  title = {The {{Detection}} of the {{Retina}}'s {{Lesions}} in {{Optical Coherence Tomography}} ({{OCT}})},
  booktitle = {Information {{Technology}} and {{Computational Physics}}},
  author = {{Swiebocka-Wiek}, Joanna},
  editor = {Kulczycki, Piotr and K{\'o}czy, L{\'a}szl{\'o} T. and Mesiar, Radko and Kacprzyk, Janusz},
  year = {2017},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {179--195},
  publisher = {Springer International Publishing},
  abstract = {The Optical Coherence Tomography (OCT) is a very modern, noninvasive, and noncontact optical imaging technique. It is dedicated to different types of ocular tissues such as for example: the retina, optic disk, or cornea. During the examination, OCT is used for the early diagnosis of diseases such as: glaucoma, macular degeneration (AMD), diabetic changes in the retina, macular hole, macular edema, and eye cancer. These may inevitably lead to blindness, hence it is so important to ensure the patient of early and accurate diagnosis. The main goal of this paper is to propose a preliminary, automated method of detecting the occurrence of pathological changes in the retina (cysts and inflammation).},
  isbn = {978-3-319-44260-0},
  langid = {english},
  keywords = {Cystoid Macular Edema,Macular Hole,Optical Coherence Tomography,Optical Coherence Tomography Image,Retina Nerve Fiber Layer}
}

@inproceedings{swiebocka-wiekDetectionRetinaLesions2017a,
  title = {The {{Detection}} of the {{Retina}}'s {{Lesions}} in {{Optical Coherence Tomography}} ({{OCT}})},
  booktitle = {Information {{Technology}} and {{Computational Physics}}},
  author = {{Swiebocka-Wiek}, Joanna},
  editor = {Kulczycki, Piotr and K{\'o}czy, L{\'a}szl{\'o} T. and Mesiar, Radko and Kacprzyk, Janusz},
  year = {2017},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {179--195},
  publisher = {Springer International Publishing},
  abstract = {The Optical Coherence Tomography (OCT) is a very modern, noninvasive, and noncontact optical imaging technique. It is dedicated to different types of ocular tissues such as for example: the retina, optic disk, or cornea. During the examination, OCT is used for the early diagnosis of diseases such as: glaucoma, macular degeneration (AMD), diabetic changes in the retina, macular hole, macular edema, and eye cancer. These may inevitably lead to blindness, hence it is so important to ensure the patient of early and accurate diagnosis. The main goal of this paper is to propose a preliminary, automated method of detecting the occurrence of pathological changes in the retina (cysts and inflammation).},
  isbn = {978-3-319-44260-0},
  langid = {english},
  keywords = {Cystoid Macular Edema,Macular Hole,Optical Coherence Tomography,Optical Coherence Tomography Image,Retina Nerve Fiber Layer}
}

@inproceedings{szegedyGoingDeeperConvolutions2015,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  month = jun,
  pages = {1--9},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298594},
  urldate = {2019-09-29},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  isbn = {978-1-4673-6964-0},
  langid = {english}
}

@inproceedings{szegedyGoingDeeperConvolutions2015a,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  month = jun,
  pages = {1--9},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298594},
  urldate = {2023-02-17},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  isbn = {978-1-4673-6964-0},
  langid = {english}
}

@inproceedings{szegedyGoingDeeperConvolutions2015b,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  month = jun,
  pages = {1--9},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298594},
  urldate = {2019-09-29},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\3CNGV9WR\Szegedy et al. - 2015 - Going deeper with convolutions.pdf}
}

@inproceedings{szegedyGoingDeeperConvolutions2015c,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  month = jun,
  pages = {1--9},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298594},
  urldate = {2023-02-17},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\N5K7KADK\Szegedy et al. - 2015 - Going deeper with convolutions.pdf}
}

@inproceedings{szegedyIntriguingPropertiesNeural2014,
  title = {Intriguing Properties of Neural Networks},
  booktitle = {2nd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2014, {{Banff}}, {{AB}}, {{Canada}}, {{April}} 14-16, 2014, {{Conference Track Proceedings}}},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2014},
  urldate = {2023-11-30}
}

@inproceedings{szegedyIntriguingPropertiesNeural2014a,
  title = {Intriguing Properties of Neural Networks},
  booktitle = {2nd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2014, {{Banff}}, {{AB}}, {{Canada}}, {{April}} 14-16, 2014, {{Conference Track Proceedings}}},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2014},
  urldate = {2023-11-30}
}

@inproceedings{szegedyRethinkingInceptionArchitecture2016,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, C. and Vanhoucke, V. and Ioffe, S. and Shlens, J. and Wojna, Z.},
  year = {2016},
  month = jun,
  pages = {2818--2826},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.308},
  abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2\% top-1 and 5:6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5\% top-5 error and 17:3\% top-1 error on the validation set and 3:6\% top-5 error on the official test set.},
  keywords = {Benchmark testing,Computational efficiency,Computational modeling,Computer architecture,computer vision,Computer vision,Convolution,deep convolutional networks,ILSVRC 2012 classification challenge validation set,image classification,inception architecture,neural nets,Training}
}

@inproceedings{szegedyRethinkingInceptionArchitecture2016a,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, C. and Vanhoucke, V. and Ioffe, S. and Shlens, J. and Wojna, Z.},
  year = {2016},
  month = jun,
  pages = {2818--2826},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.308},
  abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2\% top-1 and 5:6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5\% top-5 error and 17:3\% top-1 error on the validation set and 3:6\% top-5 error on the official test set.},
  keywords = {Benchmark testing,Computational efficiency,Computational modeling,Computer architecture,computer vision,Computer vision,Convolution,deep convolutional networks,ILSVRC 2012 classification challenge validation set,image classification,inception architecture,neural nets,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\VKI8H7AD\\Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf;C\:\\Users\\cleme\\Zotero\\storage\\GXACYNAG\\7780677.html}
}

@article{tanAutomatedSegmentationExudates2017,
  title = {Automated Segmentation of Exudates, Haemorrhages, Microaneurysms Using Single Convolutional Neural Network},
  author = {Tan, Jen Hong and Fujita, Hamido and Sivaprasad, Sobha and Bhandary, Sulatha V. and Rao, A. Krishna and Chua, Kuang Chua and Acharya, U. Rajendra},
  year = {2017},
  month = dec,
  journal = {Information Sciences},
  volume = {420},
  pages = {66--76},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2017.08.050},
  urldate = {2019-07-24},
  abstract = {Screening for vision threatening diabetic retinopathy by grading digital retinal images reduces the risk of blindness in people with diabetes. Computer-aided diagnosis can aid human graders to cope with this mounting problem. We propose to use a 10-layer convolutional neural network to automatically, simultaneously segment and discriminate exudates, haemorrhages and micro-aneurysms. Input image is normalized before segmentation. The net is trained in two stages to improve performance. On average, our net on 30,275,903 effective points achieved a sensitivity of 0.8758 and 0.7158 for exudates and dark lesions on the CLEOPATRA database. It also achieved a sensitivity of 0.6257 and 0.4606 for haemorrhages and micro-aneurysms. This study shows that it is possible to get a single convolutional neural network to segment these pathological features on a wide range of fundus images with reasonable accuracy.},
  keywords = {Convolutional neural network,Diabetic retinopathy,Exudates,Fundus image,Haemorrhages,Microaneurysms,Segmentation}
}

@article{tanAutomatedSegmentationExudates2017a,
  title = {Automated Segmentation of Exudates, Haemorrhages, Microaneurysms Using Single Convolutional Neural Network},
  author = {Tan, Jen Hong and Fujita, Hamido and Sivaprasad, Sobha and Bhandary, Sulatha V. and Rao, A. Krishna and Chua, Kuang Chua and Acharya, U. Rajendra},
  year = {2017},
  month = dec,
  journal = {Information Sciences},
  volume = {420},
  pages = {66--76},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2017.08.050},
  urldate = {2019-07-24},
  abstract = {Screening for vision threatening diabetic retinopathy by grading digital retinal images reduces the risk of blindness in people with diabetes. Computer-aided diagnosis can aid human graders to cope with this mounting problem. We propose to use a 10-layer convolutional neural network to automatically, simultaneously segment and discriminate exudates, haemorrhages and micro-aneurysms. Input image is normalized before segmentation. The net is trained in two stages to improve performance. On average, our net on 30,275,903 effective points achieved a sensitivity of 0.8758 and 0.7158 for exudates and dark lesions on the CLEOPATRA database. It also achieved a sensitivity of 0.6257 and 0.4606 for haemorrhages and micro-aneurysms. This study shows that it is possible to get a single convolutional neural network to segment these pathological features on a wide range of fundus images with reasonable accuracy.},
  keywords = {Convolutional neural network,Diabetic retinopathy,Exudates,Fundus image,Haemorrhages,Microaneurysms,Segmentation},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\P778DEW8\\Tan et al. - 2017 - Automated segmentation of exudates, haemorrhages, .pdf;C\:\\Users\\cleme\\Zotero\\storage\\3TGRAPRM\\S0020025517308927.html}
}

@article{tangInterpretableClassificationAlzheimer2019,
  title = {Interpretable Classification of {{Alzheimer}}'s Disease Pathologies with a Convolutional Neural Network Pipeline},
  author = {Tang, Ziqi and Chuang, Kangway V. and DeCarli, Charles and Jin, Lee-Way and Beckett, Laurel and Keiser, Michael J. and Dugger, Brittany N.},
  year = {2019},
  month = dec,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {2173},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10212-1},
  urldate = {2021-11-16},
  langid = {english}
}

@article{tangInterpretableClassificationAlzheimer2019a,
  title = {Interpretable Classification of {{Alzheimer}}'s Disease Pathologies with a Convolutional Neural Network Pipeline},
  author = {Tang, Ziqi and Chuang, Kangway V. and DeCarli, Charles and Jin, Lee-Way and Beckett, Laurel and Keiser, Michael J. and Dugger, Brittany N.},
  year = {2019},
  month = dec,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {2173},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10212-1},
  urldate = {2021-11-16},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\6IL32F9L\Tang et al. - 2019 - Interpretable classification of Alzheimer’s diseas.pdf}
}

@article{tangQuantitativeEvaluationPapilledema2012,
  title = {Quantitative {{Evaluation}} of {{Papilledema}} from {{Stereoscopic Color Fundus Photographs}}},
  author = {Tang, Li and Kardon, Randy H. and Wang, Jui-Kai and Garvin, Mona K. and Lee, Kyungmoo and Abr{\`a}moff, Michael D.},
  year = {2012},
  month = jul,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {53},
  number = {8},
  pages = {4490--4497},
  issn = {1552-5783},
  doi = {10.1167/iovs.12-9803},
  urldate = {2019-11-28},
  langid = {english}
}

@article{tangQuantitativeEvaluationPapilledema2012a,
  title = {Quantitative {{Evaluation}} of {{Papilledema}} from {{Stereoscopic Color Fundus Photographs}}},
  author = {Tang, Li and Kardon, Randy H. and Wang, Jui-Kai and Garvin, Mona K. and Lee, Kyungmoo and Abr{\`a}moff, Michael D.},
  year = {2012},
  month = jul,
  journal = {Investigative Ophthalmology \& Visual Science},
  volume = {53},
  number = {8},
  pages = {4490--4497},
  issn = {1552-5783},
  doi = {10.1167/iovs.12-9803},
  urldate = {2019-11-28},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\KRD7CPND\\Tang et al. - 2012 - Quantitative Evaluation of Papilledema from Stereo.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UUUCH79V\\article.html}
}

@article{tangRobustMultiscaleStereo2011,
  title = {Robust {{Multiscale Stereo Matching}} from {{Fundus Images}} with {{Radiometric Differences}}},
  author = {Tang, Li and Garvin, Mona K. and Lee, Kyungmoo and Alward, Wallace L.W. and Kwon, Young H. and Abramoff, Michael D.},
  year = {2011},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {11},
  pages = {2245--2258},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2011.69},
  abstract = {A robust multiscale stereo matching algorithm is proposed to find reliable correspondences between low contrast and weakly textured retinal image pairs with radiometric differences. Existing algorithms designed to deal with piecewise planar surfaces with distinct features and Lambertian reflectance do not apply in applications such as 3D reconstruction of medical images including stereo retinal images. In this paper, robust pixel feature vectors are formulated to extract discriminative features in the presence of noise in scale space, through which the response of low-frequency mechanisms alter and interact with the response of high-frequency mechanisms. The deep structures of the scene are represented with the evolution of disparity estimates in scale space, which distributes the matching ambiguity along the scale dimension to obtain globally coherent reconstructions. The performance is verified both qualitatively by face validity and quantitatively on our collection of stereo fundus image sets with ground truth, which have been made publicly available as an extension of standard test images for performance evaluation.},
  keywords = {3D medical image reconstruction,Algorithms,Biological,Depth from stereo,Depth Perception,electroretinography,fundus image,fundus images,Fundus Oculi,high-frequency mechanisms,Humans,image matching,image reconstruction,image resolution,image texture,Imaging,Kernel,Lambertian reflectance,low-frequency mechanisms,Measurement,medical image processing,Models,Noise,Ophthalmoscopy,Optical Coherence,performance evaluation,piecewise planar surfaces,Pixel,pixel feature vector,radiometric differences,Retina,robust multiscale stereo matching,robust pixel feature vectors,Robustness,scale space.,stereo image processing,textured retinal image pairs,Three dimensional displays,Three-Dimensional,Tomography}
}

@article{tangRobustMultiscaleStereo2011a,
  title = {Robust {{Multiscale Stereo Matching}} from {{Fundus Images}} with {{Radiometric Differences}}},
  author = {Tang, Li and Garvin, Mona K. and Lee, Kyungmoo and Alward, Wallace L.W. and Kwon, Young H. and Abramoff, Michael D.},
  year = {2011},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {11},
  pages = {2245--2258},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2011.69},
  abstract = {A robust multiscale stereo matching algorithm is proposed to find reliable correspondences between low contrast and weakly textured retinal image pairs with radiometric differences. Existing algorithms designed to deal with piecewise planar surfaces with distinct features and Lambertian reflectance do not apply in applications such as 3D reconstruction of medical images including stereo retinal images. In this paper, robust pixel feature vectors are formulated to extract discriminative features in the presence of noise in scale space, through which the response of low-frequency mechanisms alter and interact with the response of high-frequency mechanisms. The deep structures of the scene are represented with the evolution of disparity estimates in scale space, which distributes the matching ambiguity along the scale dimension to obtain globally coherent reconstructions. The performance is verified both qualitatively by face validity and quantitatively on our collection of stereo fundus image sets with ground truth, which have been made publicly available as an extension of standard test images for performance evaluation.},
  keywords = {3D medical image reconstruction,Algorithms,Depth from stereo,Depth Perception,electroretinography,fundus image,fundus images,Fundus Oculi,high-frequency mechanisms,Humans,image matching,image reconstruction,image resolution,image texture,Imaging Three-Dimensional,Kernel,Lambertian reflectance,low-frequency mechanisms,Measurement,medical image processing,Models Biological,Noise,Ophthalmoscopy,performance evaluation,piecewise planar surfaces,Pixel,pixel feature vector,radiometric differences,Retina,robust multiscale stereo matching,robust pixel feature vectors,Robustness,scale space.,stereo image processing,textured retinal image pairs,Three dimensional displays,Tomography Optical Coherence},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\THMHESWX\\Tang et al. - 2011 - Robust Multiscale Stereo Matching from Fundus Imag.pdf;C\:\\Users\\cleme\\Zotero\\storage\\33YKH5DP\\5740926.html;C\:\\Users\\cleme\\Zotero\\storage\\HDN2SMLW\\litang2011.html}
}

@article{teoGlobalPrevalenceDiabetic2021,
  title = {Global {{Prevalence}} of {{Diabetic Retinopathy}} and {{Projection}} of {{Burden}} through 2045: {{Systematic Review}} and {{Meta-analysis}}},
  shorttitle = {Global {{Prevalence}} of {{Diabetic Retinopathy}} and {{Projection}} of {{Burden}} through 2045},
  author = {Teo, Zhen Ling and Tham, Yih-Chung and Yu, Marco and Chee, Miao Li and Rim, Tyler Hyungtaek and Cheung, Ning and Bikbov, Mukharram M. and Wang, Ya Xing and Tang, Yating and Lu, Yi and Wong, Ian Y. and Ting, Daniel Shu Wei and Tan, Gavin Siew Wei and Jonas, Jost B. and Sabanayagam, Charumathi and Wong, Tien Yin and Cheng, Ching-Yu},
  year = {2021},
  month = nov,
  journal = {Ophthalmology},
  volume = {128},
  number = {11},
  pages = {1580--1591},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2021.04.027},
  abstract = {TOPIC: To provide updated estimates on the global prevalence and number of people with diabetic retinopathy (DR) through 2045. CLINICAL RELEVANCE: The International Diabetes Federation (IDF) estimated the global population with diabetes mellitus (DM) to be 463 million in 2019 and 700 million in 2045. Diabetic retinopathy remains a common complication of DM and a leading cause of preventable blindness in the adult working population. METHODS: We conducted a systematic review using PubMed, Medline, Web of Science, and Scopus for population-based studies published up to March 2020. Random effect meta-analysis with logit transformation was performed to estimate global and regional prevalence of DR, vision-threatening DR (VTDR), and clinically significant macular edema (CSME). Projections of DR, VTDR, and CSME burden were based on population data from the IDF Atlas 2019. RESULTS: We included 59 population-based studies. Among individuals with diabetes, global prevalence was 22.27\% (95\% confidence interval [CI], 19.73\%-25.03\%) for DR, 6.17\% (95\% CI, 5.43\%-6.98\%) for VTDR, and 4.07\% (95\% CI, 3.42\%-4.82\%) for CSME. In 2020, the number of adults worldwide with DR, VTDR, and CSME was estimated to be 103.12 million, 28.54 million, and 18.83 million, respectively; by 2045, the numbers are projected to increase to 160.50 million, 44.82 million, and 28.61 million, respectively. Diabetic retinopathy prevalence was highest in Africa (35.90\%) and North American and the Caribbean (33.30\%) and was lowest in South and Central America (13.37\%). In meta-regression models adjusting for habitation type, response rate, study year, and DR diagnostic method, Hispanics (odds ratio [OR], 2.92; 95\% CI, 1.22-6.98) and Middle Easterners (OR, 2.44; 95\% CI, 1.51-3.94) with diabetes were more likely to have DR compared with Asians. DISCUSSION: The global DR burden is expected to remain high through 2045, disproportionately affecting countries in the Middle East and North Africa and the Western Pacific. These updated estimates may guide DR screening, treatment, and public health care strategies.},
  langid = {english},
  pmid = {33940045},
  keywords = {Cost of Illness,Diabetes mellitus,Diabetic retinopathy,Diabetic Retinopathy,Follow-Up Studies,Forecasting,Global Health,Humans,Population,Prevalence,Risk Factors,Systematic review}
}

@article{teoGlobalPrevalenceDiabetic2021a,
  title = {Global {{Prevalence}} of {{Diabetic Retinopathy}} and {{Projection}} of {{Burden}} through 2045: {{Systematic Review}} and {{Meta-analysis}}},
  shorttitle = {Global {{Prevalence}} of {{Diabetic Retinopathy}} and {{Projection}} of {{Burden}} through 2045},
  author = {Teo, Zhen Ling and Tham, Yih-Chung and Yu, Marco and Chee, Miao Li and Rim, Tyler Hyungtaek and Cheung, Ning and Bikbov, Mukharram M. and Wang, Ya Xing and Tang, Yating and Lu, Yi and Wong, Ian Y. and Ting, Daniel Shu Wei and Tan, Gavin Siew Wei and Jonas, Jost B. and Sabanayagam, Charumathi and Wong, Tien Yin and Cheng, Ching-Yu},
  year = {2021},
  month = nov,
  journal = {Ophthalmology},
  volume = {128},
  number = {11},
  pages = {1580--1591},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2021.04.027},
  abstract = {TOPIC: To provide updated estimates on the global prevalence and number of people with diabetic retinopathy (DR) through 2045. CLINICAL RELEVANCE: The International Diabetes Federation (IDF) estimated the global population with diabetes mellitus (DM) to be 463 million in 2019 and 700 million in 2045. Diabetic retinopathy remains a common complication of DM and a leading cause of preventable blindness in the adult working population. METHODS: We conducted a systematic review using PubMed, Medline, Web of Science, and Scopus for population-based studies published up to March 2020. Random effect meta-analysis with logit transformation was performed to estimate global and regional prevalence of DR, vision-threatening DR (VTDR), and clinically significant macular edema (CSME). Projections of DR, VTDR, and CSME burden were based on population data from the IDF Atlas 2019. RESULTS: We included 59 population-based studies. Among individuals with diabetes, global prevalence was 22.27\% (95\% confidence interval [CI], 19.73\%-25.03\%) for DR, 6.17\% (95\% CI, 5.43\%-6.98\%) for VTDR, and 4.07\% (95\% CI, 3.42\%-4.82\%) for CSME. In 2020, the number of adults worldwide with DR, VTDR, and CSME was estimated to be 103.12 million, 28.54 million, and 18.83 million, respectively; by 2045, the numbers are projected to increase to 160.50 million, 44.82 million, and 28.61 million, respectively. Diabetic retinopathy prevalence was highest in Africa (35.90\%) and North American and the Caribbean (33.30\%) and was lowest in South and Central America (13.37\%). In meta-regression models adjusting for habitation type, response rate, study year, and DR diagnostic method, Hispanics (odds ratio [OR], 2.92; 95\% CI, 1.22-6.98) and Middle Easterners (OR, 2.44; 95\% CI, 1.51-3.94) with diabetes were more likely to have DR compared with Asians. DISCUSSION: The global DR burden is expected to remain high through 2045, disproportionately affecting countries in the Middle East and North Africa and the Western Pacific. These updated estimates may guide DR screening, treatment, and public health care strategies.},
  langid = {english},
  pmid = {33940045},
  keywords = {Cost of Illness,Diabetes mellitus,Diabetic retinopathy,Diabetic Retinopathy,Follow-Up Studies,Forecasting,Global Health,Humans,Population,Prevalence,Risk Factors,Systematic review}
}

@article{teoGlobalPrevalenceDiabetic2021b,
  title = {Global {{Prevalence}} of {{Diabetic Retinopathy}} and {{Projection}} of {{Burden}} through 2045: {{Systematic Review}} and {{Meta-analysis}}},
  shorttitle = {Global {{Prevalence}} of {{Diabetic Retinopathy}} and {{Projection}} of {{Burden}} through 2045},
  author = {Teo, Zhen Ling and Tham, Yih-Chung and Yu, Marco and Chee, Miao Li and Rim, Tyler Hyungtaek and Cheung, Ning and Bikbov, Mukharram M. and Wang, Ya Xing and Tang, Yating and Lu, Yi and Wong, Ian Y. and Ting, Daniel Shu Wei and Tan, Gavin Siew Wei and Jonas, Jost B. and Sabanayagam, Charumathi and Wong, Tien Yin and Cheng, Ching-Yu},
  year = {2021},
  month = nov,
  journal = {Ophthalmology},
  volume = {128},
  number = {11},
  pages = {1580--1591},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2021.04.027},
  abstract = {TOPIC: To provide updated estimates on the global prevalence and number of people with diabetic retinopathy (DR) through~2045. CLINICAL RELEVANCE: The International Diabetes Federation (IDF) estimated the global population with diabetes mellitus (DM) to be 463 million in 2019 and 700 million in 2045. Diabetic retinopathy remains a common complication of DM and a leading cause of preventable blindness in the adult working population. METHODS: We conducted a systematic review using PubMed, Medline, Web of Science, and Scopus for population-based studies published up to March 2020. Random effect meta-analysis with logit transformation was performed to estimate global and regional prevalence of DR, vision-threatening DR (VTDR), and clinically significant macular edema (CSME). Projections of DR, VTDR, and CSME burden were based on population data from the IDF Atlas~2019. RESULTS: We included 59 population-based studies. Among individuals with diabetes, global prevalence was 22.27\% (95\% confidence interval [CI], 19.73\%-25.03\%) for DR, 6.17\% (95\% CI, 5.43\%-6.98\%) for VTDR, and 4.07\% (95\% CI, 3.42\%-4.82\%) for CSME. In 2020, the number of adults worldwide with DR, VTDR, and CSME was estimated to be 103.12 million, 28.54 million, and 18.83 million, respectively; by 2045, the numbers are projected to increase to 160.50 million, 44.82 million, and 28.61 million, respectively. Diabetic retinopathy prevalence was highest in Africa (35.90\%) and North American and the Caribbean (33.30\%) and was lowest in South and Central America (13.37\%). In meta-regression models adjusting for habitation type, response rate, study year, and DR diagnostic method, Hispanics (odds ratio [OR], 2.92; 95\% CI, 1.22-6.98) and Middle Easterners (OR, 2.44; 95\% CI, 1.51-3.94) with diabetes were more likely to have DR compared with Asians. DISCUSSION: The global DR burden is expected to remain high through 2045, disproportionately affecting countries in the Middle East and North Africa and the Western Pacific. These updated estimates may guide DR screening, treatment, and public health care strategies.},
  langid = {english},
  pmid = {33940045},
  keywords = {Cost of Illness,Diabetes mellitus,Diabetic retinopathy,Diabetic Retinopathy,Follow-Up Studies,Forecasting,Global Health,Humans,Population,Prevalence,Risk Factors,Systematic review}
}

@article{teoGlobalPrevalenceDiabetic2021c,
  title = {Global {{Prevalence}} of {{Diabetic Retinopathy}} and {{Projection}} of {{Burden}} through 2045: {{Systematic Review}} and {{Meta-analysis}}},
  shorttitle = {Global {{Prevalence}} of {{Diabetic Retinopathy}} and {{Projection}} of {{Burden}} through 2045},
  author = {Teo, Zhen Ling and Tham, Yih-Chung and Yu, Marco and Chee, Miao Li and Rim, Tyler Hyungtaek and Cheung, Ning and Bikbov, Mukharram M. and Wang, Ya Xing and Tang, Yating and Lu, Yi and Wong, Ian Y. and Ting, Daniel Shu Wei and Tan, Gavin Siew Wei and Jonas, Jost B. and Sabanayagam, Charumathi and Wong, Tien Yin and Cheng, Ching-Yu},
  year = {2021},
  month = nov,
  journal = {Ophthalmology},
  volume = {128},
  number = {11},
  pages = {1580--1591},
  issn = {1549-4713},
  doi = {10.1016/j.ophtha.2021.04.027},
  abstract = {TOPIC: To provide updated estimates on the global prevalence and number of people with diabetic retinopathy (DR) through~2045. CLINICAL RELEVANCE: The International Diabetes Federation (IDF) estimated the global population with diabetes mellitus (DM) to be 463 million in 2019 and 700 million in 2045. Diabetic retinopathy remains a common complication of DM and a leading cause of preventable blindness in the adult working population. METHODS: We conducted a systematic review using PubMed, Medline, Web of Science, and Scopus for population-based studies published up to March 2020. Random effect meta-analysis with logit transformation was performed to estimate global and regional prevalence of DR, vision-threatening DR (VTDR), and clinically significant macular edema (CSME). Projections of DR, VTDR, and CSME burden were based on population data from the IDF Atlas~2019. RESULTS: We included 59 population-based studies. Among individuals with diabetes, global prevalence was 22.27\% (95\% confidence interval [CI], 19.73\%-25.03\%) for DR, 6.17\% (95\% CI, 5.43\%-6.98\%) for VTDR, and 4.07\% (95\% CI, 3.42\%-4.82\%) for CSME. In 2020, the number of adults worldwide with DR, VTDR, and CSME was estimated to be 103.12 million, 28.54 million, and 18.83 million, respectively; by 2045, the numbers are projected to increase to 160.50 million, 44.82 million, and 28.61 million, respectively. Diabetic retinopathy prevalence was highest in Africa (35.90\%) and North American and the Caribbean (33.30\%) and was lowest in South and Central America (13.37\%). In meta-regression models adjusting for habitation type, response rate, study year, and DR diagnostic method, Hispanics (odds ratio [OR], 2.92; 95\% CI, 1.22-6.98) and Middle Easterners (OR, 2.44; 95\% CI, 1.51-3.94) with diabetes were more likely to have DR compared with Asians. DISCUSSION: The global DR burden is expected to remain high through 2045, disproportionately affecting countries in the Middle East and North Africa and the Western Pacific. These updated estimates may guide DR screening, treatment, and public health care strategies.},
  langid = {english},
  pmid = {33940045},
  keywords = {Cost of Illness,Diabetes mellitus,Diabetic retinopathy,Diabetic Retinopathy,Follow-Up Studies,Forecasting,Global Health,Humans,Population,Prevalence,Risk Factors,Systematic review}
}

@article{thakurPredictingGlaucomaOnset2020,
  title = {Predicting {{Glaucoma}} before {{Onset Using Deep Learning}}},
  author = {Thakur, Anshul and Goldbaum, Michael and Yousefi, Siamak},
  year = {2020},
  journal = {Ophthalmology. Glaucoma},
  volume = {3},
  number = {4},
  pages = {262--268},
  issn = {2589-4196},
  doi = {10.1016/j.ogla.2020.04.012},
  abstract = {PURPOSE: To assess the accuracy of deep learning models to predict glaucoma development from fundus photographs several years before disease onset. DESIGN: Algorithm development for predicting glaucoma using data from a prospective longitudinal study. PARTICIPANTS: A total of 66\,721 fundus photographs from 3272 eyes of 1636 subjects who participated in the Ocular Hypertension Treatment Study (OHTS) were included. MAIN OUTCOME MEASURES: Accuracy and area under the curve (AUC). METHODS: Fundus photographs and visual fields were carefully examined by 2 independent readers from the optic disc and visual field reading centers of the OHTS. When an abnormality was detected by the readers, the subject was recalled for retesting to confirm the abnormality and for further confirmation by an end point committee. By using 66\,721 fundus photographs, deep learning models were trained and validated using 85\% of the fundus photographs and further retested (validated) on the remaining (held-out) 15\% of the fundus photographs. RESULTS: The AUC of the deep learning model in predicting glaucoma development 4 to 7 years before disease onset was 0.77 (95\% confidence interval [CI], 0.75-0.79). The accuracy of the model in predicting glaucoma development approximately 1 to 3 years before disease onset was 0.88 (95\% CI, 0.86-0.91). The accuracy of the model in detecting glaucoma after onset was 0.95 (95\% CI, 0.94-0.96). CONCLUSIONS: Deep learning models can predict glaucoma development before disease onset with reasonable accuracy. Eyes with visual field abnormality but not glaucomatous optic neuropathy had a higher tendency to be missed by deep learning algorithms.},
  langid = {english},
  pmid = {33012331},
  keywords = {Deep Learning,Female,Glaucoma,Humans,Intraocular Pressure,Male,Middle Aged,Optical Coherence,Predictive Value of Tests,Prospective Studies,Retinal Ganglion Cells,Tomography,Visual Fields}
}

@article{thakurPredictingGlaucomaOnset2020a,
  title = {Predicting {{Glaucoma}} before {{Onset Using Deep Learning}}},
  author = {Thakur, Anshul and Goldbaum, Michael and Yousefi, Siamak},
  year = {2020},
  journal = {Ophthalmology. Glaucoma},
  volume = {3},
  number = {4},
  pages = {262--268},
  issn = {2589-4196},
  doi = {10.1016/j.ogla.2020.04.012},
  abstract = {PURPOSE: To assess the accuracy of deep learning models to predict glaucoma development from fundus photographs several years before disease onset. DESIGN: Algorithm development for predicting glaucoma using data from a prospective longitudinal study. PARTICIPANTS: A total of 66\,721 fundus photographs from 3272 eyes of 1636 subjects who participated in the Ocular Hypertension Treatment Study (OHTS) were included. MAIN OUTCOME MEASURES: Accuracy and area under the curve (AUC). METHODS: Fundus photographs and visual fields were carefully examined by 2 independent readers from the optic disc and visual field reading centers of the OHTS. When an abnormality was detected by the readers, the subject was recalled for retesting to confirm the abnormality and for further confirmation by an end point committee. By using 66\,721 fundus photographs, deep learning models were trained and validated using 85\% of the fundus photographs and further retested (validated) on the remaining (held-out) 15\% of the fundus photographs. RESULTS: The AUC of the deep learning model in predicting glaucoma development 4 to 7 years before disease onset was 0.77 (95\% confidence interval [CI], 0.75-0.79). The accuracy of the model in predicting glaucoma development approximately 1 to 3 years before disease onset was 0.88 (95\% CI, 0.86-0.91). The accuracy of the model in detecting glaucoma after onset was 0.95 (95\% CI, 0.94-0.96). CONCLUSIONS: Deep learning models can predict glaucoma development before disease onset with reasonable accuracy. Eyes with visual field abnormality but not glaucomatous optic neuropathy had a higher tendency to be missed by deep learning algorithms.},
  langid = {english},
  pmid = {33012331},
  keywords = {Deep Learning,Female,Glaucoma,Humans,Intraocular Pressure,Male,Middle Aged,Predictive Value of Tests,Prospective Studies,Retinal Ganglion Cells,Tomography Optical Coherence,Visual Fields},
  file = {C:\Users\cleme\Zotero\storage\XPBXMTUB\Thakur et al. - 2020 - Predicting Glaucoma before Onset Using Deep Learni.pdf}
}

@article{TheorieGraphes2019,
  title = {{Th{\'e}orie des graphes}},
  year = {2019},
  month = aug,
  journal = {Wikip{\'e}dia},
  urldate = {2019-11-24},
  abstract = {La th{\'e}orie des graphes est la discipline math{\'e}matique et informatique qui {\'e}tudie les graphes, lesquels sont des mod{\`e}les abstraits de dessins de r{\'e}seaux reliant des objets. Ces mod{\`e}les sont constitu{\'e}s par la donn{\'e}e de sommets (aussi appel{\'e}s n{\oe}uds ou points, en r{\'e}f{\'e}rence aux poly{\`e}dres), et d'ar{\^e}tes (aussi appel{\'e}es liens ou lignes) entre ces sommets ; ces ar{\^e}tes sont parfois non-sym{\'e}triques (les graphes sont alors dits orient{\'e}s) et sont appel{\'e}s des fl{\`e}ches. Les algorithmes {\'e}labor{\'e}s pour r{\'e}soudre des probl{\`e}mes concernant les objets de cette th{\'e}orie ont de nombreuses applications dans tous les domaines li{\'e}s {\`a} la notion de r{\'e}seau (r{\'e}seau social, r{\'e}seau informatique, t{\'e}l{\'e}communications, etc.) et dans bien d'autres domaines (par exemple g{\'e}n{\'e}tique) tant le concept de graphe, {\`a} peu pr{\`e}s {\'e}quivalent {\`a} celui de relation binaire ({\`a} ne pas confondre donc avec graphe d'une fonction), est g{\'e}n{\'e}ral. De grands th{\'e}or{\`e}mes difficiles, comme le th{\'e}or{\`e}me des quatre couleurs, le th{\'e}or{\`e}me des graphes parfaits, ou encore le th{\'e}or{\`e}me de Robertson-Seymour, ont contribu{\'e} {\`a} asseoir cette mati{\`e}re aupr{\`e}s des math{\'e}maticiens, et les questions qu'elle laisse ouvertes, comme la conjecture de Hadwiger, en font une branche vivace des math{\'e}matiques discr{\`e}tes.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french}
}

@article{TheorieGraphes2019a,
  title = {{Th{\'e}orie des graphes}},
  year = {2019},
  month = aug,
  journal = {Wikip{\'e}dia},
  urldate = {2019-11-24},
  abstract = {La th{\'e}orie des graphes est la discipline math{\'e}matique et informatique qui {\'e}tudie les graphes, lesquels sont des mod{\`e}les abstraits de dessins de r{\'e}seaux reliant des objets. Ces mod{\`e}les sont constitu{\'e}s par la donn{\'e}e de sommets (aussi appel{\'e}s n{\oe}uds ou points, en r{\'e}f{\'e}rence aux poly{\`e}dres), et d'ar{\^e}tes (aussi appel{\'e}es liens ou lignes) entre ces sommets ; ces ar{\^e}tes sont parfois non-sym{\'e}triques (les graphes sont alors dits orient{\'e}s) et sont appel{\'e}s des fl{\`e}ches. Les algorithmes {\'e}labor{\'e}s pour r{\'e}soudre des probl{\`e}mes concernant les objets de cette th{\'e}orie ont de nombreuses applications dans tous les domaines li{\'e}s {\`a} la notion de r{\'e}seau (r{\'e}seau social, r{\'e}seau informatique, t{\'e}l{\'e}communications, etc.) et dans bien d'autres domaines (par exemple g{\'e}n{\'e}tique) tant le concept de graphe, {\`a} peu pr{\`e}s {\'e}quivalent {\`a} celui de relation binaire ({\`a} ne pas confondre donc avec graphe d'une fonction), est g{\'e}n{\'e}ral. De grands th{\'e}or{\`e}mes difficiles, comme le th{\'e}or{\`e}me des quatre couleurs, le th{\'e}or{\`e}me des graphes parfaits, ou encore le th{\'e}or{\`e}me de Robertson-Seymour, ont contribu{\'e} {\`a} asseoir cette mati{\`e}re aupr{\`e}s des math{\'e}maticiens, et les questions qu'elle laisse ouvertes, comme la conjecture de Hadwiger, en font une branche vivace des math{\'e}matiques discr{\`e}tes.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 161670233},
  file = {C:\Users\cleme\Zotero\storage\QFAIXJXB\index.html}
}

@article{thulasiNonmydriaticOcularFundus2013,
  title = {Nonmydriatic Ocular Fundus Photography among Headache Patients in an Emergency Department},
  author = {Thulasi, Praneetha and Fraser, Clare L. and Biousse, Val{\'e}rie and Wright, David W. and Newman, Nancy J. and Bruce, Beau B.},
  year = {2013},
  month = jan,
  journal = {Neurology},
  volume = {80},
  number = {5},
  pages = {432--437},
  issn = {0028-3878},
  doi = {10.1212/WNL.0b013e31827f0f20},
  urldate = {2019-11-12},
  abstract = {Objectives: Determine the frequency of and the predictive factors for abnormal ocular fundus findings among emergency department (ED) headache patients. Methods: Cross-sectional study of prospectively enrolled adult patients presenting to our ED with a chief complaint of headache. Ocular fundus photographs were obtained using a nonmydriatic fundus camera that does not require pupillary dilation. Demographic and neuroimaging information was collected. Photographs were reviewed independently by 2 neuroophthalmologists for findings relevant to acute care. The results were analyzed using univariate statistics and logistic regression modeling. Results: We included 497 patients (median age: 40 years, 73\% women), among whom 42 (8.5\%, 95\% confidence interval: 6\%--11\%) had ocular fundus abnormalities. Of these 42 patients, 12 had disc edema, 9 had optic nerve pallor, 6 had grade III/IV hypertensive retinopathy, and 15 had isolated retinal hemorrhages. Body mass index {$\geq$}35 kg/m2 (odds ratio [OR]: 2.3, p = 0.02), younger age (OR: 0.7 per 10-year increase, p = 0.02), and higher mean arterial blood pressure (OR: 1.3 per 10-mm Hg increase, p = 0.003) were predictive of abnormal retinal photography. Patients with an abnormal fundus had a higher percentage of hospital admission (21\% vs 10\%, p = 0.04). Among the 34 patients with abnormal ocular fundi who had brain imaging, 14 (41\%) had normal imaging. Conclusions: Ocular fundus abnormalities were found in 8.5\% of patients with headache presenting to our ED. Predictors of abnormal funduscopic findings included higher body mass index, younger age, and higher blood pressure. Our study confirms the importance of funduscopic examination in patients with headache, particularly in the ED, and reaffirms the utility of nonmydriatic fundus photography in this setting.},
  pmcid = {PMC3590046},
  pmid = {23284060}
}

@article{thulasiNonmydriaticOcularFundus2013a,
  title = {Nonmydriatic Ocular Fundus Photography among Headache Patients in an Emergency Department},
  author = {Thulasi, Praneetha and Fraser, Clare L. and Biousse, Val{\'e}rie and Wright, David W. and Newman, Nancy J. and Bruce, Beau B.},
  year = {2013},
  month = jan,
  journal = {Neurology},
  volume = {80},
  number = {5},
  pages = {432--437},
  issn = {0028-3878},
  doi = {10.1212/WNL.0b013e31827f0f20},
  urldate = {2019-11-12},
  abstract = {Objectives: Determine the frequency of and the predictive factors for abnormal ocular fundus findings among emergency department (ED) headache patients. Methods: Cross-sectional study of prospectively enrolled adult patients presenting to our ED with a chief complaint of headache. Ocular fundus photographs were obtained using a nonmydriatic fundus camera that does not require pupillary dilation. Demographic and neuroimaging information was collected. Photographs were reviewed independently by 2 neuroophthalmologists for findings relevant to acute care. The results were analyzed using univariate statistics and logistic regression modeling. Results: We included 497 patients (median age: 40 years, 73\% women), among whom 42 (8.5\%, 95\% confidence interval: 6\%--11\%) had ocular fundus abnormalities. Of these 42 patients, 12 had disc edema, 9 had optic nerve pallor, 6 had grade III/IV hypertensive retinopathy, and 15 had isolated retinal hemorrhages. Body mass index {$\geq$}35 kg/m2 (odds ratio [OR]: 2.3, p = 0.02), younger age (OR: 0.7 per 10-year increase, p = 0.02), and higher mean arterial blood pressure (OR: 1.3 per 10-mm Hg increase, p = 0.003) were predictive of abnormal retinal photography. Patients with an abnormal fundus had a higher percentage of hospital admission (21\% vs 10\%, p = 0.04). Among the 34 patients with abnormal ocular fundi who had brain imaging, 14 (41\%) had normal imaging. Conclusions: Ocular fundus abnormalities were found in 8.5\% of patients with headache presenting to our ED. Predictors of abnormal funduscopic findings included higher body mass index, younger age, and higher blood pressure. Our study confirms the importance of funduscopic examination in patients with headache, particularly in the ED, and reaffirms the utility of nonmydriatic fundus photography in this setting.},
  pmcid = {PMC3590046},
  pmid = {23284060},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\BTI3A29Q\\thulasi2013.pdf;C\:\\Users\\cleme\\Zotero\\storage\\NCKDZY8P\\Thulasi et al. - 2013 - Nonmydriatic ocular fundus photography among heada.pdf}
}

@article{tianPerformanceEvaluationAutomated2016,
  title = {Performance Evaluation of Automated Segmentation Software on Optical Coherence Tomography Volume Data},
  author = {Tian, Jing and Varga, Boglarka and Tatrai, Erika and Fanni, Palya and Somfai, Gabor Mark and Smiddy, William E. and Debuc, Delia Cabrera},
  year = {2016},
  month = may,
  journal = {Journal of biophotonics},
  volume = {9},
  number = {5},
  pages = {478--489},
  issn = {1864-063X},
  doi = {10.1002/jbio.201500239},
  urldate = {2022-07-10},
  abstract = {Over the past two decades a significant number of OCT segmentation approaches have been proposed in the literature. Each methodology has been conceived for and/or evaluated using specific datasets that do not reflect the complexities of the majority of widely available retinal features observed in clinical settings. In addition, there does not exist an appropriate OCT dataset with ground truth that reflects the realities of everyday retinal features observed in clinical settings. While the need for unbiased performance evaluation of automated segmentation algorithms is obvious, the validation process of segmentation algorithms have been usually performed by comparing with manual labelings from each study and there has been a lack of common ground truth. Therefore, a performance comparison of different algorithms using the same ground truth has never been performed. This paper reviews research-oriented tools for automated segmentation of the retinal tissue on OCT images. It also evaluates and compares the performance of these software tools with a common ground truth.},
  pmcid = {PMC5025289},
  pmid = {27159849}
}

@article{tianPerformanceEvaluationAutomated2016a,
  title = {Performance Evaluation of Automated Segmentation Software on Optical Coherence Tomography Volume Data},
  author = {Tian, Jing and Varga, Boglarka and Tatrai, Erika and Fanni, Palya and Somfai, Gabor Mark and Smiddy, William E. and Debuc, Delia Cabrera},
  year = {2016},
  month = may,
  journal = {Journal of biophotonics},
  volume = {9},
  number = {5},
  pages = {478--489},
  issn = {1864-063X},
  doi = {10.1002/jbio.201500239},
  urldate = {2022-07-10},
  abstract = {Over the past two decades a significant number of OCT segmentation approaches have been proposed in the literature. Each methodology has been conceived for and/or evaluated using specific datasets that do not reflect the complexities of the majority of widely available retinal features observed in clinical settings. In addition, there does not exist an appropriate OCT dataset with ground truth that reflects the realities of everyday retinal features observed in clinical settings. While the need for unbiased performance evaluation of automated segmentation algorithms is obvious, the validation process of segmentation algorithms have been usually performed by comparing with manual labelings from each study and there has been a lack of common ground truth. Therefore, a performance comparison of different algorithms using the same ground truth has never been performed. This paper reviews research-oriented tools for automated segmentation of the retinal tissue on OCT images. It also evaluates and compares the performance of these software tools with a common ground truth.},
  pmcid = {PMC5025289},
  pmid = {27159849},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\A7IRSC48\\tian2016.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\U6RWSWJ3\\Tian et al. - 2016 - Performance evaluation of automated segmentation s.pdf}
}

@inproceedings{tianSimultaneousSemanticSegmentation2016,
  title = {Simultaneous Semantic Segmentation of a Set of Partially Labeled Images},
  booktitle = {2016 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Tian, Qiongjie and Li, Baoxin},
  year = {2016},
  month = mar,
  pages = {1--9},
  doi = {10.1109/WACV.2016.7477639},
  urldate = {2024-03-01},
  abstract = {Semantic segmentation, by which an image is decomposed into regions with their respective semantic labels, is often the first step towards image understanding. Existing research on this regard is mainly performed under two conditions: the fully-supervised setting that relies on a set of images with pixel-level labels and the weakly-supervised one that uses only image-level labels. In both cases, the labeling task is time-consuming and laborious, and thus training data are always limited. In practice, there are voluminous on-line images, which unfortunately often have only incomplete image-level labels (tags) but would otherwise be potentially useful for a learning-based algorithm. Only limited efforts have been attempted on using such coarsely and incompletely labelled data for semantic segmentation. This paper proposes a new approach to semantic segmentation of a set of partially-labelled images, using a formulation considering information from multiple visual similar images. Experiments on several popular datasets, with comparison with existing methods, demonstrate evident performance improvement of the proposed approach.},
  keywords = {Computer science,Context,Graphical models,Image segmentation,Labeling,Semantics,Training data},
  file = {C:\Users\cleme\Zotero\storage\9CNYTLFF\7477639.html}
}

@article{tingAIMedicalImaging2018,
  title = {{{AI}} for Medical Imaging Goes Deep},
  author = {Ting, Daniel S. W. and Liu, Yong and Burlina, Philippe and Xu, Xinxing and Bressler, Neil M. and Wong, Tien Y.},
  year = {2018},
  month = may,
  journal = {Nature Medicine},
  volume = {24},
  number = {5},
  pages = {539--540},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0029-3},
  urldate = {2022-07-10},
  abstract = {An artificial intelligence (AI) using a deep-learning approach can classify retinal images from optical coherence tomography for early diagnosis of retinal diseases and has the potential to be used in other image-based medical diagnoses.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Diagnosis,Machine learning,Medical imaging}
}

@article{tingAIMedicalImaging2018a,
  title = {{{AI}} for Medical Imaging Goes Deep},
  author = {Ting, Daniel S. W. and Liu, Yong and Burlina, Philippe and Xu, Xinxing and Bressler, Neil M. and Wong, Tien Y.},
  year = {2018},
  month = may,
  journal = {Nature Medicine},
  volume = {24},
  number = {5},
  pages = {539--540},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0029-3},
  urldate = {2022-07-10},
  abstract = {An artificial intelligence (AI) using a deep-learning approach can classify retinal images from optical coherence tomography for early diagnosis of retinal diseases and has the potential to be used in other image-based medical diagnoses.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Diagnosis,Machine learning,Medical imaging},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\DBLQANB8\\ting2018.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\Y88GFXW6\\ting2018.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\YHXMQZ4G\\s41591-018-0029-3.html}
}

@article{tingArtificialIntelligenceDeep2019,
  title = {Artificial Intelligence and Deep Learning in Ophthalmology},
  author = {Ting, Daniel Shu Wei and Pasquale, Louis R. and Peng, Lily and Campbell, John Peter and Lee, Aaron Y. and Raman, Rajiv and Tan, Gavin Siew Wei and Schmetterer, Leopold and Keane, Pearse A. and Wong, Tien Yin},
  year = {2019},
  month = feb,
  journal = {British Journal of Ophthalmology},
  volume = {103},
  number = {2},
  pages = {167--175},
  publisher = {BMJ Publishing Group Ltd},
  issn = {0007-1161, 1468-2079},
  doi = {10.1136/bjophthalmol-2018-313173},
  urldate = {2022-07-10},
  abstract = {Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI `black-box' algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.},
  chapter = {Review},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0},
  langid = {english},
  pmid = {30361278},
  keywords = {glaucoma,imaging,public health,retina,telemedicine}
}

@article{tingArtificialIntelligenceDeep2019a,
  title = {Artificial Intelligence and Deep Learning in Ophthalmology},
  author = {Ting, Daniel Shu Wei and Pasquale, Louis R. and Peng, Lily and Campbell, John Peter and Lee, Aaron Y. and Raman, Rajiv and Tan, Gavin Siew Wei and Schmetterer, Leopold and Keane, Pearse A. and Wong, Tien Yin},
  year = {2019},
  month = feb,
  journal = {British Journal of Ophthalmology},
  volume = {103},
  number = {2},
  pages = {167--175},
  publisher = {BMJ Publishing Group Ltd},
  issn = {0007-1161, 1468-2079},
  doi = {10.1136/bjophthalmol-2018-313173},
  urldate = {2022-07-10},
  abstract = {Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI `black-box' algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.},
  chapter = {Review},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ..  This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0},
  langid = {english},
  pmid = {30361278},
  keywords = {glaucoma,imaging,public health,retina,telemedicine},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\7X9YHXUI\\Ting et al. - 2019 - Artificial intelligence and deep learning in ophth.pdf;C\:\\Users\\cleme\\Zotero\\storage\\MALJAS27\\ting2018.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\IL3SRF7Q\\167.html}
}

@article{tingDeepLearningOphthalmology2019,
  title = {Deep Learning in Ophthalmology: {{The}} Technical and Clinical Considerations},
  shorttitle = {Deep Learning in Ophthalmology},
  author = {Ting, Daniel S. W. and Peng, Lily and Varadarajan, Avinash V. and Keane, Pearse A. and Burlina, Philippe M. and Chiang, Michael F. and Schmetterer, Leopold and Pasquale, Louis R. and Bressler, Neil M. and Webster, Dale R. and Abramoff, Michael and Wong, Tien Y.},
  year = {2019},
  month = apr,
  journal = {Progress in Retinal and Eye Research},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2019.04.003},
  urldate = {2019-07-30},
  abstract = {The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally.}
}

@article{tingDeepLearningOphthalmology2019a,
  title = {Deep Learning in Ophthalmology: {{The}} Technical and Clinical Considerations},
  shorttitle = {Deep Learning in Ophthalmology},
  author = {Ting, Daniel S. W. and Peng, Lily and Varadarajan, Avinash V. and Keane, Pearse A. and Burlina, Philippe M. and Chiang, Michael F. and Schmetterer, Leopold and Pasquale, Louis R. and Bressler, Neil M. and Webster, Dale R. and Abramoff, Michael and Wong, Tien Y.},
  year = {2019},
  month = sep,
  journal = {Progress in Retinal and Eye Research},
  volume = {72},
  pages = {100759},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2019.04.003},
  urldate = {2022-07-10},
  abstract = {The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally.},
  langid = {english}
}

@article{tingDeepLearningOphthalmology2019b,
  title = {Deep Learning in Ophthalmology: {{The}} Technical and Clinical Considerations},
  shorttitle = {Deep Learning in Ophthalmology},
  author = {Ting, Daniel S. W. and Peng, Lily and Varadarajan, Avinash V. and Keane, Pearse A. and Burlina, Philippe M. and Chiang, Michael F. and Schmetterer, Leopold and Pasquale, Louis R. and Bressler, Neil M. and Webster, Dale R. and Abramoff, Michael and Wong, Tien Y.},
  year = {2019},
  month = apr,
  journal = {Progress in Retinal and Eye Research},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2019.04.003},
  urldate = {2019-07-30},
  abstract = {The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\VS92W2M7\\Ting et al. - 2019 - Deep learning in ophthalmology The technical and .pdf;C\:\\Users\\cleme\\Zotero\\storage\\QBQFBZY4\\S1350946218300909.html}
}

@article{tingDeepLearningOphthalmology2019c,
  title = {Deep Learning in Ophthalmology: {{The}} Technical and Clinical Considerations},
  shorttitle = {Deep Learning in Ophthalmology},
  author = {Ting, Daniel S. W. and Peng, Lily and Varadarajan, Avinash V. and Keane, Pearse A. and Burlina, Philippe M. and Chiang, Michael F. and Schmetterer, Leopold and Pasquale, Louis R. and Bressler, Neil M. and Webster, Dale R. and Abramoff, Michael and Wong, Tien Y.},
  year = {2019},
  month = sep,
  journal = {Progress in Retinal and Eye Research},
  volume = {72},
  pages = {100759},
  issn = {1350-9462},
  doi = {10.1016/j.preteyeres.2019.04.003},
  urldate = {2022-07-10},
  abstract = {The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\DBVVK8PP\\Ting et al. - 2019 - Deep learning in ophthalmology The technical and .pdf;C\:\\Users\\cleme\\Zotero\\storage\\DWHTYBR2\\ting2019.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\2EFZRPJJ\\S1350946218300909.html}
}

@article{tingDevelopmentValidationDeep2017,
  title = {Development and {{Validation}} of a {{Deep Learning System}} for {{Diabetic Retinopathy}} and {{Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes}}},
  author = {Ting, Daniel Shu Wei and Cheung, Carol Yim-Lui and Lim, Gilbert and Tan, Gavin Siew Wei and Quang, Nguyen D. and Gan, Alfred and Hamzah, Haslina and {Garcia-Franco}, Renata and Yeo, Ian Yew San and Lee, Shu Yen and Wong, Edmund Yick Mun and Sabanayagam, Charumathi and Baskaran, Mani and Ibrahim, Farah and Tan, Ngiap Chuan and Finkelstein, Eric A. and Lamoureux, Ecosse L. and Wong, Ian Y. and Bressler, Neil M. and Sivaprasad, Sobha and Varma, Rohit and Jonas, Jost B. and He, Ming Guang and Cheng, Ching-Yu and Cheung, Gemmy Chui Ming and Aung, Tin and Hsu, Wynne and Lee, Mong Li and Wong, Tien Yin},
  year = {2017},
  month = dec,
  journal = {JAMA : the journal of the American Medical Association},
  volume = {318},
  number = {22},
  pages = {2211--2223},
  issn = {0098-7484},
  doi = {10.1001/jama.2017.18152},
  urldate = {2019-11-20},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}A deep learning system (DLS) is a machine learning technology with potential for screening diabetic retinopathy and related eye diseases.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}To evaluate the performance of a DLS in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, possible glaucoma, and age-related macular degeneration (AMD) in community and clinic-based multiethnic populations with diabetes.{$<$}/p{$><$}h3{$>$}Design, Setting, and Participants{$<$}/h3{$><$}p{$>$}Diagnostic performance of a DLS for diabetic retinopathy and related eye diseases was evaluated using 494 661 retinal images. A DLS was trained for detecting diabetic retinopathy (using 76 370 images), possible glaucoma (125 189 images), and AMD (72 610 images), and performance of DLS was evaluated for detecting diabetic retinopathy (using 112 648 images), possible glaucoma (71 896 images), and AMD (35 948 images). Training of the DLS was completed in May 2016, and validation of the DLS was completed in May 2017 for detection of referable diabetic retinopathy (moderate nonproliferative diabetic retinopathy or worse) and vision-threatening diabetic retinopathy (severe nonproliferative diabetic retinopathy or worse) using a primary validation data set in the Singapore National Diabetic Retinopathy Screening Program and 10 multiethnic cohorts with diabetes.{$<$}/p{$><$}h3{$>$}Exposures{$<$}/h3{$><$}p{$>$}Use of a deep learning system.{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}Area under the receiver operating characteristic curve (AUC) and sensitivity and specificity of the DLS with professional graders (retinal specialists, general ophthalmologists, trained graders, or optometrists) as the reference standard.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}In the primary validation dataset (n = 14 880 patients; 71 896 images; mean [SD] age, 60.2 [2.2] years; 54.6\% men), the prevalence of referable diabetic retinopathy was 3.0\%; vision-threatening diabetic retinopathy, 0.6\%; possible glaucoma, 0.1\%; and AMD, 2.5\%. The AUC of the DLS for referable diabetic retinopathy was 0.936 (95\% CI, 0.925-0.943), sensitivity was 90.5\% (95\% CI, 87.3\%-93.0\%), and specificity was 91.6\% (95\% CI, 91.0\%-92.2\%). For vision-threatening diabetic retinopathy, AUC was 0.958 (95\% CI, 0.956-0.961), sensitivity was 100\% (95\% CI, 94.1\%-100.0\%), and specificity was 91.1\% (95\% CI, 90.7\%-91.4\%). For possible glaucoma, AUC was 0.942 (95\% CI, 0.929-0.954), sensitivity was 96.4\% (95\% CI, 81.7\%-99.9\%), and specificity was 87.2\% (95\% CI, 86.8\%-87.5\%). For AMD, AUC was 0.931 (95\% CI, 0.928-0.935), sensitivity was 93.2\% (95\% CI, 91.1\%-99.8\%), and specificity was 88.7\% (95\% CI, 88.3\%-89.0\%). For referable diabetic retinopathy in the 10 additional datasets, AUC range was 0.889 to 0.983 (n = 40 752 images).{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}In this evaluation of retinal images from multiethnic cohorts of patients with diabetes, the DLS had high sensitivity and specificity for identifying diabetic retinopathy and related eye diseases. Further research is necessary to evaluate the applicability of the DLS in health care settings and the utility of the DLS to improve vision outcomes.{$<$}/p{$>$}},
  langid = {english}
}

@article{tingDevelopmentValidationDeep2017a,
  title = {Development and {{Validation}} of a {{Deep Learning System}} for {{Diabetic Retinopathy}} and {{Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes}}},
  author = {Ting, Daniel Shu Wei and Cheung, Carol Yim-Lui and Lim, Gilbert and Tan, Gavin Siew Wei and Quang, Nguyen D. and Gan, Alfred and Hamzah, Haslina and {Garcia-Franco}, Renata and Yeo, Ian Yew San and Lee, Shu Yen and Wong, Edmund Yick Mun and Sabanayagam, Charumathi and Baskaran, Mani and Ibrahim, Farah and Tan, Ngiap Chuan and Finkelstein, Eric A. and Lamoureux, Ecosse L. and Wong, Ian Y. and Bressler, Neil M. and Sivaprasad, Sobha and Varma, Rohit and Jonas, Jost B. and He, Ming Guang and Cheng, Ching-Yu and Cheung, Gemmy Chui Ming and Aung, Tin and Hsu, Wynne and Lee, Mong Li and Wong, Tien Yin},
  year = {2017},
  month = dec,
  journal = {JAMA},
  volume = {318},
  number = {22},
  pages = {2211--2223},
  issn = {0098-7484},
  doi = {10.1001/jama.2017.18152},
  urldate = {2019-11-20},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}A deep learning system (DLS) is a machine learning technology with potential for screening diabetic retinopathy and related eye diseases.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}To evaluate the performance of a DLS in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, possible glaucoma, and age-related macular degeneration (AMD) in community and clinic-based multiethnic populations with diabetes.{$<$}/p{$><$}h3{$>$}Design, Setting, and Participants{$<$}/h3{$><$}p{$>$}Diagnostic performance of a DLS for diabetic retinopathy and related eye diseases was evaluated using 494 661 retinal images. A DLS was trained for detecting diabetic retinopathy (using 76 370 images), possible glaucoma (125 189 images), and AMD (72 610 images), and performance of DLS was evaluated for detecting diabetic retinopathy (using 112 648 images), possible glaucoma (71 896 images), and AMD (35 948 images). Training of the DLS was completed in May 2016, and validation of the DLS was completed in May 2017 for detection of referable diabetic retinopathy (moderate nonproliferative diabetic retinopathy or worse) and vision-threatening diabetic retinopathy (severe nonproliferative diabetic retinopathy or worse) using a primary validation data set in the Singapore National Diabetic Retinopathy Screening Program and 10 multiethnic cohorts with diabetes.{$<$}/p{$><$}h3{$>$}Exposures{$<$}/h3{$><$}p{$>$}Use of a deep learning system.{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}Area under the receiver operating characteristic curve (AUC) and sensitivity and specificity of the DLS with professional graders (retinal specialists, general ophthalmologists, trained graders, or optometrists) as the reference standard.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}In the primary validation dataset (n = 14 880 patients; 71 896 images; mean [SD] age, 60.2 [2.2] years; 54.6\% men), the prevalence of referable diabetic retinopathy was 3.0\%; vision-threatening diabetic retinopathy, 0.6\%; possible glaucoma, 0.1\%; and AMD, 2.5\%. The AUC of the DLS for referable diabetic retinopathy was 0.936 (95\% CI, 0.925-0.943), sensitivity was 90.5\% (95\% CI, 87.3\%-93.0\%), and specificity was 91.6\% (95\% CI, 91.0\%-92.2\%). For vision-threatening diabetic retinopathy, AUC was 0.958 (95\% CI, 0.956-0.961), sensitivity was 100\% (95\% CI, 94.1\%-100.0\%), and specificity was 91.1\% (95\% CI, 90.7\%-91.4\%). For possible glaucoma, AUC was 0.942 (95\% CI, 0.929-0.954), sensitivity was 96.4\% (95\% CI, 81.7\%-99.9\%), and specificity was 87.2\% (95\% CI, 86.8\%-87.5\%). For AMD, AUC was 0.931 (95\% CI, 0.928-0.935), sensitivity was 93.2\% (95\% CI, 91.1\%-99.8\%), and specificity was 88.7\% (95\% CI, 88.3\%-89.0\%). For referable diabetic retinopathy in the 10 additional datasets, AUC range was 0.889 to 0.983 (n = 40 752 images).{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}In this evaluation of retinal images from multiethnic cohorts of patients with diabetes, the DLS had high sensitivity and specificity for identifying diabetic retinopathy and related eye diseases. Further research is necessary to evaluate the applicability of the DLS in health care settings and the utility of the DLS to improve vision outcomes.{$<$}/p{$>$}},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5BEXYABZ\\Ting et al. - 2017 - Development and Validation of a Deep Learning Syst.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RH44XPDX\\2665775.html}
}

@article{tjoaSurveyExplainableArtificial2021a,
  title = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}}): {{Toward Medical XAI}}},
  shorttitle = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Tjoa, Erico and Guan, Cuntai},
  year = {2021},
  month = nov,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {11},
  pages = {4793--4813},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3027314},
  abstract = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide ``obviously'' interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
  copyright = {cc by},
  langid = {english},
  pmid = {33079674},
  keywords = {Artificial neural networks,Biomedical imaging,Explainable artificial intelligence (XAI),Heating systems,interpretability,Learning systems,machine learning (ML),medical information system,Prediction algorithms,Reliability,survey,Visualization},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\C754TVNV\\Tjoa et Guan - 2021 - A Survey on Explainable Artificial Intelligence (X.pdf;C\:\\Users\\cleme\\Zotero\\storage\\JMMTYFAU\\9233366.html}
}

@inproceedings{touvronTrainingDataefficientImage2021,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  year = {2021},
  volume = {139},
  eprint = {2012.12877},
  pages = {10347--10357},
  urldate = {2021-03-01},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{touvronTrainingDataefficientImage2021a,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  year = {2021},
  volume = {139},
  eprint = {2012.12877},
  pages = {10347--10357},
  urldate = {2021-03-01},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\GVDIM8QQ\Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf}
}

@article{traagLouvainLeidenGuaranteeing2019,
  title = {From {{Louvain}} to {{Leiden}}: Guaranteeing Well-Connected Communities},
  shorttitle = {From {{Louvain}} to {{Leiden}}},
  author = {Traag, V. A. and Waltman, L. and {van Eck}, N. J.},
  year = {2019},
  month = mar,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {5233},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-41695-z},
  urldate = {2024-02-28},
  abstract = {Community detection is often used to understand the structure of large and complex networks. One of the most popular algorithms for uncovering community structure is the so-called Louvain algorithm. We show that this algorithm has a major defect that largely went unnoticed until now: the Louvain algorithm may yield arbitrarily badly connected communities. In the worst case, communities may even be disconnected, especially when running the algorithm iteratively. In our experimental analysis, we observe that up to 25\% of the communities are badly connected and up to 16\% are disconnected. To address this problem, we introduce the Leiden algorithm. We prove that the Leiden algorithm yields communities that are guaranteed to be connected. In addition, we prove that, when the Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. We demonstrate the performance of the Leiden algorithm for several benchmark and real-world networks. We find that the Leiden algorithm is faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Applied mathematics,Computational science,Computer science},
  file = {C:\Users\cleme\Zotero\storage\KX5YBVA4\Traag et al. - 2019 - From Louvain to Leiden guaranteeing well-connecte.pdf}
}

@misc{TransFuseFusingTransformers,
  title = {{{TransFuse}}: {{Fusing Transformers}} and {{CNNs}} for {{Medical Image Segmentation}}},
  shorttitle = {{{TransFuse}}},
  urldate = {2021-11-09},
  abstract = {Medical image segmentation - the prerequisite of numerous clinical needs - has been significantly prospered by recent advances in convolutional neural networks (CNNs). However, it exhibits general limitations on modeling explicit long-range {\dots}},
  langid = {english}
}

@misc{TransFuseFusingTransformersa,
  title = {{{TransFuse}}: {{Fusing Transformers}} and {{CNNs}} for {{Medical Image Segmentation}}},
  shorttitle = {{{TransFuse}}},
  journal = {springerprofessional.de},
  urldate = {2021-11-09},
  abstract = {Medical image segmentation - the prerequisite of numerous clinical needs - has been significantly prospered by recent advances in convolutional neural networks (CNNs). However, it exhibits general limitations on modeling explicit long-range {\dots}},
  howpublished = {https://www.springerprofessional.de/en/transfuse-fusing-transformers-and-cnns-for-medical-image-segment/19687858},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\6EJGJBN8\19687858.html}
}

@article{tsaiEdgeDrivenDualBootstrapIterative2010,
  title = {The {{Edge-Driven Dual-Bootstrap Iterative Closest Point Algorithm}} for {{Registration}} of {{Multimodal Fluorescein Angiogram Sequence}}},
  author = {Tsai, Chia-Ling and Li, Chun-Yi and Yang, Gehua and Lin, Kai-Shung},
  year = {2010},
  month = mar,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {29},
  number = {3},
  pages = {636--649},
  issn = {1558-254X},
  doi = {10.1109/TMI.2009.2030324},
  abstract = {Motivated by the need for multimodal image registration in ophthalmology, this paper introduces an algorithm which is tailored to jointly align in a common reference space all the images in a complete fluorescein angiogram (FA) sequence, which contains both red-free (RF) and FA images. Our work is inspired by Generalized Dual-Bootstrap Iterative Closest Point (GDB-ICP), which rank-orders Lowe keypoint matches and refines the transformation, going from local and low-order to global and higher-order model, computed from each keypoint match in succession. Albeit GDB-ICP has been shown to be robust in registering images taken under different lighting conditions, the performance is not satisfactory for image pairs with substantial, nonlinear intensity differences. Our algorithm, named Edge-Driven DB-ICP, targeting the least reliable component of GDB-ICP, modifies generation of keypoint matches for initialization by extracting the Lowe keypoints from the gradient magnitude image and enriching the keypoint descriptor with global-shape context using the edge points. Our dataset consists of 60 randomly-selected pathological sequences, each on average having up to two RF and 13 FA images. Edge-Driven DB-ICP successfully registered 92.4\% of all pairs, and 81.1\% multimodal pairs, whereas GDB-ICP registered 80.1\% and 40.1\%, respectively. For the joint registration of all images in a sequence, Edge-Driven DB-ICP succeeded in 59 sequences, which is a 23\% improvement over GDB-ICP.},
  keywords = {Algorithms,Biomedical imaging,biomedical optical imaging,Cluster Analysis,Computer science,Computer-Assisted,Databases,edge-driven DB-ICP,edge-driven dual-bootstrap iterative closest point algorithm,eye,FA image,Factual,Fluorescein angiogram,fluorescein angiogram sequence,Fluorescein Angiography,GDB-ICP,generalized dual-bootstrap iterative closest point,Humans,Image Processing,image registration,Image registration,image sequences,Iterative algorithms,iterative closest point,Iterative closest point algorithm,iterative methods,keypoint descriptor,keypoint matching,Lowe keypoint matches,medical image processing,multimodal image registration,ophthalmology,Pathology,Radio frequency,rank-orders Lowe keypoint,red-free image,registration,Retina,retinal imaging,Retinal vessels,Robustness,statistical analysis}
}

@article{tsaiEdgeDrivenDualBootstrapIterative2010a,
  title = {The {{Edge-Driven Dual-Bootstrap Iterative Closest Point Algorithm}} for {{Registration}} of {{Multimodal Fluorescein Angiogram Sequence}}},
  author = {Tsai, Chia-Ling and Li, Chun-Yi and Yang, Gehua and Lin, Kai-Shung},
  year = {2010},
  month = mar,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {29},
  number = {3},
  pages = {636--649},
  issn = {1558-254X},
  doi = {10.1109/TMI.2009.2030324},
  abstract = {Motivated by the need for multimodal image registration in ophthalmology, this paper introduces an algorithm which is tailored to jointly align in a common reference space all the images in a complete fluorescein angiogram (FA) sequence, which contains both red-free (RF) and FA images. Our work is inspired by Generalized Dual-Bootstrap Iterative Closest Point (GDB-ICP), which rank-orders Lowe keypoint matches and refines the transformation, going from local and low-order to global and higher-order model, computed from each keypoint match in succession. Albeit GDB-ICP has been shown to be robust in registering images taken under different lighting conditions, the performance is not satisfactory for image pairs with substantial, nonlinear intensity differences. Our algorithm, named Edge-Driven DB-ICP, targeting the least reliable component of GDB-ICP, modifies generation of keypoint matches for initialization by extracting the Lowe keypoints from the gradient magnitude image and enriching the keypoint descriptor with global-shape context using the edge points. Our dataset consists of 60 randomly-selected pathological sequences, each on average having up to two RF and 13 FA images. Edge-Driven DB-ICP successfully registered 92.4\% of all pairs, and 81.1\% multimodal pairs, whereas GDB-ICP registered 80.1\% and 40.1\%, respectively. For the joint registration of all images in a sequence, Edge-Driven DB-ICP succeeded in 59 sequences, which is a 23\% improvement over GDB-ICP.},
  keywords = {Algorithms,Biomedical imaging,biomedical optical imaging,Cluster Analysis,Computer science,Databases Factual,edge-driven DB-ICP,edge-driven dual-bootstrap iterative closest point algorithm,eye,FA image,Fluorescein angiogram,fluorescein angiogram sequence,Fluorescein Angiography,GDB-ICP,generalized dual-bootstrap iterative closest point,Humans,Image Processing Computer-Assisted,image registration,Image registration,image sequences,Iterative algorithms,iterative closest point,Iterative closest point algorithm,iterative methods,keypoint descriptor,keypoint matching,Lowe keypoint matches,medical image processing,multimodal image registration,ophthalmology,Pathology,Radio frequency,rank-orders Lowe keypoint,red-free image,registration,Retina,retinal imaging,Retinal vessels,Robustness,statistical analysis},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\QRTR9QMQ\\Tsai et al. - 2010 - The Edge-Driven Dual-Bootstrap Iterative Closest P.pdf;C\:\\Users\\cleme\\Zotero\\storage\\VYQA4ZAV\\5223602.html}
}

@article{tsiknakisDeepLearningDiabetic2021,
  title = {Deep Learning for Diabetic Retinopathy Detection and Classification Based on Fundus Images: {{A}} Review},
  shorttitle = {Deep Learning for Diabetic Retinopathy Detection and Classification Based on Fundus Images},
  author = {Tsiknakis, Nikos and Theodoropoulos, Dimitris and Manikis, Georgios and Ktistakis, Emmanouil and Boutsora, Ourania and Berto, Alexa and Scarpa, Fabio and Scarpa, Alberto and Fotiadis, Dimitrios I. and Marias, Kostas},
  year = {2021},
  month = aug,
  journal = {Computers in Biology and Medicine},
  volume = {135},
  pages = {104599},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104599},
  urldate = {2021-09-14},
  abstract = {Diabetic Retinopathy is a retina disease caused by diabetes mellitus and it is the leading cause of blindness globally. Early detection and treatment are necessary in order to delay or avoid vision deterioration and vision loss. To that end, many artificial-intelligence-powered methods have been proposed by the research community for the detection and classification of diabetic retinopathy on fundus retina images. This review article provides a thorough analysis of the use of deep learning methods at the various steps of the diabetic retinopathy detection pipeline based on fundus images. We discuss several aspects of that pipeline, ranging from the datasets that are widely used by the research community, the preprocessing techniques employed and how these accelerate and improve the models' performance, to the development of such deep learning models for the diagnosis and grading of the disease as well as the localization of the disease's lesions. We also discuss certain models that have been applied in real clinical settings. Finally, we conclude with some important insights and provide future research directions.},
  langid = {english},
  keywords = {Artificial intelligence,Classification,Deep learning,Detection,Diabetic retinopathy,Fundus,Retina,Review,Segmentation}
}

@article{tsiknakisDeepLearningDiabetic2021a,
  title = {Deep Learning for Diabetic Retinopathy Detection and Classification Based on Fundus Images: {{A}} Review},
  shorttitle = {Deep Learning for Diabetic Retinopathy Detection and Classification Based on Fundus Images},
  author = {Tsiknakis, Nikos and Theodoropoulos, Dimitris and Manikis, Georgios and Ktistakis, Emmanouil and Boutsora, Ourania and Berto, Alexa and Scarpa, Fabio and Scarpa, Alberto and Fotiadis, Dimitrios I. and Marias, Kostas},
  year = {2021},
  month = aug,
  journal = {Computers in Biology and Medicine},
  volume = {135},
  pages = {104599},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104599},
  urldate = {2021-09-14},
  abstract = {Diabetic Retinopathy is a retina disease caused by diabetes mellitus and it is the leading cause of blindness globally. Early detection and treatment are necessary in order to delay or avoid vision deterioration and vision loss. To that end, many artificial-intelligence-powered methods have been proposed by the research community for the detection and classification of diabetic retinopathy on fundus retina images. This review article provides a thorough analysis of the use of deep learning methods at the various steps of the diabetic retinopathy detection pipeline based on fundus images. We discuss several aspects of that pipeline, ranging from the datasets that are widely used by the research community, the preprocessing techniques employed and how these accelerate and improve the models' performance, to the development of such deep learning models for the diagnosis and grading of the disease as well as the localization of the disease's lesions. We also discuss certain models that have been applied in real clinical settings. Finally, we conclude with some important insights and provide future research directions.},
  langid = {english},
  keywords = {Artificial intelligence,Classification,Deep learning,Detection,Diabetic retinopathy,Fundus,Retina,Review,Segmentation},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\T6H6UV73\\Tsiknakis et al. - 2021 - Deep learning for diabetic retinopathy detection a.pdf;C\:\\Users\\cleme\\Zotero\\storage\\S5VU57TP\\S0010482521003930.html}
}

@inproceedings{tzengAdversarialDiscriminativeDomain2017,
  title = {Adversarial {{Discriminative Domain Adaptation}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
  year = {2017},
  month = jul,
  pages = {2962--2971},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.316},
  urldate = {2024-06-22},
  abstract = {Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.},
  keywords = {Adaptation models,Gallium nitride,Image reconstruction,Standards,Training,Visualization},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\RRTJE9J7\\Tzeng et al. - 2017 - Adversarial Discriminative Domain Adaptation.pdf;C\:\\Users\\cleme\\Zotero\\storage\\MY7BEE8B\\8099799.html}
}

@misc{UltrawideFieldRetinal,
  title = {Ultra-Wide Field Retinal Imaging: {{A}} Wider Clinical Perspective - {{PMC}}},
  urldate = {2024-02-22},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8012972/},
  file = {C:\Users\cleme\Zotero\storage\QDLIKCK5\PMC8012972.html}
}

@article{ulyanovInstanceNormalizationMissing2016,
  title = {Instance {{Normalization}}: {{The Missing Ingredient}} for {{Fast Stylization}}},
  shorttitle = {Instance {{Normalization}}},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  year = {2016},
  month = jul,
  abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will be made available at https://github.com/DmitryUlyanov/texture\_nets.}
}

@article{ulyanovInstanceNormalizationMissing2016a,
  title = {Instance {{Normalization}}: {{The Missing Ingredient}} for {{Fast Stylization}}},
  shorttitle = {Instance {{Normalization}}},
  author = {Ulyanov, Dmitry and Vedaldi, A. and Lempitsky, V.},
  year = {2016},
  month = jul,
  journal = {ArXiv},
  urldate = {2023-02-22},
  abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at this https URL. Full paper can be found at arXiv:1701.02096.}
}

@article{ulyanovInstanceNormalizationMissing2016b,
  title = {Instance {{Normalization}}: {{The Missing Ingredient}} for {{Fast Stylization}}},
  shorttitle = {Instance {{Normalization}}},
  author = {Ulyanov, Dmitry and Vedaldi, A. and Lempitsky, V.},
  year = {2016},
  month = jul,
  journal = {ArXiv},
  urldate = {2023-02-22},
  abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at this https URL. Full paper can be found at arXiv:1701.02096.},
  file = {C:\Users\cleme\Zotero\storage\ILUI26G3\Ulyanov et al. - 2016 - Instance Normalization The Missing Ingredient for.pdf}
}

@article{ulyanovInstanceNormalizationMissing2016c,
  title = {Instance {{Normalization}}: {{The Missing Ingredient}} for {{Fast Stylization}}},
  shorttitle = {Instance {{Normalization}}},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  year = {2016},
  month = jul,
  abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will be made available at https://github.com/DmitryUlyanov/texture\_nets.},
  file = {C:\Users\cleme\Zotero\storage\V4KA924R\Ulyanov et al. - 2016 - Instance Normalization The Missing Ingredient for.pdf}
}

@misc{UNetConvolutionalNetworks,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  urldate = {2019-11-24}
}

@misc{UNetConvolutionalNetworksa,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  urldate = {2019-11-24},
  howpublished = {https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/},
  file = {C:\Users\cleme\Zotero\storage\LGBNHFIQ\u-net.html}
}

@inproceedings{valanarasuMedicalTransformerGated2021,
  title = {Medical {{Transformer}}: {{Gated Axial-Attention}} for {{Medical Image Segmentation}}},
  shorttitle = {Medical {{Transformer}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Valanarasu, Jeya Maria Jose and Oza, Poojan and Hacihaliloglu, Ilker and Patel, Vishal M.},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {36--46},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_4},
  abstract = {Over the past decade, deep convolutional neural networks have been widely adopted for medical image segmentation and shown to achieve adequate performance. However, due to inherent inductive biases present in convolutional architectures, they lack understanding of long-range dependencies in the image. Recently proposed transformer-based architectures that leverage self-attention mechanism encode long-range dependencies and learn representations that are highly expressive. This motivates us to explore transformer-based solutions and study the feasibility of using transformer-based network architectures for medical image segmentation tasks. Majority of existing transformer-based network architectures proposed for vision applications require large-scale datasets to train properly. However, compared to the datasets for vision applications, in medical imaging the number of data samples is relatively low, making it difficult to efficiently train transformers for medical imaging applications. To this end, we propose a gated axial-attention model which extends the existing architectures by introducing an additional control mechanism in the self-attention module. Furthermore, to train the model effectively on medical images, we propose a Local-Global training strategy (LoGo) which further improves the performance. Specifically, we operate on the whole image and patches to learn global and local features, respectively. The proposed Medical Transformer (MedT) is evaluated on three different medical image segmentation datasets and it is shown that it achieves better performance than the convolutional and other related transformer-based architectures. Code: https://github.com/jeya-maria-jose/Medical-Transformer},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Medical image segmentation,Self-attention,Transformers}
}

@inproceedings{valanarasuMedicalTransformerGated2021a,
  title = {Medical {{Transformer}}: {{Gated Axial-Attention}} for {{Medical Image Segmentation}}},
  shorttitle = {Medical {{Transformer}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Valanarasu, Jeya Maria Jose and Oza, Poojan and Hacihaliloglu, Ilker and Patel, Vishal M.},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {36--46},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_4},
  abstract = {Over the past decade, deep convolutional neural networks have been widely adopted for medical image segmentation and shown to achieve adequate performance. However, due to inherent inductive biases present in convolutional architectures, they lack understanding of long-range dependencies in the image. Recently proposed transformer-based architectures that leverage self-attention mechanism encode long-range dependencies and learn representations that are highly expressive. This motivates us to explore transformer-based solutions and study the feasibility of using transformer-based network architectures for medical image segmentation tasks. Majority of existing transformer-based network architectures proposed for vision applications require large-scale datasets to train properly. However, compared to the datasets for vision applications, in medical imaging the number of data samples is relatively low, making it difficult to efficiently train transformers for medical imaging applications. To this end, we propose a gated axial-attention model which extends the existing architectures by introducing an additional control mechanism in the self-attention module. Furthermore, to train the model effectively on medical images, we propose a Local-Global training strategy (LoGo) which further improves the performance. Specifically, we operate on the whole image and patches to learn global and local features, respectively. The proposed Medical Transformer (MedT) is evaluated on three different medical image segmentation datasets and it is shown that it achieves better performance than the convolutional and other related transformer-based architectures. Code: https://github.com/jeya-maria-jose/Medical-Transformer},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Medical image segmentation,Self-attention,Transformers},
  file = {C:\Users\cleme\Zotero\storage\TER9EPCB\Valanarasu et al. - 2021 - Medical Transformer Gated Axial-Attention for Med.pdf}
}

@article{vanderheijdenValidationAutomatedScreening2018,
  title = {Validation of Automated Screening for Referable Diabetic Retinopathy with the {{IDx-DR}} Device in the {{Hoorn Diabetes Care System}}},
  author = {{van der Heijden}, Amber A. and Abramoff, Michael D. and Verbraak, Frank and {van Hecke}, Manon V. and Liem, Albert and Nijpels, Giel},
  year = {2018},
  month = feb,
  journal = {Acta Ophthalmologica},
  volume = {96},
  number = {1},
  pages = {63--68},
  issn = {1755-3768},
  doi = {10.1111/aos.13613},
  abstract = {PURPOSE: To increase the efficiency of retinal image grading, algorithms for automated grading have been developed, such as the IDx-DR 2.0 device. We aimed to determine the ability of this device, incorporated in clinical work flow, to detect retinopathy in persons with type 2 diabetes. METHODS: Retinal images of persons treated by the Hoorn Diabetes Care System (DCS) were graded by the IDx-DR device and independently by three retinal specialists using the International Clinical Diabetic Retinopathy severity scale (ICDR) and EURODIAB criteria. Agreement between specialists was calculated. Results of the IDx-DR device and experts were compared using sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV), distinguishing between referable diabetic retinopathy (RDR) and vision-threatening retinopathy (VTDR). Area under the receiver operating characteristic curve (AUC) was calculated. RESULTS: Of the included 1415 persons, 898 (63.5\%) had images of sufficient quality according to the experts and the IDx-DR device. Referable diabetic retinopathy (RDR) was diagnosed in 22 persons (2.4\%) using EURODIAB and 73 persons (8.1\%) using ICDR classification. Specific intergrader agreement ranged from 40\% to 61\%. Sensitivity, specificity, PPV and NPV of IDx-DR to detect RDR were 91\% (95\% CI: 0.69-0.98), 84\% (95\% CI: 0.81-0.86), 12\% (95\% CI: 0.08-0.18) and 100\% (95\% CI: 0.99-1.00; EURODIAB) and 68\% (95\% CI: 0.56-0.79), 86\% (95\% CI: 0.84-0.88), 30\% (95\% CI: 0.24-0.38) and 97\% (95\% CI: 0.95-0.98; ICDR). The AUC was 0.94 (95\% CI: 0.88-1.00; EURODIAB) and 0.87 (95\% CI: 0.83-0.92; ICDR). For detection of VTDR, sensitivity was lower and specificity was higher compared to RDR. AUC's were comparable. CONCLUSION: Automated grading using the IDx-DR device for RDR detection is a valid method and can be used in primary care, decreasing the demand on ophthalmologists.},
  langid = {english},
  pmcid = {PMC5814834},
  pmid = {29178249},
  keywords = {Aged,Algorithms,automated grading,Computer-Assisted,Diabetes Mellitus,diabetic retinopathy,Diabetic Retinopathy,Diagnostic Techniques,Equipment Design,Female,Humans,Image Interpretation,Incidence,Male,Mass Screening,Netherlands,Ophthalmological,ROC Curve,Type 2,type 2 diabetes,validation}
}

@article{vanderheijdenValidationAutomatedScreening2018a,
  title = {Validation of Automated Screening for Referable Diabetic Retinopathy with the {{IDx-DR}} Device in the {{Hoorn Diabetes Care System}}},
  author = {{van der Heijden}, Amber A. and Abramoff, Michael D. and Verbraak, Frank and {van Hecke}, Manon V. and Liem, Albert and Nijpels, Giel},
  year = {2018},
  month = feb,
  journal = {Acta Ophthalmologica},
  volume = {96},
  number = {1},
  pages = {63--68},
  issn = {1755-3768},
  doi = {10.1111/aos.13613},
  abstract = {PURPOSE: To increase the efficiency of retinal image grading, algorithms for automated grading have been developed, such as the IDx-DR 2.0 device. We aimed to determine the ability of this device, incorporated in clinical work flow, to detect retinopathy in persons with type 2 diabetes. METHODS: Retinal images of persons treated by the Hoorn Diabetes Care System (DCS) were graded by the IDx-DR device and independently by three retinal specialists using the International Clinical Diabetic Retinopathy severity scale (ICDR) and EURODIAB criteria. Agreement between specialists was calculated. Results of the IDx-DR device and experts were compared using sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV), distinguishing between referable diabetic retinopathy (RDR) and vision-threatening retinopathy (VTDR). Area under the receiver operating characteristic curve (AUC) was calculated. RESULTS: Of the included 1415 persons, 898 (63.5\%) had images of sufficient quality according to the experts and the IDx-DR device. Referable diabetic retinopathy (RDR) was diagnosed in 22 persons (2.4\%) using EURODIAB and 73 persons (8.1\%) using ICDR classification. Specific intergrader agreement ranged from 40\% to 61\%. Sensitivity, specificity, PPV and NPV of IDx-DR to detect RDR were 91\% (95\% CI: 0.69-0.98), 84\% (95\% CI: 0.81-0.86), 12\% (95\% CI: 0.08-0.18) and 100\% (95\% CI: 0.99-1.00; EURODIAB) and 68\% (95\% CI: 0.56-0.79), 86\% (95\% CI: 0.84-0.88), 30\% (95\% CI: 0.24-0.38) and 97\% (95\% CI: 0.95-0.98; ICDR). The AUC was 0.94 (95\% CI: 0.88-1.00; EURODIAB) and 0.87 (95\% CI: 0.83-0.92; ICDR). For detection of VTDR, sensitivity was lower and specificity was higher compared to RDR. AUC's were comparable. CONCLUSION: Automated grading using the IDx-DR device for RDR detection is a valid method and can be used in primary care, decreasing the demand on ophthalmologists.},
  langid = {english},
  pmcid = {PMC5814834},
  pmid = {29178249},
  keywords = {Aged,Algorithms,automated grading,Computer-Assisted,Diabetes Mellitus,diabetic retinopathy,Diabetic Retinopathy,Diagnostic Techniques,Equipment Design,Female,Humans,Image Interpretation,Incidence,Male,Mass Screening,Netherlands,Ophthalmological,ROC Curve,Type 2,type 2 diabetes,validation}
}

@article{vanderheijdenValidationAutomatedScreening2018b,
  title = {Validation of Automated Screening for Referable Diabetic Retinopathy with the {{IDx-DR}} Device in the {{Hoorn Diabetes Care System}}},
  author = {{van der Heijden}, Amber A. and Abramoff, Michael D. and Verbraak, Frank and {van Hecke}, Manon V. and Liem, Albert and Nijpels, Giel},
  year = {2018},
  month = feb,
  journal = {Acta Ophthalmologica},
  volume = {96},
  number = {1},
  pages = {63--68},
  issn = {1755-3768},
  doi = {10.1111/aos.13613},
  abstract = {PURPOSE: To increase the efficiency of retinal image grading, algorithms for automated grading have been developed, such as the IDx-DR 2.0 device. We aimed to determine the ability of this device, incorporated in clinical work flow, to detect retinopathy in persons with type 2 diabetes. METHODS: Retinal images of persons treated by the Hoorn Diabetes Care System (DCS) were graded by the IDx-DR device and independently by three retinal specialists using the International Clinical Diabetic Retinopathy severity scale (ICDR) and EURODIAB criteria. Agreement between specialists was calculated. Results of the IDx-DR device and experts were compared using sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV), distinguishing between referable diabetic retinopathy (RDR) and vision-threatening retinopathy (VTDR). Area under the receiver operating characteristic curve (AUC) was calculated. RESULTS: Of the included 1415 persons, 898 (63.5\%) had images of sufficient quality according to the experts and the IDx-DR device. Referable diabetic retinopathy (RDR) was diagnosed in 22 persons (2.4\%) using EURODIAB and 73 persons (8.1\%) using ICDR classification. Specific intergrader agreement ranged from 40\% to 61\%. Sensitivity, specificity, PPV and NPV of IDx-DR to detect RDR were 91\% (95\% CI: 0.69-0.98), 84\% (95\% CI: 0.81-0.86), 12\% (95\% CI: 0.08-0.18) and 100\% (95\% CI: 0.99-1.00; EURODIAB) and 68\% (95\% CI: 0.56-0.79), 86\% (95\% CI: 0.84-0.88), 30\% (95\% CI: 0.24-0.38) and 97\% (95\% CI: 0.95-0.98; ICDR). The AUC was 0.94 (95\% CI: 0.88-1.00; EURODIAB) and 0.87 (95\% CI: 0.83-0.92; ICDR). For detection of VTDR, sensitivity was lower and specificity was higher compared to RDR. AUC's were comparable. CONCLUSION: Automated grading using the IDx-DR device for RDR detection is a valid method and can be used in primary care, decreasing the demand on ophthalmologists.},
  langid = {english},
  pmcid = {PMC5814834},
  pmid = {29178249},
  keywords = {Aged,Algorithms,automated grading,Diabetes Mellitus Type 2,diabetic retinopathy,Diabetic Retinopathy,Diagnostic Techniques Ophthalmological,Equipment Design,Female,Humans,Image Interpretation Computer-Assisted,Incidence,Male,Mass Screening,Netherlands,ROC Curve,type 2 diabetes,validation},
  file = {C:\Users\cleme\Zotero\storage\PMLAA6QZ\van der Heijden et al. - 2018 - Validation of automated screening for referable di.pdf}
}

@article{vanderheijdenValidationAutomatedScreening2018c,
  title = {Validation of Automated Screening for Referable Diabetic Retinopathy with the {{IDx-DR}} Device in the {{Hoorn Diabetes Care System}}},
  author = {{van der Heijden}, Amber A. and Abramoff, Michael D. and Verbraak, Frank and {van Hecke}, Manon V. and Liem, Albert and Nijpels, Giel},
  year = {2018},
  month = feb,
  journal = {Acta Ophthalmologica},
  volume = {96},
  number = {1},
  pages = {63--68},
  issn = {1755-3768},
  doi = {10.1111/aos.13613},
  abstract = {PURPOSE: To increase the efficiency of retinal image grading, algorithms for automated grading have been developed, such as the IDx-DR 2.0 device. We aimed to determine the ability of this device, incorporated in clinical work flow, to detect retinopathy in persons with type 2 diabetes. METHODS: Retinal images of persons treated by the Hoorn Diabetes Care System (DCS) were graded by the IDx-DR device and independently by three retinal specialists using the International Clinical Diabetic Retinopathy severity scale (ICDR) and EURODIAB criteria. Agreement between specialists was calculated. Results of the IDx-DR device and experts were compared using sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV), distinguishing between referable diabetic retinopathy (RDR) and vision-threatening retinopathy (VTDR). Area under the receiver operating characteristic curve (AUC) was calculated. RESULTS: Of the included 1415 persons, 898 (63.5\%) had images of sufficient quality according to the experts and the IDx-DR device. Referable diabetic retinopathy (RDR) was diagnosed in 22 persons (2.4\%) using EURODIAB and 73 persons (8.1\%) using ICDR classification. Specific intergrader agreement ranged from 40\% to 61\%. Sensitivity, specificity, PPV and NPV of IDx-DR to detect RDR were 91\% (95\% CI: 0.69-0.98), 84\% (95\% CI: 0.81-0.86), 12\% (95\% CI: 0.08-0.18) and 100\% (95\% CI: 0.99-1.00; EURODIAB) and 68\% (95\% CI: 0.56-0.79), 86\% (95\% CI: 0.84-0.88), 30\% (95\% CI: 0.24-0.38) and 97\% (95\% CI: 0.95-0.98; ICDR). The AUC was 0.94 (95\% CI: 0.88-1.00; EURODIAB) and 0.87 (95\% CI: 0.83-0.92; ICDR). For detection of VTDR, sensitivity was lower and specificity was higher compared to RDR. AUC's were comparable. CONCLUSION: Automated grading using the IDx-DR device for RDR detection is a valid method and can be used in primary care, decreasing the demand on ophthalmologists.},
  langid = {english},
  pmcid = {PMC5814834},
  pmid = {29178249},
  keywords = {Aged,Algorithms,automated grading,Diabetes Mellitus Type 2,diabetic retinopathy,Diabetic Retinopathy,Diagnostic Techniques Ophthalmological,Equipment Design,Female,Humans,Image Interpretation Computer-Assisted,Incidence,Male,Mass Screening,Netherlands,ROC Curve,type 2 diabetes,validation},
  file = {C:\Users\cleme\Zotero\storage\FLS9MN7E\van der Heijden et al. - 2018 - Validation of automated screening for referable di.pdf}
}

@article{vangrinsvenFastConvolutionalNeural2016,
  title = {Fast {{Convolutional Neural Network Training Using Selective Data Sampling}}: {{Application}} to {{Hemorrhage Detection}} in {{Color Fundus Images}}},
  shorttitle = {Fast {{Convolutional Neural Network Training Using Selective Data Sampling}}},
  author = {{van Grinsven}, Mark J. J. P. and {van Ginneken}, Bram and Hoyng, Carel B. and Theelen, Thomas and S{\'a}nchez, Clara I.},
  year = {2016},
  month = may,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {35},
  number = {5},
  pages = {1273--1284},
  issn = {1558-254X},
  doi = {10.1109/TMI.2016.2526689},
  abstract = {Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.},
  keywords = {Biomedical imaging,Convolutional neural network,Databases,deep learning,hemorrhage,Hemorrhaging,Image analysis,Image color analysis,Observers,selective sampling,Training}
}

@article{vangrinsvenFastConvolutionalNeural2016a,
  title = {Fast {{Convolutional Neural Network Training Using Selective Data Sampling}}: {{Application}} to {{Hemorrhage Detection}} in {{Color Fundus Images}}},
  shorttitle = {Fast {{Convolutional Neural Network Training Using Selective Data Sampling}}},
  author = {{van Grinsven}, Mark J. J. P. and {van Ginneken}, Bram and Hoyng, Carel B. and Theelen, Thomas and S{\'a}nchez, Clara I.},
  year = {2016},
  month = may,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {35},
  number = {5},
  pages = {1273--1284},
  issn = {1558-254X},
  doi = {10.1109/TMI.2016.2526689},
  abstract = {Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.},
  keywords = {Biomedical imaging,Convolutional neural network,Databases,deep learning,hemorrhage,Hemorrhaging,Image analysis,Image color analysis,Observers,selective sampling,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\F2YNQJQ3\\van Grinsven et al. - 2016 - Fast Convolutional Neural Network Training Using S.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6EWZYS4S\\7401052.html}
}

@article{vaswaniAttentionAllYou,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  langid = {english}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  urldate = {2021-02-27},
  langid = {english}
}

@article{vaswaniAttentionAllYou2017a,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  urldate = {2021-02-27},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\FJEBDCG5\\Vaswani et al. - 2017 - Attention is All you Need.pdf;C\:\\Users\\cleme\\Zotero\\storage\\494CB9TW\\3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@article{vaswaniAttentionAllYoua,
  title = {Attention Is {{All}} You {{Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\C6NZYYXF\Vaswani et al. - Attention is All you Need.pdf}
}

@article{veldenDevelopmentObjectiveScheme1998,
  title = {Development of an {{Objective Scheme}} to {{Estimate Tropical Cyclone Intensity}} from {{Digital Geostationary Satellite Infrared Imagery}}},
  author = {Velden, Christopher S. and Olander, Timothy L. and Zehr, Raymond M.},
  year = {1998},
  month = mar,
  journal = {Weather and Forecasting},
  volume = {13},
  number = {1},
  pages = {172--186},
  issn = {0882-8156},
  doi = {10.1175/1520-0434(1998)013<0172:DOAOST>2.0.CO;2},
  urldate = {2019-06-10},
  abstract = {The standard method for estimating the intensity of tropical cyclones is based on satellite observations (Dvorak technique) and is utilized operationally by tropical analysis centers around the world. The technique relies on image pattern recognition along with analyst interpretation of empirically based rules regarding the vigor and organization of convection surrounding the storm center. While this method performs well enough in most cases to be employed operationally, there are situations when analyst judgment can lead to discrepancies between different analysis centers estimating the same storm. In an attempt to eliminate this subjectivity, a computer-based algorithm that operates objectively on digital infrared information has been developed. An original version of this algorithm (engineered primarily by the third author) has been significantly modified and advanced to include selected ``Dvorak rules,'' additional constraints, and a time-averaging scheme. This modified version, the Objective Dvorak Technique (ODT), is applicable to tropical cyclones that have attained tropical storm or hurricane strength. The performance of the ODT is evaluated on cases from the 1995 and 1996 Atlantic hurricane seasons. Reconnaissance aircraft measurements of minimum surface pressure are used to validate the satellite-based estimates. Statistical analysis indicates the technique to be competitive with, and in some cases superior to, the Dvorak-based intensity estimates produced operationally by satellite analysts from tropical analysis centers. Further analysis reveals situations where the algorithm needs improvement, and directions for future research and modifications are suggested.}
}

@article{veldenDevelopmentObjectiveScheme1998a,
  title = {Development of an {{Objective Scheme}} to {{Estimate Tropical Cyclone Intensity}} from {{Digital Geostationary Satellite Infrared Imagery}}},
  author = {Velden, Christopher S. and Olander, Timothy L. and Zehr, Raymond M.},
  year = {1998},
  month = mar,
  journal = {Weather and Forecasting},
  volume = {13},
  number = {1},
  pages = {172--186},
  issn = {0882-8156},
  doi = {10.1175/1520-0434(1998)013<0172:DOAOST>2.0.CO;2},
  urldate = {2019-06-10},
  abstract = {The standard method for estimating the intensity of tropical cyclones is based on satellite observations (Dvorak technique) and is utilized operationally by tropical analysis centers around the world. The technique relies on image pattern recognition along with analyst interpretation of empirically based rules regarding the vigor and organization of convection surrounding the storm center. While this method performs well enough in most cases to be employed operationally, there are situations when analyst judgment can lead to discrepancies between different analysis centers estimating the same storm. In an attempt to eliminate this subjectivity, a computer-based algorithm that operates objectively on digital infrared information has been developed. An original version of this algorithm (engineered primarily by the third author) has been significantly modified and advanced to include selected ``Dvorak rules,'' additional constraints, and a time-averaging scheme. This modified version, the Objective Dvorak Technique (ODT), is applicable to tropical cyclones that have attained tropical storm or hurricane strength. The performance of the ODT is evaluated on cases from the 1995 and 1996 Atlantic hurricane seasons. Reconnaissance aircraft measurements of minimum surface pressure are used to validate the satellite-based estimates. Statistical analysis indicates the technique to be competitive with, and in some cases superior to, the Dvorak-based intensity estimates produced operationally by satellite analysts from tropical analysis centers. Further analysis reveals situations where the algorithm needs improvement, and directions for future research and modifications are suggested.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\Z6NAG7UA\\Velden et al. - 1998 - Development of an Objective Scheme to Estimate Tro.pdf;C\:\\Users\\cleme\\Zotero\\storage\\H5KEMKBV\\1520-0434(1998)0130172DOAOST2.0.html}
}

@article{velickovic2018graph,
  title = {Graph Attention Networks},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  journal = {International Conference on Learning Representations}
}

@article{velickovic2018graph,
  title = {Graph Attention Networks},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  journal = {International Conference on Learning Representations}
}

@article{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  journal = {arXiv:1710.10903 [cs, stat]},
  eprint = {1710.10903},
  primaryclass = {cs, stat},
  urldate = {2019-12-30},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning}
}

@article{velickovicGraphAttentionNetworks2018a,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  journal = {arXiv:1710.10903 [cs, stat]},
  eprint = {1710.10903},
  primaryclass = {cs, stat},
  urldate = {2019-12-30},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\WRC8JRYM\Veličković et al. - 2018 - Graph Attention Networks.pdf}
}

@article{viergeverSurveyMedicalImage2016,
  title = {A Survey of Medical Image Registration -- under Review},
  author = {Viergever, Max A. and Maintz, J. B. Antoine and Klein, Stefan and Murphy, Keelin and Staring, Marius and Pluim, Josien P. W.},
  year = {2016},
  month = oct,
  journal = {Medical Image Analysis},
  series = {20th Anniversary of the {{Medical Image Analysis}} Journal ({{MedIA}})},
  volume = {33},
  pages = {140--144},
  issn = {1361-8415},
  doi = {10.1016/j.media.2016.06.030},
  urldate = {2019-08-08},
  abstract = {A retrospective view on the past two decades of the field of medical image registration is presented, guided by the article ``A survey of medical image registration'' (Maintz and Viergever, 1998). It shows that the classification of the field introduced in that article is still usable, although some modifications to do justice to advances in the field would be due. The main changes over the last twenty years are the shift from extrinsic to intrinsic registration, the primacy of intensity-based registration, the breakthrough of nonlinear registration, the progress of inter-subject registration, and the availability of generic image registration software packages. Two problems that were called urgent already 20 years ago, are even more urgent nowadays: Validation of registration methods, and translation of results of image registration research to clinical practice. It may be concluded that the field of medical image registration has evolved, but still is in need of further development in various aspects.},
  keywords = {Medical image registration},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\YWXVPSYC\\Viergever et al. - 2016 - A survey of medical image registration – under rev.pdf;C\:\\Users\\cleme\\Zotero\\storage\\5SYG28SG\\S1361841516301074.html}
}

@article{virgiliOpticalCoherenceTomography2015,
  title = {Optical Coherence Tomography ({{OCT}}) for Detection of Macular Oedema in Patients with Diabetic Retinopathy},
  author = {Virgili, Gianni and Menchini, Francesca and Casazza, Giovanni and Hogg, Ruth and Das, Radha R. and Wang, Xue and Michelessi, Manuele},
  year = {2015},
  journal = {Cochrane Database of Systematic Reviews},
  number = {1},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1465-1858},
  doi = {10.1002/14651858.CD008081.pub3},
  urldate = {2022-06-25},
  langid = {english}
}

@article{virgiliOpticalCoherenceTomography2015a,
  title = {Optical Coherence Tomography ({{OCT}}) for Detection of Macular Oedema in Patients with Diabetic Retinopathy},
  author = {Virgili, Gianni and Menchini, Francesca and Casazza, Giovanni and Hogg, Ruth and Das, Radha R. and Wang, Xue and Michelessi, Manuele},
  year = {2015},
  journal = {Cochrane Database of Systematic Reviews},
  number = {1},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1465-1858},
  doi = {10.1002/14651858.CD008081.pub3},
  urldate = {2022-06-25},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\TANQBYLU\\Virgili et al. - 2015 - Optical coherence tomography (OCT) for detection o.pdf;C\:\\Users\\cleme\\Zotero\\storage\\96PNDE4S\\abstract.html}
}

@inproceedings{vorontsovLabelNoiseSegmentation2021,
  title = {Label {{Noise}} in {{Segmentation Networks}}: {{Mitigation Must Deal}} with {{Bias}}},
  shorttitle = {Label {{Noise}} in {{Segmentation Networks}}},
  booktitle = {Deep {{Generative Models}}, and {{Data Augmentation}}, {{Labelling}}, and {{Imperfections}}},
  author = {Vorontsov, Eugene and Kadoury, Samuel},
  editor = {Engelhardt, Sandy and Oksuz, Ilkay and Zhu, Dajiang and Yuan, Yixuan and Mukhopadhyay, Anirban and Heller, Nicholas and Huang, Sharon Xiaolei and Nguyen, Hien and Sznitman, Raphael and Xue, Yuan},
  year = {2021},
  pages = {251--258},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-88210-5_25},
  abstract = {Imperfect labels limit the quality of predictions learned by deep neural networks. This is particularly relevant in medical image segmentation, where reference annotations are difficult to collect and vary significantly even across expert annotators. Prior work on mitigating label noise focused on simple models of mostly uniform noise. In this work, we explore biased and unbiased errors artificially introduced to brain tumour annotations on MRI data. We found that supervised and semi-supervised segmentation methods are robust or fairly robust to unbiased errors but sensitive to biased errors. It is therefore important to identify the sorts of errors expected in medical image labels and especially mitigate the biased errors.},
  isbn = {978-3-030-88210-5},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\E5BW473U\Vorontsov and Kadoury - 2021 - Label Noise in Segmentation Networks Mitigation M.pdf}
}

@article{vujosevicHyperreflectiveIntraretinalSpots2013,
  title = {Hyperreflective {{Intraretinal Spots}} in {{Diabetics}} without and with {{Nonproliferative Diabetic Retinopathy}}: {{An In Vivo Study Using Spectral Domain OCT}}},
  shorttitle = {Hyperreflective {{Intraretinal Spots}} in {{Diabetics}} without and with {{Nonproliferative Diabetic Retinopathy}}},
  author = {Vujosevic, Stela and Bini, Silvia and Midena, Giulia and Berton, Marianna and Pilotto, Elisabetta and Midena, Edoardo},
  year = {2013},
  month = dec,
  journal = {Journal of Diabetes Research},
  volume = {2013},
  pages = {e491835},
  publisher = {Hindawi},
  issn = {2314-6745},
  doi = {10.1155/2013/491835},
  urldate = {2022-07-09},
  abstract = {Purpose. To evaluate the presence of hyperreflective spots (HRS) in diabetic patients without clinically detectable retinopathy (no DR) or with nonproliferative mild to moderate retinopathy (DR) without macular edema, and compare the results to controls. Methods. 36 subjects were enrolled: 12 with no DR, 12 with DR, and 12 normal subjects who served as controls. All studied subjects underwent full ophthalmologic examination and spectral domain optical coherence tomography (SD-OCT). SD-OCT images were analyzed to measure and localize HRS. Each image was analyzed by two independent, masked examiners. Results. The number of HRS was significantly higher in both diabetics without and with retinopathy versus controls ( ) and in diabetics with retinopathy versus diabetics without retinopathy ( ). The HRS were mainly located in the inner retina layers (inner limiting membrane, ganglion cell layer, and inner nuclear layer). The intraobserver and interobserver agreement was almost perfect ( ). Conclusions. SD-OCT hyperreflective spots are present in diabetic eyes even when clinical retinopathy is undetectable. Their number increases with progressing retinopathy. Initially, HRS are mainly located in the inner retina, where the resident microglia is present. With progressing retinopathy, HRS reach the outer retinal layer. HRS may represent a surrogate of microglial activation in diabetic retina.},
  langid = {english}
}

@article{vujosevicHyperreflectiveIntraretinalSpots2013a,
  title = {Hyperreflective {{Intraretinal Spots}} in {{Diabetics}} without and with {{Nonproliferative Diabetic Retinopathy}}: {{An In Vivo Study Using Spectral Domain OCT}}},
  shorttitle = {Hyperreflective {{Intraretinal Spots}} in {{Diabetics}} without and with {{Nonproliferative Diabetic Retinopathy}}},
  author = {Vujosevic, Stela and Bini, Silvia and Midena, Giulia and Berton, Marianna and Pilotto, Elisabetta and Midena, Edoardo},
  year = {2013},
  month = dec,
  journal = {Journal of Diabetes Research},
  volume = {2013},
  pages = {e491835},
  publisher = {Hindawi},
  issn = {2314-6745},
  doi = {10.1155/2013/491835},
  urldate = {2022-07-09},
  abstract = {Purpose. To evaluate the presence of hyperreflective spots (HRS) in diabetic patients without clinically detectable retinopathy (no DR) or with nonproliferative mild to moderate retinopathy (DR) without macular edema, and compare the results to controls. Methods. 36 subjects were enrolled: 12 with no DR, 12 with DR, and 12 normal subjects who served as controls. All studied subjects underwent full ophthalmologic examination and spectral domain optical coherence tomography (SD-OCT). SD-OCT images were analyzed to measure and localize HRS. Each image was analyzed by two independent, masked examiners. Results. The number of HRS was significantly higher in both diabetics without and with retinopathy versus controls ( ) and in diabetics with retinopathy versus diabetics without retinopathy ( ). The HRS were mainly located in the inner retina layers (inner limiting membrane, ganglion cell layer, and inner nuclear layer). The intraobserver and interobserver agreement was almost perfect ( ). Conclusions. SD-OCT hyperreflective spots are present in diabetic eyes even when clinical retinopathy is undetectable. Their number increases with progressing retinopathy. Initially, HRS are mainly located in the inner retina, where the resident microglia is present. With progressing retinopathy, HRS reach the outer retinal layer. HRS may represent a surrogate of microglial activation in diabetic retina.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\29N9EGFS\\Vujosevic et al. - 2013 - Hyperreflective Intraretinal Spots in Diabetics wi.pdf;C\:\\Users\\cleme\\Zotero\\storage\\2YIXX6GM\\vujosevic2013.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\IT5VU5BG\\491835.html}
}

@inproceedings{wahidClassificationDiabeticRetinopathy2021,
  title = {Classification of {{Diabetic Retinopathy}} from {{OCT Images}} Using {{Deep Convolutional Neural Network}} with {{BiLSTM}} and {{SVM}}},
  booktitle = {2021 12th {{International Conference}} on {{Computing Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
  author = {Wahid, Md. Ferdous and Aowlad Hossain, A. B. M.},
  year = {2021},
  month = jul,
  pages = {1--5},
  doi = {10.1109/ICCCNT51525.2021.9579901},
  abstract = {Diabetic retinopathy (DR) is an eye disease that, if not diagnosed at an early stage, can cause vision loss and blindness in diabetic patients. Therefore, in the field of ophthalmology, optical coherence tomography (OCT) imaging is widely used in the early diagnosis and treatment of DR. However, it is arduous and time consuming to manually classify and detect DR from retinal OCT image. In this context, this paper proposed a deep convolutional neural network combined with Bidirectional long-short term memory and support vector machine (CNN-BiLSTM+SVM) for automatic DR classification from OCT image. Here, CNN-BiLSTM architecture is used as feature extractor where CNN extracts the local features and BiLSTM learns correlation among the extracted features. SVM is subsequently trained to diagnose diabetic retinopathy using the extracted features. The effectiveness of the proposed model is assessed on blind test dataset of 1000 images (250 per class) labeled into four classes. The proposed model has attained satisfactory results in the classification of diabetic retinopathy, with an accuracy of 99.30\%, which also proves its competitiveness in comparison to other existing state-of-the-art.},
  keywords = {Biomedical Image Processing,Deep Neural Convolutional Network,Diabetic Retinopathy,Feature extraction,Feature Extraction,Neural networks,Optical coherence tomography,Optical losses,Retina,Retinal OCT image,Retinopathy,Support vector machines}
}

@inproceedings{wahidClassificationDiabeticRetinopathy2021a,
  title = {Classification of {{Diabetic Retinopathy}} from {{OCT Images}} Using {{Deep Convolutional Neural Network}} with {{BiLSTM}} and {{SVM}}},
  booktitle = {2021 12th {{International Conference}} on {{Computing Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
  author = {Wahid, Md. Ferdous and Aowlad Hossain, A. B. M.},
  year = {2021},
  month = jul,
  pages = {1--5},
  doi = {10.1109/ICCCNT51525.2021.9579901},
  abstract = {Diabetic retinopathy (DR) is an eye disease that, if not diagnosed at an early stage, can cause vision loss and blindness in diabetic patients. Therefore, in the field of ophthalmology, optical coherence tomography (OCT) imaging is widely used in the early diagnosis and treatment of DR. However, it is arduous and time consuming to manually classify and detect DR from retinal OCT image. In this context, this paper proposed a deep convolutional neural network combined with Bidirectional long-short term memory and support vector machine (CNN-BiLSTM+SVM) for automatic DR classification from OCT image. Here, CNN-BiLSTM architecture is used as feature extractor where CNN extracts the local features and BiLSTM learns correlation among the extracted features. SVM is subsequently trained to diagnose diabetic retinopathy using the extracted features. The effectiveness of the proposed model is assessed on blind test dataset of 1000 images (250 per class) labeled into four classes. The proposed model has attained satisfactory results in the classification of diabetic retinopathy, with an accuracy of 99.30\%, which also proves its competitiveness in comparison to other existing state-of-the-art.},
  keywords = {Biomedical Image Processing,Deep Neural Convolutional Network,Diabetic Retinopathy,Feature extraction,Feature Extraction,Neural networks,Optical coherence tomography,Optical losses,Retina,Retinal OCT image,Retinopathy,Support vector machines},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\2QM6K7L6\\Wahid et Aowlad Hossain - 2021 - Classification of Diabetic Retinopathy from OCT Im.pdf;C\:\\Users\\cleme\\Zotero\\storage\\7EE2DCHM\\9579901.html}
}

@inproceedings{wang2020axial,
  title = {Axial-Deeplab: {{Stand-alone}} Axial-Attention for Panoptic Segmentation},
  booktitle = {European Conference on Computer Vision ({{ECCV}})},
  author = {Wang, Huiyu and Zhu, Yukun and Green, Bradley and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh},
  year = {2020}
}

@inproceedings{wang2020axial,
  title = {Axial-Deeplab: {{Stand-alone}} Axial-Attention for Panoptic Segmentation},
  booktitle = {European Conference on Computer Vision ({{ECCV}})},
  author = {Wang, Huiyu and Zhu, Yukun and Green, Bradley and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh},
  year = {2020}
}

@inproceedings{wang2021transbts,
  title = {{{TransBTS}}: {{Multimodal}} Brain Tumor Segmentation Using Transformer},
  booktitle = {International Conference on Medical Image Computing and Computer Assisted Intervention ({{MICCAI}})},
  author = {Wang, Wenxuan and Chen, Chen and Ding, Meng and Li, Jiangyun and Yu, Hong and Zha, Sen},
  year = {2021}
}

@inproceedings{wang2021transbts,
  title = {{{TransBTS}}: {{Multimodal}} Brain Tumor Segmentation Using Transformer},
  booktitle = {International Conference on Medical Image Computing and Computer Assisted Intervention ({{MICCAI}})},
  author = {Wang, Wenxuan and Chen, Chen and Ding, Meng and Li, Jiangyun and Yu, Hong and Zha, Sen},
  year = {2021}
}

@article{wangAutomatedDiagnosisSegmentation2020,
  title = {Automated Diagnosis and Segmentation of Choroidal Neovascularization in {{OCT}} Angiography Using Deep Learning},
  author = {Wang, Jie and Wang, Jie and Hormel, Tristan T. and Gao, Liqin and Gao, Liqin and Zang, Pengxiao and Zang, Pengxiao and Guo, Yukun and Wang, Xiaogang and Bailey, Steven T. and Jia, Yali and Jia, Yali},
  year = {2020},
  month = feb,
  journal = {Biomedical Optics Express},
  volume = {11},
  number = {2},
  pages = {927--944},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.379977},
  urldate = {2022-07-08},
  abstract = {Accurate identification and segmentation of choroidal neovascularization (CNV) is essential for the diagnosis and management of exudative age-related macular degeneration (AMD). Projection-resolved optical coherence tomographic angiography (PR-OCTA) enables both cross-sectional and en face visualization of CNV. However, CNV identification and segmentation remains difficult even with PR-OCTA due to the presence of residual artifacts. In this paper, a fully automated CNV diagnosis and segmentation algorithm using convolutional neural networks (CNNs) is described. This study used a clinical dataset, including both scans with and without CNV, and scans of eyes with different pathologies. Furthermore, no scans were excluded due to image quality. In testing, all CNV cases were diagnosed from non-CNV controls with 100\&\#x0025; sensitivity and 95\&\#x0025; specificity. The mean intersection over union of CNV membrane segmentation was as high as 0.88. By enabling fully automated categorization and segmentation, the proposed algorithm should offer benefits for CNV diagnosis, visualization monitoring.},
  copyright = {\&\#169; 2020 Optical Society of America},
  langid = {english}
}

@article{wangAutomatedDiagnosisSegmentation2020a,
  title = {Automated Diagnosis and Segmentation of Choroidal Neovascularization in {{OCT}} Angiography Using Deep Learning},
  author = {Wang, Jie and Wang, Jie and Hormel, Tristan T. and Gao, Liqin and Gao, Liqin and Zang, Pengxiao and Zang, Pengxiao and Guo, Yukun and Wang, Xiaogang and Bailey, Steven T. and Jia, Yali and Jia, Yali},
  year = {2020},
  month = feb,
  journal = {Biomedical Optics Express},
  volume = {11},
  number = {2},
  pages = {927--944},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.379977},
  urldate = {2022-07-08},
  abstract = {Accurate identification and segmentation of choroidal neovascularization (CNV) is essential for the diagnosis and management of exudative age-related macular degeneration (AMD). Projection-resolved optical coherence tomographic angiography (PR-OCTA) enables both cross-sectional and en face visualization of CNV. However, CNV identification and segmentation remains difficult even with PR-OCTA due to the presence of residual artifacts. In this paper, a fully automated CNV diagnosis and segmentation algorithm using convolutional neural networks (CNNs) is described. This study used a clinical dataset, including both scans with and without CNV, and scans of eyes with different pathologies. Furthermore, no scans were excluded due to image quality. In testing, all CNV cases were diagnosed from non-CNV controls with 100\&\#x0025; sensitivity and 95\&\#x0025; specificity. The mean intersection over union of CNV membrane segmentation was as high as 0.88. By enabling fully automated categorization and segmentation, the proposed algorithm should offer benefits for CNV diagnosis, visualization monitoring.},
  copyright = {\&\#169; 2020 Optical Society of America},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\W5CJPZMW\Wang et al. - 2020 - Automated diagnosis and segmentation of choroidal .pdf}
}

@inproceedings{wangBoundaryAwareTransformersSkin2021,
  title = {Boundary-{{Aware Transformers}} for {{Skin Lesion Segmentation}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Wang, Jiacheng and Wei, Lan and Wang, Liansheng and Zhou, Qichao and Zhu, Lei and Qin, Jing},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {206--216},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_20},
  abstract = {Skin lesion segmentation from dermoscopy images is of great importance for improving the quantitative analysis of skin cancer. However, the automatic segmentation of melanoma is a very challenging task owing to the large variation of melanoma and ambiguous boundaries of lesion areas. While convolutional neutral networks (CNNs) have achieved remarkable progress in this task, most of existing solutions are still incapable of effectively capturing global dependencies to counteract the inductive bias caused by limited receptive fields. Recently, transformers have been proposed as a promising tool for global context modeling by employing a powerful global attention mechanism, but one of their main shortcomings when applied to segmentation tasks is that they cannot effectively extract sufficient local details to tackle ambiguous boundaries. We propose a novel boundary-aware transformer (BAT) to comprehensively address the challenges of automatic skin lesion segmentation. Specifically, we integrate a new boundary-wise attention gate (BAG) into transformers to enable the whole network to not only effectively model global long-range dependencies via transformers but also, simultaneously, capture more local details by making full use of boundary-wise prior knowledge. Particularly, the auxiliary supervision of BAG is capable of assisting transformers to learn position embedding as it provides much spatial information. We conducted extensive experiments to evaluate the proposed BAT and experiments corroborate its effectiveness, consistently outperforming state-of-the-art methods in two famous datasets (Code is available at https://github.com/jcwang123/BA-Transformer).},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Deep learning,Medical image segmentation,Transformer}
}

@inproceedings{wangBoundaryAwareTransformersSkin2021a,
  title = {Boundary-{{Aware Transformers}} for {{Skin Lesion Segmentation}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Wang, Jiacheng and Wei, Lan and Wang, Liansheng and Zhou, Qichao and Zhu, Lei and Qin, Jing},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {206--216},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_20},
  abstract = {Skin lesion segmentation from dermoscopy images is of great importance for improving the quantitative analysis of skin cancer. However, the automatic segmentation of melanoma is a very challenging task owing to the large variation of melanoma and ambiguous boundaries of lesion areas. While convolutional neutral networks (CNNs) have achieved remarkable progress in this task, most of existing solutions are still incapable of effectively capturing global dependencies to counteract the inductive bias caused by limited receptive fields. Recently, transformers have been proposed as a promising tool for global context modeling by employing a powerful global attention mechanism, but one of their main shortcomings when applied to segmentation tasks is that they cannot effectively extract sufficient local details to tackle ambiguous boundaries. We propose a novel boundary-aware transformer (BAT) to comprehensively address the challenges of automatic skin lesion segmentation. Specifically, we integrate a new boundary-wise attention gate (BAG) into transformers to enable the whole network to not only effectively model global long-range dependencies via transformers but also, simultaneously, capture more local details by making full use of boundary-wise prior knowledge. Particularly, the auxiliary supervision of BAG is capable of assisting transformers to learn position embedding as it provides much spatial information. We conducted extensive experiments to evaluate the proposed BAT and experiments corroborate its effectiveness, consistently outperforming state-of-the-art methods in two famous datasets (Code is available at https://github.com/jcwang123/BA-Transformer).},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Deep learning,Medical image segmentation,Transformer},
  file = {C:\Users\cleme\Zotero\storage\SMFN7QHQ\Wang et al. - 2021 - Boundary-Aware Transformers for Skin Lesion Segmen.pdf}
}

@article{wangCausalScreeningInterpret2020,
  title = {Causal {{Screening}} to {{Interpret Graph Neural Networks}}},
  author = {Wang, Xiang and Wu, Yingxin and Zhang, An and He, Xiangnan and Chua, Tat-seng},
  year = {2020},
  month = oct,
  urldate = {2023-10-03},
  abstract = {With the growing success of graph neural networks (GNNs), the explainability of GNN is attracting considerable attention. However, current works on feature attribution, which frame explanation generation as attributing a prediction to the graph features, mostly focus on the statistical interpretability. They may struggle to distinguish causal and noncausal effects of features, and quantify redundancy among features, thus resulting in unsatisfactory explanations. In this work, we focus on the causal interpretability in GNNs and propose a method, Causal Screening, from the perspective of cause-effect. It incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, Causal Screening can be used to generate faithful and concise explanations for any GNN model. Further, by conducting extensive experiments on three graph classification datasets, we observe that Causal Screening achieves significant improvements over state-of-the-art approaches w.r.t. two quantitative metrics: predictive accuracy, contrastivity, and safely passes sanity checks.},
  langid = {english}
}

@article{wangCausalScreeningInterpret2020a,
  title = {Causal {{Screening}} to {{Interpret Graph Neural Networks}}},
  author = {Wang, Xiang and Wu, Yingxin and Zhang, An and He, Xiangnan and Chua, Tat-seng},
  year = {2020},
  month = oct,
  urldate = {2023-10-03},
  abstract = {With the growing success of graph neural networks (GNNs), the explainability of GNN is attracting considerable attention. However, current works on feature attribution, which frame explanation generation as attributing a prediction to the graph features, mostly focus on the statistical interpretability. They may struggle to distinguish causal and noncausal effects of features, and quantify redundancy among features, thus resulting in unsatisfactory explanations. In this work, we focus on the causal interpretability in GNNs and propose a method, Causal Screening, from the perspective of cause-effect. It incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, Causal Screening can be used to generate faithful and concise explanations for any GNN model. Further, by conducting extensive experiments on three graph classification datasets, we observe that Causal Screening achieves significant improvements over state-of-the-art approaches w.r.t. two quantitative metrics: predictive accuracy, contrastivity, and safely passes sanity checks.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\7TY2GT2G\Wang et al. - 2020 - Causal Screening to Interpret Graph Neural Network.pdf}
}

@article{wangCharacterizationFluidFlow1995,
  title = {Characterization of Fluid Flow Velocity by Optical {{Doppler}} Tomography},
  author = {Wang, X. J. and Milner, T. E. and Nelson, J. S.},
  year = {1995},
  month = jun,
  journal = {Optics Letters},
  volume = {20},
  number = {11},
  pages = {1337--1339},
  issn = {1539-4794},
  doi = {10.1364/OL.20.001337},
  urldate = {2019-11-14},
  abstract = {The spatial profiles of fluid flow velocity in transparent glass and turbid collagen conduits are measured by optical Doppler tomography (ODT). The flow velocity at a discrete user-specified spatial location in the conduit is determined by measurement of the Doppler shift of backscattered light from microspheres suspended in the flowing fluid. Experimental data and theoretical calculations are in excellent agreement. ODT is an accurate method for the characterization of high-resolution fluid flow velocity.},
  copyright = {\&\#169; 1995 Optical Society of America},
  langid = {english},
  keywords = {Doppler effect,Interference,Laser Doppler velocimetry,Light propagation,Power spectra,Spatial resolution}
}

@article{wangCharacterizationFluidFlow1995a,
  title = {Characterization of Fluid Flow Velocity by Optical {{Doppler}} Tomography},
  author = {Wang, X. J. and Milner, T. E. and Nelson, J. S.},
  year = {1995},
  month = jun,
  journal = {Optics Letters},
  volume = {20},
  number = {11},
  pages = {1337--1339},
  issn = {1539-4794},
  doi = {10.1364/OL.20.001337},
  urldate = {2019-11-14},
  abstract = {The spatial profiles of fluid flow velocity in transparent glass and turbid collagen conduits are measured by optical Doppler tomography (ODT). The flow velocity at a discrete user-specified spatial location in the conduit is determined by measurement of the Doppler shift of backscattered light from microspheres suspended in the flowing fluid. Experimental data and theoretical calculations are in excellent agreement. ODT is an accurate method for the characterization of high-resolution fluid flow velocity.},
  copyright = {\&\#169; 1995 Optical Society of America},
  langid = {english},
  keywords = {Doppler effect,Interference,Laser Doppler velocimetry,Light propagation,Power spectra,Spatial resolution},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\JV84784A\\Wang et al. - 1995 - Characterization of fluid flow velocity by optical.pdf;C\:\\Users\\cleme\\Zotero\\storage\\CEXM94TI\\abstract.html}
}

@article{wangDeepLearningQuality2019,
  title = {Deep Learning for Quality Assessment of Retinal {{OCT}} Images},
  author = {Wang, Jing and Wang, Jing and Wang, Jing and Deng, Guohua and Deng, Guohua and Li, Wanyue and Li, Wanyue and Chen, Yiwei and Gao, Feng and Liu, Hu and Liu, Hu and He, Yi and He, Yi and Shi, Guohua and Shi, Guohua and Shi, Guohua},
  year = {2019},
  month = dec,
  journal = {Biomedical Optics Express},
  volume = {10},
  number = {12},
  pages = {6057--6072},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.10.006057},
  urldate = {2022-07-08},
  abstract = {Optical coherence tomography (OCT) is a promising high-speed, non-invasive imaging modality providing high-resolution retinal scans. However, a variety of external factors such as light occlusion and patient movement can seriously degrade OCT image quality, which complicates manual retinopathy detection and computer-aided diagnosis. As such, this study first presents an OCT image quality assessment (OCT-IQA) system, capable of automatic classification based on signal completeness, location, and effectiveness. Four CNN architectures (VGG-16, Inception-V3, ResNet-18, and ResNet-50) from the ImageNet classification task were used to train the proposed OCT-IQA system via transfer learning. The ResNet-50 with the best performance was then integrated into the final OCT-IQA network. The usefulness of this approach was evaluated using retinopathy detection results. A retinopathy classification network was first trained by fine-tuning Inception-V3 model. The model was then applied to two test datasets, created randomly from the original dataset, one of which was screened by the OCT-IQA system and only included high quality images while the other was mixed by high and low quality images. Results showed that retinopathy detection accuracy and area under curve (AUC) were 3.75\&\#x0025; and 1.56\&\#x0025; higher, respectively, for the filtered data (compared with the unfiltered data). These experimental results demonstrate the effectiveness of the proposed OCT-IQA system and suggest that deep learning could be applied to the design of computer-aided systems (CADSs) for automatic retinopathy detection.},
  copyright = {\&\#169; 2019 Optical Society of America},
  langid = {english}
}

@article{wangDeepLearningQuality2019a,
  title = {Deep Learning for Quality Assessment of Retinal {{OCT}} Images},
  author = {Wang, Jing and Wang, Jing and Wang, Jing and Deng, Guohua and Deng, Guohua and Li, Wanyue and Li, Wanyue and Chen, Yiwei and Gao, Feng and Liu, Hu and Liu, Hu and He, Yi and He, Yi and Shi, Guohua and Shi, Guohua and Shi, Guohua},
  year = {2019},
  month = dec,
  journal = {Biomedical Optics Express},
  volume = {10},
  number = {12},
  pages = {6057--6072},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.10.006057},
  urldate = {2022-07-08},
  abstract = {Optical coherence tomography (OCT) is a promising high-speed, non-invasive imaging modality providing high-resolution retinal scans. However, a variety of external factors such as light occlusion and patient movement can seriously degrade OCT image quality, which complicates manual retinopathy detection and computer-aided diagnosis. As such, this study first presents an OCT image quality assessment (OCT-IQA) system, capable of automatic classification based on signal completeness, location, and effectiveness. Four CNN architectures (VGG-16, Inception-V3, ResNet-18, and ResNet-50) from the ImageNet classification task were used to train the proposed OCT-IQA system via transfer learning. The ResNet-50 with the best performance was then integrated into the final OCT-IQA network. The usefulness of this approach was evaluated using retinopathy detection results. A retinopathy classification network was first trained by fine-tuning Inception-V3 model. The model was then applied to two test datasets, created randomly from the original dataset, one of which was screened by the OCT-IQA system and only included high quality images while the other was mixed by high and low quality images. Results showed that retinopathy detection accuracy and area under curve (AUC) were 3.75\&\#x0025; and 1.56\&\#x0025; higher, respectively, for the filtered data (compared with the unfiltered data). These experimental results demonstrate the effectiveness of the proposed OCT-IQA system and suggest that deep learning could be applied to the design of computer-aided systems (CADSs) for automatic retinopathy detection.},
  copyright = {\&\#169; 2019 Optical Society of America},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\K9YZYLU3\\Wang et al. - 2019 - Deep learning for quality assessment of retinal OC.pdf;C\:\\Users\\cleme\\Zotero\\storage\\723U234U\\fulltext.html}
}

@article{wangOCTImageClassification2019,
  title = {On {{OCT Image Classification}} via {{Deep Learning}}},
  author = {Wang, Depeng and Wang, Liejun},
  year = {2019},
  month = oct,
  journal = {IEEE Photonics Journal},
  volume = {11},
  number = {5},
  pages = {1--14},
  issn = {1943-0655},
  doi = {10.1109/JPHOT.2019.2934484},
  abstract = {Computer-aided diagnosis of retinopathy is a research hotspot in the field of medical image classification. Diabetic macular edema (DME) and age-related macular degeneration (AMD) are two common ocular diseases that can result in partial or complete loss of vision. Optical coherence tomography imaging (OCT) is widely applied to the diagnosis of ocular diseases including DME and AMD. In this paper, an automatic method based on deep learning is proposed to detect AME and AMD lesions, in which two publicly available OCT datasets of retina were adopted and a network model with effective feature of reuse feature was applied to solve the problem of small datasets and enhance the adaptation to the difference of different datasets of the approach. Several network models with effective feature of reusable feature were compared and the transfer learning on networks with pre-trained models was realized. CliqueNet achieves better, classification results compared with other network models with a more than 0.98 accuracy and 0.99 of area under the curve (AUC) value finally.},
  keywords = {Adaptation models,age-related macular degeneration automated diagnosis,computer-aided diagnosis,Convolution,Deep learning,diabetic macular edema,Feature extraction,Image recognition,Neural networks,optical coherence tomography,Retina,Training}
}

@article{wangOCTImageClassification2019a,
  title = {On {{OCT Image Classification}} via {{Deep Learning}}},
  author = {Wang, Depeng and Wang, Liejun},
  year = {2019},
  month = oct,
  journal = {IEEE Photonics Journal},
  volume = {11},
  number = {5},
  pages = {1--14},
  issn = {1943-0655},
  doi = {10.1109/JPHOT.2019.2934484},
  abstract = {Computer-aided diagnosis of retinopathy is a research hotspot in the field of medical image classification. Diabetic macular edema (DME) and age-related macular degeneration (AMD) are two common ocular diseases that can result in partial or complete loss of vision. Optical coherence tomography imaging (OCT) is widely applied to the diagnosis of ocular diseases including DME and AMD. In this paper, an automatic method based on deep learning is proposed to detect AME and AMD lesions, in which two publicly available OCT datasets of retina were adopted and a network model with effective feature of reuse feature was applied to solve the problem of small datasets and enhance the adaptation to the difference of different datasets of the approach. Several network models with effective feature of reusable feature were compared and the transfer learning on networks with pre-trained models was realized. CliqueNet achieves better, classification results compared with other network models with a more than 0.98 accuracy and 0.99 of area under the curve (AUC) value finally.},
  keywords = {Adaptation models,age-related macular degeneration automated diagnosis,computer-aided diagnosis,Convolution,Deep learning,diabetic macular edema,Feature extraction,Image recognition,Neural networks,optical coherence tomography,Retina,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\NSFVK9GS\\Wang et Wang - 2019 - On OCT Image Classification via Deep Learning.pdf;C\:\\Users\\cleme\\Zotero\\storage\\GWL8ZP8X\\8794616.html}
}

@inproceedings{wangResidualAttentionNetwork2017,
  title = {Residual {{Attention Network}} for {{Image Classification}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
  year = {2017},
  month = jul,
  pages = {6450--6458},
  publisher = {IEEE},
  address = {Honolulu, HI, USA},
  doi = {10.1109/CVPR.2017.683},
  urldate = {2023-05-16},
  abstract = {In this work, we propose ``Residual Attention Network'', a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.},
  isbn = {978-1-5386-0457-1},
  langid = {english}
}

@inproceedings{wangResidualAttentionNetwork2017a,
  title = {Residual {{Attention Network}} for {{Image Classification}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
  year = {2017},
  month = jul,
  pages = {6450--6458},
  publisher = {IEEE},
  address = {Honolulu, HI, USA},
  doi = {10.1109/CVPR.2017.683},
  urldate = {2023-05-16},
  abstract = {In this work, we propose ``Residual Attention Network'', a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\86SAXQWK\Wang et al. - 2017 - Residual Attention Network for Image Classificatio.pdf}
}

@article{wangUDMILUncertaintyDrivenDeep2020,
  title = {{{UD-MIL}}: {{Uncertainty-Driven Deep Multiple Instance Learning}} for {{OCT Image Classification}}},
  shorttitle = {{{UD-MIL}}},
  author = {Wang, Xi and Tang, Fangyao and Chen, Hao and Luo, Luyang and Tang, Ziqi and Ran, An-Ran and Cheung, Carol Y. and Heng, Pheng-Ann},
  year = {2020},
  month = dec,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {24},
  number = {12},
  pages = {3431--3442},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2020.2983730},
  abstract = {Deep learning has achieved remarkable success in the optical coherence tomography (OCT) image classification task with substantial labelled B-scan images available. However, obtaining such fine-grained expert annotations is usually quite difficult and expensive. How to leverage the volume-level labels to develop a robust classifier is very appealing. In this paper, we propose a weakly supervised deep learning framework with uncertainty estimation to address the macula-related disease classification problem from OCT images with the only volume-level label being available. First, a convolutional neural network (CNN) based instance-level classifier is iteratively refined by using the proposed uncertainty-driven deep multiple instance learning scheme. To our best knowledge, we are the first to incorporate the uncertainty evaluation mechanism into multiple instance learning (MIL) for training a robust instance classifier. The classifier is able to detect suspicious abnormal instances and abstract the corresponding deep embedding with high representation capability simultaneously. Second, a recurrent neural network (RNN) takes instance features from the same bag as input and generates the final bag-level prediction by considering the individually local instance information and globally aggregated bag-level representation. For more comprehensive validation, we built two large diabetic macular edema (DME) OCT datasets from different devices and imaging protocols to evaluate the efficacy of our method, which are composed of 30,151 B-scans in 1,396 volumes from 274 patients (Heidelberg-DME dataset) and 38,976 B-scans in 3,248 volumes from 490 patients (Triton-DME dataset), respectively. We compare the proposed method with the state-of-the-art approaches, and experimentally demonstrate that our method is superior to alternative methods, achieving volume-level accuracy, F1-score and area under the receiver operating characteristic curve (AUC) of 95.1\%, 0.939 and 0.990 on Heidelberg-DME and those of 95.1\%, 0.935 and 0.986 on Triton-DME, respectively. Furthermore, the proposed method also yields competitive results on another public age-related macular degeneration OCT dataset, indicating the high potential as an effective screening tool in the clinical practice.},
  keywords = {Biomedical imaging,classification,Informatics,Machine learning,multiple instance learning,Optical coherence tomography,Retina,Supervised learning,Training data,Uncertainty,uncertainty estimation,Visualization}
}

@article{wangUDMILUncertaintyDrivenDeep2020a,
  title = {{{UD-MIL}}: {{Uncertainty-Driven Deep Multiple Instance Learning}} for {{OCT Image Classification}}},
  shorttitle = {{{UD-MIL}}},
  author = {Wang, Xi and Tang, Fangyao and Chen, Hao and Luo, Luyang and Tang, Ziqi and Ran, An-Ran and Cheung, Carol Y. and Heng, Pheng-Ann},
  year = {2020},
  month = dec,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {24},
  number = {12},
  pages = {3431--3442},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2020.2983730},
  abstract = {Deep learning has achieved remarkable success in the optical coherence tomography (OCT) image classification task with substantial labelled B-scan images available. However, obtaining such fine-grained expert annotations is usually quite difficult and expensive. How to leverage the volume-level labels to develop a robust classifier is very appealing. In this paper, we propose a weakly supervised deep learning framework with uncertainty estimation to address the macula-related disease classification problem from OCT images with the only volume-level label being available. First, a convolutional neural network (CNN) based instance-level classifier is iteratively refined by using the proposed uncertainty-driven deep multiple instance learning scheme. To our best knowledge, we are the first to incorporate the uncertainty evaluation mechanism into multiple instance learning (MIL) for training a robust instance classifier. The classifier is able to detect suspicious abnormal instances and abstract the corresponding deep embedding with high representation capability simultaneously. Second, a recurrent neural network (RNN) takes instance features from the same bag as input and generates the final bag-level prediction by considering the individually local instance information and globally aggregated bag-level representation. For more comprehensive validation, we built two large diabetic macular edema (DME) OCT datasets from different devices and imaging protocols to evaluate the efficacy of our method, which are composed of 30,151 B-scans in 1,396 volumes from 274 patients (Heidelberg-DME dataset) and 38,976 B-scans in 3,248 volumes from 490 patients (Triton-DME dataset), respectively. We compare the proposed method with the state-of-the-art approaches, and experimentally demonstrate that our method is superior to alternative methods, achieving volume-level accuracy, F1-score and area under the receiver operating characteristic curve (AUC) of 95.1\%, 0.939 and 0.990 on Heidelberg-DME and those of 95.1\%, 0.935 and 0.986 on Triton-DME, respectively. Furthermore, the proposed method also yields competitive results on another public age-related macular degeneration OCT dataset, indicating the high potential as an effective screening tool in the clinical practice.},
  keywords = {Biomedical imaging,classification,Informatics,Machine learning,multiple instance learning,Optical coherence tomography,Retina,Supervised learning,Training data,Uncertainty,uncertainty estimation,Visualization},
  file = {C:\Users\cleme\Zotero\storage\EIGU8LU4\10.1109@JBHI.2020.2983730.pdf.pdf}
}

@article{wangWeaklySupervisedAnomaly2021,
  title = {Weakly Supervised Anomaly Segmentation in Retinal {{OCT}} Images Using an Adversarial Learning Approach},
  author = {Wang, Jing and Li, Wanyue and Chen, Yiwei and Fang, Wangyi and Kong, Wen and He, Yi and Shi, Guohua},
  year = {2021},
  month = aug,
  journal = {Biomedical Optics Express},
  volume = {12},
  number = {8},
  pages = {4713--4729},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.426803},
  urldate = {2023-02-03},
  abstract = {Lesion detection is a critical component of disease diagnosis, but the manual segmentation of lesions in medical images is time-consuming and experience-demanding. These issues have recently been addressed through deep learning models. However, most of the existing algorithms were developed using supervised training, which requires time-intensive manual labeling and prevents the model from detecting unaware lesions. As such, this study proposes a weakly supervised learning network based on CycleGAN for lesions segmentation in full-width optical coherence tomography (OCT) images. The model was trained to reconstruct underlying normal anatomic structures from abnormal input images, then the lesions can be detected by calculating the difference between the input and output images. A customized network architecture and a multi-scale similarity perceptual reconstruction loss were used to extend the CycleGAN model to transfer between objects exhibiting shape deformations. The proposed technique was validated using an open-source retinal OCT image dataset. Image-level anomaly detection and pixel-level lesion detection results were assessed using area-under-curve (AUC) and the Dice similarity coefficient, producing results of 96.94\% and 0.8239, respectively, higher than all comparative methods. The average test time required to generate a single full-width image was 0.039 s, which is shorter than that reported in recent studies. These results indicate that our model can accurately detect and segment retinopathy lesions in real-time, without the need for supervised labeling. And we hope this method will be helpful to accelerate the clinical diagnosis process and reduce the misdiagnosis rate.},
  copyright = {{\copyright} 2021 Optical Society of America},
  langid = {english},
  keywords = {Image metrics,Image quality,Medical image processing,Medical imaging,Noise reduction,Single photon emission computed tomography}
}

@article{wangWeaklySupervisedAnomaly2021a,
  title = {Weakly Supervised Anomaly Segmentation in Retinal {{OCT}} Images Using an Adversarial Learning Approach},
  author = {Wang, Jing and Li, Wanyue and Chen, Yiwei and Fang, Wangyi and Kong, Wen and He, Yi and Shi, Guohua},
  year = {2021},
  month = aug,
  journal = {Biomedical Optics Express},
  volume = {12},
  number = {8},
  pages = {4713--4729},
  publisher = {Optica Publishing Group},
  issn = {2156-7085},
  doi = {10.1364/BOE.426803},
  urldate = {2023-02-03},
  abstract = {Lesion detection is a critical component of disease diagnosis, but the manual segmentation of lesions in medical images is time-consuming and experience-demanding. These issues have recently been addressed through deep learning models. However, most of the existing algorithms were developed using supervised training, which requires time-intensive manual labeling and prevents the model from detecting unaware lesions. As such, this study proposes a weakly supervised learning network based on CycleGAN for lesions segmentation in full-width optical coherence tomography (OCT) images. The model was trained to reconstruct underlying normal anatomic structures from abnormal input images, then the lesions can be detected by calculating the difference between the input and output images. A customized network architecture and a multi-scale similarity perceptual reconstruction loss were used to extend the CycleGAN model to transfer between objects exhibiting shape deformations. The proposed technique was validated using an open-source retinal OCT image dataset. Image-level anomaly detection and pixel-level lesion detection results were assessed using area-under-curve (AUC) and the Dice similarity coefficient, producing results of 96.94\% and 0.8239, respectively, higher than all comparative methods. The average test time required to generate a single full-width image was 0.039 s, which is shorter than that reported in recent studies. These results indicate that our model can accurately detect and segment retinopathy lesions in real-time, without the need for supervised labeling. And we hope this method will be helpful to accelerate the clinical diagnosis process and reduce the misdiagnosis rate.},
  copyright = {{\copyright} 2021 Optical Society of America},
  langid = {english},
  keywords = {Image metrics,Image quality,Medical image processing,Medical imaging,Noise reduction,Single photon emission computed tomography},
  file = {C:\Users\cleme\Zotero\storage\PW2ECZZL\Wang et al. - 2021 - Weakly supervised anomaly segmentation in retinal .pdf}
}

@article{wangWeaklySupervisedLesion2019,
  title = {Weakly {{Supervised Lesion Detection From Fundus Images}}},
  author = {Wang, R. and Chen, B. and Meng, D. and Wang, L.},
  year = {2019},
  month = jun,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {38},
  number = {6},
  pages = {1501--1512},
  issn = {0278-0062},
  doi = {10.1109/TMI.2018.2885376},
  abstract = {Early diagnosis and continuous monitoring of patients suffering from eye diseases have been major concerns in the computer-aided detection techniques. Detecting one or several specific types of retinal lesions has made a significant breakthrough in computer-aided screen in the past few decades. However, due to the variety of retinal lesions and complex normal anatomical structures, automatic detection of lesions with unknown and diverse types from a retina remains a challenging task. In this paper, a weakly supervised method, requiring only a series of normal and abnormal retinal images without need to specifically annotate their locations and types, is proposed for this task. Specifically, a fundus image is understood as a superposition of background, blood vessels, and background noise (lesions included for abnormal images). Background is formulated as a low-rank structure after a series of simple preprocessing steps, including spatial alignment, color normalization, and blood vessels removal. Background noise is regarded as stochastic variable and modeled through Gaussian for normal images and mixture of Gaussian for abnormal images, respectively. The proposed method encodes both the background knowledge of fundus images and the background noise into one unique model, and corporately optimizes the model using normal and abnormal images, which fully depict the low-rank subspace of the background and distinguish the lesions from the background noise in abnormal fundus images. Experimental results demonstrate that the proposed method is of fine arts accuracy and outperforms the previous related methods.},
  keywords = {abnormal fundus images,abnormal images,abnormal retinal images,automatic detection,background knowledge,background noise,Biomedical imaging,biomedical optical imaging,blood vessel removal,blood vessels,Blood vessels,color normalization,complex normal anatomical structures,Computer-aided detection,computer-aided detection techniques,computer-aided screen,diseases,Diseases,eye,eye diseases,fundus image,image classification,image segmentation,Lesions,locations,low-rank structure,medical image processing,mixture of Gaussian,Noise measurement,normal images,Retina,retinal lesions,Task analysis,weakly supervised learning,weakly supervised lesion detection,weakly supervised method}
}

@article{wangWeaklySupervisedLesion2019a,
  title = {Weakly {{Supervised Lesion Detection From Fundus Images}}},
  author = {Wang, R. and Chen, B. and Meng, D. and Wang, L.},
  year = {2019},
  month = jun,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {38},
  number = {6},
  pages = {1501--1512},
  issn = {0278-0062},
  doi = {10.1109/TMI.2018.2885376},
  abstract = {Early diagnosis and continuous monitoring of patients suffering from eye diseases have been major concerns in the computer-aided detection techniques. Detecting one or several specific types of retinal lesions has made a significant breakthrough in computer-aided screen in the past few decades. However, due to the variety of retinal lesions and complex normal anatomical structures, automatic detection of lesions with unknown and diverse types from a retina remains a challenging task. In this paper, a weakly supervised method, requiring only a series of normal and abnormal retinal images without need to specifically annotate their locations and types, is proposed for this task. Specifically, a fundus image is understood as a superposition of background, blood vessels, and background noise (lesions included for abnormal images). Background is formulated as a low-rank structure after a series of simple preprocessing steps, including spatial alignment, color normalization, and blood vessels removal. Background noise is regarded as stochastic variable and modeled through Gaussian for normal images and mixture of Gaussian for abnormal images, respectively. The proposed method encodes both the background knowledge of fundus images and the background noise into one unique model, and corporately optimizes the model using normal and abnormal images, which fully depict the low-rank subspace of the background and distinguish the lesions from the background noise in abnormal fundus images. Experimental results demonstrate that the proposed method is of fine arts accuracy and outperforms the previous related methods.},
  keywords = {abnormal fundus images,abnormal images,abnormal retinal images,automatic detection,background knowledge,background noise,Biomedical imaging,biomedical optical imaging,blood vessel removal,blood vessels,Blood vessels,color normalization,complex normal anatomical structures,Computer-aided detection,computer-aided detection techniques,computer-aided screen,diseases,Diseases,eye,eye diseases,fundus image,image classification,image segmentation,Lesions,locations,low-rank structure,medical image processing,mixture of Gaussian,Noise measurement,normal images,Retina,retinal lesions,Task analysis,weakly supervised learning,weakly supervised lesion detection,weakly supervised method},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\KZ76N8Z9\\Wang et al. - 2019 - Weakly Supervised Lesion Detection From Fundus Ima.pdf;C\:\\Users\\cleme\\Zotero\\storage\\IZZNPPPQ\\8566017.html}
}

@article{wassersteinASAStatementPValues2016,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2019-12-04}
}

@article{wassersteinASAStatementPValues2016a,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2019-12-04},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\2ASSCFIV\\wasserstein2016.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ABH8XE7M\\Wasserstein et Lazar - 2016 - The ASA Statement on p-Values Context, Process, a.pdf;C\:\\Users\\cleme\\Zotero\\storage\\ABB9Y38U\\00031305.2016.html;C\:\\Users\\cleme\\Zotero\\storage\\HGN8QVA6\\wasserstein2016.html}
}

@article{weiLearnSegmentRetinal2020,
  title = {Learn to {{Segment Retinal Lesions}} and {{Beyond}}},
  author = {Wei, Qijie and Li, Xirong and Yu, Weihong and Zhang, Xiao and Zhang, Yongpeng and Hu, Bojie and Mo, Bin and Gong, Di and Chen, Ning and Ding, Dayong and Chen, Youxin},
  year = {2020},
  month = oct,
  journal = {arXiv:1912.11619 [cs]},
  eprint = {1912.11619},
  primaryclass = {cs},
  urldate = {2021-04-14},
  abstract = {Towards automated retinal screening, this paper makes an endeavor to simultaneously achieve pixel-level retinal lesion segmentation and image-level disease classification. Such a multi-task approach is crucial for accurate and clinically interpretable disease diagnosis. Prior art is insufficient due to three challenges, i.e., lesions lacking objective boundaries, clinical importance of lesions irrelevant to their size, and the lack of one-to-one correspondence between lesion and disease classes. This paper attacks the three challenges in the context of diabetic retinopathy (DR) grading. We propose Lesion-Net, a new variant of fully convolutional networks, with its expansive path redesigned to tackle the first challenge. A dual Dice loss that leverages both semantic segmentation and image classification losses is introduced to resolve the second challenge. Lastly, we build a multi-task network that employs Lesion-Net as a sideattention branch for both DR grading and result interpretation. A set of 12K fundus images is manually segmented by 45 ophthalmologists for 8 DR-related lesions, resulting in 290K manual segments in total. Extensive experiments on this largescale dataset show that our proposed approach surpasses the prior art for multiple tasks including lesion segmentation, lesion classification and DR grading.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{weiLearnSegmentRetinal2020a,
  title = {Learn to {{Segment Retinal Lesions}} and {{Beyond}}},
  author = {Wei, Qijie and Li, Xirong and Yu, Weihong and Zhang, Xiao and Zhang, Yongpeng and Hu, Bojie and Mo, Bin and Gong, Di and Chen, Ning and Ding, Dayong and Chen, Youxin},
  year = {2020},
  month = oct,
  journal = {arXiv:1912.11619 [cs]},
  eprint = {1912.11619},
  primaryclass = {cs},
  urldate = {2021-04-14},
  abstract = {Towards automated retinal screening, this paper makes an endeavor to simultaneously achieve pixel-level retinal lesion segmentation and image-level disease classification. Such a multi-task approach is crucial for accurate and clinically interpretable disease diagnosis. Prior art is insufficient due to three challenges, i.e., lesions lacking objective boundaries, clinical importance of lesions irrelevant to their size, and the lack of one-to-one correspondence between lesion and disease classes. This paper attacks the three challenges in the context of diabetic retinopathy (DR) grading. We propose Lesion-Net, a new variant of fully convolutional networks, with its expansive path redesigned to tackle the first challenge. A dual Dice loss that leverages both semantic segmentation and image classification losses is introduced to resolve the second challenge. Lastly, we build a multi-task network that employs Lesion-Net as a sideattention branch for both DR grading and result interpretation. A set of 12K fundus images is manually segmented by 45 ophthalmologists for 8 DR-related lesions, resulting in 290K manual segments in total. Extensive experiments on this largescale dataset show that our proposed approach surpasses the prior art for multiple tasks including lesion segmentation, lesion classification and DR grading.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\UMYTT4DY\Wei et al. - 2020 - Learn to Segment Retinal Lesions and Beyond.pdf}
}

@inproceedings{weiLearnSegmentRetinal2021,
  title = {Learn to {{Segment Retinal Lesions}} and {{Beyond}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Wei, Qijie and Li, Xirong and Yu, Weihong and Zhang, Xiao and Zhang, Yongpeng and Hu, Bojie and Mo, Bin and Gong, Di and Chen, Ning and Ding, Dayong and Chen, Youxin},
  year = {2021},
  month = jan,
  pages = {7403--7410},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412088},
  abstract = {Towards automated retinal screening, this paper makes an endeavor to simultaneously achieve pixel-level retinal lesion segmentation and image-level disease classification. Such a multi-task approach is crucial for accurate and clinically interpretable disease diagnosis. Prior art is insufficient due to three challenges, i.e., lesions lacking objective boundaries, clinical importance of lesions irrelevant to their size, and the lack of one-to-one correspondence between lesion and disease classes. This paper attacks the three challenges in the context of diabetic retinopathy (DR) grading. We propose Lesion-Net, a new variant of fully convolutional networks, with its expansive path redesigned to tackle the first challenge. A dual Dice loss that leverages both semantic segmentation and image classification losses is introduced to resolve the second challenge. Lastly, we build a multi-task network that employs Lesion-Net as a side-attention branch for both DR grading and result interpretation. A set of 12K fundus images is manually segmented by 45 ophthalmologists for 8 DR-related lesions, resulting in 290K manual segments in total. Extensive experiments on this large-scale dataset show that our proposed approach surpasses the prior art for multiple tasks including lesion segmentation, lesion classification and DR grading.},
  keywords = {Art,Image segmentation,Pattern recognition,Retina,Retinopathy,Semantics,Training data}
}

@inproceedings{weiLearnSegmentRetinal2021a,
  title = {Learn to {{Segment Retinal Lesions}} and {{Beyond}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Wei, Qijie and Li, Xirong and Yu, Weihong and Zhang, Xiao and Zhang, Yongpeng and Hu, Bojie and Mo, Bin and Gong, Di and Chen, Ning and Ding, Dayong and Chen, Youxin},
  year = {2021},
  month = jan,
  pages = {7403--7410},
  publisher = {IEEE Computer Society},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412088},
  urldate = {2022-10-23},
  abstract = {Towards automated retinal screening, this paper makes an endeavor to simultaneously achieve pixel-level retinal lesion segmentation and image-level disease classification. Such a multi-task approach is crucial for accurate and clinically interpretable disease diagnosis. Prior art is insufficient due to three challenges, i.e., lesions lacking objective boundaries, clinical importance of lesions irrelevant to their size, and the lack of one-to-one correspondence between lesion and disease classes. This paper attacks the three challenges in the context of diabetic retinopathy (DR) grading. We propose Lesion-Net, a new variant of fully convolutional networks, with its expansive path redesigned to tackle the first challenge. A dual Dice loss that leverages both semantic segmentation and image classification losses is introduced to resolve the second challenge. Lastly, we build a multi-task network that employs Lesion-Net as a side-attention branch for both DR grading and result interpretation. A set of 12K fundus images is manually segmented by 45 ophthalmologists for 8 DR-related lesions, resulting in 290K manual segments in total. Extensive experiments on this large-scale dataset show that our proposed approach surpasses the prior art for multiple tasks including lesion segmentation, lesion classification and DR grading.},
  isbn = {978-1-72818-808-9},
  langid = {english}
}

@inproceedings{weiLearnSegmentRetinal2021b,
  title = {Learn to {{Segment Retinal Lesions}} and {{Beyond}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Wei, Qijie and Li, Xirong and Yu, Weihong and Zhang, Xiao and Zhang, Yongpeng and Hu, Bojie and Mo, Bin and Gong, Di and Chen, Ning and Ding, Dayong and Chen, Youxin},
  year = {2021},
  month = jan,
  pages = {7403--7410},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412088},
  abstract = {Towards automated retinal screening, this paper makes an endeavor to simultaneously achieve pixel-level retinal lesion segmentation and image-level disease classification. Such a multi-task approach is crucial for accurate and clinically interpretable disease diagnosis. Prior art is insufficient due to three challenges, i.e., lesions lacking objective boundaries, clinical importance of lesions irrelevant to their size, and the lack of one-to-one correspondence between lesion and disease classes. This paper attacks the three challenges in the context of diabetic retinopathy (DR) grading. We propose Lesion-Net, a new variant of fully convolutional networks, with its expansive path redesigned to tackle the first challenge. A dual Dice loss that leverages both semantic segmentation and image classification losses is introduced to resolve the second challenge. Lastly, we build a multi-task network that employs Lesion-Net as a side-attention branch for both DR grading and result interpretation. A set of 12K fundus images is manually segmented by 45 ophthalmologists for 8 DR-related lesions, resulting in 290K manual segments in total. Extensive experiments on this large-scale dataset show that our proposed approach surpasses the prior art for multiple tasks including lesion segmentation, lesion classification and DR grading.},
  keywords = {Art,Image segmentation,Pattern recognition,Retina,Retinopathy,Semantics,Training data},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\CX2IMG8G\\Wei et al. - 2021 - Learn to Segment Retinal Lesions and Beyond.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UG6GBGX3\\9412088.html}
}

@inproceedings{weiLearnSegmentRetinal2021c,
  title = {Learn to {{Segment Retinal Lesions}} and {{Beyond}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Wei, Qijie and Li, Xirong and Yu, Weihong and Zhang, Xiao and Zhang, Yongpeng and Hu, Bojie and Mo, Bin and Gong, Di and Chen, Ning and Ding, Dayong and Chen, Youxin},
  year = {2021},
  month = jan,
  pages = {7403--7410},
  publisher = {IEEE Computer Society},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412088},
  urldate = {2022-10-23},
  abstract = {Towards automated retinal screening, this paper makes an endeavor to simultaneously achieve pixel-level retinal lesion segmentation and image-level disease classification. Such a multi-task approach is crucial for accurate and clinically interpretable disease diagnosis. Prior art is insufficient due to three challenges, i.e., lesions lacking objective boundaries, clinical importance of lesions irrelevant to their size, and the lack of one-to-one correspondence between lesion and disease classes. This paper attacks the three challenges in the context of diabetic retinopathy (DR) grading. We propose Lesion-Net, a new variant of fully convolutional networks, with its expansive path redesigned to tackle the first challenge. A dual Dice loss that leverages both semantic segmentation and image classification losses is introduced to resolve the second challenge. Lastly, we build a multi-task network that employs Lesion-Net as a side-attention branch for both DR grading and result interpretation. A set of 12K fundus images is manually segmented by 45 ophthalmologists for 8 DR-related lesions, resulting in 290K manual segments in total. Extensive experiments on this large-scale dataset show that our proposed approach surpasses the prior art for multiple tasks including lesion segmentation, lesion classification and DR grading.},
  isbn = {978-1-72818-808-9},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\D5N22XEB\\wei2021.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\QXEMXPC7\\Wei et al. - 2021 - Learn to Segment Retinal Lesions and Beyond.pdf}
}

@article{welikalaAutomatedDetectionProliferative2014,
  title = {Automated Detection of Proliferative Diabetic Retinopathy Using a Modified Line Operator and Dual Classification},
  author = {Welikala, R. A. and Dehmeshki, J. and Hoppe, A. and Tah, V. and Mann, S. and Williamson, T. H. and Barman, S. A.},
  year = {2014},
  month = may,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {114},
  number = {3},
  pages = {247--261},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2014.02.010},
  urldate = {2019-11-19},
  abstract = {Proliferative diabetic retinopathy (PDR) is a condition that carries a high risk of severe visual impairment. The hallmark of PDR is neovascularisation, the growth of abnormal new vessels. This paper describes an automated method for the detection of new vessels in retinal images. Two vessel segmentation approaches are applied, using the standard line operator and a novel modified line operator. The latter is designed to reduce false responses to non-vessel edges. Both generated binary vessel maps hold vital information which must be processed separately. This is achieved with a dual classification system. Local morphology features are measured from each binary vessel map to produce two separate feature sets. Independent classification is performed for each feature set using a support vector machine (SVM) classifier. The system then combines these individual classification outcomes to produce a final decision. Sensitivity and specificity results using a dataset of 60 images are 0.862 and 0.944 respectively on a per patch basis and 1.00 and 0.90 respectively on a per image basis.},
  langid = {english},
  keywords = {Dual classification,Modified line operator,New vessels,Proliferative diabetic retinopathy,Retinal images}
}

@article{welikalaAutomatedDetectionProliferative2014a,
  title = {Automated Detection of Proliferative Diabetic Retinopathy Using a Modified Line Operator and Dual Classification},
  author = {Welikala, R. A. and Dehmeshki, J. and Hoppe, A. and Tah, V. and Mann, S. and Williamson, T. H. and Barman, S. A.},
  year = {2014},
  month = may,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {114},
  number = {3},
  pages = {247--261},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2014.02.010},
  urldate = {2019-11-19},
  abstract = {Proliferative diabetic retinopathy (PDR) is a condition that carries a high risk of severe visual impairment. The hallmark of PDR is neovascularisation, the growth of abnormal new vessels. This paper describes an automated method for the detection of new vessels in retinal images. Two vessel segmentation approaches are applied, using the standard line operator and a novel modified line operator. The latter is designed to reduce false responses to non-vessel edges. Both generated binary vessel maps hold vital information which must be processed separately. This is achieved with a dual classification system. Local morphology features are measured from each binary vessel map to produce two separate feature sets. Independent classification is performed for each feature set using a support vector machine (SVM) classifier. The system then combines these individual classification outcomes to produce a final decision. Sensitivity and specificity results using a dataset of 60 images are 0.862 and 0.944 respectively on a per patch basis and 1.00 and 0.90 respectively on a per image basis.},
  langid = {english},
  keywords = {Dual classification,Modified line operator,New vessels,Proliferative diabetic retinopathy,Retinal images},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\AX4YVWMR\\welikala2014.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UWQQEH9D\\Welikala et al. - 2014 - Automated detection of proliferative diabetic reti.pdf;C\:\\Users\\cleme\\Zotero\\storage\\TXW3Q763\\S0169260714000650.html}
}

@misc{wightmanPyTorchImageModelsWightman,
  title = {{{PyTorch Image Models}}},
  author = {Wightman, Ross},
  year = {Wightman},
  lccn = {10.5281/zenodo.4414861}
}

@misc{wightmanPyTorchImageModelsWightmana,
  title = {{{PyTorch Image Models}}},
  author = {Wightman, Ross},
  year = {Wightman},
  lccn = {10.5281/zenodo.4414861}
}

@article{willoughbyAnatomyPhysiologyHuman2010,
  title = {Anatomy and Physiology of the Human Eye: {{Effects}} of Mucopolysaccharidoses Disease on Structure and Function -- a Review},
  shorttitle = {Anatomy and Physiology of the Human Eye},
  author = {Willoughby, Colin E and Ponzin, Diego and Ferrari, Stefano and Lobo, Aires and Landau, Klara and Omidi, Yadollah},
  year = {2010},
  month = aug,
  journal = {Clinical \& Experimental Ophthalmology},
  volume = {38},
  number = {s1},
  pages = {2--11},
  issn = {1442-6404},
  doi = {10.1111/j.1442-9071.2010.02363.x},
  urldate = {2019-10-30},
  abstract = {Abstract The current paper provides an overview of current knowledge on the structure and function of the eye. It describes in depth the different parts of the eye that are involved in the ocular manifestations seen in the mucopolysaccharidoses (MPS). The MPS are a group of rare inheritable lysosomal storage disorders characterized by the accumulation of glycosaminoglycans (GAGs) in cells and tissues all over the body, leading to widespread tissue and organ dysfunction. GAGs also tend to accumulate in several tissues of the eye, leading to various ocular manifestations affecting both the anterior (cornea, conjunctiva) and the posterior parts (retina, sclera, optic nerve) of the eye.},
  keywords = {anatomy,eye disease,mucopolysaccharidosis,physiology,review}
}

@article{willoughbyAnatomyPhysiologyHuman2010a,
  title = {Anatomy and Physiology of the Human Eye: Effects of Mucopolysaccharidoses Disease on Structure and Function -- a Review},
  shorttitle = {Anatomy and Physiology of the Human Eye},
  author = {Willoughby, Colin E and Ponzin, Diego and Ferrari, Stefano and Lobo, Aires and Landau, Klara and Omidi, Yadollah},
  year = {2010},
  month = aug,
  journal = {Clinical \& Experimental Ophthalmology},
  volume = {38},
  number = {s1},
  pages = {2--11},
  issn = {1442-6404},
  doi = {10.1111/j.1442-9071.2010.02363.x},
  urldate = {2019-10-30},
  abstract = {Abstract The current paper provides an overview of current knowledge on the structure and function of the eye. It describes in depth the different parts of the eye that are involved in the ocular manifestations seen in the mucopolysaccharidoses (MPS). The MPS are a group of rare inheritable lysosomal storage disorders characterized by the accumulation of glycosaminoglycans (GAGs) in cells and tissues all over the body, leading to widespread tissue and organ dysfunction. GAGs also tend to accumulate in several tissues of the eye, leading to various ocular manifestations affecting both the anterior (cornea, conjunctiva) and the posterior parts (retina, sclera, optic nerve) of the eye.},
  keywords = {anatomy,eye disease,mucopolysaccharidosis,physiology,review},
  file = {C:\Users\cleme\Zotero\storage\SY5MK4VG\j.1442-9071.2010.02363.html}
}

@article{winklerAssociationSurgicalSkin2019,
  title = {Association {{Between Surgical Skin Markings}} in {{Dermoscopic Images}} and {{Diagnostic Performance}} of a {{Deep Learning Convolutional Neural Network}} for {{Melanoma Recognition}}},
  author = {Winkler, Julia K. and Fink, Christine and Toberer, Ferdinand and Enk, Alexander and Deinlein, Teresa and {Hofmann-Wellenhof}, Rainer and Thomas, Luc and Lallas, Aimilios and Blum, Andreas and Stolz, Wilhelm and Haenssle, Holger A.},
  year = {2019},
  month = oct,
  journal = {JAMA dermatology},
  volume = {155},
  number = {10},
  pages = {1135--1141},
  issn = {2168-6084},
  doi = {10.1001/jamadermatol.2019.1735},
  abstract = {IMPORTANCE: Deep learning convolutional neural networks (CNNs) have shown a performance at the level of dermatologists in the diagnosis of melanoma. Accordingly, further exploring the potential limitations of CNN technology before broadly applying it is of special interest. OBJECTIVE: To investigate the association between gentian violet surgical skin markings in dermoscopic images and the diagnostic performance of a CNN approved for use as a medical device in the European market. DESIGN AND SETTING: A cross-sectional analysis was conducted from August 1, 2018, to November 30, 2018, using a CNN architecture trained with more than 120 000 dermoscopic images of skin neoplasms and corresponding diagnoses. The association of gentian violet skin markings in dermoscopic images with the performance of the CNN was investigated in 3 image sets of 130 melanocytic lesions each (107 benign nevi, 23 melanomas). EXPOSURES: The same lesions were sequentially imaged with and without the application of a gentian violet surgical skin marker and then evaluated by the CNN for their probability of being a melanoma. In addition, the markings were removed by manually cropping the dermoscopic images to focus on the melanocytic lesion. MAIN OUTCOMES AND MEASURES: Sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curve for the CNN's diagnostic classification in unmarked, marked, and cropped images. RESULTS: In all, 130 melanocytic lesions (107 benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN achieved a sensitivity of 95.7\% (95\% CI, 79\%-99.2\%) and a specificity of 84.1\% (95\% CI, 76.0\%-89.8\%). The ROC AUC was 0.969. In marked lesions, an increase in melanoma probability scores was observed that resulted in a sensitivity of 100\% (95\% CI, 85.7\%-100\%) and a significantly reduced specificity of 45.8\% (95\% CI, 36.7\%-55.2\%, P\,{$<$}\,.001). The ROC AUC was 0.922. Cropping images led to the highest sensitivity of 100\% (95\% CI, 85.7\%-100\%), specificity of 97.2\% (95\% CI, 92.1\%-99.0\%), and ROC AUC of 0.993. Heat maps created by vanilla gradient descent backpropagation indicated that the blue markings were associated with the increased false-positive rate. CONCLUSIONS AND RELEVANCE: This study's findings suggest that skin markings significantly interfered with the CNN's correct diagnosis of nevi by increasing the melanoma probability scores and consequently the false-positive rate. A predominance of skin markings in melanoma training images may have induced the CNN's association of markings with a melanoma diagnosis. Accordingly, these findings suggest that skin markings should be avoided in dermoscopic images intended for analysis by a CNN. TRIAL REGISTRATION: German Clinical Trial Register (DRKS) Identifier: DRKS00013570.},
  langid = {english},
  pmcid = {PMC6694463},
  pmid = {31411641}
}

@article{winklerAssociationSurgicalSkin2019a,
  title = {Association {{Between Surgical Skin Markings}} in {{Dermoscopic Images}} and {{Diagnostic Performance}} of a {{Deep Learning Convolutional Neural Network}} for {{Melanoma Recognition}}},
  author = {Winkler, Julia K. and Fink, Christine and Toberer, Ferdinand and Enk, Alexander and Deinlein, Teresa and {Hofmann-Wellenhof}, Rainer and Thomas, Luc and Lallas, Aimilios and Blum, Andreas and Stolz, Wilhelm and Haenssle, Holger A.},
  year = {2019},
  month = oct,
  journal = {JAMA dermatology},
  volume = {155},
  number = {10},
  pages = {1135--1141},
  issn = {2168-6084},
  doi = {10.1001/jamadermatol.2019.1735},
  abstract = {IMPORTANCE: Deep learning convolutional neural networks (CNNs) have shown a performance at the level of dermatologists in the diagnosis of melanoma. Accordingly, further exploring the potential limitations of CNN technology before broadly applying it is of special interest. OBJECTIVE: To investigate the association between gentian violet surgical skin markings in dermoscopic images and the diagnostic performance of a CNN approved for use as a medical device in the European market. DESIGN AND SETTING: A cross-sectional analysis was conducted from August 1, 2018, to November 30, 2018, using a CNN architecture trained with more than 120 000 dermoscopic images of skin neoplasms and corresponding diagnoses. The association of gentian violet skin markings in dermoscopic images with the performance of the CNN was investigated in 3 image sets of 130 melanocytic lesions each (107 benign nevi, 23 melanomas). EXPOSURES: The same lesions were sequentially imaged with and without the application of a gentian violet surgical skin marker and then evaluated by the CNN for their probability of being a melanoma. In addition, the markings were removed by manually cropping the dermoscopic images to focus on the melanocytic lesion. MAIN OUTCOMES AND MEASURES: Sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curve for the CNN's diagnostic classification in unmarked, marked, and cropped images. RESULTS: In all, 130 melanocytic lesions (107 benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN achieved a sensitivity of 95.7\% (95\% CI, 79\%-99.2\%) and a specificity of 84.1\% (95\% CI, 76.0\%-89.8\%). The ROC AUC was 0.969. In marked lesions, an increase in melanoma probability scores was observed that resulted in a sensitivity of 100\% (95\% CI, 85.7\%-100\%) and a significantly reduced specificity of 45.8\% (95\% CI, 36.7\%-55.2\%, P\,{$<$}\,.001). The ROC AUC was 0.922. Cropping images led to the highest sensitivity of 100\% (95\% CI, 85.7\%-100\%), specificity of 97.2\% (95\% CI, 92.1\%-99.0\%), and ROC AUC of 0.993. Heat maps created by vanilla gradient descent backpropagation indicated that the blue markings were associated with the increased false-positive rate. CONCLUSIONS AND RELEVANCE: This study's findings suggest that skin markings significantly interfered with the CNN's correct diagnosis of nevi by increasing the melanoma probability scores and consequently the false-positive rate. A predominance of skin markings in melanoma training images may have induced the CNN's association of markings with a melanoma diagnosis. Accordingly, these findings suggest that skin markings should be avoided in dermoscopic images intended for analysis by a CNN. TRIAL REGISTRATION: German Clinical Trial Register (DRKS) Identifier: DRKS00013570.},
  langid = {english},
  pmcid = {PMC6694463},
  pmid = {31411641},
  file = {C:\Users\cleme\Zotero\storage\TAZCL7J5\Winkler et al. - 2019 - Association Between Surgical Skin Markings in Derm.pdf}
}

@misc{WMAWorldMedical,
  title = {{{WMA}} - {{The World Medical Association-WMA Declaration}} of {{Helsinki}} -- {{Ethical Principles}} for {{Medical Research Involving Human Subjects}}},
  urldate = {2020-02-20},
  langid = {american}
}

@misc{WMAWorldMedicala,
  title = {{{WMA}} - {{The World Medical Association-WMA Declaration}} of {{Helsinki}} -- {{Ethical Principles}} for {{Medical Research Involving Human Subjects}}},
  urldate = {2020-02-20},
  langid = {american},
  file = {C:\Users\cleme\Zotero\storage\XZ97HY5V\wma-declaration-of-helsinki-ethical-principles-for-medical-research-involving-human-subjects.html}
}

@article{wongEnhancedDepthImaging2011,
  title = {Enhanced {{Depth Imaging Optical Coherence Tomography}}},
  author = {Wong, Ian Y. and Koizumi, Hideki and Lai, Wico W.},
  year = {2011},
  month = jul,
  journal = {Ophthalmic Surgery, Lasers, and Imaging},
  volume = {42},
  number = {4},
  pages = {S75-S84},
  issn = {1542-8877},
  doi = {10.3928/15428877-20110627-07},
  urldate = {2019-11-18},
  langid = {english}
}

@article{wongEnhancedDepthImaging2011a,
  title = {Enhanced {{Depth Imaging Optical Coherence Tomography}}},
  author = {Wong, Ian Y. and Koizumi, Hideki and Lai, Wico W.},
  year = {2011},
  month = jul,
  journal = {Ophthalmic Surgery, Lasers, and Imaging},
  volume = {42},
  number = {4},
  pages = {S75-S84},
  issn = {1542-8877},
  doi = {10.3928/15428877-20110627-07},
  urldate = {2019-11-18},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\WKIUSN76\Wong et al. - 2011 - Enhanced Depth Imaging Optical Coherence Tomograph.pdf}
}

@article{wongGlobalPrevalenceAgerelated2014,
  title = {Global Prevalence of Age-Related Macular Degeneration and Disease Burden Projection for 2020 and 2040: A Systematic Review and Meta-Analysis},
  shorttitle = {Global Prevalence of Age-Related Macular Degeneration and Disease Burden Projection for 2020 and 2040},
  author = {Wong, Wan Ling and Su, Xinyi and Li, Xiang and Cheung, Chui Ming G and Klein, Ronald and Cheng, Ching-Yu and Wong, Tien Yin},
  year = {2014},
  month = feb,
  journal = {The Lancet Global Health},
  volume = {2},
  number = {2},
  pages = {e106-e116},
  issn = {2214-109X},
  doi = {10.1016/S2214-109X(13)70145-1},
  urldate = {2019-12-27},
  abstract = {Background Numerous population-based studies of age-related macular degeneration have been reported around the world, with the results of some studies suggesting racial or ethnic differences in disease prevalence. Integrating these resources to provide summarised data to establish worldwide prevalence and to project the number of people with age-related macular degeneration from 2020 to 2040 would be a useful guide for global strategies. Methods We did a systematic literature review to identify all population-based studies of age-related macular degeneration published before May, 2013. Only studies using retinal photographs and standardised grading classifications (the Wisconsin age-related maculopathy grading system, the international classification for age-related macular degeneration, or the Rotterdam staging system) were included. Hierarchical Bayesian approaches were used to estimate the pooled prevalence, the 95\% credible intervals (CrI), and to examine the difference in prevalence by ethnicity (European, African, Hispanic, Asian) and region (Africa, Asia, Europe, Latin America and the Caribbean, North America, and Oceania). UN World Population Prospects were used to project the number of people affected in 2014 and 2040. Bayes factor was calculated as a measure of statistical evidence, with a score above three indicating substantial evidence. Findings Analysis of 129,664 individuals (aged 30--97 years), with 12,727 cases from 39 studies, showed the pooled prevalence (mapped to an age range of 45--85 years) of early, late, and any age-related macular degeneration to be 8{$\cdot$}01\% (95\% CrI 3{$\cdot$}98--15{$\cdot$}49), 0{$\cdot$}37\% (0{$\cdot$}18--0{$\cdot$}77), and 8{$\cdot$}69\% (4{$\cdot$}26--17{$\cdot$}40), respectively. We found a higher prevalence of early and any age-related macular degeneration in Europeans than in Asians (early: 11{$\cdot$}2\% vs 6{$\cdot$}8\%, Bayes factor 3{$\cdot$}9; any: 12{$\cdot$}3\% vs 7{$\cdot$}4\%, Bayes factor 4{$\cdot$}3), and early, late, and any age-related macular degeneration to be more prevalent in Europeans than in Africans (early: 11{$\cdot$}2\% vs 7{$\cdot$}1\%, Bayes factor 12{$\cdot$}2; late: 0{$\cdot$}5\% vs 0{$\cdot$}3\%, 3{$\cdot$}7; any: 12{$\cdot$}3\% vs 7{$\cdot$}5\%, 31{$\cdot$}3). There was no difference in prevalence between Asians and Africans (all Bayes factors {$<$}1). Europeans had a higher prevalence of geographic atrophy subtype (1{$\cdot$}11\%, 95\% CrI 0{$\cdot$}53--2{$\cdot$}08) than Africans (0{$\cdot$}14\%, 0{$\cdot$}04--0{$\cdot$}45), Asians (0{$\cdot$}21\%, 0{$\cdot$}04--0{$\cdot$}87), and Hispanics (0{$\cdot$}16\%, 0{$\cdot$}05--0{$\cdot$}46). Between geographical regions, cases of early and any age-related macular degeneration were less prevalent in Asia than in Europe and North America (early: 6{$\cdot$}3\% vs 14.3\% and 12{$\cdot$}8\% [Bayes factor 2{$\cdot$}3 and 7{$\cdot$}6]; any: 6{$\cdot$}9\% vs 18{$\cdot$}3\% and 14{$\cdot$}3\% [3{$\cdot$}0 and 3{$\cdot$}8]). No significant gender effect was noted in prevalence (Bayes factor {$<$}1{$\cdot$}0). The projected number of people with age-related macular degeneration in 2020 is 196 million (95\% CrI 140--261), increasing to 288 million in 2040 (205--399). Interpretation These estimates indicate the substantial global burden of age-related macular degeneration. Summarised data provide information for understanding the effect of the condition and provide data towards designing eye-care strategies and health services around the world. Funding National Medical Research Council, Singapore.},
  langid = {english}
}

@article{wongGlobalPrevalenceAgerelated2014a,
  title = {Global Prevalence of Age-Related Macular Degeneration and Disease Burden Projection for 2020 and 2040: A Systematic Review and Meta-Analysis},
  shorttitle = {Global Prevalence of Age-Related Macular Degeneration and Disease Burden Projection for 2020 and 2040},
  author = {Wong, Wan Ling and Su, Xinyi and Li, Xiang and Cheung, Chui Ming G and Klein, Ronald and Cheng, Ching-Yu and Wong, Tien Yin},
  year = {2014},
  month = feb,
  journal = {The Lancet Global Health},
  volume = {2},
  number = {2},
  pages = {e106-e116},
  issn = {2214-109X},
  doi = {10.1016/S2214-109X(13)70145-1},
  urldate = {2019-12-27},
  abstract = {Background Numerous population-based studies of age-related macular degeneration have been reported around the world, with the results of some studies suggesting racial or ethnic differences in disease prevalence. Integrating these resources to provide summarised data to establish worldwide prevalence and to project the number of people with age-related macular degeneration from 2020 to 2040 would be a useful guide for global strategies. Methods We did a systematic literature review to identify all population-based studies of age-related macular degeneration published before May, 2013. Only studies using retinal photographs and standardised grading classifications (the Wisconsin age-related maculopathy grading system, the international classification for age-related macular degeneration, or the Rotterdam staging system) were included. Hierarchical Bayesian approaches were used to estimate the pooled prevalence, the 95\% credible intervals (CrI), and to examine the difference in prevalence by ethnicity (European, African, Hispanic, Asian) and region (Africa, Asia, Europe, Latin America and the Caribbean, North America, and Oceania). UN World Population Prospects were used to project the number of people affected in 2014 and 2040. Bayes factor was calculated as a measure of statistical evidence, with a score above three indicating substantial evidence. Findings Analysis of 129\hphantom{,}664 individuals (aged 30--97 years), with 12\hphantom{,}727 cases from 39 studies, showed the pooled prevalence (mapped to an age range of 45--85 years) of early, late, and any age-related macular degeneration to be 8{$\cdot$}01\% (95\% CrI 3{$\cdot$}98--15{$\cdot$}49), 0{$\cdot$}37\% (0{$\cdot$}18--0{$\cdot$}77), and 8{$\cdot$}69\% (4{$\cdot$}26--17{$\cdot$}40), respectively. We found a higher prevalence of early and any age-related macular degeneration in Europeans than in Asians (early: 11{$\cdot$}2\% vs 6{$\cdot$}8\%, Bayes factor 3{$\cdot$}9; any: 12{$\cdot$}3\% vs 7{$\cdot$}4\%, Bayes factor 4{$\cdot$}3), and early, late, and any age-related macular degeneration to be more prevalent in Europeans than in Africans (early: 11{$\cdot$}2\% vs 7{$\cdot$}1\%, Bayes factor 12{$\cdot$}2; late: 0{$\cdot$}5\% vs 0{$\cdot$}3\%, 3{$\cdot$}7; any: 12{$\cdot$}3\% vs 7{$\cdot$}5\%, 31{$\cdot$}3). There was no difference in prevalence between Asians and Africans (all Bayes factors {$<$}1). Europeans had a higher prevalence of geographic atrophy subtype (1{$\cdot$}11\%, 95\% CrI 0{$\cdot$}53--2{$\cdot$}08) than Africans (0{$\cdot$}14\%, 0{$\cdot$}04--0{$\cdot$}45), Asians (0{$\cdot$}21\%, 0{$\cdot$}04--0{$\cdot$}87), and Hispanics (0{$\cdot$}16\%, 0{$\cdot$}05--0{$\cdot$}46). Between geographical regions, cases of early and any age-related macular degeneration were less prevalent in Asia than in Europe and North America (early: 6{$\cdot$}3\% vs 14.3\% and 12{$\cdot$}8\% [Bayes factor 2{$\cdot$}3 and 7{$\cdot$}6]; any: 6{$\cdot$}9\% vs 18{$\cdot$}3\% and 14{$\cdot$}3\% [3{$\cdot$}0 and 3{$\cdot$}8]). No significant gender effect was noted in prevalence (Bayes factor {$<$}1{$\cdot$}0). The projected number of people with age-related macular degeneration in 2020 is 196 million (95\% CrI 140--261), increasing to 288 million in 2040 (205--399). Interpretation These estimates indicate the substantial global burden of age-related macular degeneration. Summarised data provide information for understanding the effect of the condition and provide data towards designing eye-care strategies and health services around the world. Funding National Medical Research Council, Singapore.},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\CHLFEQYG\\Wong et al. - 2014 - Global prevalence of age-related macular degenerat.pdf;C\:\\Users\\cleme\\Zotero\\storage\\BSARZD2I\\S2214109X13701451.html}
}

@inproceedings{wongLevelsetBasedAutomatic2008,
  title = {Level-Set Based Automatic Cup-to-Disc Ratio Determination Using Retinal Fundus Images in {{ARGALI}}},
  booktitle = {2008 30th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Wong, D. W. K. and Liu, J. and Lim, J.H. and Jia, X. and Yin, F. and Li, H. and Wong, T. Y.},
  year = {2008},
  month = aug,
  pages = {2266--2269},
  issn = {1558-4615},
  doi = {10.1109/IEMBS.2008.4649648},
  abstract = {Glaucoma is a leading cause of permanent blindness. However, disease progression can be limited if detected early. The optic cup-to-disc ratio (CDR) is one of the main clinical indicators of glaucoma, and is currently determined manually, limiting its potential in mass screening. In this paper, we propose an automatic CDR determination method using a variational level-set approach to segment the optic disc and cup from retinal fundus images. The method is a core component of ARGALI, a system for automated glaucoma risk assessment. Threshold analysis is used in pre-processing to estimate the initial contour. Due to the presence of retinal vasculature traversing the disc and cup boundaries which can cause inaccuracies in the detected contours, an ellipse-fitting post-processing step is also introduced. The method was tested on 104 images from the Singapore Malay Eye Study, and it was found the results produced a clinically acceptable variation of up to 0.2 CDR units from the manually graded samples, with potential use in mass screening.},
  keywords = {Algorithms,automated level-set segmentation,Computer-Assisted,Diagnosis,Fundus Oculi,Glaucoma,Humans,Mass Screening,medical image processing,Ophthalmoscopy,optic cup-to-disc ratio,Optic Disk,Risk Factors}
}

@inproceedings{wongLevelsetBasedAutomatic2008a,
  title = {Level-Set Based Automatic Cup-to-Disc Ratio Determination Using Retinal Fundus Images in {{ARGALI}}},
  booktitle = {2008 30th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Wong, D. W. K. and Liu, J. and Lim, J.H. and Jia, X. and Yin, F. and Li, H. and Wong, T. Y.},
  year = {2008},
  month = aug,
  pages = {2266--2269},
  issn = {1558-4615},
  doi = {10.1109/IEMBS.2008.4649648},
  abstract = {Glaucoma is a leading cause of permanent blindness. However, disease progression can be limited if detected early. The optic cup-to-disc ratio (CDR) is one of the main clinical indicators of glaucoma, and is currently determined manually, limiting its potential in mass screening. In this paper, we propose an automatic CDR determination method using a variational level-set approach to segment the optic disc and cup from retinal fundus images. The method is a core component of ARGALI, a system for automated glaucoma risk assessment. Threshold analysis is used in pre-processing to estimate the initial contour. Due to the presence of retinal vasculature traversing the disc and cup boundaries which can cause inaccuracies in the detected contours, an ellipse-fitting post-processing step is also introduced. The method was tested on 104 images from the Singapore Malay Eye Study, and it was found the results produced a clinically acceptable variation of up to 0.2 CDR units from the manually graded samples, with potential use in mass screening.},
  keywords = {Algorithms,automated level-set segmentation,Diagnosis Computer-Assisted,Fundus Oculi,Glaucoma,Humans,Mass Screening,medical image processing,Ophthalmoscopy,optic cup-to-disc ratio,Optic Disk,Risk Factors},
  file = {C:\Users\cleme\Zotero\storage\PX4VDT8W\4649648.html}
}

@inproceedings{wongTHALIAAutomaticHierarchical2013,
  title = {{{THALIA}} - {{An}} Automatic Hierarchical Analysis System to Detect Drusen Lesion Images for Amd Assessment},
  booktitle = {2013 {{IEEE}} 10th {{International Symposium}} on {{Biomedical Imaging}}},
  author = {Wong, Damon W.K. and Liu, Jiang and Cheng, Xiangang and Zhang, Jielin and Yin, Fengshou and Bhargava, Mayuri and Cheung, Gemmy C.M. and Wong, Tien Yin},
  year = {2013},
  month = apr,
  pages = {884--887},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2013.6556617},
  abstract = {Age-related macular degeneration (AMD) is a leading cause of permanent blindness. In its early stage AMD is characterized by drusen which are extracellelur deposits in the retina. In this paper, we present THALIA, an automatic system for the detection of drusen images for AMD assessment. First, the macular region of interest is detected using a seeded mode tracking approach. The macular region of interest is then mapped into a new representation using a hierarchicial word transform (HWI). In HWI, dense sampling is first carried out to generate structured pixels which embed local context. These structured pixels are then clustered using hierarchical k-means. The HWI image is subsequently classified using a SVM-based classifier. We have tested THALIA on a dataset of 350 images and obtained an accuracy of 95.46\%. Results are promising for further validation of the THALIA system.},
  keywords = {age-related macular degeneration assessment,AMD,automatic hierarchical analysis system,Blindness,Context,dense sampling,drusen,drusen lesion image detection,extracellular deposits,eye,hierarchical k-means clustering,hierarchicial word transform,Histograms,image classification,Image color analysis,Image recognition,image sampling,macular region-of-interest,medical image processing,permanent blindness,retina,Retina,retinal image,seeded mode tracking approach,structured pixels,support vector machines,SVM-based classifier,THALIA,vision defects,Visualization}
}

@inproceedings{wongTHALIAAutomaticHierarchical2013a,
  title = {{{THALIA}} - {{An}} Automatic Hierarchical Analysis System to Detect Drusen Lesion Images for Amd Assessment},
  booktitle = {2013 {{IEEE}} 10th {{International Symposium}} on {{Biomedical Imaging}}},
  author = {Wong, Damon W.K. and Liu, Jiang and Cheng, Xiangang and Zhang, Jielin and Yin, Fengshou and Bhargava, Mayuri and Cheung, Gemmy C.M. and Wong, Tien Yin},
  year = {2013},
  month = apr,
  pages = {884--887},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2013.6556617},
  abstract = {Age-related macular degeneration (AMD) is a leading cause of permanent blindness. In its early stage AMD is characterized by drusen which are extracellelur deposits in the retina. In this paper, we present THALIA, an automatic system for the detection of drusen images for AMD assessment. First, the macular region of interest is detected using a seeded mode tracking approach. The macular region of interest is then mapped into a new representation using a hierarchicial word transform (HWI). In HWI, dense sampling is first carried out to generate structured pixels which embed local context. These structured pixels are then clustered using hierarchical k-means. The HWI image is subsequently classified using a SVM-based classifier. We have tested THALIA on a dataset of 350 images and obtained an accuracy of 95.46\%. Results are promising for further validation of the THALIA system.},
  keywords = {age-related macular degeneration assessment,AMD,automatic hierarchical analysis system,Blindness,Context,dense sampling,drusen,drusen lesion image detection,extracellular deposits,eye,hierarchical k-means clustering,hierarchicial word transform,Histograms,image classification,Image color analysis,Image recognition,image sampling,macular region-of-interest,medical image processing,permanent blindness,retina,Retina,retinal image,seeded mode tracking approach,structured pixels,support vector machines,SVM-based classifier,THALIA,vision defects,Visualization},
  file = {C:\Users\cleme\Zotero\storage\W7RFY4CA\6556617.html}
}

@misc{WorldHealthDay,
  title = {World {{Health Day}} 2016: {{WHO}} Calls for Global Action to Halt Rise in and Improve Care for People with Diabetes},
  shorttitle = {World {{Health Day}} 2016},
  urldate = {2019-11-19},
  abstract = {The number of people living with diabetes has almost quadrupled since 1980 to 422 million adults, with most living in developing countries. Factors driving this dramatic rise include overweight and obesity, WHO announced ahead of World Health Day.},
  langid = {english}
}

@misc{WorldHealthDaya,
  title = {World {{Health Day}} 2016: {{WHO}} Calls for Global Action to Halt Rise in and Improve Care for People with Diabetes},
  shorttitle = {World {{Health Day}} 2016},
  urldate = {2019-11-19},
  abstract = {The number of people living with diabetes has almost quadrupled since 1980 to 422 million adults, with most living in developing countries. Factors driving this dramatic rise include overweight and obesity, WHO announced ahead of World Health Day.},
  howpublished = {https://www.who.int/news-room/detail/06-04-2016-world-health-day-2016-who-calls-for-global-action-to-halt-rise-in-and-improve-care-for-people-with-diabetes},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\3B8MJZAY\06-04-2016-world-health-day-2016-who-calls-for-global-action-to-halt-rise-in-and-improve-care-f.html}
}

@inproceedings{wortsman2022model,
  title = {Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy without Increasing Inference Time},
  booktitle = {International Conference on Machine Learning},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and {Gontijo-Lopes}, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  year = {2022},
  pages = {23965--23998},
  publisher = {PMLR}
}

@inproceedings{wortsman2022model,
  title = {Model Soups: {{Averaging}} Weights of Multiple Fine-Tuned Models Improves Accuracy without Increasing Inference Time},
  booktitle = {International Conference on Machine Learning},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and {Gontijo-Lopes}, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  year = {2022},
  pages = {23965--23998},
  publisher = {PMLR}
}

@article{wortsmanModelSoupsAveraging,
  title = {Model Soups: {{Averaging}} Weights of Multiple Fine-Tuned Models Improves Accuracy without Increasing Inference Time},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and {Gontijo-Lopes}, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  pages = {34},
  abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models finetuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs---we call the results ``model soups.'' When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pretrained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logitensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github. com/mlfoundations/model-soups.},
  langid = {english}
}

@article{wortsmanModelSoupsAveraginga,
  title = {Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy without Increasing Inference Time},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and {Gontijo-Lopes}, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  pages = {34},
  abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models finetuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs---we call the results ``model soups.'' When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pretrained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94\% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logitensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github. com/mlfoundations/model-soups.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\68TQ95FZ\Wortsman et al. - Model soups averaging weights of multiple fine-tu.pdf}
}

@inproceedings{wuAttenNetDeepAttention2020,
  title = {{{AttenNet}}: {{Deep Attention Based Retinal Disease Classification}} in {{OCT Images}}},
  shorttitle = {{{AttenNet}}},
  booktitle = {{{MultiMedia Modeling}}},
  author = {Wu, Jun and Zhang, Yao and Wang, Jie and Zhao, Jianchun and Ding, Dayong and Chen, Ningjiang and Wang, Lingling and Chen, Xuan and Jiang, Chunhui and Zou, Xuan and Liu, Xing and Xiao, Hui and Tian, Yuan and Shang, Zongjiang and Wang, Kaiwei and Li, Xirong and Yang, Gang and Fan, Jianping},
  editor = {Ro, Yong Man and Cheng, Wen-Huang and Kim, Junmo and Chu, Wei-Ta and Cui, Peng and Choi, Jung-Woo and Hu, Min-Chun and De Neve, Wesley},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {565--576},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-37734-2_46},
  abstract = {An optical coherence tomography (OCT) image is becoming the standard imaging modality in diagnosing retinal diseases and the assessment of their progression. However, the manual evaluation of the volumetric scan is time consuming, expensive and the signs of the early disease are easy to miss. In this paper, we mainly present an attention-based deep learning method for the retinal disease classification in OCT images, which can assist the large-scale screening or the diagnosis recommendation for an ophthalmologist. First, according to the unique characteristic of a retinal OCT image, we design a customized pre-processing method to improve image quality. Second, in order to guide the network optimization more effectively, a specially designed attention model, which pays more attention to critical regions containing pathological anomalies, is integrated into a typical deep learning network. We evaluate our proposed method on two data sets, and the results consistently show that it outperforms the state-of-the-art methods. We report an overall four-class accuracy of 97.4\%, a two-class sensitivity of 100.0\%, and a two-class specificity of 100.0\% on a public data set shared by Zhang et al. with 1,000 testing B-scans in four disease classes. Compared to their work, our method improves the numbers by 0.8\%, 2.2\%, and 2.6\% respectively.},
  isbn = {978-3-030-37734-2},
  langid = {english},
  keywords = {Attention model,Deep learning,Optical Coherence Tomography (OCT),Retinal disease classification}
}

@inproceedings{wuAttenNetDeepAttention2020a,
  title = {{{AttenNet}}: {{Deep Attention Based Retinal Disease Classification}} in {{OCT Images}}},
  shorttitle = {{{AttenNet}}},
  booktitle = {{{MultiMedia Modeling}}},
  author = {Wu, Jun and Zhang, Yao and Wang, Jie and Zhao, Jianchun and Ding, Dayong and Chen, Ningjiang and Wang, Lingling and Chen, Xuan and Jiang, Chunhui and Zou, Xuan and Liu, Xing and Xiao, Hui and Tian, Yuan and Shang, Zongjiang and Wang, Kaiwei and Li, Xirong and Yang, Gang and Fan, Jianping},
  editor = {Ro, Yong Man and Cheng, Wen-Huang and Kim, Junmo and Chu, Wei-Ta and Cui, Peng and Choi, Jung-Woo and Hu, Min-Chun and De Neve, Wesley},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {565--576},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-37734-2_46},
  abstract = {An optical coherence tomography (OCT) image is becoming the standard imaging modality in diagnosing retinal diseases and the assessment of their progression. However, the manual evaluation of the volumetric scan is time consuming, expensive and the signs of the early disease are easy to miss. In this paper, we mainly present an attention-based deep learning method for the retinal disease classification in OCT images, which can assist the large-scale screening or the diagnosis recommendation for an ophthalmologist. First, according to the unique characteristic of a retinal OCT image, we design a customized pre-processing method to improve image quality. Second, in order to guide the network optimization more effectively, a specially designed attention model, which pays more attention to critical regions containing pathological anomalies, is integrated into a typical deep learning network. We evaluate our proposed method on two data sets, and the results consistently show that it outperforms the state-of-the-art methods. We report an overall four-class accuracy of 97.4\%, a two-class sensitivity of 100.0\%, and a two-class specificity of 100.0\% on a public data set shared by Zhang et al. with 1,000 testing B-scans in four disease classes. Compared to their work, our method improves the numbers by 0.8\%, 2.2\%, and 2.6\% respectively.},
  isbn = {978-3-030-37734-2},
  langid = {english},
  keywords = {Attention model,Deep learning,Optical Coherence Tomography (OCT),Retinal disease classification}
}

@article{wuComprehensiveSurveyGraph2019,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  year = {2019},
  month = aug,
  journal = {arXiv:1901.00596 [cs, stat]},
  eprint = {1901.00596},
  primaryclass = {cs, stat},
  urldate = {2019-11-25},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this rapidly growing field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\EM5YY3J5\\Wu et al. - 2019 - A Comprehensive Survey on Graph Neural Networks.pdf;C\:\\Users\\cleme\\Zotero\\storage\\96MKTB8K\\1901.html}
}

@article{wuJCSExplainableCOVID192021,
  title = {{{JCS}}: {{An Explainable COVID-19 Diagnosis System}} by {{Joint Classification}} and {{Segmentation}}},
  shorttitle = {{{JCS}}},
  author = {Wu, Yu-Huan and Gao, Shang-Hua and Mei, Jie and Xu, Jun and Fan, Deng-Ping and Zhang, Rong-Guo and Cheng, Ming-Ming},
  year = {2021},
  journal = {IEEE transactions on image processing: a publication of the IEEE Signal Processing Society},
  volume = {30},
  pages = {3113--3126},
  issn = {1941-0042},
  doi = {10.1109/TIP.2021.3058783},
  abstract = {Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic disease in over 200 countries, influencing billions of humans. To control the infection, identifying and separating the infected people is the most crucial step. The main diagnostic tool is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high enough to effectively prevent the pandemic. The chest CT scan test provides a valuable complementary tool to the RT-PCR test, and it can identify the patients in the early-stage with high sensitivity. However, the chest CT scan test is usually time-consuming, requiring about 21.5 minutes per case. This paper develops a novel Joint Classification and Segmentation (JCS) system to perform real-time and explainable COVID- 19 chest CT diagnosis. To train our JCS system, we construct a large scale COVID- 19 Classification and Segmentation (COVID-CS) dataset, with 144,167 chest CT images of 400 COVID- 19 patients and 350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with fine-grained pixel-level labels of opacifications, which are increased attenuation of the lung parenchyma. We also have annotated lesion counts, opacification areas, and locations and thus benefit various diagnosis aspects. Extensive experiments demonstrate that the proposed JCS diagnosis system is very efficient for COVID-19 classification and segmentation. It obtains an average sensitivity of 95.0\% and a specificity of 93.0\% on the classification test set, and 78.5\% Dice score on the segmentation test set of our COVID-CS dataset. The COVID-CS dataset and code are available at https://github.com/yuhuan-wu/JCS.},
  langid = {english},
  pmid = {33600316},
  keywords = {80 and over,Adolescent,Adult,Aged,Computer-Assisted,COVID-19,Databases,Deep Learning,Factual,Female,Humans,Lung,Male,Middle Aged,Radiographic Image Interpretation,SARS-CoV-2,Tomography,X-Ray Computed,Young Adult}
}

@article{wuJCSExplainableCOVID192021a,
  title = {{{JCS}}: {{An Explainable COVID-19 Diagnosis System}} by {{Joint Classification}} and {{Segmentation}}},
  shorttitle = {{{JCS}}},
  author = {Wu, Yu-Huan and Gao, Shang-Hua and Mei, Jie and Xu, Jun and Fan, Deng-Ping and Zhang, Rong-Guo and Cheng, Ming-Ming},
  year = {2021},
  journal = {IEEE transactions on image processing: a publication of the IEEE Signal Processing Society},
  volume = {30},
  pages = {3113--3126},
  issn = {1941-0042},
  doi = {10.1109/TIP.2021.3058783},
  abstract = {Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic disease in over 200 countries, influencing billions of humans. To control the infection, identifying and separating the infected people is the most crucial step. The main diagnostic tool is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high enough to effectively prevent the pandemic. The chest CT scan test provides a valuable complementary tool to the RT-PCR test, and it can identify the patients in the early-stage with high sensitivity. However, the chest CT scan test is usually time-consuming, requiring about 21.5 minutes per case. This paper develops a novel Joint Classification and Segmentation (JCS) system to perform real-time and explainable COVID- 19 chest CT diagnosis. To train our JCS system, we construct a large scale COVID- 19 Classification and Segmentation (COVID-CS) dataset, with 144,167 chest CT images of 400 COVID- 19 patients and 350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with fine-grained pixel-level labels of opacifications, which are increased attenuation of the lung parenchyma. We also have annotated lesion counts, opacification areas, and locations and thus benefit various diagnosis aspects. Extensive experiments demonstrate that the proposed JCS diagnosis system is very efficient for COVID-19 classification and segmentation. It obtains an average sensitivity of 95.0\% and a specificity of 93.0\% on the classification test set, and 78.5\% Dice score on the segmentation test set of our COVID-CS dataset. The COVID-CS dataset and code are available at https://github.com/yuhuan-wu/JCS.},
  langid = {english},
  pmid = {33600316},
  keywords = {Adolescent,Adult,Aged,Aged 80 and over,COVID-19,Databases Factual,Deep Learning,Female,Humans,Lung,Male,Middle Aged,Radiographic Image Interpretation Computer-Assisted,SARS-CoV-2,Tomography X-Ray Computed,Young Adult},
  file = {C:\Users\cleme\Zotero\storage\JE53H5QB\Wu et al. - 2021 - JCS An Explainable COVID-19 Diagnosis System by J.pdf}
}

@article{wuVisionTransformerbasedRecognition2021,
  title = {Vision {{Transformer-based}} Recognition of Diabetic Retinopathy Grade},
  author = {Wu, Jianfang and Hu, Ruo and Xiao, Zhenghong and Chen, Jiaxu and Liu, Jingwei},
  year = {2021},
  month = dec,
  journal = {Medical Physics},
  volume = {48},
  number = {12},
  pages = {7850--7863},
  issn = {2473-4209},
  doi = {10.1002/mp.15312},
  abstract = {BACKGROUND: In the domain of natural language processing, Transformers are recognized as state-of-the-art models, which opposing to typical convolutional neural networks (CNNs) do not rely on convolution layers. Instead, Transformers employ multi-head attention mechanisms as the main building block to capture long-range contextual relations between image pixels. Recently, CNNs dominated the deep learning solutions for diabetic retinopathy grade recognition. However, spurred by the advantages of Transformers, we propose a Transformer-based method that is appropriate for recognizing the grade of diabetic retinopathy. PURPOSE: The purposes of this work are to demonstrate that (i) the pure attention mechanism is suitable for diabetic retinopathy grade recognition and (ii) Transformers can replace traditional CNNs for diabetic retinopathy grade recognition. METHODS: This paper proposes a Vision Transformer-based method to recognize the grade of diabetic retinopathy. Fundus images are subdivided into non-overlapping patches, which are then converted into sequences by flattening, and undergo a linear and positional embedding process to preserve positional information. Then, the generated sequence is input into several multi-head attention layers to generate the final representation. The first token sequence is input to a softmax classification layer to produce the recognition output in the classification stage. RESULTS: The dataset for training and testing employs fundus images of different resolutions, subdivided into patches. We challenge our method against current CNNs and extreme learning machines and achieve an appealing performance. Specifically, the suggested deep learning architecture attains an accuracy of 91.4\%, specificity = 0.977 (95\% confidence interval (CI) (0.951-1)), precision = 0.928 (95\% CI (0.852-1)), sensitivity = 0.926 (95\% CI (0.863-0.989)), quadratic weighted kappa score = 0.935, and area under curve (AUC) = 0.986. CONCLUSION: Our comparative experiments against current methods conclude that our model is competitive and highlight that an attention mechanism based on a Vision Transformer model is promising for the diabetic retinopathy grade recognition task.},
  langid = {english},
  pmid = {34693536},
  keywords = {Area Under Curve,Computer,deep learning,Diabetes Mellitus,diabetic retinopathy,Diabetic Retinopathy,Fundus Oculi,Humans,multi-head attention,Neural Networks,Vision Transformer}
}

@article{wuVisionTransformerbasedRecognition2021a,
  title = {Vision {{Transformer-based}} Recognition of Diabetic Retinopathy Grade},
  author = {Wu, Jianfang and Hu, Ruo and Xiao, Zhenghong and Chen, Jiaxu and Liu, Jingwei},
  year = {2021},
  month = dec,
  journal = {Medical Physics},
  volume = {48},
  number = {12},
  pages = {7850--7863},
  issn = {2473-4209},
  doi = {10.1002/mp.15312},
  abstract = {BACKGROUND: In the domain of natural language processing, Transformers are recognized as state-of-the-art models, which opposing to typical convolutional neural networks (CNNs) do not rely on convolution layers. Instead, Transformers employ multi-head attention mechanisms as the main building block to capture long-range contextual relations between image pixels. Recently, CNNs dominated the deep learning solutions for diabetic retinopathy grade recognition. However, spurred by the advantages of Transformers, we propose a Transformer-based method that is appropriate for recognizing the grade of diabetic retinopathy. PURPOSE: The purposes of this work are to demonstrate that (i) the pure attention mechanism is suitable for diabetic retinopathy grade recognition and (ii) Transformers can replace traditional CNNs for diabetic retinopathy grade recognition. METHODS: This paper proposes a Vision Transformer-based method to recognize the grade of diabetic retinopathy. Fundus images are subdivided into non-overlapping patches, which are then converted into sequences by flattening, and undergo a linear and positional embedding process to preserve positional information. Then, the generated sequence is input into several multi-head attention layers to generate the final representation. The first token sequence is input to a softmax classification layer to produce the recognition output in the classification stage. RESULTS: The dataset for training and testing employs fundus images of different resolutions, subdivided into patches. We challenge our method against current CNNs and extreme learning machines and achieve an appealing performance. Specifically, the suggested deep learning architecture attains an accuracy of 91.4\%, specificity~=~0.977 (95\% confidence interval (CI) (0.951-1)), precision~=~0.928 (95\% CI (0.852-1)), sensitivity~=~0.926 (95\% CI (0.863-0.989)), quadratic weighted kappa score~=~0.935, and area under curve (AUC)~=~0.986. CONCLUSION: Our comparative experiments against current methods conclude that our model is competitive and highlight that an attention mechanism based on a Vision Transformer model is promising for the diabetic retinopathy grade recognition task.},
  langid = {english},
  pmid = {34693536},
  keywords = {Area Under Curve,deep learning,Diabetes Mellitus,diabetic retinopathy,Diabetic Retinopathy,Fundus Oculi,Humans,multi-head attention,Neural Networks Computer,Vision Transformer}
}

@article{xiaoAssessmentEarlyDiabetic2023,
  title = {Assessment of Early Diabetic Retinopathy Severity Using Ultra-Widefield {{Clarus}} versus Conventional Five-Field and Ultra-Widefield {{Optos}} Fundus Imaging},
  author = {Xiao, Yuanyuan and Dan, Handong and Du, Xiaofeng and Michaelide, Michel and Nie, Xiaodong and Wang, Wanxiao and Zheng, Miao and Wang, Dongdong and Huang, Zixu and Song, Zongming},
  year = {2023},
  month = oct,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {17131},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-43947-5},
  urldate = {2024-02-22},
  abstract = {To compare early diabetic retinopathy (DR) severity level and the abilities in detecting early DR lesions among conventional five-field, ultrawide-field (UWF) Optos, and UWF Clarus fundus imaging methods. This was a single-center, prospective, clinic-based, and comparative study. In total, 157 consecutive patients with diabetes mellitus were enrolled in this study. All patients underwent comprehensive ophthalmological examinations. Following mydriasis, each eye was examined with conventional five-field, UWF Optos, and UWF Clarus fundus imaging methods. The initial UWF images were overlaid with a template mask that obscured the retina, which created a five-field view from UWF images (covered UWF images). The covered UWF images were then graded, after which the template mask was removed, and the original UWF images were also evaluated. All images were graded using the International Clinical DR severity scale. DR grades were compared and analyzed by weighted kappa statistics among the three fundus imaging methods. In total, 157 consecutive patients with diabetes (302 eyes) were enrolled in this study. Weighted kappa statistics for agreement were 0.471 (five-field vs. covered Optos), 0.809 (five-field vs. covered Clarus), 0.396 (covered Optos vs. covered Clarus), 0.463 (five-field vs. Optos), 0.521 (five-field vs. Clarus 133{$^\circ$}), 0.500 (five-field vs. Clarus 200{$^\circ$}), 0.323 (Optos vs. Clarus 133{$^\circ$}), and 0.349 (Optos vs. Clarus 200{$^\circ$}). The area under curve of covered Clarus images was higher than that of conventional five-field images at three different thresholds. Compared with conventional five-field and Optos fundus imaging methods, Clarus fundus imaging methods exhibited excellent performance in assessing early DR severity. Thus, Clarus fundus imaging methods were superior for early detection of DR.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Diseases,Health care,Risk factors},
  file = {C:\Users\cleme\Zotero\storage\VLXLPXI6\Xiao et al. - 2023 - Assessment of early diabetic retinopathy severity .pdf}
}

@article{xiaoComparisonQuantitativeAssessment2023,
  title = {Comparison of Quantitative Assessment and Efficiency of Diabetic Retinopathy Diagnosis Using {{ETDRS}} Seven-Field Imaging and Two Ultra-Widefield Imaging},
  author = {Xiao, Yuanyuan and Huang, Zixu and Yuan, Qiongqiong and Du, Xiaofeng and Li, Zeyu and Nie, Xiaodong and Shi, Qianqian and Dan, Handong and Song, Zongming},
  year = {2023},
  month = dec,
  journal = {Eye},
  volume = {37},
  number = {17},
  pages = {3558--3564},
  publisher = {Nature Publishing Group},
  issn = {1476-5454},
  doi = {10.1038/s41433-023-02549-1},
  urldate = {2024-02-22},
  abstract = {This study compared the efficiency of diabetic retinopathy (DR) diagnosis and differences in the relative visible retinal area among the Early Treatment Diabetic Retinopathy Study (ETDRS) seven-field, ultra-widefield (UWF)-Optos, and UWF-Clarus fundus imaging methods.},
  copyright = {2023 The Author(s), under exclusive licence to The Royal College of Ophthalmologists},
  langid = {english},
  keywords = {Eye abnormalities,Retinal diseases},
  file = {C:\Users\cleme\Zotero\storage\8CRS3PVS\Xiao et al. - 2023 - Comparison of quantitative assessment and efficien.pdf}
}

@inproceedings{xie2020self,
  title = {Self-Training with Noisy Student Improves Imagenet Classification},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
  year = {2020},
  pages = {10687--10698}
}

@inproceedings{xie2020self,
  title = {Self-Training with Noisy Student Improves Imagenet Classification},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
  year = {2020},
  pages = {10687--10698}
}

@inproceedings{xie2021segformer,
  title = {{{SegFormer}}: {{Simple}} and Efficient Design for Semantic Segmentation with Transformers},
  booktitle = {Neural Information Processing Systems ({{NeurIPS}})},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  year = {2021}
}

@inproceedings{xie2021segformer,
  title = {{{SegFormer}}: {{Simple}} and Efficient Design for Semantic Segmentation with Transformers},
  booktitle = {Neural Information Processing Systems ({{NeurIPS}})},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  year = {2021}
}

@article{xieSegFormerSimpleEfficient,
  title = {{{SegFormer}}: {{Simple}} and {{Efficient Design}} for {{Semantic Segmentation}} with {{Transformers}}},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5{\texttimes} smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code is available at: github.com/NVlabs/SegFormer.},
  langid = {english}
}

@article{xieSegFormerSimpleEfficienta,
  title = {{{SegFormer}}: {{Simple}} and {{Efficient Design}} for {{Semantic Segmentation}} with {{Transformers}}},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5{\texttimes} smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code is available at: github.com/NVlabs/SegFormer.},
  langid = {english}
}

@article{xieSegFormerSimpleEfficientb,
  title = {{{SegFormer}}: {{Simple}} and {{Efficient Design}} for {{Semantic Segmentation}} with {{Transformers}}},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5{\texttimes} smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code is available at: github.com/NVlabs/SegFormer.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\FFC96E94\Xie et al. - SegFormer Simple and Efﬁcient Design for Semantic.pdf}
}

@article{xieSegFormerSimpleEfficientc,
  title = {{{SegFormer}}: {{Simple}} and {{Efficient Design}} for {{Semantic Segmentation}} with {{Transformers}}},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5{\texttimes} smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code is available at: github.com/NVlabs/SegFormer.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\AYKMBN3V\Xie et al. - SegFormer Simple and Efﬁcient Design for Semantic.pdf}
}

@article{xieSurveyIncorporatingDomain2021,
  title = {A Survey on Incorporating Domain Knowledge into Deep Learning for Medical Image Analysis},
  author = {Xie, Xiaozheng and Niu, Jianwei and Liu, Xuefeng and Chen, Zhengsu and Tang, Shaojie and Yu, Shui},
  year = {2021},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {69},
  pages = {101985},
  issn = {1361-8415},
  doi = {10.1016/j.media.2021.101985},
  urldate = {2022-07-10},
  abstract = {Although deep learning models like CNNs have achieved great success in medical image analysis, the small size of medical datasets remains a major bottleneck in this area. To address this problem, researchers have started looking for external information beyond current available medical datasets. Traditional approaches generally leverage the information from natural images via transfer learning. More recent works utilize the domain knowledge from medical doctors, to create networks that resemble how medical doctors are trained, mimic their diagnostic patterns, or focus on the features or areas they pay particular attention to. In this survey, we summarize the current progress on integrating medical domain knowledge into deep learning models for various tasks, such as disease diagnosis, lesion, organ and abnormality detection, lesion and organ segmentation. For each task, we systematically categorize different kinds of medical domain knowledge that have been utilized and their corresponding integrating methods. We also provide current challenges and directions for future research.},
  langid = {english},
  keywords = {Deep learning models,Medical domain knowledge,Medical image analysis},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\5CQM7MCD\\xie2021.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\SQAF2JUD\\Xie et al. - 2021 - A survey on incorporating domain knowledge into de.pdf;C\:\\Users\\cleme\\Zotero\\storage\\NMJQMQYN\\S1361841521000311.html}
}

@article{xiongNumberLinearRegions,
  title = {On the {{Number}} of {{Linear Regions}} of {{Convolutional Neural Networks}}},
  author = {Xiong, Huan and Huang, Lei and Yu, Mengyang and Liu, Li and Zhu, Fan and Shao, Ling},
  abstract = {One fundamental problem in deep learning is understanding the outstanding performance of deep Neural Networks (NNs) in practice. One explanation for the superiority of NNs is that they can realize a large class of complicated functions, i.e., they have powerful expressivity. The expressivity of a ReLU NN can be quantified by the maximal number of linear regions it can separate its input space into. In this paper, we provide several mathematical results needed for studying the linear regions of CNNs, and use them to derive the maximal and average numbers of linear regions for one-layer ReLU CNNs. Furthermore, we obtain upper and lower bounds for the number of linear regions of multi-layer ReLU CNNs. Our results suggest that deeper CNNs have more powerful expressivity than their shallow counterparts, while CNNs have more expressivity than fully-connected NNs per parameter.},
  langid = {english}
}

@article{xiongNumberLinearRegionsa,
  title = {On the {{Number}} of {{Linear Regions}} of {{Convolutional Neural Networks}}},
  author = {Xiong, Huan and Huang, Lei and Yu, Mengyang and Liu, Li and Zhu, Fan and Shao, Ling},
  abstract = {One fundamental problem in deep learning is understanding the outstanding performance of deep Neural Networks (NNs) in practice. One explanation for the superiority of NNs is that they can realize a large class of complicated functions, i.e., they have powerful expressivity. The expressivity of a ReLU NN can be quantified by the maximal number of linear regions it can separate its input space into. In this paper, we provide several mathematical results needed for studying the linear regions of CNNs, and use them to derive the maximal and average numbers of linear regions for one-layer ReLU CNNs. Furthermore, we obtain upper and lower bounds for the number of linear regions of multi-layer ReLU CNNs. Our results suggest that deeper CNNs have more powerful expressivity than their shallow counterparts, while CNNs have more expressivity than fully-connected NNs per parameter.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\XAGSVKTC\Xiong et al. - On the Number of Linear Regions of Convolutional N.pdf}
}

@article{xuFFUNetFeatureFusion2021,
  title = {{{FFU-Net}}: {{Feature Fusion U-Net}} for {{Lesion Segmentation}} of {{Diabetic Retinopathy}}},
  shorttitle = {{{FFU-Net}}},
  author = {Xu, Yifei and Zhou, Zhuming and Li, Xiao and Zhang, Nuo and Zhang, Meizi and Wei, Pingping},
  year = {2021},
  month = jan,
  journal = {BioMed Research International},
  volume = {2021},
  pages = {6644071},
  issn = {2314-6133},
  doi = {10.1155/2021/6644071},
  urldate = {2022-10-23},
  abstract = {Diabetic retinopathy is one of the main causes of blindness in human eyes, and lesion segmentation is an important basic work for the diagnosis of diabetic retinopathy. Due to the small lesion areas scattered in fundus images, it is laborious to segment the lesion of diabetic retinopathy effectively with the existing U-Net model. In this paper, we proposed a new lesion segmentation model named FFU-Net (Feature Fusion U-Net) that enhances U-Net from the following points. Firstly, the pooling layer in the network is replaced with a convolutional layer to reduce spatial loss of the fundus image. Then, we integrate multiscale feature fusion (MSFF) block into the encoders which helps the network to learn multiscale features efficiently and enrich the information carried with skip connection and lower-resolution decoder by fusing contextual channel attention (CCA) models. Finally, in order to solve the problems of data imbalance and misclassification, we present a Balanced Focal Loss function. In the experiments on benchmark dataset IDRID, we make an ablation study to verify the effectiveness of each component and compare FFU-Net against several state-of-the-art models. In comparison with baseline U-Net, FFU-Net improves the segmentation performance by 11.97\%, 10.68\%, and 5.79\% on metrics SEN, IOU, and DICE, respectively. The quantitative and qualitative results demonstrate the superiority of our FFU-Net in the task of lesion segmentation of diabetic retinopathy.},
  pmcid = {PMC7801055},
  pmid = {33490274}
}

@article{xuFFUNetFeatureFusion2021a,
  title = {{{FFU-Net}}: {{Feature Fusion U-Net}} for {{Lesion Segmentation}} of {{Diabetic Retinopathy}}},
  shorttitle = {{{FFU-Net}}},
  author = {Xu, Yifei and Zhou, Zhuming and Li, Xiao and Zhang, Nuo and Zhang, Meizi and Wei, Pingping},
  year = {2021},
  month = jan,
  journal = {BioMed Research International},
  volume = {2021},
  pages = {6644071},
  issn = {2314-6133},
  doi = {10.1155/2021/6644071},
  urldate = {2022-10-23},
  abstract = {Diabetic retinopathy is one of the main causes of blindness in human eyes, and lesion segmentation is an important basic work for the diagnosis of diabetic retinopathy. Due to the small lesion areas scattered in fundus images, it is laborious to segment the lesion of diabetic retinopathy effectively with the existing U-Net model. In this paper, we proposed a new lesion segmentation model named FFU-Net (Feature Fusion U-Net) that enhances U-Net from the following points. Firstly, the pooling layer in the network is replaced with a convolutional layer to reduce spatial loss of the fundus image. Then, we integrate multiscale feature fusion (MSFF) block into the encoders which helps the network to learn multiscale features efficiently and enrich the information carried with skip connection and lower-resolution decoder by fusing contextual channel attention (CCA) models. Finally, in order to solve the problems of data imbalance and misclassification, we present a Balanced Focal Loss function. In the experiments on benchmark dataset IDRID, we make an ablation study to verify the effectiveness of each component and compare FFU-Net against several state-of-the-art models. In comparison with baseline U-Net, FFU-Net improves the segmentation performance by 11.97\%, 10.68\%, and 5.79\% on metrics SEN, IOU, and DICE, respectively. The quantitative and qualitative results demonstrate the superiority of our FFU-Net in the task of lesion segmentation of diabetic retinopathy.},
  pmcid = {PMC7801055},
  pmid = {33490274},
  file = {C:\Users\cleme\Zotero\storage\3NIUSPQC\Xu et al. - 2021 - FFU-Net Feature Fusion U-Net for Lesion Segmentat.pdf}
}

@article{xuHowPowerfulAre2018,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.00826 [cs, stat]},
  eprint = {1810.00826},
  primaryclass = {cs, stat},
  urldate = {2019-08-15},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the WeisfeilerLehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{xuHowPowerfulAre2018a,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.00826 [cs, stat]},
  eprint = {1810.00826},
  primaryclass = {cs, stat},
  urldate = {2019-08-15},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the WeisfeilerLehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\cleme\Zotero\storage\EWB8H7HY\Xu et al. - 2018 - How Powerful are Graph Neural Networks.pdf}
}

@inproceedings{xuNoisyLabelsAre2021,
  title = {Noisy {{Labels}} Are {{Treasure}}: {{Mean-Teacher-Assisted Confident Learning}} for {{Hepatic Vessel Segmentation}}},
  shorttitle = {Noisy {{Labels}} Are {{Treasure}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Xu, Zhe and Lu, Donghuan and Wang, Yixin and Luo, Jie and Jayender, Jagadeesan and Ma, Kai and Zheng, Yefeng and Li, Xiu},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--13},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_1},
  abstract = {Manually segmenting the hepatic vessels from Computer Tomography (CT) is far more expertise-demanding and laborious than other structures due to the low-contrast and complex morphology of vessels, resulting in the extreme lack of high-quality labeled data. Without sufficient high-quality annotations, the usual data-driven learning-based approaches struggle with deficient training. On the other hand, directly introducing additional data with low-quality annotations may confuse the network, leading to undesirable performance degradation. To address this issue, we propose a novel mean-teacher-assisted confident learning framework to robustly exploit the noisy labeled data for the challenging hepatic vessel segmentation task. Specifically, with the adapted confident learning assisted by a third party, i.e., the weight-averaged teacher model, the noisy labels in the additional low-quality dataset can be transformed from `encumbrance' to `treasure' via progressive pixel-wise soft-correction, thus providing productive guidance. Extensive experiments using two public datasets demonstrate the superiority of the proposed framework as well as the effectiveness of each component.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Confident learning,Hepatic vessel,Noisy label}
}

@inproceedings{xuNoisyLabelsAre2021a,
  title = {Noisy {{Labels}} Are {{Treasure}}: {{Mean-Teacher-Assisted Confident Learning}} for {{Hepatic Vessel Segmentation}}},
  shorttitle = {Noisy {{Labels}} Are {{Treasure}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Xu, Zhe and Lu, Donghuan and Wang, Yixin and Luo, Jie and Jayender, Jagadeesan and Ma, Kai and Zheng, Yefeng and Li, Xiu},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--13},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_1},
  abstract = {Manually segmenting the hepatic vessels from Computer Tomography (CT) is far more expertise-demanding and laborious than other structures due to the low-contrast and complex morphology of vessels, resulting in the extreme lack of high-quality labeled data. Without sufficient high-quality annotations, the usual data-driven learning-based approaches struggle with deficient training. On the other hand, directly introducing additional data with low-quality annotations may confuse the network, leading to undesirable performance degradation. To address this issue, we propose a novel mean-teacher-assisted confident learning framework to robustly exploit the noisy labeled data for the challenging hepatic vessel segmentation task. Specifically, with the adapted confident learning assisted by a third party, i.e., the weight-averaged teacher model, the noisy labels in the additional low-quality dataset can be transformed from `encumbrance' to `treasure' via progressive pixel-wise soft-correction, thus providing productive guidance. Extensive experiments using two public datasets demonstrate the superiority of the proposed framework as well as the effectiveness of each component.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Confident learning,Hepatic vessel,Noisy label}
}

@inproceedings{xuNoisyLabelsAre2021b,
  title = {Noisy {{Labels}} Are {{Treasure}}: {{Mean-Teacher-Assisted Confident Learning}} for {{Hepatic Vessel Segmentation}}},
  shorttitle = {Noisy {{Labels}} Are {{Treasure}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Xu, Zhe and Lu, Donghuan and Wang, Yixin and Luo, Jie and Jayender, Jagadeesan and Ma, Kai and Zheng, Yefeng and Li, Xiu},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--13},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_1},
  abstract = {Manually segmenting the hepatic vessels from Computer Tomography (CT) is far more expertise-demanding and laborious than other structures due to the low-contrast and complex morphology of vessels, resulting in the extreme lack of high-quality labeled data. Without sufficient high-quality annotations, the usual data-driven learning-based approaches struggle with deficient training. On the other hand, directly introducing additional data with low-quality annotations may confuse the network, leading to undesirable performance degradation. To address this issue, we propose a novel mean-teacher-assisted confident learning framework to robustly exploit the noisy labeled data for the challenging hepatic vessel segmentation task. Specifically, with the adapted confident learning assisted by a third party, i.e., the weight-averaged teacher model, the noisy labels in the additional low-quality dataset can be transformed from `encumbrance' to `treasure' via progressive pixel-wise soft-correction, thus providing productive guidance. Extensive experiments using two public datasets demonstrate the superiority of the proposed framework as well as the effectiveness of each component.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Confident learning,Hepatic vessel,Noisy label},
  file = {C:\Users\cleme\Zotero\storage\4LNB2JAD\Xu et al. - 2021 - Noisy Labels are Treasure Mean-Teacher-Assisted C.pdf}
}

@inproceedings{xuNoisyLabelsAre2021c,
  title = {Noisy {{Labels}} Are {{Treasure}}: {{Mean-Teacher-Assisted Confident Learning}} for {{Hepatic Vessel Segmentation}}},
  shorttitle = {Noisy {{Labels}} Are {{Treasure}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Xu, Zhe and Lu, Donghuan and Wang, Yixin and Luo, Jie and Jayender, Jagadeesan and Ma, Kai and Zheng, Yefeng and Li, Xiu},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--13},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_1},
  abstract = {Manually segmenting the hepatic vessels from Computer Tomography (CT) is far more expertise-demanding and laborious than other structures due to the low-contrast and complex morphology of vessels, resulting in the extreme lack of high-quality labeled data. Without sufficient high-quality annotations, the usual data-driven learning-based approaches struggle with deficient training. On the other hand, directly introducing additional data with low-quality annotations may confuse the network, leading to undesirable performance degradation. To address this issue, we propose a novel mean-teacher-assisted confident learning framework to robustly exploit the noisy labeled data for the challenging hepatic vessel segmentation task. Specifically, with the adapted confident learning assisted by a third party, i.e., the weight-averaged teacher model, the noisy labels in the additional low-quality dataset can be transformed from `encumbrance' to `treasure' via progressive pixel-wise soft-correction, thus providing productive guidance. Extensive experiments using two public datasets demonstrate the superiority of the proposed framework as well as the effectiveness of each component.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Confident learning,Hepatic vessel,Noisy label},
  file = {C:\Users\cleme\Zotero\storage\NTIQ54RG\Xu et al. - 2021 - Noisy Labels are Treasure Mean-Teacher-Assisted C.pdf}
}

@inproceedings{xuRegistrationMultimodalTemporal2012,
  title = {Registration of Multimodal and Temporal Images of the Retina Using a Combined Feature-Based and Statistics-Based Method},
  booktitle = {2012 5th {{International Conference}} on {{BioMedical Engineering}} and {{Informatics}}},
  author = {Xu, M. and Sun, Y.},
  year = {2012},
  month = oct,
  pages = {353--357},
  doi = {10.1109/BMEI.2012.6513145},
  abstract = {Registration of retinal images is important for doctors to diagnose because it merges information of several images. This paper presents an automated multimodal and temporal retinal image registration method which is especially robust to infrared radiation (IR) fundus images, which may suffer terrible imaging artifacts, such as extremely low contrast, overexposed edges, or invisibility of blood vessels. The method we propose combines the feature-based registration method and the statistics-based registration method by using the OD detection method based on Line operator to detect the OD for the fundamental registration and use the maximal mutual information method for fine registration and improve them by down-sampling, getting Region of Interest (ROI), bilateral filter and so on to get a better effect. Experiments over optical coherence tomography (OCT) and IR dataset show that our proposed method outperforms other methods by obtaining an accuracy of 82\% and an average time consuming of 5.1 second.},
  keywords = {bilateral filter,biomedical optical imaging,blood vessels,down-sampling,eye,feature-based registration method,image registration,image sampling,infrared imaging,infrared radiation (IR) fundus,infrared radiation fundus images,line operator,maximal mutual information method,medical image processing,multimodal,multimodal retinal image registration method,mutual information,OD detection,OD Detection,optic disc detection,optical coherence tomography,optical tomography,patient diagnose,statistical analysis,statistics-based registration method,temporal retinal image registration method}
}

@inproceedings{xuRegistrationMultimodalTemporal2012a,
  title = {Registration of Multimodal and Temporal Images of the Retina Using a Combined Feature-Based and Statistics-Based Method},
  booktitle = {2012 5th {{International Conference}} on {{BioMedical Engineering}} and {{Informatics}}},
  author = {Xu, M. and Sun, Y.},
  year = {2012},
  month = oct,
  pages = {353--357},
  doi = {10.1109/BMEI.2012.6513145},
  abstract = {Registration of retinal images is important for doctors to diagnose because it merges information of several images. This paper presents an automated multimodal and temporal retinal image registration method which is especially robust to infrared radiation (IR) fundus images, which may suffer terrible imaging artifacts, such as extremely low contrast, overexposed edges, or invisibility of blood vessels. The method we propose combines the feature-based registration method and the statistics-based registration method by using the OD detection method based on Line operator to detect the OD for the fundamental registration and use the maximal mutual information method for fine registration and improve them by down-sampling, getting Region of Interest (ROI), bilateral filter and so on to get a better effect. Experiments over optical coherence tomography (OCT) and IR dataset show that our proposed method outperforms other methods by obtaining an accuracy of 82\% and an average time consuming of 5.1 second.},
  keywords = {bilateral filter,biomedical optical imaging,blood vessels,down-sampling,eye,feature-based registration method,image registration,image sampling,infrared imaging,infrared radiation (IR) fundus,infrared radiation fundus images,line operator,maximal mutual information method,medical image processing,multimodal,multimodal retinal image registration method,mutual information,OD detection,OD Detection,optic disc detection,optical coherence tomography,optical tomography,patient diagnose,statistical analysis,statistics-based registration method,temporal retinal image registration method},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\3RZSXNJB\\Xu et Sun - 2012 - Registration of multimodal and temporal images of .pdf;C\:\\Users\\cleme\\Zotero\\storage\\HBHKRCFB\\6513145.html}
}

@article{xuThreeDimensionalOptic2006,
  title = {Three Dimensional Optic Disc Visualisation from Stereo Images via Dual Registration and Ocular Media Optical Correction},
  author = {Xu, J and Chutatape, O and Zheng, C and Kuan, P C T},
  year = {2006},
  month = feb,
  journal = {The British Journal of Ophthalmology},
  volume = {90},
  number = {2},
  pages = {181--185},
  issn = {0007-1161},
  doi = {10.1136/bjo.2005.082313},
  urldate = {2019-11-12},
  abstract = {Background/aims The three dimensional (3-D) visualisation of the optic disc in true colour will give essential meaning in clinical application. It is not only useful for clinicians in the evaluation of the condition of the optic disc, but it also simplifies the pathological diagnosis and disease progression monitoring. This paper describes a complete 3-D optic disc reconstruction method from a pair of stereo images by a series of robust procedures including camera calibration, image registration, depth recovery, and ocular media optical inclusion. Methods Two registration techniques (correlation and feature based methods) are combined together to prune the uncertain matching points in order to improve the overall accuracy of registration. The ocular media within the eyeball are lump modelled as a single lens and integrated into the reconstruction process to obtain an accurate 3-D optic disc image. Conclusion The recovered 3-D optic disc images show good consistency and compatibility when compared with the results from Heidelberg retina tomography (HRT) under clinical validation, with an additional advantage of implementing a more economical and conventional mode of retinal image acquisition.},
  pmcid = {PMC1860159},
  pmid = {16424530}
}

@article{xuThreeDimensionalOptic2006a,
  title = {Three Dimensional Optic Disc Visualisation from Stereo Images via Dual Registration and Ocular Media Optical Correction},
  author = {Xu, J and Chutatape, O and Zheng, C and Kuan, P C T},
  year = {2006},
  month = feb,
  journal = {The British Journal of Ophthalmology},
  volume = {90},
  number = {2},
  pages = {181--185},
  issn = {0007-1161},
  doi = {10.1136/bjo.2005.082313},
  urldate = {2019-11-12},
  abstract = {Background/aims The three dimensional (3-D) visualisation of the optic disc in true colour will give essential meaning in clinical application. It is not only useful for clinicians in the evaluation of the condition of the optic disc, but it also simplifies the pathological diagnosis and disease progression monitoring. This paper describes a complete 3-D optic disc reconstruction method from a pair of stereo images by a series of robust procedures including camera calibration, image registration, depth recovery, and ocular media optical inclusion. Methods Two registration techniques (correlation and feature based methods) are combined together to prune the uncertain matching points in order to improve the overall accuracy of registration. The ocular media within the eyeball are lump modelled as a single lens and integrated into the reconstruction process to obtain an accurate 3-D optic disc image. Conclusion The recovered 3-D optic disc images show good consistency and compatibility when compared with the results from Heidelberg retina tomography (HRT) under clinical validation, with an additional advantage of implementing a more economical and conventional mode of retinal image acquisition.},
  pmcid = {PMC1860159},
  pmid = {16424530},
  file = {C:\Users\cleme\Zotero\storage\9SQRYYA8\Xu et al. - 2006 - Three dimensional optic disc visualisation from st.pdf}
}

@article{yamaguchiTropicalCycloneIntensity2018,
  title = {Tropical {{Cyclone Intensity Prediction}} in the {{Western North Pacific Basin Using SHIPS}} and {{JMA}}/{{GSM}}},
  author = {Yamaguchi, Munehiko and Owada, Hiromi and Shimada, Udai and Sawada, Masahiro and Iriguchi, Takeshi and Musgrave, Kate D. and DeMaria, Mark},
  year = {2018},
  journal = {Sola},
  volume = {14},
  pages = {138--143},
  doi = {10.2151/sola.2018-024},
  abstract = {This study investigates prediction of TC intensity in the western North Pacific basin using a statistical-dynamical model called the Statistical Hurricane Intensity Prediction Scheme (SHIPS), with data sources in operations at the Japan Meteorological Agency (JMA) such as the JMA/Global Spectral Model forecast fields. In addition to predicting the change in the maximum wind (Vmax) as in the original SHIPS technique, another version of SHIPS for predicting the change in the minimum sea-level pressure (Pmin) has been developed. With 13 years of training samples, a total of 26 predictors were selected from among 52 through stepwise regression. Based on three years of independent samples, the root mean square errors of both Vmax and Pmin by the 26-predictor SHIPS model were found to be much smaller than those of the JMA/GSM and a simple climatology and persistence intensity model, which JMA official intensity forecasts are currently mainly based on. The prediction accuracy was not sensitive to the number of predictors as long as the leading predictors were included. Benefits of operationalizing SHIPS include a reduction in the errors of the JMA official intensity forecasts and an extension of their forecast length beyond the current 3 days (e.g., 5 days).}
}

@article{yamaguchiTropicalCycloneIntensity2018a,
  title = {Tropical {{Cyclone Intensity Prediction}} in the {{Western North Pacific Basin Using SHIPS}} and {{JMA}}/{{GSM}}},
  author = {Yamaguchi, Munehiko and Owada, Hiromi and Shimada, Udai and Sawada, Masahiro and Iriguchi, Takeshi and Musgrave, Kate D. and DeMaria, Mark},
  year = {2018},
  journal = {Sola},
  volume = {14},
  pages = {138--143},
  doi = {10.2151/sola.2018-024},
  abstract = {This study investigates prediction of TC intensity in the western North Pacific basin using a statistical-dynamical model called the Statistical Hurricane Intensity Prediction Scheme (SHIPS), with data sources in operations at the Japan Meteorological Agency (JMA) such as the JMA/Global Spectral Model forecast fields. In addition to predicting the change in the maximum wind (Vmax) as in the original SHIPS technique, another version of SHIPS for predicting the change in the minimum sea-level pressure (Pmin) has been developed. With 13 years of training samples, a total of 26 predictors were selected from among 52 through stepwise regression. Based on three years of independent samples, the root mean square errors of both Vmax and Pmin by the 26-predictor SHIPS model were found to be much smaller than those of the JMA/GSM and a simple climatology and persistence intensity model, which JMA official intensity forecasts are currently mainly based on. The prediction accuracy was not sensitive to the number of predictors as long as the leading predictors were included. Benefits of operationalizing SHIPS include a reduction in the errors of the JMA official intensity forecasts and an extension of their forecast length beyond the current 3 days (e.g., 5 days).},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\QSB5TAH4\\Yamaguchi et al. - 2018 - Tropical Cyclone Intensity Prediction in the Weste.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UNLMW8D8\\en.html}
}

@incollection{yamamotoAutomaticRecognitionColor1976,
  title = {Automatic {{Recognition}} of {{Color Fundus Photographs}}},
  booktitle = {Digital {{Processing}} of {{Biomedical Images}}},
  author = {Yamamoto, S. and Yokouchi, H.},
  editor = {Preston, K. and Onoe, M.},
  year = {1976},
  pages = {385--398},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4684-0769-3_24},
  urldate = {2019-11-01},
  abstract = {The evaluation of several findings taken from fundus photographs is a technique widely used in mass screening of geriatric disorders and diagnoses of hypertension. The examination of fundus photographs is left to the pattern recognition ability of the individual physician. Along with the increasing occurrence of geriatric disorders, the quantity of work could well come to exceed the capacity of the physicians available. Nevertheless, there has been no research to date on the automatic reading of fundus photographs nor has there been any direct discussion of the technical possibility of automation in this area.},
  isbn = {978-1-4684-0769-3},
  langid = {english},
  keywords = {Automatic Recognition,Contour Line,Fundus Photograph,Picture Element,Smoothing Process}
}

@incollection{yamamotoAutomaticRecognitionColor1976a,
  title = {Automatic {{Recognition}} of {{Color Fundus Photographs}}},
  booktitle = {Digital {{Processing}} of {{Biomedical Images}}},
  author = {Yamamoto, S. and Yokouchi, H.},
  editor = {Preston, K. and Onoe, M.},
  year = {1976},
  pages = {385--398},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4684-0769-3_24},
  urldate = {2019-11-01},
  isbn = {978-1-4684-0771-6 978-1-4684-0769-3},
  langid = {english}
}

@incollection{yamamotoAutomaticRecognitionColor1976b,
  title = {Automatic {{Recognition}} of {{Color Fundus Photographs}}},
  booktitle = {Digital {{Processing}} of {{Biomedical Images}}},
  author = {Yamamoto, S. and Yokouchi, H.},
  editor = {Preston, K. and Onoe, M.},
  year = {1976},
  pages = {385--398},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4684-0769-3_24},
  urldate = {2019-11-01},
  isbn = {978-1-4684-0771-6 978-1-4684-0769-3},
  langid = {english}
}

@incollection{yamamotoAutomaticRecognitionColor1976c,
  title = {Automatic {{Recognition}} of {{Color Fundus Photographs}}},
  booktitle = {Digital {{Processing}} of {{Biomedical Images}}},
  author = {Yamamoto, S. and Yokouchi, H.},
  editor = {Preston, K. and Onoe, M.},
  year = {1976},
  pages = {385--398},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4684-0769-3_24},
  urldate = {2019-11-01},
  abstract = {The evaluation of several findings taken from fundus photographs is a technique widely used in mass screening of geriatric disorders and diagnoses of hypertension. The examination of fundus photographs is left to the pattern recognition ability of the individual physician. Along with the increasing occurrence of geriatric disorders, the quantity of work could well come to exceed the capacity of the physicians available. Nevertheless, there has been no research to date on the automatic reading of fundus photographs nor has there been any direct discussion of the technical possibility of automation in this area.},
  isbn = {978-1-4684-0769-3},
  langid = {english},
  keywords = {Automatic Recognition,Contour Line,Fundus Photograph,Picture Element,Smoothing Process}
}

@incollection{yamamotoAutomaticRecognitionColor1976d,
  title = {Automatic {{Recognition}} of {{Color Fundus Photographs}}},
  booktitle = {Digital {{Processing}} of {{Biomedical Images}}},
  author = {Yamamoto, S. and Yokouchi, H.},
  editor = {Preston, K. and Onoe, M.},
  year = {1976},
  pages = {385--398},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4684-0769-3_24},
  urldate = {2019-11-01},
  isbn = {978-1-4684-0771-6 978-1-4684-0769-3},
  langid = {english}
}

@incollection{yamamotoAutomaticRecognitionColor1976e,
  title = {Automatic {{Recognition}} of {{Color Fundus Photographs}}},
  booktitle = {Digital {{Processing}} of {{Biomedical Images}}},
  author = {Yamamoto, S. and Yokouchi, H.},
  editor = {Preston, K. and Onoe, M.},
  year = {1976},
  pages = {385--398},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4684-0769-3_24},
  urldate = {2019-11-01},
  isbn = {978-1-4684-0771-6 978-1-4684-0769-3},
  langid = {english}
}

@article{yanDeeplearningbasedPredictionLate2020,
  title = {Deep-Learning-Based {{Prediction}} of {{Late Age-Related Macular Degeneration Progression}}},
  author = {Yan, Qi and Weeks, Daniel E. and Xin, Hongyi and Swaroop, Anand and Chew, Emily Y. and Huang, Heng and Ding, Ying and Chen, Wei},
  year = {2020},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {2},
  pages = {141--150},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0154-9},
  abstract = {Both genetic and environmental factors influence the etiology of age-related macular degeneration (AMD), a leading cause of blindness. AMD severity is primarily measured by fundus images and recently developed machine learning methods can successfully predict AMD progression using image data. However, none of these methods have utilized both genetic and image data for predicting AMD progression. Here we jointly used genotypes and fundus images to predict an eye as having progressed to late AMD with a modified deep convolutional neural network (CNN). In total, we used 31,262 fundus images and 52 AMD-associated genetic variants from 1,351 subjects from the Age-Related Eye Disease Study (AREDS) with disease severity phenotypes and fundus images available at baseline and follow-up visits over a period of 12 years. Our results showed that fundus images coupled with genotypes could predict late AMD progression with an averaged area under the curve (AUC) value of 0.85 (95\%CI: 0.83-0.86). The results using fundus images alone showed an averaged AUC of 0.81 (95\%CI: 0.80-0.83). We implemented our model in a cloud-based application for individual risk assessment.},
  langid = {english},
  pmcid = {PMC7153739},
  pmid = {32285025}
}

@article{yanDeeplearningbasedPredictionLate2020a,
  title = {Deep-Learning-Based {{Prediction}} of {{Late Age-Related Macular Degeneration Progression}}},
  author = {Yan, Qi and Weeks, Daniel E. and Xin, Hongyi and Swaroop, Anand and Chew, Emily Y. and Huang, Heng and Ding, Ying and Chen, Wei},
  year = {2020},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {2},
  pages = {141--150},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0154-9},
  abstract = {Both genetic and environmental factors influence the etiology of age-related macular degeneration (AMD), a leading cause of blindness. AMD severity is primarily measured by fundus images and recently developed machine learning methods can successfully predict AMD progression using image data. However, none of these methods have utilized both genetic and image data for predicting AMD progression. Here we jointly used genotypes and fundus images to predict an eye as having progressed to late AMD with a modified deep convolutional neural network (CNN). In total, we used 31,262 fundus images and 52 AMD-associated genetic variants from 1,351 subjects from the Age-Related Eye Disease Study (AREDS) with disease severity phenotypes and fundus images available at baseline and follow-up visits over a period of 12 years. Our results showed that fundus images coupled with genotypes could predict late AMD progression with an averaged area under the curve (AUC) value of 0.85 (95\%CI: 0.83-0.86). The results using fundus images alone showed an averaged AUC of 0.81 (95\%CI: 0.80-0.83). We implemented our model in a cloud-based application for individual risk assessment.},
  langid = {english},
  pmcid = {PMC7153739},
  pmid = {32285025},
  file = {C:\Users\cleme\Zotero\storage\UQPPJ3VC\Yan et al. - 2020 - Deep-learning-based Prediction of Late Age-Related.pdf}
}

@article{yangClassificationDiabeticRetinopathy2022,
  title = {Classification of Diabetic Retinopathy: {{Past}}, Present and Future},
  shorttitle = {Classification of Diabetic Retinopathy},
  author = {Yang, Zhengwei and Tan, Tien-En and Shao, Yan and Wong, Tien Yin and Li, Xiaorong},
  year = {2022},
  journal = {Frontiers in Endocrinology},
  volume = {13},
  issn = {1664-2392},
  urldate = {2023-08-25},
  abstract = {Diabetic retinopathy (DR) is a leading cause of visual impairment and blindness worldwide. Since DR was first recognized as an important complication of diabetes, there have been many attempts to accurately classify the severity and stages of disease. These historical classification systems evolved as understanding of disease pathophysiology improved, methods of imaging and assessing DR changed, and effective treatments were developed. Current DR classification systems are effective, and have been the basis of major research trials and clinical management guidelines for decades. However, with further new developments such as recognition of diabetic retinal neurodegeneration, new imaging platforms such as optical coherence tomography and ultra wide-field retinal imaging, artificial intelligence and new treatments, our current classification systems have significant limitations that need to be addressed. In this paper, we provide a historical review of different classification systems for DR, and discuss the limitations of our current classification systems in the context of new developments. We also review the implications of new developments in the field, to see how they might feature in a future, updated classification.}
}

@article{yangClassificationDiabeticRetinopathy2022a,
  title = {Classification of Diabetic Retinopathy: {{Past}}, Present and Future},
  shorttitle = {Classification of Diabetic Retinopathy},
  author = {Yang, Zhengwei and Tan, Tien-En and Shao, Yan and Wong, Tien Yin and Li, Xiaorong},
  year = {2022},
  journal = {Frontiers in Endocrinology},
  volume = {13},
  issn = {1664-2392},
  urldate = {2023-08-25},
  abstract = {Diabetic retinopathy (DR) is a leading cause of visual impairment and blindness worldwide. Since DR was first recognized as an important complication of diabetes, there have been many attempts to accurately classify the severity and stages of disease. These historical classification systems evolved as understanding of disease pathophysiology improved, methods of imaging and assessing DR changed, and effective treatments were developed. Current DR classification systems are effective, and have been the basis of major research trials and clinical management guidelines for decades. However, with further new developments such as recognition of diabetic retinal neurodegeneration, new imaging platforms such as optical coherence tomography and ultra wide-field retinal imaging, artificial intelligence and new treatments, our current classification systems have significant limitations that need to be addressed. In this paper, we provide a historical review of different classification systems for DR, and discuss the limitations of our current classification systems in the context of new developments. We also review the implications of new developments in the field, to see how they might feature in a future, updated classification.},
  file = {C:\Users\cleme\Zotero\storage\XES337GW\Yang et al. - 2022 - Classification of diabetic retinopathy Past, pres.pdf}
}

@inproceedings{yangFundusDiseaseImage2021,
  title = {Fundus {{Disease Image Classification}} Based on {{Improved Transformer}}},
  booktitle = {2021 {{International Conference}} on {{Neuromorphic Computing}} ({{ICNC}})},
  author = {Yang, Honggang and Chen, Jiejie and Xu, Mengfei},
  year = {2021},
  month = oct,
  pages = {207--214},
  doi = {10.1109/ICNC52316.2021.9608181},
  abstract = {In recent years, the progress of deep learning and fundus camera technology makes it possible to diagnose fundus diseases by computer. However, the fundus image dataset is relatively small, which makes the pure transformer model challenging to be applied to medical disease analysis. Therefore, this paper proposes a Transformer Eye (TransEye) fine-grained fundus disease image classification method based on the self-attention mechanism to assist diagnosis. TransEye combines the advantages of Convolution Neural Network (CNN) and Transformer model. It can not only effectively extract the underlying features, but also establish the remote dependence of the image. So, it can locate the most discriminative image area and complete end-to-end training. Evaluated the classification effect of our method on the preprocessed OIA dataset, the experimental results show the superiority of TransEye.},
  keywords = {Convolutional neural networks,deep learning,Feature extraction,fundus images,Neural networks,Neuromorphic engineering,OIA dataset,Semantics,Training,transformer,Transformers}
}

@inproceedings{yangFundusDiseaseImage2021a,
  title = {Fundus {{Disease Image Classification}} Based on {{Improved Transformer}}},
  booktitle = {2021 {{International Conference}} on {{Neuromorphic Computing}} ({{ICNC}})},
  author = {Yang, Honggang and Chen, Jiejie and Xu, Mengfei},
  year = {2021},
  month = oct,
  pages = {207--214},
  doi = {10.1109/ICNC52316.2021.9608181},
  abstract = {In recent years, the progress of deep learning and fundus camera technology makes it possible to diagnose fundus diseases by computer. However, the fundus image dataset is relatively small, which makes the pure transformer model challenging to be applied to medical disease analysis. Therefore, this paper proposes a Transformer Eye (TransEye) fine-grained fundus disease image classification method based on the self-attention mechanism to assist diagnosis. TransEye combines the advantages of Convolution Neural Network (CNN) and Transformer model. It can not only effectively extract the underlying features, but also establish the remote dependence of the image. So, it can locate the most discriminative image area and complete end-to-end training. Evaluated the classification effect of our method on the preprocessed OIA dataset, the experimental results show the superiority of TransEye.},
  keywords = {Convolutional neural networks,deep learning,Feature extraction,fundus images,Neural networks,Neuromorphic engineering,OIA dataset,Semantics,Training,transformer,Transformers},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\DPD6VXS9\\Yang et al. - 2021 - Fundus Disease Image Classification based on Impro.pdf;C\:\\Users\\cleme\\Zotero\\storage\\R7MD58V9\\9608181.html}
}

@inproceedings{yangLearningTextureTransformer2020,
  title = {Learning {{Texture Transformer Network}} for {{Image Super-Resolution}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Fuzhi and Yang, Huan and Fu, Jianlong and Lu, Hongtao and Guo, Baining},
  year = {2020},
  month = jun,
  pages = {5790--5799},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00583},
  urldate = {2021-11-05},
  abstract = {We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images. However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases. In this paper, we propose a novel Texture Transformer Network for Image Super-Resolution (TTSR), in which the LR and Ref images are formulated as queries and keys in a transformer, respectively. TTSR consists of four closely-related modules optimized for image generation tasks, including a learnable texture extractor by DNN, a relevance embedding module, a hard-attention module for texture transfer, and a softattention module for texture synthesis. Such a design encourages joint feature learning across LR and Ref images, in which deep feature correspondences can be discovered by attention, and thus accurate texture features can be transferred. The proposed texture transformer can be further stacked in a cross-scale way, which enables texture recovery from different levels (e.g., from 1{\texttimes} to 4{\texttimes} magnification). Extensive experiments show that TTSR achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@inproceedings{yangLearningTextureTransformer2020a,
  title = {Learning {{Texture Transformer Network}} for {{Image Super-Resolution}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Fuzhi and Yang, Huan and Fu, Jianlong and Lu, Hongtao and Guo, Baining},
  year = {2020},
  month = jun,
  pages = {5790--5799},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00583},
  urldate = {2021-11-05},
  abstract = {We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images. However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases. In this paper, we propose a novel Texture Transformer Network for Image Super-Resolution (TTSR), in which the LR and Ref images are formulated as queries and keys in a transformer, respectively. TTSR consists of four closely-related modules optimized for image generation tasks, including a learnable texture extractor by DNN, a relevance embedding module, a hard-attention module for texture transfer, and a softattention module for texture synthesis. Such a design encourages joint feature learning across LR and Ref images, in which deep feature correspondences can be discovered by attention, and thus accurate texture features can be transferred. The proposed texture transformer can be further stacked in a cross-scale way, which enables texture recovery from different levels (e.g., from 1{\texttimes} to 4{\texttimes} magnification). Extensive experiments show that TTSR achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\WI8DX4IA\Yang et al. - 2020 - Learning Texture Transformer Network for Image Sup.pdf}
}

@inproceedings{yanLearningMutuallyLocalGlobal2019,
  title = {Learning {{Mutually Local-Global U-Nets For High-Resolution Retinal Lesion Segmentation In Fundus Images}}},
  booktitle = {2019 {{IEEE}} 16th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2019)},
  author = {Yan, Zizheng and Han, Xiaoguang and Wang, Changmiao and Qiu, Yuda and Xiong, Zixiang and Cui, Shuguang},
  year = {2019},
  month = apr,
  pages = {597--600},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2019.8759579},
  abstract = {Diabetic retinopathy is the most important complication of diabetes. Early diagnosis of retinal lesions helps to avoid visual loss or blindness. Due to high-resolution and small-size lesion regions, applying existing methods, such as U-Nets, to perform segmentation on fundus photography is very challenging. Although downsampling the input images could simplify the problem, it loses detailed information. Conducting patch-level analysis helps reaching fine-scale segmentation yet usually leads to misunderstanding as the lack of context information. In this paper, we propose an efficient network that combines them together, not only being aware of local details but also taking fully use of the context perceptions. This is implemented by integrating the decoder parts of a global-level U-net and a patch-level one. The two streams are jointly optimized, ensuring that they are enhanced mutually. Experimental results demonstrate our new framework significantly outperforms existing patch-based and global-based methods, especially when the lesion regions are scattered and small-scaled.},
  keywords = {Decoding,deep learning,Diabetes,diabetic retinopathy,Image segmentation,lesion segmentation,Lesions,local-global U-nets,Retina,Retinopathy,Training}
}

@inproceedings{yanLearningMutuallyLocalGlobal2019a,
  title = {Learning {{Mutually Local-Global U-Nets For High-Resolution Retinal Lesion Segmentation In Fundus Images}}},
  booktitle = {2019 {{IEEE}} 16th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2019)},
  author = {Yan, Zizheng and Han, Xiaoguang and Wang, Changmiao and Qiu, Yuda and Xiong, Zixiang and Cui, Shuguang},
  year = {2019},
  month = apr,
  pages = {597--600},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2019.8759579},
  abstract = {Diabetic retinopathy is the most important complication of diabetes. Early diagnosis of retinal lesions helps to avoid visual loss or blindness. Due to high-resolution and small-size lesion regions, applying existing methods, such as U-Nets, to perform segmentation on fundus photography is very challenging. Although downsampling the input images could simplify the problem, it loses detailed information. Conducting patch-level analysis helps reaching fine-scale segmentation yet usually leads to misunderstanding as the lack of context information. In this paper, we propose an efficient network that combines them together, not only being aware of local details but also taking fully use of the context perceptions. This is implemented by integrating the decoder parts of a global-level U-net and a patch-level one. The two streams are jointly optimized, ensuring that they are enhanced mutually. Experimental results demonstrate our new framework significantly outperforms existing patch-based and global-based methods, especially when the lesion regions are scattered and small-scaled.},
  keywords = {Decoding,deep learning,Diabetes,diabetic retinopathy,Image segmentation,lesion segmentation,Lesions,local-global U-nets,Retina,Retinopathy,Training}
}

@inproceedings{yanLearningMutuallyLocalGlobal2019b,
  title = {Learning {{Mutually Local-Global U-Nets For High-Resolution Retinal Lesion Segmentation In Fundus Images}}},
  booktitle = {2019 {{IEEE}} 16th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2019)},
  author = {Yan, Zizheng and Han, Xiaoguang and Wang, Changmiao and Qiu, Yuda and Xiong, Zixiang and Cui, Shuguang},
  year = {2019},
  month = apr,
  pages = {597--600},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2019.8759579},
  abstract = {Diabetic retinopathy is the most important complication of diabetes. Early diagnosis of retinal lesions helps to avoid visual loss or blindness. Due to high-resolution and small-size lesion regions, applying existing methods, such as U-Nets, to perform segmentation on fundus photography is very challenging. Although downsampling the input images could simplify the problem, it loses detailed information. Conducting patch-level analysis helps reaching fine-scale segmentation yet usually leads to misunderstanding as the lack of context information. In this paper, we propose an efficient network that combines them together, not only being aware of local details but also taking fully use of the context perceptions. This is implemented by integrating the decoder parts of a global-level U-net and a patch-level one. The two streams are jointly optimized, ensuring that they are enhanced mutually. Experimental results demonstrate our new framework significantly outperforms existing patch-based and global-based methods, especially when the lesion regions are scattered and small-scaled.},
  keywords = {Decoding,deep learning,Diabetes,diabetic retinopathy,Image segmentation,lesion segmentation,Lesions,local-global U-nets,Retina,Retinopathy,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\H6TT9V46\\Yan et al. - 2019 - Learning Mutually Local-Global U-Nets For High-Res.pdf;C\:\\Users\\cleme\\Zotero\\storage\\XPZCFTZJ\\8759579.html}
}

@inproceedings{yanLearningMutuallyLocalGlobal2019c,
  title = {Learning {{Mutually Local-Global U-Nets For High-Resolution Retinal Lesion Segmentation In Fundus Images}}},
  booktitle = {2019 {{IEEE}} 16th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2019)},
  author = {Yan, Zizheng and Han, Xiaoguang and Wang, Changmiao and Qiu, Yuda and Xiong, Zixiang and Cui, Shuguang},
  year = {2019},
  month = apr,
  pages = {597--600},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2019.8759579},
  abstract = {Diabetic retinopathy is the most important complication of diabetes. Early diagnosis of retinal lesions helps to avoid visual loss or blindness. Due to high-resolution and small-size lesion regions, applying existing methods, such as U-Nets, to perform segmentation on fundus photography is very challenging. Although downsampling the input images could simplify the problem, it loses detailed information. Conducting patch-level analysis helps reaching fine-scale segmentation yet usually leads to misunderstanding as the lack of context information. In this paper, we propose an efficient network that combines them together, not only being aware of local details but also taking fully use of the context perceptions. This is implemented by integrating the decoder parts of a global-level U-net and a patch-level one. The two streams are jointly optimized, ensuring that they are enhanced mutually. Experimental results demonstrate our new framework significantly outperforms existing patch-based and global-based methods, especially when the lesion regions are scattered and small-scaled.},
  keywords = {Decoding,deep learning,Diabetes,diabetic retinopathy,Image segmentation,lesion segmentation,Lesions,local-global U-nets,Retina,Retinopathy,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\8K7EIYDU\\Yan et al. - 2019 - Learning Mutually Local-Global U-Nets For High-Res.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RPZT3E7Q\\10.1109@ISBI.2019.8759579.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\PQWE9Q9C\\8759579.html}
}

@article{yaqoobSpectralDomainOptical2005,
  title = {Spectral Domain Optical Coherence Tomography: A Better {{OCT}} Imaging Strategy},
  shorttitle = {Spectral Domain Optical Coherence Tomography},
  author = {Yaqoob, Zahid and Wu, Jigang and Yang, Changhuei},
  year = {2005},
  month = dec,
  journal = {BioTechniques},
  volume = {39},
  number = {6S},
  pages = {S6-S13},
  issn = {0736-6205},
  doi = {10.2144/000112090},
  urldate = {2019-11-14},
  abstract = {This paper reviews the current state of research in spectral domain optical coherence tomography (SDOCT). SDOCT is an interferometric technique that provides depth-resolved tissue structure information encoded in the magnitude and delay of the back-scattered light by spectral analysis of the interference fringe pattern. There are two approaches to SDOCT---one that uses a broadband source and a spectrometer to measure the interference pattern as a function of wavelength and the other that utilizes a narrowband tunable laser that is swept linearly in k {$\sim$} 1/{$\lambda$} space during spectral fringe data acquisition. Unlike time domain (TD) OCT, the reference arm is stationary in both SDOCT methods, which allows for ultra high-speed OCT imaging. Owing to its high speed and superior sensitivity, SDOCT has become indispensable in biomedical imaging applications. After a brief introduction and a discussion on sensitivity advantage, methods of implementation of the two SDOCT schemes will be presented. The two peer approaches are compared in speed, scan depth range, complexity, spectral regions of operation, and methods of detection. The review also discusses OCT enhancements and functional methods based on SDOCT format and concludes with possible directions that this research may take in the near future.}
}

@article{yaqoobSpectralDomainOptical2005a,
  title = {Spectral Domain Optical Coherence Tomography: A Better {{OCT}} Imaging Strategy},
  shorttitle = {Spectral Domain Optical Coherence Tomography},
  author = {Yaqoob, Zahid and Wu, Jigang and Yang, Changhuei},
  year = {2005},
  month = dec,
  journal = {BioTechniques},
  volume = {39},
  number = {6S},
  pages = {S6-S13},
  issn = {0736-6205},
  doi = {10.2144/000112090},
  urldate = {2019-11-14},
  abstract = {This paper reviews the current state of research in spectral domain optical coherence tomography (SDOCT). SDOCT is an interferometric technique that provides depth-resolved tissue structure information encoded in the magnitude and delay of the back-scattered light by spectral analysis of the interference fringe pattern. There are two approaches to SDOCT---one that uses a broadband source and a spectrometer to measure the interference pattern as a function of wavelength and the other that utilizes a narrowband tunable laser that is swept linearly in k {$\sim$} 1/{$\lambda$} space during spectral fringe data acquisition. Unlike time domain (TD) OCT, the reference arm is stationary in both SDOCT methods, which allows for ultra high-speed OCT imaging. Owing to its high speed and superior sensitivity, SDOCT has become indispensable in biomedical imaging applications. After a brief introduction and a discussion on sensitivity advantage, methods of implementation of the two SDOCT schemes will be presented. The two peer approaches are compared in speed, scan depth range, complexity, spectral regions of operation, and methods of detection. The review also discusses OCT enhancements and functional methods based on SDOCT format and concludes with possible directions that this research may take in the near future.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\7MEMMHHE\\Yaqoob et al. - 2005 - Spectral domain optical coherence tomography a be.pdf;C\:\\Users\\cleme\\Zotero\\storage\\3PQA7QT2\\000112090.html}
}

@article{yiLearningPixelLevelLabel2022,
  title = {Learning {{From Pixel-Level Label Noise}}: {{A New Perspective}} for {{Semi-Supervised Semantic Segmentation}}},
  shorttitle = {Learning {{From Pixel-Level Label Noise}}},
  author = {Yi, Rumeng and Huang, Yaping and Guan, Qingji and Pu, Mengyang and Zhang, Runsheng},
  year = {2022},
  journal = {IEEE Transactions on Image Processing},
  volume = {31},
  pages = {623--635},
  issn = {1941-0042},
  doi = {10.1109/TIP.2021.3134142},
  abstract = {This paper addresses semi-supervised semantic segmentation by exploiting a small set of images with pixel-level annotations (strong supervisions) and a large set of images with only image-level annotations (weak supervisions). Most existing approaches aim to generate accurate pixel-level labels from weak supervisions. However, we observe that those generated labels still inevitably contain noisy labels. Motivated by this observation, we present a novel perspective and formulate this task as a problem of learning with pixel-level label noise. Existing noisy label methods, nevertheless, mainly aim at image-level tasks, which can not capture the relationship between neighboring labels in one image. Therefore, we propose a graph-based label noise detection and correction framework to deal with pixel-level noisy labels. In particular, for the generated pixel-level noisy labels from weak supervisions by Class Activation Map (CAM), we train a clean segmentation model with strong supervisions to detect the clean labels from these noisy labels according to the cross-entropy loss. Then, we adopt a superpixel-based graph to represent the relations of spatial adjacency and semantic similarity between pixels in one image. Finally we correct the noisy labels using a Graph Attention Network (GAT) supervised by detected clean labels. We comprehensively conduct experiments on PASCAL VOC 2012, PASCAL-Context, MS-COCO and Cityscapes datasets. The experimental results show that our proposed semi-supervised method achieves the state-of-the-art performances and even outperforms the fully-supervised models on PASCAL VOC 2012 and MS-COCO datasets in some cases.},
  keywords = {Annotations,graph neural network,Image segmentation,label noise,Noise measurement,Predictive models,Semantics,Semi-supervised semantic segmentation,Task analysis,Training}
}

@article{yiLearningPixelLevelLabel2022a,
  title = {Learning {{From Pixel-Level Label Noise}}: {{A New Perspective}} for {{Semi-Supervised Semantic Segmentation}}},
  shorttitle = {Learning {{From Pixel-Level Label Noise}}},
  author = {Yi, Rumeng and Huang, Yaping and Guan, Qingji and Pu, Mengyang and Zhang, Runsheng},
  year = {2022},
  journal = {IEEE Transactions on Image Processing},
  volume = {31},
  pages = {623--635},
  issn = {1941-0042},
  doi = {10.1109/TIP.2021.3134142},
  abstract = {This paper addresses semi-supervised semantic segmentation by exploiting a small set of images with pixel-level annotations (strong supervisions) and a large set of images with only image-level annotations (weak supervisions). Most existing approaches aim to generate accurate pixel-level labels from weak supervisions. However, we observe that those generated labels still inevitably contain noisy labels. Motivated by this observation, we present a novel perspective and formulate this task as a problem of learning with pixel-level label noise. Existing noisy label methods, nevertheless, mainly aim at image-level tasks, which can not capture the relationship between neighboring labels in one image. Therefore, we propose a graph-based label noise detection and correction framework to deal with pixel-level noisy labels. In particular, for the generated pixel-level noisy labels from weak supervisions by Class Activation Map (CAM), we train a clean segmentation model with strong supervisions to detect the clean labels from these noisy labels according to the cross-entropy loss. Then, we adopt a superpixel-based graph to represent the relations of spatial adjacency and semantic similarity between pixels in one image. Finally we correct the noisy labels using a Graph Attention Network (GAT) supervised by detected clean labels. We comprehensively conduct experiments on PASCAL VOC 2012, PASCAL-Context, MS-COCO and Cityscapes datasets. The experimental results show that our proposed semi-supervised method achieves the state-of-the-art performances and even outperforms the fully-supervised models on PASCAL VOC 2012 and MS-COCO datasets in some cases.},
  keywords = {Annotations,graph neural network,Image segmentation,label noise,Noise measurement,Predictive models,Semantics,Semi-supervised semantic segmentation,Task analysis,Training},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\QCRXVAUS\\Yi et al. - 2022 - Learning From Pixel-Level Label Noise A New Persp.pdf;C\:\\Users\\cleme\\Zotero\\storage\\6T38XAT6\\9652059.html}
}

@article{yimPredictingConversionWet2020,
  title = {Predicting Conversion to Wet Age-Related Macular Degeneration Using Deep Learning},
  author = {Yim, Jason and Chopra, Reena and Spitz, Terry and Winkens, Jim and Obika, Annette and Kelly, Christopher and Askham, Harry and Lukic, Marko and Huemer, Josef and Fasler, Katrin and Moraes, Gabriella and Meyer, Clemens and Wilson, Marc and Dixon, Jonathan and Hughes, Cian and Rees, Geraint and Khaw, Peng T. and Karthikesalingam, Alan and King, Dominic and Hassabis, Demis and Suleyman, Mustafa and Back, Trevor and Ledsam, Joseph R. and Keane, Pearse A. and De Fauw, Jeffrey},
  year = {2020},
  month = jun,
  journal = {Nature Medicine},
  volume = {26},
  number = {6},
  pages = {892--899},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-020-0867-7},
  urldate = {2023-06-20},
  langid = {english}
}

@article{yimPredictingConversionWet2020a,
  title = {Predicting Conversion to Wet Age-Related Macular Degeneration Using Deep Learning},
  author = {Yim, Jason and Chopra, Reena and Spitz, Terry and Winkens, Jim and Obika, Annette and Kelly, Christopher and Askham, Harry and Lukic, Marko and Huemer, Josef and Fasler, Katrin and Moraes, Gabriella and Meyer, Clemens and Wilson, Marc and Dixon, Jonathan and Hughes, Cian and Rees, Geraint and Khaw, Peng T. and Karthikesalingam, Alan and King, Dominic and Hassabis, Demis and Suleyman, Mustafa and Back, Trevor and Ledsam, Joseph R. and Keane, Pearse A. and De Fauw, Jeffrey},
  year = {2020},
  month = jun,
  journal = {Nature Medicine},
  volume = {26},
  number = {6},
  pages = {892--899},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-020-0867-7},
  urldate = {2023-06-20},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\AK4GA4Q7\Yim et al. - 2020 - Predicting conversion to wet age-related macular d.pdf}
}

@article{yingGNNExplainerGeneratingExplanations2019,
  title = {{{GNNExplainer}}: {{Generating Explanations}} for {{Graph Neural Networks}}},
  shorttitle = {{{GNNExplainer}}},
  author = {Ying, Rex and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
  year = {2019},
  month = dec,
  journal = {Advances in neural information processing systems},
  volume = {32},
  pages = {9240--9251},
  issn = {1049-5258},
  urldate = {2023-10-03},
  abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GnnExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GnnExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GnnExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GnnExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0\% in explanation accuracy. GnnExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.},
  pmcid = {PMC7138248},
  pmid = {32265580}
}

@article{yingGNNExplainerGeneratingExplanations2019a,
  title = {{{GNNExplainer}}: {{Generating Explanations}} for {{Graph Neural Networks}}},
  shorttitle = {{{GNNExplainer}}},
  author = {Ying, Rex and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
  year = {2019},
  month = dec,
  journal = {Advances in neural information processing systems},
  volume = {32},
  pages = {9240--9251},
  issn = {1049-5258},
  urldate = {2023-10-03},
  abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GnnExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GnnExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GnnExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GnnExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0\% in explanation accuracy. GnnExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.},
  pmcid = {PMC7138248},
  pmid = {32265580},
  file = {C:\Users\cleme\Zotero\storage\TJLPW8NP\Ying et al. - 2019 - GNNExplainer Generating Explanations for Graph Ne.pdf}
}

@article{yooFeasibilityStudyImprove2021,
  title = {Feasibility Study to Improve Deep Learning in {{OCT}} Diagnosis of Rare Retinal Diseases with Few-Shot Classification},
  author = {Yoo, Tae Keun and Choi, Joon Yul and Kim, Hong Kyu},
  year = {2021},
  month = feb,
  journal = {Medical \& Biological Engineering \& Computing},
  volume = {59},
  number = {2},
  pages = {401--415},
  issn = {1741-0444},
  doi = {10.1007/s11517-021-02321-1},
  urldate = {2022-07-08},
  abstract = {Deep learning (DL) has been successfully applied to the diagnosis of ophthalmic diseases. However, rare diseases are commonly neglected due to insufficient data. Here, we demonstrate that few-shot learning (FSL) using a generative adversarial network (GAN) can improve the applicability of DL in the optical coherence tomography (OCT) diagnosis of rare diseases. Four major classes with a large number of datasets and five rare disease classes with a few-shot dataset are included in this study. Before training the classifier, we constructed GAN models to generate pathological OCT images of each rare disease from normal OCT images. The Inception-v3 architecture was trained using an augmented training dataset, and the final model was validated using an independent test dataset. The synthetic images helped in the extraction of the characteristic features of each rare disease. The proposed DL model demonstrated a significant improvement in the accuracy of the OCT diagnosis of rare retinal diseases and outperformed the traditional DL models, Siamese network, and prototypical network. By increasing the accuracy of diagnosing rare retinal diseases through FSL, clinicians can avoid neglecting rare diseases with DL assistance, thereby reducing diagnosis delay and patient burden.},
  langid = {english},
  keywords = {Deep learning,Few-shot learning,Generative adversarial network,Optical coherence tomography,Rare diseases}
}

@article{yooFeasibilityStudyImprove2021a,
  title = {Feasibility Study to Improve Deep Learning in {{OCT}} Diagnosis of Rare Retinal Diseases with Few-Shot Classification},
  author = {Yoo, Tae Keun and Choi, Joon Yul and Kim, Hong Kyu},
  year = {2021},
  month = feb,
  journal = {Medical \& Biological Engineering \& Computing},
  volume = {59},
  number = {2},
  pages = {401--415},
  issn = {1741-0444},
  doi = {10.1007/s11517-021-02321-1},
  urldate = {2022-07-08},
  abstract = {Deep learning (DL) has been successfully applied to the diagnosis of ophthalmic diseases. However, rare diseases are commonly neglected due to insufficient data. Here, we demonstrate that few-shot learning (FSL) using a generative adversarial network (GAN) can improve the applicability of DL in the optical coherence tomography (OCT) diagnosis of rare diseases. Four major classes with a large number of datasets and five rare disease classes with a few-shot dataset are included in this study. Before training the classifier, we constructed GAN models to generate pathological OCT images of each rare disease from normal OCT images. The Inception-v3 architecture was trained using an augmented training dataset, and the final model was validated using an independent test dataset. The synthetic images helped in the extraction of the characteristic features of each rare disease. The proposed DL model demonstrated a significant improvement in the accuracy of the OCT diagnosis of rare retinal diseases and outperformed the traditional DL models, Siamese network, and prototypical network. By increasing the accuracy of diagnosing rare retinal diseases through FSL, clinicians can avoid neglecting rare diseases with DL assistance, thereby reducing diagnosis delay and patient burden.},
  langid = {english},
  keywords = {Deep learning,Few-shot learning,Generative adversarial network,Optical coherence tomography,Rare diseases},
  file = {C:\Users\cleme\Zotero\storage\582GR2RV\Yoo et al. - 2021 - Feasibility study to improve deep learning in OCT .pdf}
}

@inproceedings{Yuan_2021_ICCV,
  title = {Tokens-to-Token {{ViT}}: {{Training}} Vision Transformers from Scratch on {{ImageNet}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis E.H. and Feng, Jiashi and Yan, Shuicheng},
  year = {2021},
  month = oct,
  pages = {558--567}
}

@inproceedings{Yuan_2021_ICCV,
  title = {Tokens-to-Token {{ViT}}: {{Training}} Vision Transformers from Scratch on {{ImageNet}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis E.H. and Feng, Jiashi and Yan, Shuicheng},
  year = {2021},
  month = oct,
  pages = {558--567}
}

@inproceedings{Yuan_2021_ICCV,
  title = {Tokens-to-Token {{ViT}}: {{Training}} Vision Transformers from Scratch on {{ImageNet}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis E.H. and Feng, Jiashi and Yan, Shuicheng},
  year = {2021},
  month = oct,
  pages = {558--567}
}

@inproceedings{Yuan_2021_ICCV,
  title = {Tokens-to-Token {{ViT}}: {{Training}} Vision Transformers from Scratch on {{ImageNet}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis E.H. and Feng, Jiashi and Yan, Shuicheng},
  year = {2021},
  month = oct,
  pages = {558--567}
}

@article{yuanTokenstoTokenViTTraining2021,
  title = {Tokens-to-{{Token ViT}}: {{Training Vision Transformers}} from {{Scratch}} on {{ImageNet}}},
  shorttitle = {Tokens-to-{{Token ViT}}},
  author = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.11986 [cs]},
  eprint = {2101.11986},
  primaryclass = {cs},
  urldate = {2021-03-01},
  abstract = {Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformers (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance compared with CNNs when trained from scratch on a midsize dataset (e.g., ImageNet). We find it is because: 1) the simple tokenization of input images fails to model the important local structure (e.g., edges, lines) among neighboring pixels, leading to its low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness in fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformers (T2T-ViT), which introduces 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure presented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformers motivated by CNN architecture design after extensive study. Notably, T2T-ViT reduces the parameter counts and MACs of vanilla ViT by 200{\textbackslash}\%, while achieving more than 2.5{\textbackslash}\% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets when directly training on ImageNet. For example, T2T-ViT with ResNet50 comparable size can achieve 80.7{\textbackslash}\% top-1 accuracy on ImageNet. (Code: https://github.com/yitu-opensource/T2T-ViT)},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{yuanTokenstoTokenViTTraining2021a,
  title = {Tokens-to-{{Token ViT}}: {{Training Vision Transformers}} from {{Scratch}} on {{ImageNet}}},
  shorttitle = {Tokens-to-{{Token ViT}}},
  author = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.11986 [cs]},
  eprint = {2101.11986},
  primaryclass = {cs},
  urldate = {2021-03-01},
  abstract = {Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformers (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance compared with CNNs when trained from scratch on a midsize dataset (e.g., ImageNet). We find it is because: 1) the simple tokenization of input images fails to model the important local structure (e.g., edges, lines) among neighboring pixels, leading to its low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness in fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformers (T2T-ViT), which introduces 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure presented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformers motivated by CNN architecture design after extensive study. Notably, T2T-ViT reduces the parameter counts and MACs of vanilla ViT by 200{\textbackslash}\%, while achieving more than 2.5{\textbackslash}\% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets when directly training on ImageNet. For example, T2T-ViT with ResNet50 comparable size can achieve 80.7{\textbackslash}\% top-1 accuracy on ImageNet. (Code: https://github.com/yitu-opensource/T2T-ViT)},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\23X46G87\\Yuan et al. - 2021 - Tokens-to-Token ViT Training Vision Transformers .pdf;C\:\\Users\\cleme\\Zotero\\storage\\6BN3AX2R\\2101.html}
}

@article{yueVisionTransformerProgressive2021,
  title = {Vision {{Transformer With Progressive Sampling}}},
  author = {Yue, Xiaoyu and Sun, Shuyang and Kuang, Zhanghui and Wei, Meng and Torr, Philip H S and Zhang, Wayne and Lin, Dahua},
  year = {2021},
  journal = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages = {387--396},
  abstract = {Transformers with powerful global relation modeling abilities have been introduced to fundamental computer vision tasks recently. As a typical example, the Vision Transformer (ViT) directly applies a pure transformer architecture on image classification, by simply splitting images into tokens with a fixed length, and employing transformers to learn relations between these tokens. However, such naive tokenization could destruct object structures, assign grids to uninterested regions such as background, and introduce interference signals. To mitigate the above issues, in this paper, we propose an iterative and progressive sampling strategy to locate discriminative regions. At each iteration, embeddings of the current sampling step are fed into a transformer encoder layer, and a group of sampling offsets is predicted to update the sampling locations for the next step. The progressive sampling is differentiable. When combined with the Vision Transformer, the obtained PS-ViT network can adaptively learn where to look. The proposed PS-ViT is both effective and efficient. When trained from scratch on ImageNet, PS-ViT performs 3.8\% higher than the vanilla ViT in terms of top-1 accuracy with about 4{\texttimes} fewer parameters and 10{\texttimes} fewer FLOPs. Code is available at https://github.com/yuexy/PS-ViT.},
  langid = {english}
}

@article{yueVisionTransformerProgressive2021a,
  title = {Vision {{Transformer With Progressive Sampling}}},
  author = {Yue, Xiaoyu and Sun, Shuyang and Kuang, Zhanghui and Wei, Meng and Torr, Philip H S and Zhang, Wayne and Lin, Dahua},
  year = {2021},
  journal = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages = {387--396},
  abstract = {Transformers with powerful global relation modeling abilities have been introduced to fundamental computer vision tasks recently. As a typical example, the Vision Transformer (ViT) directly applies a pure transformer architecture on image classification, by simply splitting images into tokens with a fixed length, and employing transformers to learn relations between these tokens. However, such naive tokenization could destruct object structures, assign grids to uninterested regions such as background, and introduce interference signals. To mitigate the above issues, in this paper, we propose an iterative and progressive sampling strategy to locate discriminative regions. At each iteration, embeddings of the current sampling step are fed into a transformer encoder layer, and a group of sampling offsets is predicted to update the sampling locations for the next step. The progressive sampling is differentiable. When combined with the Vision Transformer, the obtained PS-ViT network can adaptively learn where to look. The proposed PS-ViT is both effective and efficient. When trained from scratch on ImageNet, PS-ViT performs 3.8\% higher than the vanilla ViT in terms of top-1 accuracy with about 4{\texttimes} fewer parameters and 10{\texttimes} fewer FLOPs. Code is available at https://github.com/yuexy/PS-ViT.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\GN9P4GGQ\Yue et al. - Vision Transformer With Progressive Sampling.pdf}
}

@inproceedings{yuMILVTMultipleInstance2021,
  title = {{{MIL-VT}}: {{Multiple Instance Learning Enhanced Vision Transformer}} for {{Fundus Image Classification}}},
  shorttitle = {{{MIL-VT}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Yu, Shuang and Ma, Kai and Bi, Qi and Bian, Cheng and Ning, Munan and He, Nanjun and Li, Yuexiang and Liu, Hanruo and Zheng, Yefeng},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {45--54},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87237-3_5},
  abstract = {With the advancement and prevailing success of Transformer models in the natural language processing (NLP) field, an increasing number of research works have explored the applicability of Transformer for various vision tasks and reported superior performance compared with convolutional neural networks (CNNs). However, as the proper training of Transformer generally requires an extremely large quantity of data, it has rarely been explored for the medical imaging tasks. In this paper, we attempt to adopt the Vision Transformer for the retinal disease classification tasks, by pre-training the Transformer model on a large fundus image database and then fine-tuning on downstream retinal disease classification tasks. In addition, to fully exploit the feature representations extracted by individual image patches, we propose a multiple instance learning (MIL) based `MIL head', which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. The proposed MIL-VT framework achieves superior performance over CNN models on two publicly available datasets when being trained and tested under the same setup. The implementation code and pre-trained weights are released for public access (Code link: https://github.com/greentreeys/MIL-VT).},
  isbn = {978-3-030-87237-3},
  langid = {english},
  keywords = {Deep learning,Fundus image,Multiple instance learning,Vision Transformer}
}

@inproceedings{yuMILVTMultipleInstance2021a,
  title = {{{MIL-VT}}: {{Multiple Instance Learning Enhanced Vision Transformer}} for {{Fundus Image Classification}}},
  shorttitle = {{{MIL-VT}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Yu, Shuang and Ma, Kai and Bi, Qi and Bian, Cheng and Ning, Munan and He, Nanjun and Li, Yuexiang and Liu, Hanruo and Zheng, Yefeng},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {45--54},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87237-3_5},
  abstract = {With the advancement and prevailing success of Transformer models in the natural language processing (NLP) field, an increasing number of research works have explored the applicability of Transformer for various vision tasks and reported superior performance compared with convolutional neural networks (CNNs). However, as the proper training of Transformer generally requires an extremely large quantity of data, it has rarely been explored for the medical imaging tasks. In this paper, we attempt to adopt the Vision Transformer for the retinal disease classification tasks, by pre-training the Transformer model on a large fundus image database and then fine-tuning on downstream retinal disease classification tasks. In addition, to fully exploit the feature representations extracted by individual image patches, we propose a multiple instance learning (MIL) based `MIL head', which can be conveniently attached to the Vision Transformer in a plug-and-play manner and effectively enhances the model performance for the downstream fundus image classification tasks. The proposed MIL-VT framework achieves superior performance over CNN models on two publicly available datasets when being trained and tested under the same setup. The implementation code and pre-trained weights are released for public access (Code link: https://github.com/greentreeys/MIL-VT).},
  isbn = {978-3-030-87237-3},
  langid = {english},
  keywords = {Deep learning,Fundus image,Multiple instance learning,Vision Transformer}
}

@article{yungClinicalApplicationsFundus2016,
  title = {Clinical Applications of Fundus Autofluorescence in Retinal Disease},
  author = {Yung, Madeline and Klufas, Michael A. and Sarraf, David},
  year = {2016},
  month = apr,
  journal = {International Journal of Retina and Vitreous},
  volume = {2},
  number = {1},
  pages = {12},
  issn = {2056-9920},
  doi = {10.1186/s40942-016-0035-x},
  urldate = {2019-11-27},
  abstract = {Fundus autofluorescence (FAF) is a non-invasive retinal imaging modality used in clinical practice to provide a density map of lipofuscin, the predominant ocular fluorophore, in the retinal pigment epithelium. Multiple commercially available imaging systems, including the fundus camera, the confocal scanning laser ophthalmoscope, and the ultra-widefield imaging device, are available to the clinician. Each offers unique advantages for evaluating various retinal diseases. The clinical applications of FAF continue to expand. It is now an essential tool for evaluating age related macular degeneration, macular dystrophies, retinitis pigmentosa, white dot syndromes, retinal drug toxicities, and various other retinal disorders. FAF may detect abnormalities beyond those detected on funduscopic exam, fluorescein angiography, or optical coherence tomography, and can be used to elucidate disease pathogenesis, form genotype-phenotype correlations, diagnose and monitor disease, and evaluate novel therapies. Given its ease of use, non-invasive nature, and value in characterizing retinal disease, FAF enjoys increasing clinical relevance. This review summarizes common ocular fluorophores, imaging modalities, and FAF findings for a wide spectrum of retinal disorders.}
}

@article{yungClinicalApplicationsFundus2016a,
  title = {Clinical Applications of Fundus Autofluorescence in Retinal Disease},
  author = {Yung, Madeline and Klufas, Michael A. and Sarraf, David},
  year = {2016},
  month = apr,
  journal = {International Journal of Retina and Vitreous},
  volume = {2},
  number = {1},
  pages = {12},
  issn = {2056-9920},
  doi = {10.1186/s40942-016-0035-x},
  urldate = {2019-11-27},
  abstract = {Fundus autofluorescence (FAF) is a non-invasive retinal imaging modality used in clinical practice to provide a density map of lipofuscin, the predominant ocular fluorophore, in the retinal pigment epithelium. Multiple commercially available imaging systems, including the fundus camera, the confocal scanning laser ophthalmoscope, and the ultra-widefield imaging device, are available to the clinician. Each offers unique advantages for evaluating various retinal diseases. The clinical applications of FAF continue to expand. It is now an essential tool for evaluating age related macular degeneration, macular dystrophies, retinitis pigmentosa, white dot syndromes, retinal drug toxicities, and various other retinal disorders. FAF may detect abnormalities beyond those detected on funduscopic exam, fluorescein angiography, or optical coherence tomography, and can be used to elucidate disease pathogenesis, form genotype-phenotype correlations, diagnose and monitor disease, and evaluate novel therapies. Given its ease of use, non-invasive nature, and value in characterizing retinal disease, FAF enjoys increasing clinical relevance. This review summarizes common ocular fluorophores, imaging modalities, and FAF findings for a wide spectrum of retinal disorders.},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\LCBGPV7A\\yung2016.pdf;C\:\\Users\\cleme\\Zotero\\storage\\N7NKTAAD\\Yung et al. - 2016 - Clinical applications of fundus autofluorescence i.pdf;C\:\\Users\\cleme\\Zotero\\storage\\XZDSVG9F\\s40942-016-0035-x.html}
}

@inproceedings{zagoruykoWideResidualNetworks2016,
  title = {Wide {{Residual Networks}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2016},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2016},
  pages = {87.1-87.12},
  publisher = {British Machine Vision Association},
  address = {York, UK},
  doi = {10.5244/C.30.87},
  urldate = {2021-03-02},
  abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN. Our code is available at https://github.com/szagoruyko/wide-residual-networks.},
  isbn = {978-1-901725-59-9},
  langid = {english}
}

@inproceedings{zagoruykoWideResidualNetworks2016a,
  title = {Wide {{Residual Networks}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2016},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2016},
  pages = {87.1-87.12},
  publisher = {British Machine Vision Association},
  address = {York, UK},
  doi = {10.5244/C.30.87},
  urldate = {2021-03-02},
  abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN. Our code is available at https://github.com/szagoruyko/wide-residual-networks.},
  isbn = {978-1-901725-59-9},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\HBVEPFBQ\Zagoruyko et Komodakis - 2016 - Wide Residual Networks.pdf}
}

@article{zagoruykoWideResidualNetworks2017,
  title = {Wide {{Residual Networks}}},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2017},
  month = jun,
  journal = {arXiv:1605.07146 [cs]},
  eprint = {1605.07146},
  primaryclass = {cs},
  urldate = {2021-03-02},
  abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{zagoruykoWideResidualNetworks2017a,
  title = {Wide {{Residual Networks}}},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2017},
  month = jun,
  journal = {arXiv:1605.07146 [cs]},
  eprint = {1605.07146},
  primaryclass = {cs},
  urldate = {2021-03-02},
  abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\cleme\Zotero\storage\EBVAVSVT\Zagoruyko et Komodakis - 2017 - Wide Residual Networks.pdf}
}

@article{zanaMultimodalRegistrationAlgorithm1999,
  title = {A Multimodal Registration Algorithm of Eye Fundus Images Using Vessels Detection and {{Hough}} Transform},
  author = {Zana, F. and Klein, J.C.},
  year = {1999},
  month = may,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {18},
  number = {5},
  pages = {419--428},
  issn = {1558-254X},
  doi = {10.1109/42.774169},
  abstract = {Image registration is a real challenge because physicians handle many images. Temporal registration is necessary in order to follow the various steps of a disease, whereas multimodal registration allows us to improve the identification of some lesions or to compare pieces of information gathered from different sources. This paper presents an algorithm for temporal and/or multimodal registration of retinal images based on point correspondence. As an example, the algorithm has been applied to the registration of fluorescein images (obtained after a fluorescein dye injection) with green images (green filter of a color image). The vascular tree is first detected in each type of images and bifurcation points are labeled with surrounding vessel orientations. An angle-based invariant is then computed in order to give a probability for two points to match. Then a Bayesian Hough transform is used to sort the transformations with their respective likelihoods. A precise affine estimate is finally computed for most likely transformations. The best transformation is chosen for registration.},
  keywords = {Algorithm design and analysis,Algorithms,angle-based invariant,Bayes Theorem,Bayesian Hough transform,bifurcation points,Biomedical imaging,blood vessels,Computer-Assisted,Diabetes,Diabetic Retinopathy,disease steps,Diseases,eye,eye fundus images,Filters,Fluorescein Angiography,fluorescein dye injection,fluorescein images,Fundus Oculi,green filter,green images,Hough transforms,Humans,Image Processing,Image Processing Computer-Assisted,image registration,Image registration,Lesions,mathematical morphology,medical diagnostic imaging,Medical diagnostic imaging,medical image processing,multimodal registration algorithm,ophthalmology,photographic applications,precise affine estimate,Retina,Retinal Vessels,Retinopathy,surrounding vessel orientations,temporal registration,Time Factors,vascular tree,vessels detection},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\36CGB7SY\\zana1999.pdf;C\:\\Users\\cleme\\Zotero\\storage\\MQAP6SKG\\Zana et Klein - 1999 - A multimodal registration algorithm of eye fundus .pdf;C\:\\Users\\cleme\\Zotero\\storage\\WR23H26A\\774169.html}
}

@article{zangDcardNetDiabeticRetinopathy2021,
  title = {{{DcardNet}}: {{Diabetic Retinopathy Classification}} at {{Multiple Levels Based}} on {{Structural}} and {{Angiographic Optical Coherence Tomography}}},
  shorttitle = {{{DcardNet}}},
  author = {Zang, Pengxiao and Gao, Liqin and Hormel, Tristan T. and Wang, Jie and You, Qisheng and Hwang, Thomas S. and Jia, Yali},
  year = {2021},
  month = jun,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {68},
  number = {6},
  pages = {1859--1870},
  issn = {1558-2531},
  doi = {10.1109/TBME.2020.3027231},
  abstract = {Objective: Optical coherence tomography (OCT) and its angiography (OCTA) have several advantages for the early detection and diagnosis of diabetic retinopathy (DR). However, automated, complete DR classification frameworks based on both OCT and OCTA data have not been proposed. In this study, a convolutional neural network (CNN) based method is proposed to fulfill a DR classification framework using en face OCT and OCTA. Methods: A densely and continuously connected neural network with adaptive rate dropout (DcardNet) is designed for the DR classification. In addition, adaptive label smoothing was proposed and used to suppress overfitting. Three separate classification levels are generated for each case based on the International Clinical Diabetic Retinopathy scale. At the highest level the network classifies scans as referable or non-referable for DR. The second level classifies the eye as non-DR, non-proliferative DR (NPDR), or proliferative DR (PDR). The last level classifies the case as no DR, mild and moderate NPDR, severe NPDR, and PDR. Results: We used 10-fold cross-validation with 10\% of the data to assess the network's performance. The overall classification accuracies of the three levels were 95.7\%, 85.0\%, and 71.0\% respectively. Conclusion/Significance: A reliable, sensitive and specific automated classification framework for referral to an ophthalmologist can be a key technology for reducing vision loss related to DR.},
  keywords = {Diabetes,Eye,Faces,Feature extraction,image classification,neural networks,optical coherence tomography,Reflectivity,Retina,Retinopathy,Smoothing methods}
}

@article{zangDcardNetDiabeticRetinopathy2021a,
  title = {{{DcardNet}}: {{Diabetic Retinopathy Classification}} at {{Multiple Levels Based}} on {{Structural}} and {{Angiographic Optical Coherence Tomography}}},
  shorttitle = {{{DcardNet}}},
  author = {Zang, Pengxiao and Gao, Liqin and Hormel, Tristan T. and Wang, Jie and You, Qisheng and Hwang, Thomas S. and Jia, Yali},
  year = {2021},
  month = jun,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {68},
  number = {6},
  pages = {1859--1870},
  issn = {1558-2531},
  doi = {10.1109/TBME.2020.3027231},
  abstract = {Objective: Optical coherence tomography (OCT) and its angiography (OCTA) have several advantages for the early detection and diagnosis of diabetic retinopathy (DR). However, automated, complete DR classification frameworks based on both OCT and OCTA data have not been proposed. In this study, a convolutional neural network (CNN) based method is proposed to fulfill a DR classification framework using en face OCT and OCTA. Methods: A densely and continuously connected neural network with adaptive rate dropout (DcardNet) is designed for the DR classification. In addition, adaptive label smoothing was proposed and used to suppress overfitting. Three separate classification levels are generated for each case based on the International Clinical Diabetic Retinopathy scale. At the highest level the network classifies scans as referable or non-referable for DR. The second level classifies the eye as non-DR, non-proliferative DR (NPDR), or proliferative DR (PDR). The last level classifies the case as no DR, mild and moderate NPDR, severe NPDR, and PDR. Results: We used 10-fold cross-validation with 10\% of the data to assess the network's performance. The overall classification accuracies of the three levels were 95.7\%, 85.0\%, and 71.0\% respectively. Conclusion/Significance: A reliable, sensitive and specific automated classification framework for referral to an ophthalmologist can be a key technology for reducing vision loss related to DR.},
  keywords = {Diabetes,Eye,Faces,Feature extraction,image classification,neural networks,optical coherence tomography,Reflectivity,Retina,Retinopathy,Smoothing methods},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\9PQDVMES\\Zang et al. - 2021 - DcardNet Diabetic Retinopathy Classification at M.pdf;C\:\\Users\\cleme\\Zotero\\storage\\WB4CUXN3\\9207828.html}
}

@incollection{zeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8689},
  pages = {818--833},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10590-1_53},
  urldate = {2021-11-15},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  langid = {english}
}

@inproceedings{zeilerVisualizingUnderstandingConvolutional2014a,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {818--833},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10590-1_53},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10590-1},
  langid = {english},
  keywords = {Convolutional Neural Network,Input Image,Pixel Space,Stochastic Gradient Descent,Training Image}
}

@incollection{zeilerVisualizingUnderstandingConvolutional2014b,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8689},
  pages = {818--833},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10590-1_53},
  urldate = {2021-11-15},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\7CD3HQWQ\Zeiler et Fergus - 2014 - Visualizing and Understanding Convolutional Networ.pdf}
}

@inproceedings{zeilerVisualizingUnderstandingConvolutional2014c,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {818--833},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10590-1_53},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10590-1},
  langid = {english},
  keywords = {Convolutional Neural Network,Input Image,Pixel Space,Stochastic Gradient Descent,Training Image},
  file = {C:\Users\cleme\Zotero\storage\B38TFL8P\Zeiler et Fergus - 2014 - Visualizing and Understanding Convolutional Networ.pdf}
}

@inproceedings{zepf2023label,
  title = {That Label's Got Style: {{Handling}} Label Style Bias for Uncertain Image Segmentation},
  booktitle = {The Eleventh International Conference on Learning Representations},
  author = {Zepf, Kilian and Petersen, Eike and Frellsen, Jes and Feragen, Aasa},
  year = {2023}
}

@inproceedings{zhangACHIKOKDatabaseFundus2013,
  title = {{{ACHIKO-K}}: {{Database}} of Fundus Images from Glaucoma Patients},
  shorttitle = {{{ACHIKO-K}}},
  booktitle = {2013 {{IEEE}} 8th {{Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  author = {Zhang, Zhuo and Liu, Jiang and Yin, Fengshou and Lee, Beng-Hai and Wong, Damon Wing Kee and Sung, Kyung Rim},
  year = {2013},
  month = jun,
  pages = {228--231},
  issn = {2158-2297},
  doi = {10.1109/ICIEA.2013.6566371},
  abstract = {Developing image-based automatic glaucoma diagnosis systems requires a large amount of fundus image groundtruth data, especially that from glaucomatous patients. We develop ACHIKO-K database which contains 258 manually annotated retinal images and their clinical diagnosis information. All images in ACHIKO-K are taken from glaucoma patients and contain rich information on glaucoma related pathological signs, e.g. hemorrhage, optic nerve drusen and optic cup notching etc. Using our in-house developed image marking tool, optic cup/disc boundaries and various pathological signs are carefully annotated, providing accurate groundtruth for the development of image based glaucoma detection algorithms. The metadata embedded in the digital image files are extracted and make available in ACHIKO-K database, which, possesses rich information on camera setting and are critical for developing preprocess components for image normalization. Furthermore, the fundus images in ACHIKO-K contains timely information recording retinal changes and can be precious resources for retrospective study.},
  keywords = {ACHIKO-K database,Adaptive optics,Biomedical optical imaging,camera setting,clinical diagnosis information,Databases,digital image file extraction,diseases,eye,feature extraction,fundus image database,fundus image groundtruth data,glaucoma related pathological sign,glaucomatous patient,hemorrhage,image based glaucoma detection algorithm,image marking tool,image normalization,image-based automatic glaucoma diagnosis system,Integrated optics,manually annotated retinal image,medical image processing,medical information systems,meta data,metadata,optic cup notching,optic cup/disc boundary,optic nerve drusen,Optical imaging,preprocess component,Retina,vision defects,visual databases},
  file = {C:\Users\cleme\Zotero\storage\D9MA2UH5\6566371.html}
}

@inproceedings{zhangCharacterizingLabelErrors2020,
  title = {Characterizing {{Label Errors}}: {{Confident Learning}} for {{Noisy-Labeled Image Segmentation}}},
  shorttitle = {Characterizing {{Label Errors}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2020},
  author = {Zhang, Minqing and Gao, Jiantao and Lyu, Zhen and Zhao, Weibing and Wang, Qin and Ding, Weizhen and Wang, Sheng and Li, Zhen and Cui, Shuguang},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {721--730},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-59710-8_70},
  abstract = {Convolutional neural networks (CNNs) have achieved remarkable performance in image processing for its mighty capability to fit huge amount of data. However, if the training data are corrupted by noisy labels, the resulting performance might be deteriorated. In the domain of medical image analysis, this dilemma becomes extremely severe. This is because the medical image annotation always requires medical expertise and clinical experience, which would inevitably introduce subjectivity. In this paper, we design a novel algorithm based on the teacher-student architecture for noisy-labeled medical image segmentation. Creatively, We introduce confident learning (CL) method to identify the corrupted labels and endow CNN an anti-interference ability to the noises. Specifically, the CL technique is introduced to the teacher model to characterize the suspected wrong-labeled pixels. Since the noise identification maps are a little away from sufficient precision, the spatial label smoothing regularization technique is utilized to generate soft-corrected masks for training the student model. Since our method identifies and revises the noisy labels of the training data in a pixel-level rather than simply assigns lower weights to the noisy masks, it outperforms the state-of-the-art method in the noisy-labeled image segmentation task on the JSRT dataset, especially when the training data are severely corrupted by noises.},
  isbn = {978-3-030-59710-8},
  langid = {english}
}

@inproceedings{zhangCharacterizingLabelErrors2020a,
  title = {Characterizing {{Label Errors}}: {{Confident Learning}} for {{Noisy-Labeled Image Segmentation}}},
  shorttitle = {Characterizing {{Label Errors}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2020},
  author = {Zhang, Minqing and Gao, Jiantao and Lyu, Zhen and Zhao, Weibing and Wang, Qin and Ding, Weizhen and Wang, Sheng and Li, Zhen and Cui, Shuguang},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {721--730},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-59710-8_70},
  abstract = {Convolutional neural networks (CNNs) have achieved remarkable performance in image processing for its mighty capability to fit huge amount of data. However, if the training data are corrupted by noisy labels, the resulting performance might be deteriorated. In the domain of medical image analysis, this dilemma becomes extremely severe. This is because the medical image annotation always requires medical expertise and clinical experience, which would inevitably introduce subjectivity. In this paper, we design a novel algorithm based on the teacher-student architecture for noisy-labeled medical image segmentation. Creatively, We introduce confident learning (CL) method to identify the corrupted labels and endow CNN an anti-interference ability to the noises. Specifically, the CL technique is introduced to the teacher model to characterize the suspected wrong-labeled pixels. Since the noise identification maps are a little away from sufficient precision, the spatial label smoothing regularization technique is utilized to generate soft-corrected masks for training the student model. Since our method identifies and revises the noisy labels of the training data in a pixel-level rather than simply assigns lower weights to the noisy masks, it outperforms the state-of-the-art method in the noisy-labeled image segmentation task on the JSRT dataset, especially when the training data are severely corrupted by noises.},
  isbn = {978-3-030-59710-8},
  langid = {english}
}

@article{zhangDiabeticRetinopathyGrading2022,
  title = {Diabetic {{Retinopathy Grading}} by {{Deep Graph Correlation Network}} on {{Retinal Images Without Manual Annotations}}},
  author = {Zhang, Guanghua and Sun, Bin and Chen, Zhixian and Gao, Yuxi and Zhang, Zhaoxia and Li, Keran and Yang, Weihua},
  year = {2022},
  journal = {Frontiers in Medicine},
  volume = {9},
  pages = {872214},
  issn = {2296-858X},
  doi = {10.3389/fmed.2022.872214},
  abstract = {BACKGROUND: Diabetic retinopathy, as a severe public health problem associated with vision loss, should be diagnosed early using an accurate screening tool. While many previous deep learning models have been proposed for this disease, they need sufficient professional annotation data to train the model, requiring more expensive and time-consuming screening skills. METHOD: This study aims to economize manual power and proposes a deep graph correlation network (DGCN) to develop automated diabetic retinopathy grading without any professional annotations. DGCN involves the novel deep learning algorithm of a graph convolutional network to exploit inherent correlations from independent retinal image features learned by a convolutional neural network. Three designed loss functions of graph-center, pseudo-contrastive, and transformation-invariant constrain the optimisation and application of the DGCN model in an automated diabetic retinopathy grading task. RESULTS: To evaluate the DGCN model, this study employed EyePACS-1 and Messidor-2 sets to perform grading results. It achieved an accuracy of 89.9\% (91.8\%), sensitivity of 88.2\% (90.2\%), and specificity of 91.3\% (93.0\%) on EyePACS-1 (Messidor-2) data set with a confidence index of 95\% and commendable effectiveness on receiver operating characteristic (ROC) curve and t-SNE plots. CONCLUSION: The grading capability of this study is close to that of retina specialists, but superior to that of trained graders, which demonstrates that the proposed DGCN provides an innovative route for automated diabetic retinopathy grading and other computer-aided diagnostic systems.},
  langid = {english},
  pmcid = {PMC9046841},
  pmid = {35492360},
  keywords = {automated diagnosis,diabetic retinopathy,graph correlation network,retinal image classification,unsupervised learning}
}

@article{zhangDiabeticRetinopathyGrading2022a,
  title = {Diabetic {{Retinopathy Grading}} by {{Deep Graph Correlation Network}} on {{Retinal Images Without Manual Annotations}}},
  author = {Zhang, Guanghua and Sun, Bin and Chen, Zhixian and Gao, Yuxi and Zhang, Zhaoxia and Li, Keran and Yang, Weihua},
  year = {2022},
  journal = {Frontiers in Medicine},
  volume = {9},
  pages = {872214},
  issn = {2296-858X},
  doi = {10.3389/fmed.2022.872214},
  abstract = {BACKGROUND: Diabetic retinopathy, as a severe public health problem associated with vision loss, should be diagnosed early using an accurate screening tool. While many previous deep learning models have been proposed for this disease, they need sufficient professional annotation data to train the model, requiring more expensive and time-consuming screening skills. METHOD: This study aims to economize manual power and proposes a deep graph correlation network (DGCN) to develop automated diabetic retinopathy grading without any professional annotations. DGCN involves the novel deep learning algorithm of a graph convolutional network to exploit inherent correlations from independent retinal image features learned by a convolutional neural network. Three designed loss functions of graph-center, pseudo-contrastive, and transformation-invariant constrain the optimisation and application of the DGCN model in an automated diabetic retinopathy grading task. RESULTS: To evaluate the DGCN model, this study employed EyePACS-1 and Messidor-2 sets to perform grading results. It achieved an accuracy of 89.9\% (91.8\%), sensitivity of 88.2\% (90.2\%), and specificity of 91.3\% (93.0\%) on EyePACS-1 (Messidor-2) data set with a confidence index of 95\% and commendable effectiveness on receiver operating characteristic (ROC) curve and t-SNE plots. CONCLUSION: The grading capability of this study is close to that of retina specialists, but superior to that of trained graders, which demonstrates that the proposed DGCN provides an innovative route for automated diabetic retinopathy grading and other computer-aided diagnostic systems.},
  langid = {english},
  pmcid = {PMC9046841},
  pmid = {35492360},
  keywords = {automated diagnosis,diabetic retinopathy,graph correlation network,retinal image classification,unsupervised learning},
  file = {C:\Users\cleme\Zotero\storage\SZNU9H4Z\Zhang et al. - 2022 - Diabetic Retinopathy Grading by Deep Graph Correla.pdf}
}

@article{zhangOrientationAwareSemanticSegmentation,
  title = {Orientation-{{Aware Semantic Segmentation}} on {{Icosahedron Spheres}}},
  author = {Zhang, Chao and Liwicki, Stephan and Smith, William and Cipolla, Roberto},
  pages = {9},
  abstract = {We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640{\texttimes}1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.},
  langid = {english}
}

@article{zhangOrientationAwareSemanticSegmentationa,
  title = {Orientation-{{Aware Semantic Segmentation}} on {{Icosahedron Spheres}}},
  author = {Zhang, Chao and Liwicki, Stephan and Smith, William and Cipolla, Roberto},
  pages = {9},
  abstract = {We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640{\texttimes}1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\2JXI3ZVJ\Zhang et al. - Orientation-Aware Semantic Segmentation on Icosahe.pdf}
}

@inproceedings{zhangProbabilisticModelControlling2022,
  title = {A {{Probabilistic Model}} for {{Controlling Diversity}} and {{Accuracy}} of {{Ambiguous Medical Image Segmentation}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Multimedia}}},
  author = {Zhang, Wei and Zhang, Xiaohong and Huang, Sheng and Lu, Yuting and Wang, Kun},
  year = {2022},
  month = oct,
  series = {{{MM}} '22},
  pages = {4751--4759},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3503161.3548115},
  urldate = {2023-09-06},
  abstract = {Medical image segmentation tasks often have more than one plausible annotation for a given input image due to its inherent ambiguity. Generating multiple plausible predictions for a single image is of interest for medical critical applications. Many methods estimate the distribution of the annotation space by developing probabilistic models to generate multiple hypotheses. However, these methods aim to improve the diversity of predictions at the expense of the more important accuracy. In this paper, we propose a novel probabilistic segmentation model, called Joint Probabilistic U-net, which successfully achieves flexible control over the two abstract conceptions of diversity and accuracy. Specifically, we (i) model the joint distribution of images and annotations to learn a latent space, which is used to decouple diversity and accuracy, and (ii) transform the Gaussian distribution in the latent space to a complex distribution to improve model's expressiveness. In addition, we explore two strategies for preventing the latent space collapse, which are effective in improving the model's performance on datasets with limited annotation. We demonstrate the effectiveness of the proposed model on two medical image datasets, i.e. LIDC-IDRI and ISBI 2016, and achieved state-of-the-art results on several metrics.},
  isbn = {978-1-4503-9203-7},
  keywords = {medical image segmentation,probabilistic segmentation},
  file = {C:\Users\cleme\Zotero\storage\KLIZQMNS\Zhang et al. - 2022 - A Probabilistic Model for Controlling Diversity an.pdf}
}

@inproceedings{zhangResNeStSplitAttentionNetworks2022,
  title = {{{ResNeSt}}: {{Split-Attention Networks}}},
  shorttitle = {{{ResNeSt}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Lin, Haibin and Zhang, Zhi and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
  year = {2022},
  month = jun,
  pages = {2735--2745},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPRW56347.2022.00309},
  urldate = {2023-03-21},
  abstract = {The ability to learn richer network representations generally boosts the performance of deep learning models. To improve representation-learning in convolutional neural networks, we present a multi-branch architecture, which applies channel-wise attention across different network branches to leverage the complementary strengths of both feature-map attention and multi-path representation. Our proposed Split-Attention module provides a simple and modular computation block that can serve as a drop-in replacement for the popular residual block, while producing more diverse representations via cross-feature interactions. Adding a Split-Attention module into the architecture design space of RegNet-Y and FBNetV2 directly improves the performance of the resulting network. Replacing residual blocks with our Split-Attention module, we further design a new variant of the ResNet model, named ResNeSt, which outperforms EfficientNet in terms of the accuracy/latency trade-off.},
  isbn = {978-1-66548-739-9},
  langid = {english}
}

@inproceedings{zhangResNeStSplitAttentionNetworks2022a,
  title = {{{ResNeSt}}: {{Split-Attention Networks}}},
  shorttitle = {{{ResNeSt}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Lin, Haibin and Zhang, Zhi and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
  year = {2022},
  month = jun,
  pages = {2735--2745},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPRW56347.2022.00309},
  urldate = {2023-03-21},
  abstract = {The ability to learn richer network representations generally boosts the performance of deep learning models. To improve representation-learning in convolutional neural networks, we present a multi-branch architecture, which applies channel-wise attention across different network branches to leverage the complementary strengths of both feature-map attention and multi-path representation. Our proposed Split-Attention module provides a simple and modular computation block that can serve as a drop-in replacement for the popular residual block, while producing more diverse representations via cross-feature interactions. Adding a Split-Attention module into the architecture design space of RegNet-Y and FBNetV2 directly improves the performance of the resulting network. Replacing residual blocks with our Split-Attention module, we further design a new variant of the ResNet model, named ResNeSt, which outperforms EfficientNet in terms of the accuracy/latency trade-off.},
  isbn = {978-1-66548-739-9},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\KMGRRS6X\Zhang et al. - 2022 - ResNeSt Split-Attention Networks.pdf}
}

@inproceedings{zhangTransFuseFusingTransformers2021,
  title = {{{TransFuse}}: {{Fusing Transformers}} and {{CNNs}} for {{Medical Image Segmentation}}},
  shorttitle = {{{TransFuse}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Zhang, Yundong and Liu, Huiye and Hu, Qiang},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {14--24},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_2},
  abstract = {Medical image segmentation - the prerequisite of numerous clinical needs - has been significantly prospered by recent advances in convolutional neural networks (CNNs). However, it exhibits general limitations on modeling explicit long-range relation, and existing cures, resorting to building deep encoders along with aggressive downsampling operations, leads to redundant deepened networks and loss of localized details. Hence, the segmentation task awaits a better solution to improve the efficiency of modeling global contexts while maintaining a strong grasp of low-level details. In this paper, we propose a novel parallel-in-branch architecture, TransFuse, to address this challenge. TransFuse combines Transformers and CNNs in a parallel style, where both global dependency and low-level spatial details can be efficiently captured in a much shallower manner. Besides, a novel fusion technique - BiFusion module is created to efficiently fuse the multi-level features from both branches. Extensive experiments demonstrate that TransFuse achieves the newest state-of-the-art results on both 2D and 3D medical image sets including polyp, skin lesion, hip, and prostate segmentation, with significant parameter decrease and inference speed improvement.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Convolutional neural networks,Fusion,Medical image segmentation,Transformers}
}

@inproceedings{zhangTransFuseFusingTransformers2021a,
  title = {{{TransFuse}}: {{Fusing Transformers}} and {{CNNs}} for {{Medical Image Segmentation}}},
  shorttitle = {{{TransFuse}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2021},
  author = {Zhang, Yundong and Liu, Huiye and Hu, Qiang},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {14--24},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87193-2_2},
  abstract = {Medical image segmentation - the prerequisite of numerous clinical needs - has been significantly prospered by recent advances in convolutional neural networks (CNNs). However, it exhibits general limitations on modeling explicit long-range relation, and existing cures, resorting to building deep encoders along with aggressive downsampling operations, leads to redundant deepened networks and loss of localized details. Hence, the segmentation task awaits a better solution to improve the efficiency of modeling global contexts while maintaining a strong grasp of low-level details. In this paper, we propose a novel parallel-in-branch architecture, TransFuse, to address this challenge. TransFuse combines Transformers and CNNs in a parallel style, where both global dependency and low-level spatial details can be efficiently captured in a much shallower manner. Besides, a novel fusion technique - BiFusion module is created to efficiently fuse the multi-level features from both branches. Extensive experiments demonstrate that TransFuse achieves the newest state-of-the-art results on both 2D and 3D medical image sets including polyp, skin lesion, hip, and prostate segmentation, with significant parameter decrease and inference speed improvement.},
  isbn = {978-3-030-87193-2},
  langid = {english},
  keywords = {Convolutional neural networks,Fusion,Medical image segmentation,Transformers},
  file = {C:\Users\cleme\Zotero\storage\3F2FQV76\Zhang et al. - 2021 - TransFuse Fusing Transformers and CNNs for Medica.pdf}
}

@article{zhangUnderstandingDeepLearning2021,
  title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2021},
  month = mar,
  journal = {Communications of the ACM},
  volume = {64},
  number = {3},
  pages = {107--115},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3446776},
  urldate = {2023-04-05},
  abstract = {Despite their massive size, successful deep artificial \-neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.},
  langid = {english}
}

@article{zhangUnderstandingDeepLearning2021a,
  title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2021},
  month = mar,
  journal = {Communications of the ACM},
  volume = {64},
  number = {3},
  pages = {107--115},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3446776},
  urldate = {2023-04-05},
  abstract = {Despite their massive size, successful deep artificial \-neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training.},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\6FGURUF9\Zhang et al. - 2021 - Understanding deep learning (still) requires rethi.pdf}
}

@inproceedings{zhaoContrastiveLearningLabel2021,
  title = {Contrastive {{Learning}} for {{Label Efficient Semantic Segmentation}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhao, Xiangyun and Vemulapalli, Raviteja and Mansfield, Philip Andrew and Gong, Boqing and Green, Bradley and Shapira, Lior and Wu, Ying},
  year = {2021},
  month = oct,
  pages = {10603--10613},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.01045},
  urldate = {2023-07-04},
  abstract = {Collecting labeled data for the task of semantic segmentation is expensive and time-consuming, as it requires dense pixel-level annotations. While recent Convolutional Neural Network (CNN) based semantic segmentation approaches have achieved impressive results by using large amounts of labeled training data, their performance drops significantly as the amount of labeled data decreases. This happens because deep CNNs trained with the de facto cross-entropy loss can easily overfit to small amounts of labeled data. To address this issue, we propose a simple and effective contrastive learning-based training strategy in which we first pretrain the network using a pixel-wise, label-based contrastive loss, and then fine-tune it using the cross-entropy loss. This approach increases intra-class compactness and inter-class separability, thereby resulting in a better pixel classifier. We demonstrate the effectiveness of the proposed training strategy using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our results show that pretraining with the proposed contrastive loss results in large performance gains (more than 20\% absolute improvement in some settings) when the amount of labeled data is limited. In many settings, the proposed contrastive pretraining strategy, which does not use any additional data, is able to match or outperform the widely-used ImageNet pretraining strategy that uses more than a million additional labeled images.},
  isbn = {978-1-66542-812-5},
  langid = {english}
}

@inproceedings{zhaoContrastiveLearningLabel2021a,
  title = {Contrastive {{Learning}} for {{Label Efficient Semantic Segmentation}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhao, Xiangyun and Vemulapalli, Raviteja and Mansfield, Philip Andrew and Gong, Boqing and Green, Bradley and Shapira, Lior and Wu, Ying},
  year = {2021},
  month = oct,
  pages = {10603--10613},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.01045},
  urldate = {2023-07-04},
  abstract = {Collecting labeled data for the task of semantic segmentation is expensive and time-consuming, as it requires dense pixel-level annotations. While recent Convolutional Neural Network (CNN) based semantic segmentation approaches have achieved impressive results by using large amounts of labeled training data, their performance drops significantly as the amount of labeled data decreases. This happens because deep CNNs trained with the de facto cross-entropy loss can easily overfit to small amounts of labeled data. To address this issue, we propose a simple and effective contrastive learning-based training strategy in which we first pretrain the network using a pixel-wise, label-based contrastive loss, and then fine-tune it using the cross-entropy loss. This approach increases intra-class compactness and inter-class separability, thereby resulting in a better pixel classifier. We demonstrate the effectiveness of the proposed training strategy using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our results show that pretraining with the proposed contrastive loss results in large performance gains (more than 20\% absolute improvement in some settings) when the amount of labeled data is limited. In many settings, the proposed contrastive pretraining strategy, which does not use any additional data, is able to match or outperform the widely-used ImageNet pretraining strategy that uses more than a million additional labeled images.},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\HGLCGYBK\Zhao et al. - 2021 - Contrastive Learning for Label Efficient Semantic .pdf}
}

@article{zhaoRealtimeSurgicalInstrument2019,
  title = {Real-Time Surgical Instrument Detection in Robot-Assisted Surgery Using a Convolutional Neural Network Cascade},
  author = {Zhao, Zijian and Cai, Tongbiao and Chang, Faliang and Cheng, Xiaolin},
  year = {2019},
  month = nov,
  journal = {Healthcare Technology Letters},
  volume = {6},
  number = {6},
  pages = {275--279},
  issn = {2053-3713},
  doi = {10.1049/htl.2019.0064},
  urldate = {2020-05-06},
  abstract = {Surgical instrument detection in robot-assisted surgery videos is an import vision component for these systems. Most of the current deep learning methods focus on single-tool detection and suffer from low detection speed. To address this, the authors propose a novel frame-by-frame detection method using a cascading convolutional neural network (CNN) which consists of two different CNNs for real-time multi-tool detection. An hourglass network and a modified visual geometry group (VGG) network are applied to jointly predict the localisation. The former CNN outputs detection heatmaps representing the location of tool tip areas, and the latter performs bounding-box regression for tool tip areas on these heatmaps stacked with input RGB image frames. The authors' method is tested on the publicly available EndoVis Challenge dataset and the ATLAS Dione dataset. The experimental results show that their method achieves better performance than mainstream detection methods in terms of detection accuracy and speed.},
  pmcid = {PMC6952255},
  pmid = {32038871}
}

@article{zhaoRealtimeSurgicalInstrument2019a,
  title = {Real-Time Surgical Instrument Detection in Robot-Assisted Surgery Using a Convolutional Neural Network Cascade},
  author = {Zhao, Zijian and Cai, Tongbiao and Chang, Faliang and Cheng, Xiaolin},
  year = {2019},
  month = nov,
  journal = {Healthcare Technology Letters},
  volume = {6},
  number = {6},
  pages = {275--279},
  issn = {2053-3713},
  doi = {10.1049/htl.2019.0064},
  urldate = {2020-05-06},
  abstract = {Surgical instrument detection in robot-assisted surgery videos is an import vision component for these systems. Most of the current deep learning methods focus on single-tool detection and suffer from low detection speed. To address this, the authors propose a novel frame-by-frame detection method using a cascading convolutional neural network (CNN) which consists of two different CNNs for real-time multi-tool detection. An hourglass network and a modified visual geometry group (VGG) network are applied to jointly predict the localisation. The former CNN outputs detection heatmaps representing the location of tool tip areas, and the latter performs bounding-box regression for tool tip areas on these heatmaps stacked with input RGB image frames. The authors' method is tested on the publicly available EndoVis Challenge dataset and the ATLAS Dione dataset. The experimental results show that their method achieves better performance than mainstream detection methods in terms of detection accuracy and speed.},
  pmcid = {PMC6952255},
  pmid = {32038871},
  file = {C:\Users\cleme\Zotero\storage\TRMSNYDG\Zhao et al. - 2019 - Real-time surgical instrument detection in robot-a.pdf}
}

@incollection{zhaoUnpairedImagetoImageTranslation2020,
  title = {Unpaired {{Image-to-Image Translation Using Adversarial Consistency Loss}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Zhao, Yihao and Wu, Ruihai and Dong, Hao},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12354},
  pages = {800--815},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58545-7_46},
  urldate = {2023-06-23},
  abstract = {Unpaired image-to-image translation is a class of vision problems whose goal is to find the mapping between different image domains using unpaired training data. Cycle-consistency loss is a widely used constraint for such problems. However, due to the strict pixel-level constraint, it cannot perform shape changes, remove large objects, or ignore irrelevant texture. In this paper, we propose a novel adversarialconsistency loss for image-to-image translation. This loss does not require the translated image to be translated back to be a specific source image but can encourage the translated images to retain important features of the source images and overcome the drawbacks of cycle-consistency loss noted above. Our method achieves state-of-the-art results on three challenging tasks: glasses removal, male-to-female translation, and selfieto-anime translation.},
  isbn = {978-3-030-58544-0 978-3-030-58545-7},
  langid = {english}
}

@incollection{zhaoUnpairedImagetoImageTranslation2020a,
  title = {Unpaired {{Image-to-Image Translation Using Adversarial Consistency Loss}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020},
  author = {Zhao, Yihao and Wu, Ruihai and Dong, Hao},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12354},
  pages = {800--815},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58545-7_46},
  urldate = {2023-06-23},
  abstract = {Unpaired image-to-image translation is a class of vision problems whose goal is to find the mapping between different image domains using unpaired training data. Cycle-consistency loss is a widely used constraint for such problems. However, due to the strict pixel-level constraint, it cannot perform shape changes, remove large objects, or ignore irrelevant texture. In this paper, we propose a novel adversarialconsistency loss for image-to-image translation. This loss does not require the translated image to be translated back to be a specific source image but can encourage the translated images to retain important features of the source images and overcome the drawbacks of cycle-consistency loss noted above. Our method achieves state-of-the-art results on three challenging tasks: glasses removal, male-to-female translation, and selfieto-anime translation.},
  isbn = {978-3-030-58544-0 978-3-030-58545-7},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\US99DMJQ\Zhao et al. - 2020 - Unpaired Image-to-Image Translation Using Adversar.pdf}
}

@article{zhengDevelopmentClinicalValidation2022,
  title = {Development and {{Clinical Validation}} of {{Semi-Supervised Generative Adversarial Networks}} for {{Detection}} of {{Retinal Disorders}} in {{Optical Coherence Tomography Images Using Small Dataset}}},
  author = {Zheng, Ce and Ye, Hongfei and Yang, Jianlong and Fei, Ping and Qiu, Yingping and Xie, Xiaolin and Wang, Zilei and Chen, Jili and Zhao, Peiquan},
  year = {2022},
  month = may,
  journal = {The Asia-Pacific Journal of Ophthalmology},
  volume = {11},
  number = {3},
  pages = {219--226},
  issn = {2162-0989},
  doi = {10.1097/APO.0000000000000498},
  urldate = {2022-07-10},
  abstract = {Purpose:  To develop and test semi-supervised generative adversarial networks (GANs) that detect retinal disorders on optical coherence tomography (OCT) images using a small-labeled dataset. Methods:  From a public database, we randomly chose a small supervised dataset with 400 OCT images (100 choroidal neovascularization, 100 diabetic macular edema, 100 drusen, and 100 normal) and assigned all other OCT images to unsupervised dataset (107,912 images without labeling). We adopted a semi-supervised GAN and a supervised deep learning (DL) model for automatically detecting retinal disorders from OCT images. The performance of the 2 models was compared in 3 testing datasets with different OCT devices. The evaluation metrics included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curves. Results:  The local validation dataset included 1000 images with 250 from each category. The independent clinical dataset included 366 OCT images using Cirrus OCT Shanghai Shibei Hospital and 511 OCT images using RTVue OCT from Xinhua Hospital respectively. The semi-supervised GANs classifier achieved better accuracy than supervised DL model (0.91 vs 0.86 for local cell validation dataset, 0.91 vs 0.86 in the Shanghai Shibei Hospital testing dataset, and 0.93 vs 0.92 in Xinhua Hospital testing dataset). For detecting urgent referrals (choroidal neo-vascularization and diabetic macular edema) from nonurgent referrals (drusen and normal) on OCT images, the semi-supervised GANs classifier also achieved better area under the receiver operating characteristic curves than supervised DL model (0.99 vs 0.97, 0.97 vs 0.96, and 0.99 vs 0.99, respectively). Conclusions:  A semi-supervised GAN can achieve better performance than that of a supervised DL model when the labeled dataset is limited. The current study offers utility to various research and clinical studies using DL with relatively small datasets. Semi-supervised GANs can detect retinal disorders from OCT images using relatively small dataset.},
  langid = {american}
}

@article{zhengDevelopmentClinicalValidation2022a,
  title = {Development and {{Clinical Validation}} of {{Semi-Supervised Generative Adversarial Networks}} for {{Detection}} of {{Retinal Disorders}} in {{Optical Coherence Tomography Images Using Small Dataset}}},
  author = {Zheng, Ce and Ye, Hongfei and Yang, Jianlong and Fei, Ping and Qiu, Yingping and Xie, Xiaolin and Wang, Zilei and Chen, Jili and Zhao, Peiquan},
  year = {2022-05/2022-06},
  journal = {The Asia-Pacific Journal of Ophthalmology},
  volume = {11},
  number = {3},
  pages = {219--226},
  issn = {2162-0989},
  doi = {10.1097/APO.0000000000000498},
  urldate = {2022-07-10},
  abstract = {Purpose:~         To develop and test semi-supervised generative adversarial networks (GANs) that detect retinal disorders on optical coherence tomography (OCT) images using a small-labeled dataset.         Methods:~         From a public database, we randomly chose a small supervised dataset with 400 OCT images (100 choroidal neovascularization, 100 diabetic macular edema, 100 drusen, and 100 normal) and assigned all other OCT images to unsupervised dataset (107,912 images without labeling). We adopted a semi-supervised GAN and a supervised deep learning (DL) model for automatically detecting retinal disorders from OCT images. The performance of the 2 models was compared in 3 testing datasets with different OCT devices. The evaluation metrics included accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curves.         Results:~         The local validation dataset included 1000 images with 250 from each category. The independent clinical dataset included 366 OCT images using Cirrus OCT Shanghai Shibei Hospital and 511 OCT images using RTVue OCT from Xinhua Hospital respectively. The semi-supervised GANs classifier achieved better accuracy than supervised DL model (0.91 vs 0.86 for local cell validation dataset, 0.91 vs 0.86 in the Shanghai Shibei Hospital testing dataset, and 0.93 vs 0.92 in Xinhua Hospital testing dataset). For detecting urgent referrals (choroidal neo-vascularization and diabetic macular edema) from nonurgent referrals (drusen and normal) on OCT images, the semi-supervised GANs classifier also achieved better area under the receiver operating characteristic curves than supervised DL model (0.99 vs 0.97, 0.97 vs 0.96, and 0.99 vs 0.99, respectively).         Conclusions:~         A semi-supervised GAN can achieve better performance than that of a supervised DL model when the labeled dataset is limited. The current study offers utility to various research and clinical studies using DL with relatively small datasets. Semi-supervised GANs can detect retinal disorders from OCT images using relatively small dataset.},
  langid = {american}
}

@article{zhengRethinkingSemanticSegmentation2021,
  title = {Rethinking {{Semantic Segmentation}} from a {{Sequence-to-Sequence Perspective}} with {{Transformers}}},
  author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H. S. and Zhang, Li},
  year = {2021},
  month = mar,
  journal = {arXiv:2012.15840 [cs]},
  eprint = {2012.15840},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoderdecoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{zhengRethinkingSemanticSegmentation2021a,
  title = {Rethinking {{Semantic Segmentation}} from a {{Sequence-to-Sequence Perspective}} with {{Transformers}}},
  author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H. S. and Zhang, Li},
  year = {2021},
  month = mar,
  journal = {arXiv:2012.15840 [cs]},
  eprint = {2012.15840},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoderdecoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{zhengRethinkingSemanticSegmentation2021b,
  title = {Rethinking {{Semantic Segmentation}} from a {{Sequence-to-Sequence Perspective}} with {{Transformers}}},
  author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H. S. and Zhang, Li},
  year = {2021},
  month = mar,
  journal = {arXiv:2012.15840 [cs]},
  eprint = {2012.15840},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoderdecoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\IN3YX2ZZ\Zheng et al. - 2021 - Rethinking Semantic Segmentation from a Sequence-t.pdf}
}

@article{zhengRethinkingSemanticSegmentation2021c,
  title = {Rethinking {{Semantic Segmentation}} from a {{Sequence-to-Sequence Perspective}} with {{Transformers}}},
  author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H. S. and Zhang, Li},
  year = {2021},
  month = mar,
  journal = {arXiv:2012.15840 [cs]},
  eprint = {2012.15840},
  primaryclass = {cs},
  urldate = {2021-05-20},
  abstract = {Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoderdecoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\cleme\Zotero\storage\3YYM2K2M\Zheng et al. - 2021 - Rethinking Semantic Segmentation from a Sequence-t.pdf}
}

@inproceedings{zhengUnsupervisedSceneAdaptation2020,
  title = {Unsupervised {{Scene Adaptation}} with {{Memory Regularization}} in Vivo},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zheng, Zhedong and Yang, Yi},
  year = {2020},
  month = jul,
  pages = {1076--1082},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Yokohama, Japan},
  doi = {10.24963/ijcai.2020/150},
  urldate = {2023-06-23},
  abstract = {This work focuses on the unsupervised scene adaptation problem of learning from both labeled source data and unlabeled target data. Existing approaches focus on minoring the inter-domain gap between the source and target domains. However, the intra-domain knowledge and inherent uncertainty learned by the network are under-explored. In this paper, we propose an orthogonal method, called memory regularization in vivo, to exploit the intradomain knowledge and regularize the model training. Specifically, we refer to the segmentation model itself as the memory module, and minor the discrepancy of the two classifiers, i.e., the primary classifier and the auxiliary classifier, to reduce the prediction inconsistency. Without extra parameters, the proposed method is complementary to most existing domain adaptation methods and could generally improve the performance of existing methods. Albeit simple, we verify the effectiveness of memory regularization on two synthetic-to-real benchmarks: GTA5 {$\rightarrow$} Cityscapes and SYNTHIA {$\rightarrow$} Cityscapes, yielding +11.1\% and +11.3\% mIoU improvement over the baseline model, respectively. Besides, a similar +12.0\% mIoU improvement is observed on the cross-city benchmark: Cityscapes {$\rightarrow$} Oxford RobotCar.},
  isbn = {978-0-9992411-6-5},
  langid = {english}
}

@inproceedings{zhengUnsupervisedSceneAdaptation2020a,
  title = {Unsupervised {{Scene Adaptation}} with {{Memory Regularization}} in Vivo},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zheng, Zhedong and Yang, Yi},
  year = {2020},
  month = jul,
  pages = {1076--1082},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Yokohama, Japan},
  doi = {10.24963/ijcai.2020/150},
  urldate = {2023-06-23},
  abstract = {This work focuses on the unsupervised scene adaptation problem of learning from both labeled source data and unlabeled target data. Existing approaches focus on minoring the inter-domain gap between the source and target domains. However, the intra-domain knowledge and inherent uncertainty learned by the network are under-explored. In this paper, we propose an orthogonal method, called memory regularization in vivo, to exploit the intradomain knowledge and regularize the model training. Specifically, we refer to the segmentation model itself as the memory module, and minor the discrepancy of the two classifiers, i.e., the primary classifier and the auxiliary classifier, to reduce the prediction inconsistency. Without extra parameters, the proposed method is complementary to most existing domain adaptation methods and could generally improve the performance of existing methods. Albeit simple, we verify the effectiveness of memory regularization on two synthetic-to-real benchmarks: GTA5 {$\rightarrow$} Cityscapes and SYNTHIA {$\rightarrow$} Cityscapes, yielding +11.1\% and +11.3\% mIoU improvement over the baseline model, respectively. Besides, a similar +12.0\% mIoU improvement is observed on the cross-city benchmark: Cityscapes {$\rightarrow$} Oxford RobotCar.},
  isbn = {978-0-9992411-6-5},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\ZMPHBP5R\Zheng et Yang - 2020 - Unsupervised Scene Adaptation with Memory Regulari.pdf}
}

@article{zhouBenchmarkStudyingDiabetic2021,
  title = {A {{Benchmark}} for {{Studying Diabetic Retinopathy}}: {{Segmentation}}, {{Grading}}, and {{Transferability}}},
  shorttitle = {A {{Benchmark}} for {{Studying Diabetic Retinopathy}}},
  author = {Zhou, Y. and Wang, B. and Huang, L. and Cui, S. and Shao, L.},
  year = {2021},
  month = mar,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {40},
  number = {3},
  pages = {818--828},
  issn = {1558-254X},
  doi = {10.1109/TMI.2020.3037771},
  abstract = {People with diabetes are at risk of developing an eye disease called diabetic retinopathy (DR). This disease occurs when high blood glucose levels cause damage to blood vessels in the retina. Computer-aided DR diagnosis has become a promising tool for the early detection and severity grading of DR, due to the great success of deep learning. However, most current DR diagnosis systems do not achieve satisfactory performance or interpretability for ophthalmologists, due to the lack of training data with consistent and fine-grained annotations. To address this problem, we construct a large fine-grained annotated DR dataset containing 2,842 images (FGADR). Specifically, this dataset has 1,842 images with pixel-level DR-related lesion annotations, and 1,000 images with image-level labels graded by six board-certified ophthalmologists with intra-rater consistency. The proposed dataset will enable extensive studies on DR diagnosis. Further, we establish three benchmark tasks for evaluation: 1. DR lesion segmentation; 2. DR grading by joint classification and segmentation; 3. Transfer learning for ocular multi-disease identification. Moreover, a novel inductive transfer learning method is introduced for the third task. Extensive experiments using different state-of-the-art methods are conducted on our FGADR dataset, which can serve as baselines for future research. Our dataset will be released in https://csyizhou.github.io/FGADR/.},
  keywords = {Benchmark testing,Diabetes,Diabetic retinopathy,grading,Image segmentation,lesion segmentation,Lesions,Retinopathy,Task analysis,transfer learning,Transfer learning},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\2ALQGR9G\\Zhou et al. - 2021 - A Benchmark for Studying Diabetic Retinopathy Seg.pdf;C\:\\Users\\cleme\\Zotero\\storage\\CW2EB3MA\\9257400.html}
}

@inproceedings{zhouCollaborativeLearningSemiSupervised2019,
  title = {Collaborative {{Learning}} of {{Semi-Supervised Segmentation}} and {{Classification}} for {{Medical Images}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Yi and He, Xiaodong and Huang, Lei and Liu, Li and Zhu, Fan and Cui, Shanshan and Shao, Ling},
  year = {2019},
  month = jun,
  pages = {2074--2083},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00218},
  urldate = {2021-10-04},
  abstract = {Medical image analysis has two important research areas: disease grading and fine-grained lesion segmentation. Although the former problem often relies on the latter, the two are usually studied separately. Disease severity grading can be treated as a classification problem, which only requires image-level annotations, while the lesion segmentation requires stronger pixel-level annotations. However, pixel-wise data annotation for medical images is highly time-consuming and requires domain experts. In this paper, we propose a collaborative learning method to jointly improve the performance of disease grading and lesion segmentation by semi-supervised learning with an attention mechanism. Given a small set of pixel-level annotated data, a multi-lesion mask generation model first performs the traditional semantic segmentation task. Then, based on initially predicted lesion maps for large quantities of imagelevel annotated data, a lesion attentive disease grading model is designed to improve the severity classification accuracy. Meanwhile, the lesion attention model can refine the lesion maps using class-specific information to fine-tune the segmentation model in a semi-supervised manner. An adversarial architecture is also integrated for training. With extensive experiments on a representative medical problem called diabetic retinopathy (DR), we validate the effectiveness of our method and achieve consistent improvements over state-of-the-art methods on three public datasets.},
  isbn = {978-1-72813-293-8},
  langid = {english}
}

@inproceedings{zhouCollaborativeLearningSemiSupervised2019a,
  title = {Collaborative {{Learning}} of {{Semi-Supervised Segmentation}} and {{Classification}} for {{Medical Images}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Yi and He, Xiaodong and Huang, Lei and Liu, Li and Zhu, Fan and Cui, Shanshan and Shao, Ling},
  year = {2019},
  month = jun,
  pages = {2074--2083},
  publisher = {IEEE},
  address = {Long Beach, CA, USA},
  doi = {10.1109/CVPR.2019.00218},
  urldate = {2021-10-04},
  abstract = {Medical image analysis has two important research areas: disease grading and fine-grained lesion segmentation. Although the former problem often relies on the latter, the two are usually studied separately. Disease severity grading can be treated as a classification problem, which only requires image-level annotations, while the lesion segmentation requires stronger pixel-level annotations. However, pixel-wise data annotation for medical images is highly time-consuming and requires domain experts. In this paper, we propose a collaborative learning method to jointly improve the performance of disease grading and lesion segmentation by semi-supervised learning with an attention mechanism. Given a small set of pixel-level annotated data, a multi-lesion mask generation model first performs the traditional semantic segmentation task. Then, based on initially predicted lesion maps for large quantities of imagelevel annotated data, a lesion attentive disease grading model is designed to improve the severity classification accuracy. Meanwhile, the lesion attention model can refine the lesion maps using class-specific information to fine-tune the segmentation model in a semi-supervised manner. An adversarial architecture is also integrated for training. With extensive experiments on a representative medical problem called diabetic retinopathy (DR), we validate the effectiveness of our method and achieve consistent improvements over state-of-the-art methods on three public datasets.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\MUBFJ72Q\Zhou et al. - 2019 - Collaborative Learning of Semi-Supervised Segmenta.pdf}
}

@inproceedings{zhouEndtoEndDenseVideo2018,
  title = {End-to-{{End Dense Video Captioning}} with {{Masked Transformer}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhou, Luowei and Zhou, Yingbo and Corso, Jason J. and Socher, Richard and Xiong, Caiming},
  year = {2018},
  month = jun,
  pages = {8739--8748},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00911},
  urldate = {2021-11-05},
  abstract = {Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively.},
  isbn = {978-1-5386-6420-9},
  langid = {english}
}

@inproceedings{zhouEndtoEndDenseVideo2018a,
  title = {End-to-{{End Dense Video Captioning}} with {{Masked Transformer}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhou, Luowei and Zhou, Yingbo and Corso, Jason J. and Socher, Richard and Xiong, Caiming},
  year = {2018},
  month = jun,
  pages = {8739--8748},
  publisher = {IEEE},
  address = {Salt Lake City, UT},
  doi = {10.1109/CVPR.2018.00911},
  urldate = {2021-11-05},
  abstract = {Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\FZXJLPFH\Zhou et al. - 2018 - End-to-End Dense Video Captioning with Masked Tran.pdf}
}

@article{zhouFoundationModelGeneralizable2023,
  title = {A Foundation Model for Generalizable Disease Detection from Retinal Images},
  author = {Zhou, Yukun and Chia, Mark A. and Wagner, Siegfried K. and Ayhan, Murat S. and Williamson, Dominic J. and Struyven, Robbert R. and Liu, Timing and Xu, Moucheng and Lozano, Mateo G. and {Woodward-Court}, Peter and Kihara, Yuka and Altmann, Andre and Lee, Aaron Y. and Topol, Eric J. and Denniston, Alastair K. and Alexander, Daniel C. and Keane, Pearse A.},
  year = {2023},
  month = sep,
  journal = {Nature},
  pages = {1--8},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06555-x},
  urldate = {2023-09-30},
  abstract = {Medical artificial intelligence (AI) offers great potential for recognizing signs of health conditions in retinal images and expediting the diagnosis of eye diseases and systemic disorders1. However, the development of AI models requires substantial annotation and models are usually task-specific with limited generalizability to different clinical applications2. Here, we present RETFound, a foundation model for retinal images that learns generalizable representations from unlabelled retinal images and provides a basis for label-efficient model adaptation in several applications. Specifically, RETFound is trained on 1.6\,million unlabelled retinal images by means of self-supervised learning and then adapted to disease detection tasks with explicit labels. We show that adapted RETFound consistently outperforms several comparison models in the diagnosis and prognosis of sight-threatening eye diseases, as well as incident prediction of complex systemic disorders such as heart failure and myocardial infarction with fewer labelled data. RETFound provides a generalizable solution to improve model performance and alleviate the annotation workload of experts to enable broad clinical AI applications from retinal imaging.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Cardiovascular diseases,Medical imaging,Prognosis,Retinal diseases,Translational research},
  file = {C:\Users\cleme\Zotero\storage\E6TYF7IL\Zhou et al. - 2023 - A foundation model for generalizable disease detec.pdf}
}

@inproceedings{zhouLearningDeepFeatures2016,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2016},
  pages = {2921--2929},
  urldate = {2019-12-13}
}

@inproceedings{zhouLearningDeepFeatures2016a,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2016},
  month = jun,
  pages = {2921--2929},
  publisher = {IEEE Computer Society},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.319},
  urldate = {2023-06-01},
  abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.},
  isbn = {978-1-4673-8851-1},
  langid = {english}
}

@inproceedings{zhouLearningDeepFeatures2016b,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2016},
  pages = {2921--2929},
  urldate = {2019-12-13},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\Q3VQM3CZ\\Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf;C\:\\Users\\cleme\\Zotero\\storage\\DRZJNCRG\\Zhou_Learning_Deep_Features_CVPR_2016_paper.html}
}

@inproceedings{zhouLearningDeepFeatures2016c,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2016},
  month = jun,
  pages = {2921--2929},
  publisher = {IEEE Computer Society},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.319},
  urldate = {2023-06-01},
  abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\87C34KDA\Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf}
}

@inproceedings{zhouUNetNestedUNet2018,
  title = {{{UNet}}++: {{A Nested U-Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  booktitle = {Deep {{Learning}} in {{Medical Image Analysis}} and {{Multimodal Learning}} for {{Clinical Decision Support}}},
  author = {Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
  editor = {Stoyanov, Danail and Taylor, Zeike and Carneiro, Gustavo and {Syeda-Mahmood}, Tanveer and Martel, Anne and {Maier-Hein}, Lena and Tavares, Jo{\~a}o Manuel R.S. and Bradley, Andrew and Papa, Jo{\~a}o Paulo and Belagiannis, Vasileios and Nascimento, Jacinto C. and Lu, Zhi and Conjeti, Sailesh and Moradi, Mehdi and Greenspan, Hayit and Madabhushi, Anant},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--11},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-00889-5_1},
  abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
  isbn = {978-3-030-00889-5},
  langid = {english}
}

@inproceedings{zhouUNetNestedUNet2018a,
  title = {{{UNet}}++: {{A Nested U-Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  booktitle = {Deep {{Learning}} in {{Medical Image Analysis}} and {{Multimodal Learning}} for {{Clinical Decision Support}}},
  author = {Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
  editor = {Stoyanov, Danail and Taylor, Zeike and Carneiro, Gustavo and {Syeda-Mahmood}, Tanveer and Martel, Anne and {Maier-Hein}, Lena and Tavares, Jo{\~a}o Manuel R.S. and Bradley, Andrew and Papa, Jo{\~a}o Paulo and Belagiannis, Vasileios and Nascimento, Jacinto C. and Lu, Zhi and Conjeti, Sailesh and Moradi, Mehdi and Greenspan, Hayit and Madabhushi, Anant},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--11},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-00889-5_1},
  abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
  isbn = {978-3-030-00889-5},
  langid = {english},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\C3IUUKZB\\zhou2018.pdf.pdf;C\:\\Users\\cleme\\Zotero\\storage\\XQEG2XJJ\\Zhou et al. - 2018 - UNet++ A Nested U-Net Architecture for Medical Im.pdf}
}

@article{zhouUNetRedesigningSkip2020,
  title = {{{UNet}}++: {{Redesigning Skip Connections}} to {{Exploit Multiscale Features}} in {{Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  author = {Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
  year = {2020},
  month = jun,
  journal = {IEEE transactions on medical imaging},
  volume = {39},
  number = {6},
  pages = {1856--1867},
  issn = {0278-0062},
  doi = {10.1109/TMI.2019.2959609},
  urldate = {2023-05-18},
  abstract = {The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects---an improvement over the fixed-depth UNet; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.},
  pmcid = {PMC7357299},
  pmid = {31841402}
}

@article{zhouUNetRedesigningSkip2020a,
  title = {{{UNet}}++: {{Redesigning Skip Connections}} to {{Exploit Multiscale Features}} in {{Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  author = {Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
  year = {2020},
  month = jun,
  journal = {IEEE transactions on medical imaging},
  volume = {39},
  number = {6},
  pages = {1856--1867},
  issn = {0278-0062},
  doi = {10.1109/TMI.2019.2959609},
  urldate = {2023-05-18},
  abstract = {The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects---an improvement over the fixed-depth UNet; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.},
  pmcid = {PMC7357299},
  pmid = {31841402},
  file = {C:\Users\cleme\Zotero\storage\4Y9PPD5E\Zhou et al. - 2020 - UNet++ Redesigning Skip Connections to Exploit Mu.pdf}
}

@article{zhouwangImageQualityAssessment2004,
  title = {Image Quality Assessment: {{From}} Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {{Zhou Wang} and Bovik, A. C. and Sheikh, H. R. and Simoncelli, E. P.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1057-7149},
  doi = {10.1109/TIP.2003.819861},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  keywords = {Algorithms,Automated,Computer-Assisted,data compression,Data Interpretation,Data mining,Degradation,distorted image,error sensitivity,error visibility,human visual perception,human visual system,Humans,Hypermedia,image coding,image compression,image database,Image Enhancement,Image Interpretation,Image quality,Indexes,Information Storage and Retrieval,JPEG,JPEG2000,Layout,Models,Pattern Recognition,perceptual image quality assessment,Quality assessment,Quality Control,reference image,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,Statistical,structural information,structural similarity index,Subtraction Technique,Transform coding,visual perception,Visual perception,Visual system}
}

@article{zhouwangImageQualityAssessment2004a,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {{Zhou Wang} and Bovik, A. C. and Sheikh, H. R. and Simoncelli, E. P.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1057-7149},
  doi = {10.1109/TIP.2003.819861},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  keywords = {Algorithms,data compression,Data Interpretation Statistical,Data mining,Degradation,distorted image,error sensitivity,error visibility,human visual perception,human visual system,Humans,Hypermedia,image coding,image compression,image database,Image Enhancement,Image Interpretation Computer-Assisted,Image quality,Indexes,Information Storage and Retrieval,JPEG,JPEG2000,Layout,Models Statistical,Pattern Recognition Automated,perceptual image quality assessment,Quality assessment,Quality Control,reference image,Reproducibility of Results,Sensitivity and Specificity,Signal Processing Computer-Assisted,structural information,structural similarity index,Subtraction Technique,Transform coding,visual perception,Visual perception,Visual system},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\ETNDARZJ\\Zhou Wang et al. - 2004 - Image quality assessment from error visibility to.pdf;C\:\\Users\\cleme\\Zotero\\storage\\K42K7Q9H\\1284395.html}
}

@inproceedings{zhuoranEfficientAttentionAttention2021,
  title = {Efficient {{Attention}}: {{Attention}} with {{Linear Complexities}}},
  shorttitle = {Efficient {{Attention}}},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Zhuoran, Shen and Mingyuan, Zhang and Haiyu, Zhao and Shuai, Yi and Hongsheng, Li},
  year = {2021},
  month = jan,
  pages = {3530--3538},
  publisher = {IEEE},
  address = {Waikoloa, HI, USA},
  doi = {10.1109/WACV48630.2021.00357},
  urldate = {2021-12-13},
  abstract = {Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on highresolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-ofthe-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github. com/cmsflash/efficient-attention.},
  isbn = {978-1-66540-477-8},
  langid = {english}
}

@inproceedings{zhuoranEfficientAttentionAttention2021a,
  title = {Efficient {{Attention}}: {{Attention}} with {{Linear Complexities}}},
  shorttitle = {Efficient {{Attention}}},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Zhuoran, Shen and Mingyuan, Zhang and Haiyu, Zhao and Shuai, Yi and Hongsheng, Li},
  year = {2021},
  month = jan,
  pages = {3530--3538},
  publisher = {IEEE},
  address = {Waikoloa, HI, USA},
  doi = {10.1109/WACV48630.2021.00357},
  urldate = {2021-12-13},
  abstract = {Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on highresolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-ofthe-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github. com/cmsflash/efficient-attention.},
  isbn = {978-1-66540-477-8},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\UUPNPEGY\Zhuoran et al. - 2021 - Efficient Attention Attention with Linear Complex.pdf}
}

@inproceedings{zhuPickandLearnAutomaticQuality2019,
  title = {Pick-and-{{Learn}}: {{Automatic Quality Evaluation}} for {{Noisy-Labeled Image Segmentation}}},
  shorttitle = {Pick-and-{{Learn}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2019},
  author = {Zhu, Haidong and Shi, Jialin and Wu, Ji},
  editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {576--584},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-32226-7_64},
  abstract = {Deep learning methods have achieved promising performance in many areas, but they are still struggling with noisy-labeled images during the training process. Considering that the annotation quality indispensably relies on great expertise, the problem is even more crucial in the medical image domain. How to eliminate the disturbance from noisy labels for segmentation tasks without further annotations is still a significant challenge. In this paper, we introduce our label quality evaluation strategy for deep neural networks automatically assessing the quality of each label, which is not explicitly provided, and training on clean-annotated ones. We propose a solution for network automatically evaluating the relative quality of the labels in the training set and using good ones to tune the network parameters. We also design an overfitting control module to let the network maximally learn from the precise annotations during the training process. Experiments on the public biomedical image segmentation dataset have proved the method outperforms baseline methods and retains both high accuracy and good generalization at different noise levels.},
  isbn = {978-3-030-32226-7},
  langid = {english},
  keywords = {Image segmentation,Noisy labels,Quality evaluation}
}

@inproceedings{zhuPickandLearnAutomaticQuality2019a,
  title = {Pick-and-{{Learn}}: {{Automatic Quality Evaluation}} for {{Noisy-Labeled Image Segmentation}}},
  shorttitle = {Pick-and-{{Learn}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2019},
  author = {Zhu, Haidong and Shi, Jialin and Wu, Ji},
  editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {576--584},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-32226-7_64},
  abstract = {Deep learning methods have achieved promising performance in many areas, but they are still struggling with noisy-labeled images during the training process. Considering that the annotation quality indispensably relies on great expertise, the problem is even more crucial in the medical image domain. How to eliminate the disturbance from noisy labels for segmentation tasks without further annotations is still a significant challenge. In this paper, we introduce our label quality evaluation strategy for deep neural networks automatically assessing the quality of each label, which is not explicitly provided, and training on clean-annotated ones. We propose a solution for network automatically evaluating the relative quality of the labels in the training set and using good ones to tune the network parameters. We also design an overfitting control module to let the network maximally learn from the precise annotations during the training process. Experiments on the public biomedical image segmentation dataset have proved the method outperforms baseline methods and retains both high accuracy and good generalization at different noise levels.},
  isbn = {978-3-030-32226-7},
  langid = {english},
  keywords = {Image segmentation,Noisy labels,Quality evaluation},
  file = {C:\Users\cleme\Zotero\storage\KUQLSNBZ\Zhu et al. - 2019 - Pick-and-Learn Automatic Quality Evaluation for N.pdf}
}

@misc{zhuSuperpixelTransformersEfficient2023,
  title = {Superpixel {{Transformers}} for {{Efficient Semantic Segmentation}}},
  author = {Zhu, Alex Zihao and Mei, Jieru and Qiao, Siyuan and Yan, Hang and Zhu, Yukun and Chen, Liang-Chieh and Kretzschmar, Henrik},
  year = {2023},
  month = oct,
  number = {arXiv:2309.16889},
  eprint = {2309.16889},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.16889},
  urldate = {2024-02-13},
  abstract = {Semantic segmentation, which aims to classify every pixel in an image, is a key task in machine perception, with many applications across robotics and autonomous driving. Due to the high dimensionality of this task, most existing approaches use local operations, such as convolutions, to generate per-pixel features. However, these methods are typically unable to effectively leverage global context information due to the high computational costs of operating on a dense image. In this work, we propose a solution to this issue by leveraging the idea of superpixels, an over-segmentation of the image, and applying them with a modern transformer framework. In particular, our model learns to decompose the pixel space into a spatially low dimensional superpixel space via a series of local cross-attentions. We then apply multi-head self-attention to the superpixels to enrich the superpixel features with global context and then directly produce a class prediction for each superpixel. Finally, we directly project the superpixel class predictions back into the pixel space using the associations between the superpixels and the image pixel features. Reasoning in the superpixel space allows our method to be substantially more computationally efficient compared to convolution-based decoder methods. Yet, our method achieves state-of-the-art performance in semantic segmentation due to the rich superpixel features generated by the global self-attention mechanism. Our experiments on Cityscapes and ADE20K demonstrate that our method matches the state of the art in terms of accuracy, while outperforming in terms of model parameters and latency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\A3L83SNQ\\Zhu et al. - 2023 - Superpixel Transformers for Efficient Semantic Seg.pdf;C\:\\Users\\cleme\\Zotero\\storage\\RWYK4JPH\\2309.html}
}

@article{zitovaImageRegistrationMethods2003,
  title = {Image Registration Methods: A Survey},
  shorttitle = {Image Registration Methods},
  author = {Zitov{\'a}, Barbara and Flusser, Jan},
  year = {2003},
  month = oct,
  journal = {Image and Vision Computing},
  volume = {21},
  number = {11},
  pages = {977--1000},
  issn = {0262-8856},
  doi = {10.1016/S0262-8856(03)00137-9},
  urldate = {2019-08-08},
  abstract = {This paper aims to present a review of recent as well as classic image registration methods. Image registration is the process of overlaying images (two or more) of the same scene taken at different times, from different viewpoints, and/or by different sensors. The registration geometrically align two images (the reference and sensed images). The reviewed approaches are classified according to their nature (area-based and feature-based) and according to four basic steps of image registration procedure: feature detection, feature matching, mapping function design, and image transformation and resampling. Main contributions, advantages, and drawbacks of the methods are mentioned in the paper. Problematic issues of image registration and outlook for the future research are discussed too. The major goal of the paper is to provide a comprehensive reference source for the researchers involved in image registration, regardless of particular application areas.},
  keywords = {Feature detection,Feature matching,Image registration,Mapping function,Resampling}
}

@article{zitovaImageRegistrationMethods2003a,
  title = {Image Registration Methods: A Survey},
  shorttitle = {Image Registration Methods},
  author = {Zitov{\'a}, Barbara and Flusser, Jan},
  year = {2003},
  month = oct,
  journal = {Image and Vision Computing},
  volume = {21},
  number = {11},
  pages = {977--1000},
  issn = {0262-8856},
  doi = {10.1016/S0262-8856(03)00137-9},
  urldate = {2019-08-08},
  abstract = {This paper aims to present a review of recent as well as classic image registration methods. Image registration is the process of overlaying images (two or more) of the same scene taken at different times, from different viewpoints, and/or by different sensors. The registration geometrically align two images (the reference and sensed images). The reviewed approaches are classified according to their nature (area-based and feature-based) and according to four basic steps of image registration procedure: feature detection, feature matching, mapping function design, and image transformation and resampling. Main contributions, advantages, and drawbacks of the methods are mentioned in the paper. Problematic issues of image registration and outlook for the future research are discussed too. The major goal of the paper is to provide a comprehensive reference source for the researchers involved in image registration, regardless of particular application areas.},
  keywords = {Feature detection,Feature matching,Image registration,Mapping function,Resampling},
  file = {C\:\\Users\\cleme\\Zotero\\storage\\M3JMLFRK\\Zitová et Flusser - 2003 - Image registration methods a survey.pdf;C\:\\Users\\cleme\\Zotero\\storage\\UW22Q4BT\\S0262885603001379.html}
}

@incollection{zouUnsupervisedDomainAdaptation2018,
  title = {Unsupervised {{Domain Adaptation}} for {{Semantic Segmentation}} via {{Class-Balanced Self-training}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2018},
  author = {Zou, Yang and Yu, Zhiding and Vijaya Kumar, B. V. K. and Wang, Jinsong},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11207},
  pages = {297--313},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-01219-9_18},
  urldate = {2023-06-23},
  abstract = {Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world ``wild tasks'' where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as ``domain gap'', and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training (ST) procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of ST, we also propose a novel classbalanced self-training (CBST) framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to refine generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.},
  isbn = {978-3-030-01218-2 978-3-030-01219-9},
  langid = {english}
}

@incollection{zouUnsupervisedDomainAdaptation2018a,
  title = {Unsupervised {{Domain Adaptation}} for {{Semantic Segmentation}} via {{Class-Balanced Self-training}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2018},
  author = {Zou, Yang and Yu, Zhiding and Vijaya Kumar, B. V. K. and Wang, Jinsong},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11207},
  pages = {297--313},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-01219-9_18},
  urldate = {2023-06-23},
  abstract = {Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world ``wild tasks'' where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as ``domain gap'', and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training (ST) procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of ST, we also propose a novel classbalanced self-training (CBST) framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to refine generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.},
  isbn = {978-3-030-01218-2 978-3-030-01219-9},
  langid = {english},
  file = {C:\Users\cleme\Zotero\storage\B3NPZDX2\Zou et al. - 2018 - Unsupervised Domain Adaptation for Semantic Segmen.pdf}
}

@book{zwiggelaarMedicalImageUnderstanding2007,
  title = {Medical Image Understanding and Analysis 2007: {{University}} of {{Wales Aberystwyth}}, 17 - 18th {{July}}},
  shorttitle = {Medical Image Understanding and Analysis 2007},
  editor = {Zwiggelaar, Reyer},
  year = {2007},
  publisher = {BMVA},
  address = {S.l.},
  isbn = {978-1-901725-33-9},
  langid = {english}
}

@book{zwiggelaarMedicalImageUnderstanding2007a,
  title = {Medical Image Understanding and Analysis 2007: {{University}} of {{Wales Aberystwyth}}, 17 - 18th {{July}}},
  shorttitle = {Medical Image Understanding and Analysis 2007},
  editor = {Zwiggelaar, Reyer},
  year = {2007},
  publisher = {BMVA},
  address = {S.l.},
  isbn = {978-1-901725-33-9},
  langid = {english}
}

@book{zwiggelaarMedicalImageUnderstanding2007b,
  title = {Medical Image Understanding and Analysis 2007: {{University}} of {{Wales Aberystwyth}}, 17 - 18th {{July}}},
  shorttitle = {Medical Image Understanding and Analysis 2007},
  editor = {Zwiggelaar, Reyer},
  year = {2007},
  publisher = {BMVA},
  address = {S.l.},
  isbn = {978-1-901725-33-9},
  langid = {english}
}

@book{zwiggelaarMedicalImageUnderstanding2007c,
  title = {Medical Image Understanding and Analysis 2007: {{University}} of {{Wales Aberystwyth}}, 17 - 18th {{July}}},
  shorttitle = {Medical Image Understanding and Analysis 2007},
  editor = {Zwiggelaar, Reyer},
  year = {2007},
  publisher = {BMVA},
  address = {S.l.},
  isbn = {978-1-901725-33-9},
  langid = {english}
}

@book{zwiggelaarMedicalImageUnderstanding2007d,
  title = {Medical Image Understanding and Analysis 2007: {{University}} of {{Wales Aberystwyth}}, 17 - 18th {{July}}},
  shorttitle = {Medical Image Understanding and Analysis 2007},
  editor = {Zwiggelaar, Reyer},
  year = {2007},
  publisher = {BMVA},
  address = {S.l.},
  isbn = {978-1-901725-33-9},
  langid = {english},
  annotation = {OCLC: 930715463},
  file = {C:\Users\cleme\Zotero\storage\LVM3YSMS\Zwiggelaar - 2007 - Medical image understanding and analysis 2007 Uni.pdf}
}

@book{zwiggelaarMedicalImageUnderstanding2007e,
  title = {Medical Image Understanding and Analysis 2007: {{University}} of {{Wales Aberystwyth}}, 17 - 18th {{July}}},
  shorttitle = {Medical Image Understanding and Analysis 2007},
  editor = {Zwiggelaar, Reyer},
  year = {2007},
  publisher = {BMVA},
  address = {S.l.},
  isbn = {978-1-901725-33-9},
  langid = {english},
  annotation = {OCLC: 930715463},
  file = {C:\Users\cleme\Zotero\storage\6VULNMVI\Zwiggelaar - 2007 - Medical image understanding and analysis 2007 Uni.pdf}
}

@book{zwiggelaarMedicalImageUnderstanding2007f,
  title = {Medical Image Understanding and Analysis 2007: {{University}} of {{Wales Aberystwyth}}, 17 - 18th {{July}}},
  shorttitle = {Medical Image Understanding and Analysis 2007},
  editor = {Zwiggelaar, Reyer},
  year = {2007},
  publisher = {BMVA},
  address = {S.l.},
  isbn = {978-1-901725-33-9},
  langid = {english},
  annotation = {OCLC: 930715463},
  file = {C:\Users\cleme\Zotero\storage\8E3YD2YD\Zwiggelaar - 2007 - Medical image understanding and analysis 2007 Uni.pdf}
}

@book{zwiggelaarMedicalImageUnderstanding2007g,
  title = {Medical Image Understanding and Analysis 2007: {{University}} of {{Wales Aberystwyth}}, 17 - 18th {{July}}},
  shorttitle = {Medical Image Understanding and Analysis 2007},
  editor = {Zwiggelaar, Reyer},
  year = {2007},
  publisher = {BMVA},
  address = {S.l.},
  isbn = {978-1-901725-33-9},
  langid = {english},
  annotation = {OCLC: 930715463},
  file = {C:\Users\cleme\Zotero\storage\IN4TKYJH\Zwiggelaar - 2007 - Medical image understanding and analysis 2007 Uni.pdf}
}
